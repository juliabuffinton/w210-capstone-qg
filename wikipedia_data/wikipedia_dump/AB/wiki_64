{"id": "19903", "url": "https://en.wikipedia.org/wiki?curid=19903", "title": "Marlon Brando", "text": "Marlon Brando\n\nMarlon Brando Jr. (April 3, 1924 – July 1, 2004) was an American actor and film director. With a career spanning 60 years, he is regarded for his cultural influence on 20th-century film. Brando's Academy Award-winning performances include that of Terry Malloy in \"On the Waterfront\" (1954) and Don Vito Corleone in \"The Godfather\" (1972). Brando was an activist for many causes, notably the civil rights movement and various Native American movements. He is credited with helping to popularize the Stanislavski system of acting having studied with Stella Adler in the 1940s.\n\nHe initially gained acclaim and an Academy Award nomination for reprising the role of Stanley Kowalski in the 1951 film adaptation of Tennessee Williams' play \"A Streetcar Named Desire\", a role that he originated successfully on Broadway. He received further praise for his performance as Terry Malloy in \"On the Waterfront\", and his portrayal of the rebellious motorcycle gang leader Johnny Strabler in \"The Wild One\" proved to be a lasting image in popular culture. Brando received Academy Award nominations for playing Emiliano Zapata in \"Viva Zapata!\"; Mark Antony in Joseph L. Mankiewicz's 1953 film adaptation of Shakespeare's \"Julius Caesar\"; and Air Force Major Lloyd Gruver in \"Sayonara\" (1957), an adaptation of James Michener's 1954 novel. Brando was included in a list of Top Ten Money Making Stars three times in the 1950s, coming in at number 10 in 1954, number 6 in 1955, and number 4 in 1958.\n\nThe 1960s saw Brando's career take a downturn. He directed and starred in the cult western film \"One-Eyed Jacks\", a critical and commercial flop, after which he delivered a series of box-office failures, beginning with the 1962 film adaptation of the novel \"Mutiny on the Bounty\". After 10 years, during which he did not appear in a successful film, he won his second Academy Award for playing Vito Corleone in Francis Ford Coppola's \"The Godfather\", a role critics consider among his greatest. \"The Godfather\" was then one of the most commercially successful films of all time. With that and his Oscar-nominated performance in \"Last Tango in Paris\", Brando re-established himself in the ranks of top box-office stars, placing sixth and tenth in the Money Making Stars poll in 1972 and 1973, respectively. Brando took a four-year hiatus before appearing in \"The Missouri Breaks\" (1976). After this, he was content with being a highly paid character actor in cameo roles, such as in \"Superman\" (1978) and \"The Formula\" (1980), before taking a nine-year break from motion pictures. According to the \"Guinness Book of World Records\", Brando was paid a record $3.7 million ($ million in inflation-adjusted dollars) and 11.75% of the gross profits for 13 days' work on \"Superman\". He finished out the 1970s with his controversial performance as Colonel Kurtz in another Coppola film, \"Apocalypse Now\", a box-office hit for which he was highly paid and which helped finance his career layoff during the 1980s.\n\nBrando was ranked by the American Film Institute as the fourth-greatest movie star among male movie stars whose screen debuts occurred in or before 1950. He was one of six professional actors, along with Charlie Chaplin, U.S. President Ronald Reagan, Lucille Ball, Frank Sinatra, and Marilyn Monroe, named in 1999 by \"Time\" magazine as one of its .\n\nBrando was born on April 3, 1924, in Omaha, Nebraska, to Marlon Brando, Sr. (1895–1965), a pesticide and chemical feed manufacturer, and Dorothy Julia (née Pennebaker; 1897–1954). Brando had two older sisters, Jocelyn Brando (1919–2005) and Frances (1922–1994). His ancestry was German, Dutch, English, and Irish. His patrilineal immigrant ancestor, Johann Wilhelm Brandau, arrived in New York in the early 1700s from the Palatinate in Germany. Brando was raised a Christian Scientist.\n\nHis mother, known as Dodie, was unconventional for her time; she smoked, wore pants and drove cars. An actress herself and even a theatre administrator, she helped Henry Fonda begin his acting career. However, she was an alcoholic and often had to be brought home from Chicago bars by her husband. In his autobiography, \"Songs My Mother Taught Me\", Brando expressed sadness when writing about his mother: \"The anguish that her drinking produced was that she preferred getting drunk to caring for us.\" Dodie and Brando's father eventually joined Alcoholics Anonymous. Brando harbored far more enmity for his father, stating, \"I was his namesake, but nothing I did ever pleased or even interested him. He enjoyed telling me I couldn't do anything right. He had a habit of telling me I would never amount to anything.\" Brando's parents moved to Evanston, Illinois, when his father's work took him to Chicago, but separated when Brando was 11 years old. His mother took the three children to Santa Ana, California, where they lived with her mother. In 1937, Brando's parents reconciled and moved together to Libertyville, Illinois, a small town north of Chicago. In 1939 and 1941, he worked as an usher at the town's only movie theatre, The Liberty.\n\nBrando, whose childhood nickname was \"Bud\", was a mimic from his youth. He developed an ability to absorb the mannerisms of kids he played with and display them dramatically while staying in character. He was introduced to neighborhood boy Wally Cox and the two were unlikely closest friends until Cox's death in 1973. In the 2007 TCM biopic, \"Brando: The Documentary\", childhood friend George Englund recalls Brando's earliest acting as imitating the cows and horses on the family farm as a way to distract his mother from drinking. His sister Jocelyn was the first to pursue an acting career, going to study at the American Academy of Dramatic Arts in New York City. She appeared on Broadway, then films and television. Brando's sister Frances left college in California to study art in New York. Brando had been held back a year in school and was later expelled from Libertyville High School for riding his motorcycle through the corridors.\n\nHe was sent to Shattuck Military Academy, where his father had studied before him. Brando excelled at theatre and did well in the school. In his final year (1943), he was put on probation for being insubordinate to a visiting army colonel during maneuvers. He was confined to his room, but snuck into town and was caught. The faculty voted to expel him, though he was supported by the students, who thought expulsion was too harsh. He was invited back for the following year, but decided instead to drop out of high school. Brando worked as a ditch-digger as a summer job arranged by his father. He tried to enlist in the Army, but his induction physical revealed that a football injury he had sustained at Shattuck had left him with a trick knee. He was classified 4-F and not inducted.\n\nBrando decided to follow his sisters to New York, studying at the American Theatre Wing Professional School, part of the Dramatic Workshop of the New School, with influential German director Erwin Piscator. In a 1988 documentary, \"Marlon Brando: The Wild One\", Brando's sister Jocelyn remembered, \"He was in a school play and enjoyed it ... So he decided he would go to New York and study acting because that was the only thing he had enjoyed. That was when he was 18.\" In the A&E \"Biography\" episode on Brando, George Englund said Brando fell into acting in New York because \"he was accepted there. He wasn't criticized. It was the first time in his life that he heard good things about himself.\"\n\nBrando was an avid student and proponent of Stella Adler, from whom he learned the techniques of the Stanislavski system. This technique encouraged the actor to explore both internal and external aspects to fully realize the character being portrayed. Brando's remarkable insight and sense of realism were evident early on. Adler used to recount that when teaching Brando, she had instructed the class to act like chickens, and added that a nuclear bomb was about to fall on them. Most of the class clucked and ran around wildly, but Brando sat calmly and pretended to lay an egg. Asked by Adler why he had chosen to react this way, he said, \"I'm a chicken—what do I know about bombs?\" Despite being commonly regarded as a method actor, Brando disagreed. He claimed to have abhorred Lee Strasberg's teachings:\n\nBrando was the first to bring a natural approach to acting on film. According to Dustin Hoffman in his online Masterclass, Brando would often talk to camera men and fellow actors about their weekend even after the director would call action. Once Brando felt he could deliver the dialogue as natural as that conversation he would start the dialogue. In his 2015 documentary, \"Listen To Me Marlon\", he said before that actors were like breakfast cereals. He was calling them predictable. Critics would later say this was Brando being difficult, but actors who worked opposite would say it was just all part of his technique.\n\nBrando used his Stanislavski System skills for his first summer stock roles in Sayville, New York, on Long Island. Brando established a pattern of erratic, insubordinate behavior in the few shows he had been in. His behavior had him kicked out of the cast of the New School's production in Sayville, but he was soon afterwards discovered in a locally produced play there. Then, in 1944, he made it to Broadway in the bittersweet drama \"I Remember Mama\", playing the son of Mady Christians. The Lunts wanted Brando to play the role of Alfred Lunt's son in \"O Mistress Mine\", and Lunt even coached him for his audition, but Brando's reading during the audition was so desultory that they couldn't hire him. New York Drama Critics voted him \"Most Promising Young Actor\" for his role as an anguished veteran in \"Truckline Café\", although the play was a commercial failure. In 1946, he appeared on Broadway as the young hero in the political drama \"A Flag is Born\", refusing to accept wages above the Actors' Equity rate. In that same year, Brando played the role of Marchbanks alongside Katharine Cornell in her production's revival of \"Candida\", one of her signature roles. Cornell also cast him as the Messenger in her production of Jean Anouilh's \"Antigone\" that same year. He was also offered the opportunity to portray one of the principal characters in the Broadway premiere of Eugene O'Neill's \"The Iceman Cometh\", but turned the part down after falling asleep while trying to read the massive script and pronouncing the play \"ineptly written and poorly constructed\".\n\nIn 1945, Brando's agent recommended he take a co-starring role in \"The Eagle Has Two Heads\" with Tallulah Bankhead, produced by Jack Wilson. Bankhead had turned down the role of Blanche Dubois in \"A Streetcar Named Desire\", which Williams had written for her, to tour the play for the 1946–1947 season. Bankhead recognized Brando's potential, despite her disdain (which most Broadway veterans shared) for method acting, and agreed to hire him even though he auditioned poorly. The two clashed greatly during the pre-Broadway tour, with Bankhead reminding Brando of his mother, being her age and also having a drinking problem. Wilson was largely tolerant of Brando's behavior, but he reached his limit when Brando mumbled through a dress rehearsal shortly before the November 28, 1946, opening. \"I don't care what your grandmother did,\" Wilson exclaimed, \"and that Method stuff, I want to know what you're going to do!\" Brando in turn raised his voice, and acted with great power and passion. \"It was marvelous,\" a cast member recalled. \"Everybody hugged him and kissed him. He came ambling offstage and said to me, 'They don't think you can act unless you can yell.'\"\n\nCritics were not as kind, however. A review of Brando's performance in the opening assessed that Brando was \"still building his character, but at present fails to impress.\" One Boston critic remarked of Brando's prolonged death scene, \"Brando looked like a car in midtown Manhattan searching for a parking space.\" He received better reviews at subsequent tour stops, but what his colleagues recalled was only occasional indications of the talent he would later demonstrate. \"There were a few times when he was really magnificent,\" Bankhead admitted to an interviewer in 1962. \"He was a great young actor when he wanted to be, but most of the time I couldn't even hear him on the stage.\"\n\nBrando displayed his apathy for the production by demonstrating some shocking onstage manners. He \"tried everything in the world to ruin it for her,\" Bankhead's stage manager claimed. \"He nearly drove her crazy: scratching his crotch, picking his nose, doing anything.\" After several weeks on the road, they reached Boston, by which time Bankhead was ready to dismiss him. This proved to be one of the greatest blessings of his career, as it freed him up to play the role of Stanley Kowalski in Tennessee Williams's 1947 play \"A Streetcar Named Desire\", directed by Elia Kazan. Bankhead had recommended him to Williams for the role of Stanley, thinking he was perfect for the part.\n\nPierpont writes that John Garfield was first choice for the role, but \"made impossible demands.\" It was Kazan's decision to fall back on the far less experienced (and technically too young for the role) Brando. In a letter dated August 29, 1947, Williams confided to his agent Audrey Wood: \"It had not occurred to me before what an excellent value would come through casting a very young actor in this part. It humanizes the character of Stanley in that it becomes the brutality and callousness of youth rather than a vicious old man ... A new value came out of Brando's reading which was by far the best reading I have ever heard.\" Brando based his portrayal of Kowalski on the boxer Rocky Graziano, whom he had studied at a local gymnasium. Graziano did not know who Brando was, but attended the production with tickets provided by the young man. He said, \"The curtain went up and on the stage is that son of a bitch from the gym, and he's playing me.\"\n\nIn 1947, Brando performed a screen test for an early Warner Brothers script for the novel \"Rebel Without a Cause\" (1944), which bore no relation to the film eventually produced in 1955. The screen test is included as an extra in the 2006 DVD release of \"A Streetcar Named Desire\".\n\nBrando's first screen role was a bitter paraplegic veteran in \"The Men\" (1950). He spent a month in bed at the Birmingham Army Hospital in Van Nuys to prepare for the role. \"The New York Times\" reviewer Bosley Crowther wrote that Brando as Ken \"is so vividly real, dynamic and sensitive that his illusion is complete\" and noted, \"Out of stiff and frozen silences he can lash into a passionate rage with the tearful and flailing frenzy of a taut cable suddenly cut.\"\n\nBy Brando's own account, it may have been because of this film that his draft status was changed from 4-F to 1-A. He had had surgery on his trick knee, and it was no longer physically debilitating enough to incur exclusion from the draft. When Brando reported to the induction center, he answered a questionnaire by saying his race was \"human\", his color was \"Seasonal-oyster white to beige\", and he told an Army doctor that he was psychoneurotic. When the draft board referred him to a psychiatrist, Brando explained that he had been expelled from military school and had severe problems with authority. Coincidentally, the psychiatrist knew a doctor friend of Brando. Brando avoided military service during the Korean War.\n\nEarly in his career, Brando began using cue cards instead of memorizing his lines. Despite the objections of several of the film directors he worked with, Brando felt that this helped bring realism and spontaneity to his performances. He felt otherwise he would appear to be reciting a writer's speech. In the TV documentary \"The Making of Superman: The Movie\", Brando explained:\n\nHowever, some thought Brando used the cards out of laziness or an inability to memorize his lines. Once on \"The Godfather\" set, Brando was asked why he wanted his lines printed out. He responded, \"Because I can read them that way.\"\n\nBrando brought his performance as Stanley Kowalski to the screen in Tennessee William's \"A Streetcar Named Desire\" (1951). The role is regarded as one of Brando's greatest. The reception of Brando's performance was so positive that Brando quickly became a male sex symbol in Hollywood. The role earned him his first Academy Award nomination in the Best Actor category.\nHe was also nominated the next year for \"Viva Zapata!\" (1952), a fictionalized account of the life of Mexican revolutionary Emiliano Zapata. It recounted his peasant upbringing, his rise to power in the early 20th century, and death. The film was directed by Elia Kazan and co-starred Anthony Quinn. In the biopic \"Marlon Brando: The Wild One\", Sam Shaw says, \"Secretly, before the picture started, he went to Mexico to the very town where Zapata lived and was born in and it was there that he studied the speech patterns of people, their behavior, movement.\" Most critics focused on the actor rather than the film, with \"Time\" and \"Newsweek\" publishing rave reviews.\n\nYears later, in his autobiography, Brando remarked: \"Tony Quinn, whom I admired professionally and liked personally, played my brother, but he was extremely cold to me while we shot that picture. During our scenes together, I sensed a bitterness toward me, and if I suggested a drink after work, he either turned me down or else was sullen and said little. Only years later did I learn why.\" Brando related that, to create on-screen tension between the two, \"Gadg\" (Kazan) had told Quinn—who had taken over the role of Stanley Kowalski on Broadway after Brando had finished—that Brando had been unimpressed with his work. After achieving the desired effect, Kazan never told Quinn that he had misled him. It was only many years later, after comparing notes, that Brando and Quinn realized the deception.\n\nBrando's next film, \"Julius Caesar\" (1953), received highly favorable reviews. Brando portrayed Mark Antony. While most acknowledged Brando's talent, some critics felt Brando's \"mumbling\" and other idiosyncrasies betrayed a lack of acting fundamentals and, when his casting was announced, many remained dubious about his prospects for success. Directed by Joseph L. Mankiewicz and co-starring British stage actor John Gielgud, Brando delivered an impressive performance, especially during Antony's noted \"Friends, Romans, countrymen ...\" speech. Gielgud was so impressed that he offered Brando a full season at the Hammersmith Theatre, an offer he declined. In his biography on the actor, Stefan Kanfer writes, \"Marlon's autobiography devotes one line to his work on that film: Among all those British professionals, 'for me to walk onto a movie set and play Mark Anthony was asinine'—yet another example of his persistent self-denigration, and wholly incorrect.\" Kanfer adds that after a screening of the film, director John Huston commented, \"Christ! It was like a furnace door opening—the heat came off the screen. I don't know another actor who could do that.\" During the filming of \"Julius Caesar\", Brando learned that Elia Kazan had cooperated with congressional investigators, naming a whole string of \"subversives\" to the House Committee on Un-American Activities. By all accounts, Brando was upset by his mentor's decision, but he worked with him again in \"On The Waterfront\". \"None of us is perfect,\" he later wrote in his memoir, \"and I think that Gadg has done injury to others, but mostly to himself.\"\n\nIn 1953, Brando also starred in \"The Wild One\", riding his own Triumph Thunderbird 6T motorcycle. Triumph's importers were ambivalent at the exposure, as the subject matter was rowdy motorcycle gangs taking over a small town. The film was criticized for its perceived gratuitous violence at the time, with \"Time\" stating, \"The effect of the movie is not to throw light on the public problem, but to shoot adrenaline through the moviegoer's veins.\" Brando allegedly did not see eye to eye with the Hungarian director László Benedek and did not get on with costar Lee Marvin.\n\nTo Brando's expressed puzzlement, the movie inspired teen rebellion and made him a role model to the nascent rock-and-roll generation and future stars such as James Dean and Elvis Presley. After the movie's release, the sales of leather jackets and blue jeans skyrocketed. Reflecting on the movie in his autobiography, Brando concluded that it had not aged very well but said:\n\nLater that same year, Brando starred in Lee Falk's production of George Bernard Shaw's \"Arms and the Man\" in Boston. Falk was proud to tell people that Brando turned down an offer of $10,000 per week on Broadway, in favor of working in his production in Boston, for less than $500 per week. It was the last time Brando acted in a stage play.\n\nIn 1954, Brando starred in \"On the Waterfront\", a crime drama film about union violence and corruption among longshoremen. The film was directed by Elia Kazan and written by Budd Schulberg; it also stars Karl Malden, Lee J. Cobb, Rod Steiger and, in her film debut, Eva Marie Saint. When initially offered the role, Brando—still stung by Kazan's testimony to HUAC—demurred and the part of Terry Malloy nearly went to Frank Sinatra. According to biographer Stefan Kanfer, the director believed that Sinatra, who grew up in Hoboken, would work as Malloy, but eventually producer Sam Spiegel wooed Brando to the part, signing him for $100,000. \"Kazan made no protest because, he subsequently confessed, 'I always preferred Brando to anybody.'\"\nBrando won the Oscar for his role as Irish-American stevedore Terry Malloy in \"On the Waterfront\". His performance, spurred on by his rapport with Eva Marie Saint and Kazan's direction, was praised as a \"tour de force\". For the famous \"I coulda been a contender\" scene, he convinced Kazan that the scripted scene was unrealistic. Schulberg's script had Brando acting the entire scene with his character being held at gunpoint by his brother Charlie, played by Rod Steiger. Brando insisted on gently pushing away the gun, saying that Terry would never believe that his brother would pull the trigger and doubting that he could continue his speech while fearing a gun on him. Kazan let Brando improvise and later expressed deep admiration for Brando's instinctive understanding, saying:\n\nUpon its release, \"On the Waterfront\" received glowing reviews from critics and was a commercial success, earning an estimated $4.2 million in rentals at the North American box office in 1954. In his July 29, 1954, review, \"The New York Times\" critic A. H. Weiler praised the film, calling it \"an uncommonly powerful, exciting, and imaginative use of the screen by gifted professionals.\" Film critic Roger Ebert lauded the film, stating that Brando and Kazan changed acting in American films forever and added it to his \"Great Movies\" list. In his autobiography, Brando was typically dismissive of his performance: \"On the day Gadg showed me the complete picture, I was so depressed by my performance I got up and left the screening room ... I thought I was a huge failure.\" After Brando won the Academy Award for Best Actor, the statue was stolen. Much later, it turned up at a London auction house, which contacted the actor and informed him of its whereabouts.\n\nBrando would later say in his 2015 documentary, \"Listen to me Marlon\", that he felt the part in \"On the Waterfront\" that won him an Oscar was to him personally, not good, and that he knew he had done better acting. In the behind the scenes of \"On the Waterfront\" film experts and critics would say he would change lines because he didn't like the script, and that he'd let the director know by having regular conversations. However, this was a way for Brando to prepare before a scene by warming up before starting the dialogue, in order to deliver it as natural as possible. Brando often saw a shrink, and reportedly wasn't there for some of the shooting of the scene when the close up was on the opposite actor. This is why this part of his technique is recognized mistakenly as him being difficult, and not a part of his approach.\n\nFollowing \"On the Waterfront\", Brando remained a top box office draw, but critics increasingly felt his performances were half-hearted, lacking the intensity and commitment found in his earlier work, especially in his work with Kazan. He portrayed Napoleon in the 1954 film \"Désirée\". According to co-star Jean Simmons, Brando's contract forced him to star in the movie. He put little effort into the role, claiming he didn't like the script, and later dismissed the entire movie as \"superficial and dismal\". Brando was especially contemptuous of director Henry Koster.\n\nBrando and Simmons were paired together again in the film adaptation of the musical \"Guys and Dolls\" (1955). \"Guys and Dolls\" would be Brando's first and last musical role. \"Time\" found the picture \"false to the original in its feeling\", remarking that Brando \"sings in a faraway tenor that sometimes tends to be flat.\" Appearing in Edward Murrow's \"Person to Person\" interview in early 1955, he admitted to having problems with his singing voice, which he called \"pretty terrible.\" In the 1965 documentary \"Meet Marlon Brando\", he revealed that the final product heard in the movie was a result of countless singing takes being cut into one and later joked, \"I couldn't hit a note with a baseball bat; some notes I missed by extraordinary margins ... They sewed my words together on one song so tightly that when I mouthed it in front of the camera, I nearly asphyxiated myself\". Relations between Brando and costar Frank Sinatra were also frosty, with Stefan Kanfer observing: \"The two men were diametrical opposites: Marlon required multiple takes; Frank detested repeating himself.\" Upon their first meeting Sinatra reportedly scoffed, \"Don't give me any of that Actors Studio shit.\" Brando later famously quipped, \"Frank is the kind of guy, when he dies, he's going to heaven and give God a hard time for making him bald.\" Frank Sinatra famously called Brando \"the world's most overrated actor\", and referred to him as \"mumbles\". The film was commercially though not critically successful, costing $5.5 million to make and grossing $13 million.\n\nBrando played Sakini, a Japanese interpreter for the U.S. Army in postwar Japan, in \"The Teahouse of the August Moon\" (1956). Pauline Kael was not particularly impressed by the movie, but noted \"Marlon Brando starved himself to play the pixie interpreter Sakini, and he looks as if he's enjoying the stunt—talking with a mad accent, grinning boyishly, bending forward, and doing tricky movements with his legs. He's harmlessly genial (and he is certainly missed when he's offscreen), though the fey, roguish role doesn't allow him to do what he's great at and it's possible that he's less effective in it than a lesser actor might have been.\" In \"Sayonara\" (1957) he appeared as a United States Air Force officer. \"Newsweek\" found the film a \"dull tale of the meeting of the twain\", but it was nevertheless a box-office success. According to Stefan Kanfer's biography of the actor, Brando's manager Jay Kanter negotiated a profitable contract with ten percent of the gross going to Brando, which put him in the millionaire category. The movie was controversial due to openly discussing interracial marriage, but proved a great success, earning 10 Academy Award nominations, with Brando being nominated for Best Actor. The film went on to win four Academy Awards. \"Teahouse\" and \"Sayonara\" were the first in a string of films Brando would strive to make over the next decade which contained socially relevant messages, and he formed a partnership with Paramount to establish his own production company called Pennebaker, its declared purpose to develop films that contained \"social value that would improve the world.\" The name was a tribute in honor of his mother, who had died in 1954. By all accounts, Brando was devastated by her death, with biographer Peter Manso telling A&E's \"Biography\", \"She was the one who could give him approval like no one else could and, after his mother died, it seems that Marlon stops caring.\" Brando appointed his father to run Pennebaker. In the same A&E special, George Englund claims that Brando gave his father the job because \"it gave Marlon a chance to take shots at him, to demean and diminish him\".\n\nIn 1958, Brando appeared in \"The Young Lions\", dyeing his hair blonde and assuming a German accent for the role, which he later admitted was not convincing. The film is based on the novel by Irwin Shaw, and Brando's portrayal of the character Christian Diestl was controversial for its time. He later wrote, \"The original script closely followed the book, in which Shaw painted all Germans as evil caricatures, especially Christian, whom he portrayed as a symbol of everything that was bad about Nazism; he was mean, nasty, vicious, a cliché of evil ... I thought the story should demonstrate that there are no inherently 'bad' people in the world, but they can easily be misled.\" Shaw and Brando even appeared together for a televised interview with CBS correspondent David Schoenbrun and, during a bombastic exchange, Shaw charged that, like most actors, Brando was incapable of playing flat-out villainy; Brando responded by stating \"Nobody creates a character but an actor. I play the role; now he exists. He is my creation.\" \"The Young Lions\" also features Brando's only appearance in a film with friend and rival Montgomery Clift (although they shared no scenes together). Brando closed out the decade by appearing in \"The Fugitive Kind\" (1960) opposite Anna Magnani. The film was based on another play by Tennessee Williams but was hardly the success \"A Streetcar Named Desire\" had been, with the \"Los Angeles Times\" labeling Williams's personae \"psychologically sick or just plain ugly\" and \"The New Yorker\" calling it a \"cornpone melodrama\".\n\nIn 1961, Brando made his directorial debut in the western \"One-Eyed Jacks\". The picture was originally directed by Stanley Kubrick, but he was fired early in the production. Paramount then made Brando the director. Brando portrays the lead character Rio, and Karl Malden plays his partner \"Dad\" Longworth. The supporting cast features Katy Jurado, Ben Johnson, and Slim Pickens. Brando's penchant for multiple retakes and character exploration as an actor carried over into his directing, however, and the film soon went over budget; Paramount expected the film to take three months to complete but shooting stretched to six and the cost doubled to more than six million dollars. Brando's inexperience as an editor also delayed postproduction and Paramount eventually took control of the film. Brando later wrote, \"Paramount said it didn't like my version of the story; I'd had everyone lie except Karl Malden. The studio cut the movie to pieces and made him a liar, too. By then, I was bored with the whole project and walked away from it.\" \"One-Eyed Jacks\" was poorly reviewed by critics. While the film did solid business, it ran so over budget that it lost money.\n\nBrando's revulsion with the film industry reportedly boiled over on the set of his next film, Metro-Goldwyn-Mayer's remake of \"Mutiny on the Bounty\", which was filmed in Tahiti. The actor was accused of deliberately sabotaging nearly every aspect of the production. On June 16, 1962, \"The Saturday Evening Post\" ran an article by Bill Davidson with the headline \"Six million dollars down the drain: the mutiny of Marlon Brando\". \"Mutiny\" director Lewis Milestone claimed that the executives \"deserve what they get when they give a ham actor, a petulant child, complete control over an expensive picture.\" \" Mutiny on the Bounty\" nearly capsized MGM and, while the project had indeed been hampered with delays other than Brando's behavior, the accusations would dog the actor for years as studios began to fear Brando's difficult reputation. Critics also began taking note of his fluctuating weight.\n\nDistracted by his personal life and becoming disillusioned with his career, Brando began to view acting as a means to a financial end. Critics protested when he started accepting roles in films many perceived as being beneath his talent, or criticized him for failing to live up to the better roles. Previously only signing short term deals with film studios, in 1961 Brando uncharacteristically signed a five-picture deal with Universal Studios that would haunt him for the rest of the decade. \"The Ugly American\" (1963) was the first of these films. Based on the 1958 novel of the same title that Pennebaker had optioned, the film, which featured Brando's sister Jocelyn, was rated fairly positively but died at the box office. Brando was nominated for a Golden Globe for his performance. All of Brando's other Universal films during this period, including \"Bedtime Story\" (1964), \"The Appaloosa\" (1966), \"A Countess from Hong Kong\" (1967) and \"The Night of the Following Day\" (1969), were also critical and commercial flops. \"Countess\" in particular was a disappointment for Brando, who had looked forward to working with one of his heroes, director Charlie Chaplin. The experience turned out to be an unhappy one; Brando was horrified at Chaplin's didactic style of direction and his authoritarian approach. Brando had also appeared in the spy thriller \"Morituri\" in 1965; that, too, failed to attract an audience.\n\nBrando acknowledged his professional decline, writing later, \"Some of the films I made during the sixties were successful; some weren't. Some, like \"The Night of the Following Day\", I made only for the money; others, like \"Candy\", I did because a friend asked me to and I didn't want to turn him down ... In some ways I think of my middle age as the Fuck You Years.\" \"Candy\" was especially appalling for many; a 1968 sex farce film directed by Christian Marquand and based on the 1958 novel by Terry Southern, the film satirizes pornographic stories through the adventures of its naive heroine, Candy, played by Ewa Aulin. It is generally regarded as the nadir of Brando's career. \"The Washington Post\" observed: \"Brando's self-indulgence over a dozen years is costing him and his public his talents.\" In the March 1966 issue of \"The Atlantic\", Pauline Kael wrote that in his rebellious days, Brando \"was antisocial because he knew society was crap; he was a hero to youth because he was strong enough not to take the crap\", but now Brando and others like him had become \"buffoons, shamelessly, pathetically mocking their public reputations.\" In an earlier review of \"The Appaloosa\" in 1966, Kael wrote that the actor was \"trapped in another dog of a movie ... Not for the first time, Mr. Brando gives us a heavy-lidded, adenoidally openmouthed caricature of the inarticulate, stalwart loner.\" Although he feigned indifference, Brando was hurt by the critical mauling, admitting in the 2015 film \"Listen to Me Marlon\", \"They can hit you every day and you have no way of fighting back. I was very convincing in my pose of indifference, but I was very sensitive and it hurt a lot.\"\n\nBrando portrayed a repressed gay army officer in \"Reflections in a Golden Eye\", directed by John Huston and costarring Elizabeth Taylor. The role turned out as one of his most acclaimed in years, with Stanley Crouch marveling, \"Brando's main achievement was to portray the taciturn but stoic gloom of those pulverized by circumstances.\" The film overall received mixed reviews. Another notable film was \"The Chase\" (1966), which paired the actor with Arthur Penn, Robert Duvall, Jane Fonda and Robert Redford. The film deals with themes of racism, sexual revolution, small-town corruption, and vigilantism. The film was received mostly positively.\n\nBrando cited \"Burn!\" (1969) as his personal favorite of the films he had made, writing in his autobiography, \"I think I did some of the best acting I've ever done in that picture, but few people came to see it.\" Brando dedicated a full chapter to the film in his memoir, stating that the director, Gillo Pontecorvo, was the best director he had ever worked with next to Kazan and Bernardo Bertolucci. Brando also detailed his clashes with Pontecorvo on the set and how \"we nearly killed each other.\" Loosely based on events in the history of Guadeloupe, the film got a hostile reception from critics. In 1971, Michael Winner directed him in the British horror film \"The Nightcomers\" with Stephanie Beacham, Thora Hird, Harry Andrews and Anna Palk. It is a prequel to \"The Turn of the Screw\", which later became the 1961 film \"The Innocents\". Brando's performance earned him a nomination for a Best Actor BAFTA, but the film bombed at the box office.\n\nDuring the 1970s, Brando was considered \"unbankable\". Critics were becoming increasingly dismissive of his work and he had not appeared in a box office hit since \"The Young Lions\" in 1958, the last year he had ranked as one of the Top Ten Box Office Stars and the year of his last Academy Award nomination, for \"Sayonara.\" Brando's performance as Vito Corleone, the \"Don,\" in \"The Godfather\" (1972), Francis Ford Coppola's adaptation of Mario Puzo's 1969 best-selling novel of the same name, was a career turning point, putting him back in the Top Ten and winning him his second Best Actor Oscar.\n\nParamount production chief Robert Evans, who had given Puzo an advance to write \"The Godfather\" so that Paramount would own the film rights, hired Coppola after many major directors had turned the film down. Evans wanted an Italian-American director who could provide the film with cultural authenticity. Coppola also came cheap. Evans was conscious of the fact that Paramount's last Mafia film, \"The Brotherhood\" (1968) had been a box office bomb, and he believed it was partly due to the fact that the director, Martin Ritt, and the star, Kirk Douglas, were Jews and the film lacked an authentic Italian flavor. The studio originally intended the film to be a low-budget production set in contemporary times without any major actors, but the phenomenal success of the novel gave Evans the clout to turn \"The Godfather\" into a prestige picture.\n\nCoppola had developed a list of actors for all the roles, and his list of potential Dons included the Oscar-winning Italian-American Ernest Borgnine, the Italian-American Frank de Kova (best known for playing Chief Wild Eagle on the TV sitcom \"F-Troop\"), John Marley (a Best Supporting Oscar-nominee for Paramount's 1970 hit film \"Love Story\" who was cast as the film producer Jack Woltz in the picture), the Italian-American Richard Conte (who was cast as Don Corleone's deadly rival Don Emilio Barzini), and Italian film producer Carlo Ponti. Coppola admitted in a 1975 interview, \"We finally figured we had to lure the \"best\" actor in the world. It was that simple. That boiled down to Laurence Olivier or Marlon Brando, who \"are\" the greatest actors in the world.\" The holographic copy of Coppola's cast list shows Brando's name underlined.\n\nEvans told Coppola that he had been thinking of Brando for the part two years earlier, and Puzo had imagined Brando in the part when he wrote the novel and had actually written to him about the part, so Coppola and Evans narrowed it down to Brando. (Ironically, Olivier would compete with Brando for the Best Actor Oscar for his part in \"Sleuth.\" He bested Brando at the 1972 New York Film Critics Circle Awards.) Albert S. Ruddy, whom Paramount assigned to produce the film, agreed with the choice of Brando. However, Paramount studio heads were opposed to casting Brando due to his reputation for difficulty and his long string of box office flops. Brando also had \"One-Eyed Jacks\" working against him, a troubled production that lost money for Paramount when it was released in 1961. Paramount Pictures President Stanley Jaffe told an exasperated Coppola, \"As long as I'm president of this studio, Marlon Brando will not be in this picture, and I will no longer allow you to discuss it.\"\n\nJaffe eventually set three conditions for the casting of Brando: That he would have to take a fee far below what he typically received; he'd have to agree to accept financial responsibility for any production delays his behavior cost; and he had to submit to a screen test. Coppola convinced Brando to a videotaped \"make-up\" test, in which Brando did his own makeup (he used cotton balls to simulate the character's puffed cheeks). Coppola had feared Brando might be too young to play the Don, but was electrified by the actor's characterization as the head of a crime family. Even so, he had to fight the studio in order to cast the temperamental actor. Brando had doubts himself, stating in his autobiography, \"I had never played an Italian before, and I didn't think I could do it successfully.\" Eventually, Charles Bluhdorn, the president of Paramount parent Gulf+Western, was won over to letting Brando have the role; when he saw the screen test, he asked in amazement, \"What are we watching? Who is this old guinea?\" Brando was signed for a low fee of $50,000, but in his contract, he was given a percentage of the gross on a sliding scale: 1% of the gross for each $10 million over a $10 million threshold, up to 5% if the picture exceeded $60 million. According to Evans, Brando sold back his points in the picture for $100,000, as he was in dire need of funds. \"That $100,000 cost him $11 million,\" Evans claimed.\n\nIn a 1994 interview that can be found on the Academy of Achievement website, Coppola insisted, \"\"The Godfather\" was a very unappreciated movie when we were making it. They were very unhappy with it. They didn't like the cast. They didn't like the way I was shooting it. I was always on the verge of getting fired.\" When word of this reached Brando, he threatened to walk off the picture, writing in his memoir, \"I strongly believe that directors are entitled to independence and freedom to realize their vision, though Francis left the characterizations in our hands and we had to figure out what to do.\" In a 2010 television interview with Larry King, Al Pacino also talked about how Brando's support helped him keep the role of Michael Corleone in the movie—despite the fact Coppola wanted to fire him. Brando was on his best behavior during filming, buoyed by a cast that included Pacino, Robert Duvall, James Caan, and Diane Keaton. In the \"Vanity Fair\" article \"The Godfather Wars\", Mark Seal writes, \"With the actors, as in the movie, Brando served as the head of the family. He broke the ice by toasting the group with a glass of wine.\" 'When we were young, Brando was like the godfather of actors,' says Robert Duvall. 'I used to meet with Dustin Hoffman in Cromwell's Drugstore, and if we mentioned his name once, we mentioned it 25 times in a day.' Caan adds, 'The first day we met Brando everybody was in awe.'\"\n\nBrando's performance was glowingly reviewed by critics. \"I thought it would be interesting to play a gangster, maybe for the first time in the movies, who wasn't like those bad guys Edward G. Robinson played, but who is kind of a hero, a man to be respected,\" Brando recalled in his autobiography. \"Also, because he had so much power and unquestioned authority, I thought it would be an interesting contrast to play him as a gentle man, unlike Al Capone, who beat up people with baseball bats.\" Duvall later marveled to A&E's \"Biography\", \"He minimized the sense of beginning. In other words he, like, deemphasized the word \"action\". He would go in front of that camera just like he was before. \"Cut!\" It was all the same. There was really no beginning. I learned a lot from watching that.\" Brando won the Academy Award for Best Actor for his performance, but he declined it, becoming the second actor to refuse a Best Actor award (after George C. Scott for \"Patton\"). He boycotted the award ceremony, instead sending aboriginal American rights activist Sacheen Littlefeather, who appeared in full Apache attire, to state Brando's reasons, which were based on his objection to the depiction of aboriginal Americans by Hollywood and television.\n\nThe actor followed \"The Godfather\" with Bernardo Bertolucci's 1972 film \"Last Tango in Paris\" opposite Maria Schneider, but Brando's highly noted performance threatened to be overshadowed by an uproar over the sexual content of the film. Brando portrays a recent American widower named Paul, who begins an anonymous sexual relationship with a young, betrothed Parisian woman named Jeanne. As with previous films, Brando refused to memorize his lines for many scenes; instead, he wrote his lines on cue cards and posted them around the set for easy reference, leaving Bertolucci with the problem of keeping them out of the picture frame. The film features several intense, graphic scenes involving Brando, including Paul anally raping Jeanne using butter as a lubricant, which, it was alleged was not consensual, and Paul's angry, emotionally charged final confrontation with the corpse of his dead wife. The controversial movie was a hit, however, and Brando made the list of Top Ten Box Office Stars for the last time. The voting membership of the Academy of Motion Picture Arts & Sciences again nominated Brando for Best Actor, his seventh nomination. Although Brando won the 1973 New York Film Critics Circle Awards, the actor did not appear at the ceremony or send a representative to pick up the award if he won.\n\nCritic Pauline Kael, in her famous \"New Yorker\" review, wrote \"The movie breakthrough has finally come. Bertolucci and Brando have altered the face of an art form.\" Brando confessed in his autobiography, \"To this day I can't say what \"Last Tango in Paris\" was about,\" and added the film \"required me to do a lot of emotional arm wrestling with myself, and when it was finished, I decided that I wasn't ever again going to destroy myself emotionally to make a movie. I felt I had violated my innermost self and I didn't want to suffer like that anymore ... You can't fake it.\"\n\nIn 1973, Brando was devastated by the death of his childhood best friend Wally Cox. Brando slept in Cox's pajamas and wrenched his ashes from his widow. She was going to sue for their return, but finally said \"I think Marlon needs the ashes more than I do.\"\n\nIn 1976, Brando appeared in \"The Missouri Breaks\" with his friend Jack Nicholson. The movie also reunited the actor with director Arthur Penn. As biographer Stefan Kanfer describes, Penn had difficulty controlling Brando, who seemed intent on going over the top with his border-ruffian-turned-contract-killer Robert E. Lee Clayton: \"Marlon made him a cross-dressing psychopath. Absent for the first hour of the movie, Clayton enters on horseback, dangling upside down, caparisoned in white buckskin, Littlefeather-style. He speaks in an Irish accent for no apparent reason. Over the next hour, also for no apparent reason, Clayton assumes the intonation of a British upper-class twit and an elderly frontier woman, complete with a granny dress and matching bonnet. Penn, who believed in letting actors do their thing, indulged Marlon all the way.\" Critics were unkind, with \"The Observer\" calling Brando's performance \"one of the most extravagant displays of \"grandedamerie\" since Sarah Bernhardt\", while \"The Sun\" complained, \"Marlon Brando at fifty-two has the sloppy belly of a sixty-two-year-old, the white hair of a seventy-two-year-old, and the lack of discipline of a precocious twelve-year-old.\" However, Kanfer noted: \"Even though his late work was met with disapproval, a re-examination shows that often, in the middle of the most pedestrian scene, there would be a sudden, luminous occurrence, a flash of the old Marlon that showed how capable he remained.\"\n\nIn 1977, Brando made a rare television appearance in the miniseries \"\", portraying George Lincoln Rockwell; he won a Primetime Emmy Award for Outstanding Supporting Actor in a Miniseries or a Movie for his performance. In 1978, he narrated the English version of \"Raoni\", a French-Belgian documentary film directed by Jean-Pierre Dutilleux and Luiz Carlos Saldanha that focused on the life of Raoni Metuktire and issues surrounding the survival of the indigenous Indian tribes of north central Brazil. Brando portrayed Superman's father Jor-El in the 1978 film \"Superman\". He agreed to the role only on assurance that he would be paid a large sum for what amounted to a small part, that he would not have to read the script beforehand, and that his lines would be displayed somewhere off-camera. It was revealed in a documentary contained in the 2001 DVD release of \"Superman\" that he was paid $3.7 million for two weeks of work. Brando also filmed scenes for the movie's sequel, \"Superman II\", but after producers refused to pay him the same percentage he received for the first movie, he denied them permission to use the footage. \"I asked for my usual percentage,\" he recollected in his memoir, \"but they refused, and so did I.\" However, after Brando's death, the footage was reincorporated into the 2006 re-cut of the film, \"\" and in the 2006 \"loose sequel\" \"Superman Returns\", in which both used and unused archive footage of him as Jor-El from the first two \"Superman\" films was remastered for a scene in the Fortress of Solitude, and Brando's voice-overs were used throughout the film.\n\nBrando starred as Colonel Walter E. Kurtz in Francis Ford Coppola's Vietnam epic \"Apocalypse Now\" (1979). He plays a highly decorated U.S. Army Special Forces officer who goes renegade, running his own operation based in Cambodia and is feared by the U.S. military as much as the Vietnamese. Brando was paid $1 million a week for 3 weeks work. The film drew attention for its lengthy and troubled production, as Eleanor Coppola's documentary \"Hearts of Darkness: A Filmmaker's Apocalypse\" documents: Brando showed up on the set overweight, Martin Sheen suffered a heart attack, and severe weather destroyed several expensive sets. The film's release was also postponed several times while Coppola edited millions of feet of footage. In the documentary, Coppola talks about how astonished he was when an overweight Brando turned up for his scenes and, feeling desperate, decided to portray Kurtz, who appears emaciated in the original story, as a man who had indulged every aspect of himself. Coppola: \"He was already heavy when I hired him and he promised me that he was going to get in shape and I imagined that I would, if he were heavy, I could use that. But he was \"so\" fat, he was very, very shy about it ... He was very, very adamant about how he didn't want to portray himself that way.\" Brando admitted to Coppola that he had not read the book, \"Heart of Darkness\", as the director had asked him to, and the pair spent days exploring the story and the character of Kurtz, much to the actor's financial benefit, according to producer Fred Roos: \"The clock was ticking on this deal he had and we had to finish him within three weeks or we'd go into this very expensive overage ... And Francis and Marlon would be talking about the character and whole days would go by. And this is at Marlon's urging—and yet he's getting paid for it.\"\n\nUpon release, \"Apocalypse Now\" earned critical acclaim, as did Brando's performance. His whispering of Kurtz's final words \"\"The horror! The horror!\"\", has become particularly famous. Roger Ebert, writing in the \"Chicago Sun-Times\", defended the movie's controversial \"denouement\", opining that the ending, \"with Brando's fuzzy, brooding monologues and final violence, feels more satisfactory than any conventional ending possibly could.\"\n\nAfter appearing as oil tycoon Adam Steiffel in 1980's \"The Formula\", which was poorly received critically, Brando announced his retirement from acting. However, he returned in 1989 in \"A Dry White Season\", based on André Brink's 1979 anti-apartheid novel. Brando agreed to do the film for free, but fell out with director Euzhan Palcy over how the film was edited; he even made a rare television appearance in an interview with Connie Chung to voice his disapproval. In his memoir, he maintained that Palcy \"had cut the picture so poorly, I thought, that the inherent drama of this conflict was vague at best.\" Brando received praise for his performance, earning an Academy Award nomination for Best Supporting Actor and winning the Best Actor Award at the Tokyo Film Festival. Brando also scored enthusiastic reviews for his caricature of his Vito Corleone role as Carmine Sabatini in 1990's \"The Freshman.\" In his original review, Roger Ebert wrote, \"There have been a lot of movies where stars have repeated the triumphs of their parts—but has any star ever done it more triumphantly than Marlon Brando does in \"The Freshman\"?\" \"Variety\" also praised Brando's performance as Sabatini and noted, \"Marlon Brando's sublime comedy performance elevates \"The Freshman\" from screwball comedy to a quirky niche in film history.\" Brando also starred alongside his friend Johnny Depp in the box office hit \"Don Juan DeMarco\" (1995) and in Depp's controversial \"The Brave\" (1997), which was never released in the United States. Later performances, such as his appearance in \"\" (1992) (for which he was nominated for a Raspberry as \"Worst Supporting Actor\"), \"The Island of Dr. Moreau\" (in which he won a \"Worst Supporting Actor\" Raspberry) (1996), and his barely recognizable appearance in \"Free Money\" (1998), resulted in some of the worst reviews of his career. However, his last completed film, \"The Score\" (2001), was received generally positively. In the film, in which he portrays a fence, he starred with Robert De Niro, who had portrayed Vito Corleone in \"The Godfather Part II\".\nBrando conceived the idea of a novel called \"Fan-Tan\" with director Donald Cammell in 1979, which was not released until 2005.\n\nBrando's notoriety, his troubled family life, and his obesity attracted more attention than his late acting career. He gained a great deal of weight in the 1970s and by the early to mid-1990s he weighed over and suffered from Type 2 diabetes. He had a history of weight fluctuation throughout his career that, by and large, he attributed to his years of stress-related overeating followed by compensatory dieting. He also earned a reputation for being difficult on the set, often unwilling or unable to memorize his lines and less interested in taking direction than in confronting the film director with odd demands.\n\nHe also dabbled with some innovation in his last years. He had several patents issued in his name from the U.S. Patent and Trademark Office, all of which involve a method of tensioning drumheads, in June 2002 – November 2004. (For example, see and its equivalents).\n\nIn 2004, Brando recorded voice tracks for the character Mrs. Sour in the unreleased animated film \"Big Bug Man\". This was his last role and his only role as a female character.\n\nThe actor was a longtime close friend of entertainer Michael Jackson and paid regular visits to his Neverland Ranch, resting there for weeks at a time. Brando also participated in the singer's two-day solo career 30th-anniversary celebration concerts in 2001, and starred in his 13-minute-long music video, \"You Rock My World,\" in the same year. On Jackson's 30th anniversary concert, Brando gave a rambling speech to the audience on humanitarian work which received a poor reaction and was unaired.\n\nThe actor's son, Miko, was Jackson's bodyguard and assistant for several years, and was a friend of the singer. \"The last time my father left his house to go anywhere, to spend any kind of time, it was with Michael Jackson\", Miko stated. \"He loved it ... He had a 24-hour chef, 24-hour security, 24-hour help, 24-hour kitchen, 24-hour maid service. Just carte blanche.\" \"Michael was instrumental helping my father through the last few years of his life. For that I will always be indebted to him. Dad had a hard time breathing in his final days, and he was on oxygen much of the time. He loved the outdoors, so Michael would invite him over to Neverland. Dad could name all the trees there, and the flowers, but being on oxygen it was hard for him to get around and see them all, it's such a big place. So Michael got Dad a golf cart with a portable oxygen tank so he could go around and enjoy Neverland. They'd just drive around—Michael Jackson, Marlon Brando, with an oxygen tank in a golf cart.\"\n\nIn April 2001, Brando was hospitalized with pneumonia.\n\nIn 2004, Brando signed with Tunisian film director Ridha Behi and began pre-production on a project to be titled \"Brando and Brando\". Up to a week before his death, he was working on the script in anticipation of a July/August 2004 start date. Production was suspended in July 2004 following Brando's death, at which time Behi stated that he would continue the film as an homage to Brando, with a new title of \"Citizen Brando\".\n\nOn July 1, 2004, Brando died of respiratory failure from pulmonary fibrosis with congestive heart failure at the UCLA Medical Center. The cause of death was initially withheld, with his lawyer citing privacy concerns. He also suffered from failing eyesight caused by diabetes and liver cancer. Shortly before his death and despite needing an oxygen mask to breathe, he recorded his voice to appear in \"The Godfather: The Game\", once again as Don Vito Corleone. However, Brando recorded only one line due to his health, and an impersonator was hired to finish his lines. Some lines from his character were directly lifted from the film.\n\nKarl Malden—a fellow actor in \"A Streetcar Named Desire\", \"On the Waterfront\", and \"One-Eyed Jacks\" (the only film directed by Brando)—talks in a documentary accompanying the DVD of \"A Streetcar Named Desire\" about a phone call he received from Brando shortly before Brando's death. A distressed Brando told Malden he kept falling over. Malden wanted to come over, but Brando put him off, telling him there was no point. Three weeks later, Brando was dead. Shortly before his death, he had apparently refused permission for tubes carrying oxygen to be inserted into his lungs, which, he was told, was the only way to prolong his life.\n\nBrando was cremated, and his ashes were put in with those of his childhood friend, comedian and actor Wally Cox and another longtime friend, Sam Gilman. They were then scattered partly in Tahiti and partly in Death Valley. In 2007, a 165-minute biopic of Brando for Turner Classic Movies, \"Brando: The Documentary\", produced by Mike Medavoy (the executor of Brando's will), was released.\n\nBrando was known for his tumultuous personal life and his large number of partners and children. He was the father to 11 children, three of whom were adopted. In 1976, he told a French journalist, \"Homosexuality is so much in fashion, it no longer makes news. Like a large number of men, I, too, have had homosexual experiences, and I am not ashamed. I have never paid much attention to what people think about me. But if there is someone who is convinced that Jack Nicholson and I are lovers, may they continue to do so. I find it amusing.\"\n\nIn \"Songs My Mother Taught Me\", Brando wrote he met Marilyn Monroe at a party where she played piano, unnoticed by anybody else there, that they had an affair and maintained an intermittent relationship for many years, and that he received a telephone call from her several days before she died. He also claimed numerous other romances, although he did not discuss his marriages, his wives, or his children in his autobiography.\n\nHe met nisei actress and dancer Reiko Sato in the early 1950s; in 1954 Dorothy Kilgallen reported they were an item. Though their relationship cooled, they remained friends for the rest of Sato's life, with her dividing her time between Los Angeles and Tetiaroa in her later years.\n\nBrando met actress Rita Moreno in 1954, beginning their torrid love affair. Moreno revealed in her memoir that when she became pregnant by Brando, he arranged for an abortion. After a botched abortion she tried to commit suicide by overdosing on his sleeping pills. Years after they broke up Moreno played his love interest in the film \"The Night of the Following Day\".\n\nBrando married actress Anna Kashfi in 1957. Kashfi was born in Calcutta and moved to Wales from India in 1947. She is said to have been the daughter of a Welsh steel worker of Irish descent, William O'Callaghan, who had been superintendent on the Indian State railways. However, in her book, \"Brando for Breakfast\", she claimed that she really is half Indian and that the press incorrectly thought that her stepfather, O'Callaghan, was her biological father. She said that her biological father was Indian and that she was the result of an \"unregistered alliance\" between her parents. Brando and Kashfi had a son, Christian Brando, on May 11, 1958; they divorced in 1959.\n\nIn 1960, Brando married Movita Castaneda, a Mexican-American actress seven years his senior; they were divorced in 1962. Castaneda had appeared in the first \"Mutiny on the Bounty\" film in 1935, some 27 years before the 1962 remake with Brando as Fletcher Christian. They had two children together: Miko Castaneda Brando (born 1961) and Rebecca Brando (born 1966).\n\nFrench actress Tarita Teriipaia, who played Brando's love interest in \"Mutiny on the Bounty\", became his third wife on August 10, 1962. She was 20 years old, 18 years younger than Brando, who was reportedly delighted by her naïveté. Because Teriipaia was a native French speaker, Brando became fluent in the language and gave numerous interviews in French. Teriipaia became the mother of two of his children: Simon Teihotu Brando (born 1963) and Tarita Cheyenne Brando (born 1970). Brando also adopted Teriipaia's daughter, Maimiti Brando (born 1977) and niece, Raiatua Brando (born 1982). Brando and Teriipaia divorced in July 1972.\n\nAfter Brando's death, the daughter of actress Cynthia Lynn claimed that Brando had had a short-lived affair with her mother, who appeared with Brando in \"Bedtime Story\", and that this affair resulted in her birth in 1964. Throughout the late 1960s and into the early 1980s, he had a tempestuous, long-term relationship with actress Jill Banner.\n\nBrando had a long-term relationship with his housekeeper Maria Cristina Ruiz, with whom he had three children: Ninna Priscilla Brando (born May 13, 1989), Myles Jonathan Brando (born January 16, 1992), and Timothy Gahan Brando (born January 6, 1994). Brando also adopted Petra Brando-Corval (born 1972), the daughter of his assistant Caroline Barrett and novelist James Clavell.\n\nBrando's close friendship with Wally Cox was the subject of rumors. Brando told a journalist: \"If Wally had been a woman, I would have married him and we would have lived happily ever after.\" Two of Cox's wives, however, dismissed the suggestion that the love was more than platonic.\n\nBrando's grandson Tuki Brando (born 1990), son of Cheyenne Brando, is a fashion model. His numerous grandchildren also include Michael Brando (born 1988), son of Christian Brando, Prudence Brando and Shane Brando, children of Miko C. Brando, the children of Rebecca Brando, and the three children of Teihotu Brando among others.\n\nStephen Blackehart has been reported to be the son of Brando but Blackehart disputes this claim.\nBrando earned a reputation as a 'bad boy' for his public outbursts and antics. According to \"Los Angeles\" magazine, \"Brando was rock and roll before anybody knew what rock and roll was.\" His behavior during the filming of \"Mutiny on the Bounty\" (1962) seemed to bolster his reputation as a difficult star. He was blamed for a change in director and a runaway budget, though he disclaimed responsibility for either. On June 12, 1973, Brando broke paparazzo Ron Galella's jaw. Galella had followed Brando, who was accompanied by talk show host Dick Cavett, after a taping of \"The Dick Cavett Show\" in New York City. He reportedly paid a $40,000 out-of-court settlement and suffered an infected hand as a result. Galella wore a football helmet the next time he photographed Brando at a gala benefiting the American Indians Development Association.\n\nThe filming of \"Mutiny on the Bounty\" affected Brando's life in a profound way, as he fell in love with Tahiti and its people. He bought a 12-island atoll, Tetiaroa, and in 1970 hired an award-winning young Los Angeles architect, Bernard Judge, to build his home and natural village there without despoiling the environment. An environmental laboratory protecting sea birds and turtles was established and student groups were welcomed there for many years. Tragically, the 1983 hurricane destroyed many of the structures including his resort. A hotel using Brando's name, The Brando Resort was officially opened to the public in 2014. Brando was an active ham radio operator, with the call signs KE6PZH and FO5GJ (the latter from his island). He was listed in the Federal Communications Commission (FCC) records as Martin Brandeaux to preserve his privacy.\n\nIn the A&E \"Biography\" episode on Brando, biographer Peter Manso comments, \"On the one hand, being a celebrity allowed Marlon to take his revenge on the world that had so deeply hurt him, so deeply scarred him. On the other hand he hated it because he knew it was false and ephemeral.\" In the same program another biographer, David Thomson, relates, \"Many, many people who worked with him, and came to work with him with the best intentions, went away in despair saying he's a spoiled kid. It has to be done his way or he goes away with some vast story about how he was wronged, he was offended, and I think that fits with the psychological pattern that he was a wronged kid.\"\n\nIn 1946, Brando performed in Ben Hecht's Zionist play \"A Flag is Born\". He attended some fundraisers for John F. Kennedy in the 1960 presidential election. In August 1963, he participated in the March on Washington along with fellow celebrities Harry Belafonte, James Garner, Charlton Heston, Burt Lancaster and Sidney Poitier. Along with Paul Newman, Brando also participated in the freedom rides.\n\nIn the aftermath of the 1968 assassination of Martin Luther King, Jr., Brando made one of the strongest commitments to furthering King's work. Shortly after King's death, he announced that he was bowing out of the lead role of a major film (\"The Arrangement\") (1969) which was about to begin production in order to devote himself to the civil rights movement. \"I felt I'd better go find out where it is; what it is to be black in this country; what this rage is all about,\" Brando said on the late-night ABC-TV talk show \"Joey Bishop Show\". In A&E's \"Biography\" episode on Brando, actor and co-star Martin Sheen states, \"I'll never forget the night that Reverend King was shot and I turned on the news and Marlon was walking through Harlem with Mayor Lindsay. And there were snipers and there was a lot of unrest and he kept walking and talking through those neighborhoods with Mayor Lindsay. It was one of the most incredible acts of courage I ever saw, and it meant a lot and did a lot.\"\n\nBrando's participation in the civil rights movement actually began well before King's death. In the early 1960s, he contributed thousands of dollars to both the Southern Christian Leadership Conference (S.C.L.C.) and to a scholarship fund established for the children of slain Mississippi N.A.A.C.P. leader Medgar Evers. In 1964 Brando was arrested at a \"fish-in\" held to protest a broken treaty that had promised Native Americans fishing rights in Puget Sound. By this time, Brando was already involved in films that carried messages about human rights: \"Sayonara\", which addressed interracial romance, and \"The Ugly American\", depicting the conduct of U.S. officials abroad and the deleterious effect on the citizens of foreign countries. For a time, he was also donating money to the Black Panther Party and considered himself a friend of founder Bobby Seale. Brando ended his financial support for the group over his perception of its increasing radicalization, specifically a passage in a Panther pamphlet put out by Eldridge Cleaver advocating indiscriminate violence, \"for the Revolution.\"\n\nAt the 1973 Academy Awards ceremony, Brando refused to accept the Oscar for his performance in \"The Godfather\". Sacheen Littlefeather represented him at the ceremony. She appeared in full Apache attire and stated that owing to the \"poor treatment of Native Americans in the film industry\", Brando would not accept the award. This occurred while the standoff at Wounded Knee was ongoing. The event grabbed the attention of the US and the world media. This was considered a major event and victory for the movement by its supporters and participants.\n\nOutside of his film work, Brando appeared before the California Assembly in support of a fair housing law and personally joined picket lines in demonstrations protesting discrimination in housing developments.\n\nHe was also an activist against apartheid. In 1964, he favored a boycott of his films in South Africa to prevent them from being shown to a segregated audience. He took part at a 1975 protest rally against American investments in South Africa and for the release of Nelson Mandela. In 1989, Brando also starred in the film \"A Dry White Season\", based upon André Brink's novel of the same name.\n\nIn an interview in \"Playboy\" magazine in January 1979, Brando said: \"You've seen every single race besmirched, but you never saw an image of the kike because the Jews were ever so watchful for that—and rightly so. They never allowed it to be shown on screen. The Jews have done so much for the world that, I suppose, you get extra disappointed because they didn't pay attention to that.\"\nBrando made a similar comment on \"Larry King Live\" in April 1996, saying \"Hollywood is run by Jews; it is owned by Jews, and they should have a greater sensitivity about the issue of—of people who are suffering. Because they've exploited—we have seen the—we have seen the nigger and greaseball, we've seen the chink, we've seen the slit-eyed dangerous Jap, we have seen the wily Filipino, we've seen everything, but we never saw the kike. Because they knew perfectly well, that that is where you draw the wagons around.\" Larry King, who is Jewish, replied, \"When you say—when you say something like that, you are playing right in, though, to anti-Semitic people who say the Jews are—\" Brando interrupted: \"No, no, because I will be the first one who will appraise the Jews honestly and say 'Thank God for the Jews'.\"\nJay Kanter, Brando's agent, producer, and friend, defended him in \"Daily Variety\": \"Marlon has spoken to me for hours about his fondness for the Jewish people, and he is a well-known supporter of Israel.\" Similarly, Louie Kemp, in his article for \"Jewish Journal\", wrote: \"You might remember him as Don Vito Corleone, Stanley Kowalski or the eerie Col. Walter E. Kurtz in 'Apocalypse Now', but I remember Marlon Brando as a mensch and a personal friend of the Jewish people when they needed it most.\"\n\nBrando was one of the most respected actors of the post-war era. He is listed by the American Film Institute as the fourth greatest male star whose screen debut occurred before or during 1950 (it occurred in 1950). He earned respect among critics for his memorable performances and charismatic screen presence. He helped popularize Method acting. He is regarded as one of the greatest cinema actors of the 20th century.\n\n\"Encyclopedia Britannica\" describes him as \"the most celebrated of the method actors, and his slurred, mumbling delivery marked his rejection of classical dramatic training. His true and passionate performances proved him one of the greatest actors of his generation\". It also notes the apparent paradox of his talent: \"He is regarded as the most influential actor of his generation, yet his open disdain for the acting profession ... often manifested itself in the form of questionable choices and uninspired performances. Nevertheless, he remains a riveting screen presence with a vast emotional range and an endless array of compulsively watchable idiosyncrasies.\"\n\nMarlon Brando is a cultural icon with an enduring popularity. His rise to national attention in the 1950s had a profound effect on American culture.\nAccording to film critic Pauline Kael, \"Brando represented a reaction against the post-war mania for security. As a protagonist, the Brando of the early fifties had no code, only his instincts. He was a development from the gangster leader and the outlaw. He was antisocial because he knew society was crap; he was a hero to youth because he was strong enough not to take the crap ... Brando represented a contemporary version of the free American ... Brando is still the most exciting American actor on the screen.\" Sociologist Dr. Suzanne Mcdonald-Walker states: \"Marlon Brando, sporting leather jacket, jeans, and moody glare, became a cultural icon summing up 'the road' in all its maverick glory.\" His portrayal of the gang leader Johnny Strabler in \"The Wild One\" has become an iconic image, used both as a symbol of rebelliousness and a fashion accessory that includes a Perfecto style motorcycle jacket, a tilted cap, jeans and sunglasses. Johnny's haircut inspired a craze for sideburns, followed by James Dean and Elvis Presley, among others. Dean copied Brando's acting style extensively and Presley used Brando's image as a model for his role in \"Jailhouse Rock\". The \"I coulda been a contender\" scene from \"On the Waterfront\", according to the author of\" Brooklyn Boomer\", Martin H. Levinson, is \"one of the most famous scenes in motion picture history, and the line itself has become part of America's cultural lexicon.\" An example of the endurance of Brando's popular \"Wild One\" image was the 2009 release of replicas of the leather jacket worn by Brando's Johnny Strabler character. The jackets were marketed by Triumph, the manufacturer of the Triumph Thunderbird motorcycles featured in \"The Wild One\", and were officially licensed by Brando's estate.\n\nBrando was also considered a male sex symbol. Linda Williams writes: \"Marlon Brando [was] the quintessential American male sex symbol of the late fifties and early sixties\". Brando was an early lesbian icon who, along with James Dean, influenced the butch look and self-image in the 1950s and after.\n\nBrando has also been immortalized in music; most notably, he was mentioned in the lyrics of \"Vogue\" by Madonna.\n\nThe character Dio Brando from the popular Japanese Manga series JoJo's Bizarre Adventure gets his name from Brando, as well as American heavy metal band Dio (band) and its lead singer, Ronnie James Dio.\n\nIn his autobiography \"Songs My Mother Taught Me\", Brando observed:\n\nHe also confessed that, while having great admiration for the theater, he did not return to it after his initial success primarily because the work left him drained emotionally:\n\nBrando repeatedly credited Stella Adler and her understanding of the Stanislavsky acting technique for bringing realism to American cinema, but also added:\n\nIn the 2015 documentary \"Listen to Me Marlon\", Brando shared his thoughts on playing a death scene, stating, \"That's a tough scene to play. You have to make 'em believe that you are dying ... Try to think of the most intimate moment you've ever had in your life.\" Brando's favorite actors were Spencer Tracy, John Barrymore, Fredric March, James Cagney and Paul Muni.\n\nUpon his death in 2004, Brando left an estate valued at $21.6 million. Brando's estate still earned about $9 million in 2005, the year following his death, according to \"Forbes\". That year Brando was named one of the top-earning deceased celebrities in the world by the magazine.\n\nBrando was named the fourth greatest male star whose screen debut occurred before or during 1950 by the American Film Institute, and part of \"TIME\" magazine's . He was also named one of the top 10 \"Icons of the Century\" by \"Variety\" magazine.\n\n\nNotes\nCitations\nBibliography\n\n"}
{"id": "19904", "url": "https://en.wikipedia.org/wiki?curid=19904", "title": "Meteorology", "text": "Meteorology\n\nMeteorology is a branch of the atmospheric sciences which includes atmospheric chemistry and atmospheric physics, with a major focus on weather forecasting. The study of meteorology dates back millennia, though significant progress in meteorology did not occur until the 18th century. The 19th century saw modest progress in the field after weather observation networks were formed across broad regions. Prior attempts at prediction of weather depended on historical data. It was not until after the elucidation of the laws of physics and more particularly, the development of the computer, allowing for the automated solution of a great many equations that model the weather, in the latter half of the 20th century that significant breakthroughs in weather forecasting were achieved.\n\nMeteorological phenomena are observable weather events that are explained by the science of meteorology. Meteorological phenomena are described and quantified by the variables of Earth's atmosphere: temperature, air pressure, water vapour, mass flow, and the variations and interactions of those variables, and how they change over time. Different spatial scales are used to describe and predict weather on local, regional, and global levels.\nMeteorology, climatology, atmospheric physics, and atmospheric chemistry are sub-disciplines of the atmospheric sciences. Meteorology and hydrology compose the interdisciplinary field of hydrometeorology. The interactions between Earth's atmosphere and its oceans are part of a coupled ocean-atmosphere system. Meteorology has application in many diverse fields such as the military, energy production, transport, agriculture, and construction.\n\nThe word \"meteorology\" is from the Ancient Greek μετέωρος \"metéōros\" (\"meteor\") and -λογία \"-logia\" (\"-(o)logy\"), meaning \"the study of things high in the air\".\n\nThe ability to predict rains and floods based on annual cycles was evidently used by humans at least from the time of agricultural settlement if not earlier. Early approaches to predicting weather were based on astrology and were practiced by priests. Cuneiform inscriptions on Babylonian tablets included associations between thunder and rain. The Chaldeans differentiated the 22° and 46° halos.\n\nAncient Indian Upanishads contain mentions of clouds and seasons. The Samaveda mentions sacrifices to be performed when certain phenomena were noticed. Varāhamihira's classical work \"Brihatsamhita\", written about 500 AD, provides evidence of weather observation.\n\nIn 350 BC, Aristotle wrote \"Meteorology\". Aristotle is considered the founder of meteorology. One of the most impressive achievements described in the \"Meteorology\" is the description of what is now known as the hydrologic cycle.\n\nThe book De Mundo (composed before 250 BC or between 350 and 200 BC) noted\n\nThe Greek scientist Theophrastus compiled a book on weather forecasting, called the \"Book of Signs\". The work of Theophrastus remained a dominant influence in the study of weather and in weather forecasting for nearly 2,000 years. In 25 AD, Pomponius Mela, a geographer for the Roman Empire, formalized the climatic zone system. According to Toufic Fahd, around the 9th century, Al-Dinawari wrote the \"Kitab al-Nabat\" (\"Book of Plants\"), in which he deals with the application of meteorology to agriculture during the Muslim Agricultural Revolution. He describes the meteorological character of the sky, the planets and constellations, the sun and moon, the lunar phases indicating seasons and rain, the \"anwa\" (heavenly bodies of rain), and atmospheric phenomena such as winds, thunder, lightning, snow, floods, valleys, rivers, lakes.\n\nEarly attempts at predicting weather were often related to prophesy and divining and sometimes based on astrological ideas. Admiral FitzRoy tried to separate scientific approaches from prophetic ones.\n\nPtolemy wrote on the atmospheric refraction of light in the context of astronomical observations. In 1021, Alhazen showed that atmospheric refraction is also responsible for twilight; he estimated that twilight begins when the sun is 19 degrees below the horizon, and also used a geometric determination based on this to estimate the maximum possible height of the Earth's atmosphere as 52,000 \"passim\" (about 49 miles, or 79 km).\n\nSt. Albert the Great was the first to propose that each drop of falling rain had the form of a small sphere, and that this form meant that the rainbow was produced by light interacting with each raindrop. Roger Bacon was the first to calculate the angular size of the rainbow. He stated that a rainbow summit can not appear higher than 42 degrees above the horizon. In the late 13th century and early 14th century, Kamāl al-Dīn al-Fārisī and Theodoric of Freiberg were the first to give the correct explanations for the primary rainbow phenomenon. Theoderic went further and also explained the secondary rainbow. In 1716, Edmund Halley suggested that aurorae are caused by \"magnetic effluvia\" moving along the Earth's magnetic field lines.\n\nIn 1441, King Sejong's son, Prince Munjong of Korea, invented the first standardized rain gauge. These were sent throughout the Joseon Dynasty of Korea as an official tool to assess land taxes based upon a farmer's potential harvest. In 1450, Leone Battista Alberti developed a swinging-plate anemometer, and was known as the first \"anemometer\". In 1607, Galileo Galilei constructed a thermoscope. In 1611, Johannes Kepler wrote the first scientific treatise on snow crystals: \"Strena Seu de Nive Sexangula (A New Year's Gift of Hexagonal Snow)\". In 1643, Evangelista Torricelli invented the mercury barometer. In 1662, Sir Christopher Wren invented the mechanical, self-emptying, tipping bucket rain gauge. In 1714, Gabriel Fahrenheit created a reliable scale for measuring temperature with a mercury-type thermometer. In 1742, Anders Celsius, a Swedish astronomer, proposed the \"centigrade\" temperature scale, the predecessor of the current Celsius scale. In 1783, the first hair hygrometer was demonstrated by Horace-Bénédict de Saussure. In 1802–1803, Luke Howard wrote \"On the Modification of Clouds\", in which he assigns cloud types Latin names. In 1806, Francis Beaufort introduced his system for classifying wind speeds. Near the end of the 19th century the first cloud atlases were published, including the \"International Cloud Atlas\", which has remained in print ever since. The April 1960 launch of the first successful weather satellite, TIROS-1, marked the beginning of the age where weather information became available globally.\n\nIn 1648, Blaise Pascal rediscovered that atmospheric pressure decreases with height, and deduced that there is a vacuum above the atmosphere. In 1738, Daniel Bernoulli published \"Hydrodynamics\", initiating the Kinetic theory of gases and established the basic laws for the theory of gases. In 1761, Joseph Black discovered that ice absorbs heat without changing its temperature when melting. In 1772, Black's student Daniel Rutherford discovered nitrogen, which he called \"phlogisticated air\", and together they developed the phlogiston theory. In 1777, Antoine Lavoisier discovered oxygen and developed an explanation for combustion. In 1783, in Lavoisier's essay \"Reflexions sur le phlogistique\", he deprecates the phlogiston theory and proposes a caloric theory. In 1804, Sir John Leslie observed that a matte black surface radiates heat more effectively than a polished surface, suggesting the importance of black body radiation. In 1808, John Dalton defended caloric theory in \"A New System of Chemistry\" and described how it combines with matter, especially gases; he proposed that the heat capacity of gases varies inversely with atomic weight. In 1824, Sadi Carnot analyzed the efficiency of steam engines using caloric theory; he developed the notion of a reversible process and, in postulating that no such thing exists in nature, laid the foundation for the second law of thermodynamics.\n\nIn 1494, Christopher Columbus experienced a tropical cyclone, which led to the first written European account of a hurricane. In 1686, Edmund Halley presented a systematic study of the trade winds and monsoons and identified solar heating as the cause of atmospheric motions. In 1735, an \"ideal\" explanation of global circulation through study of the trade winds was written by George Hadley. In 1743, when Benjamin Franklin was prevented from seeing a lunar eclipse by a hurricane, he decided that cyclones move in a contrary manner to the winds at their periphery. Understanding the kinematics of how exactly the rotation of the Earth affects airflow was partial at first. Gaspard-Gustave Coriolis published a paper in 1835 on the energy yield of machines with rotating parts, such as waterwheels. In 1856, William Ferrel proposed the existence of a circulation cell in the mid-latitudes, and the air within deflected by the Coriolis force resulting in the prevailing westerly winds. Late in the 19th century, the motion of air masses along isobars was understood to be the result of the large-scale interaction of the pressure gradient force and the deflecting force. By 1912, this deflecting force was named the Coriolis effect. Just after World War I, a group of meteorologists in Norway led by Vilhelm Bjerknes developed the Norwegian cyclone model that explains the generation, intensification and ultimate decay (the life cycle) of mid-latitude cyclones, and introduced the idea of fronts, that is, sharply defined boundaries between air masses. The group included Carl-Gustaf Rossby (who was the first to explain the large scale atmospheric flow in terms of fluid dynamics), Tor Bergeron (who first determined how rain forms) and Jacob Bjerknes.\n\nIn the late 16th century and first half of the 17th century a range of meteorological instruments was invented – the thermometer, barometer, hydrometer, as well as wind and rain gauges. In the 1650s natural philosophers started using these instruments to systematically record weather observations. Scientific academies established weather diaries and organised observational networks. In 1654, Ferdinando II de Medici established the first \"weather observing\" network, that consisted of meteorological stations in Florence, Cutigliano, Vallombrosa, Bologna, Parma, Milan, Innsbruck, Osnabrück, Paris and Warsaw. The collected data were sent to Florence at regular time intervals. In the 1660s Robert Hooke of the Royal Society of London sponsored networks of weather observers. Hippocrates' treatise \"Airs, Waters, and Places\" had linked weather to disease. Thus early meteorologists attempted to correlate weather patterns with epidemic outbreaks, and the climate with public health.\n\nDuring the Age of Enlightenment meteorology tried to rationalise traditional weather lore, including astrological meteorology. But there were also attempts to establish a theoretical understanding of weather phenomena. Edmond Halley and George Hadley tried to explain trade winds. They reasoned that the rising mass of heated equator air is replaced by an inflow of cooler air from high latitudes. A flow of warm air at high altitude from equator to poles in turn established an early picture of circulation. Frustration with the lack of discipline among weather observers, and the poor quality of the instruments, led the early modern nation states to organise large observation networks. Thus by the end of the 18th century meteorologists had access to large quantities of reliable weather date. In 1832, an electromagnetic telegraph was created by Baron Schilling. The arrival of the electrical telegraph in 1837 afforded, for the first time, a practical method for quickly gathering surface weather observations from a wide area.\n\nThis data could be used to produce maps of the state of the atmosphere for a region near the Earth's surface and to study how these states evolved through time. To make frequent weather forecasts based on these data required a reliable network of observations, but it was not until 1849 that the Smithsonian Institution began to establish an observation network across the United States under the leadership of Joseph Henry. Similar observation networks were established in Europe at this time. The Reverend William Clement Ley was key in understanding of cirrus clouds and early understandings of Jet Streams. Charles Kenneth Mackinnon Douglas, known as 'CKM' Douglas read Ley's papers after his death and carried on the early study of weather systems.\nNineteenth century researchers in meteorology were drawn from military or medical backgrounds, rather than trained as dedicated scientists. In 1854, the United Kingdom government appointed Robert FitzRoy to the new office of \"Meteorological Statist to the Board of Trade\" with the task of gathering weather observations at sea. FitzRoy's office became the United Kingdom Meteorological Office in 1854, the second oldest national meteorological service in the world (the Central Institution for Meteorology and Geodynamics (ZAMG) in Austria was founded in 1851 and is the oldest weather service in the world). The first daily weather forecasts made by FitzRoy's Office were published in \"The Times\" newspaper in 1860. The following year a system was introduced of hoisting storm warning cones at principal ports when a gale was expected.\n\nOver the next 50 years many countries established national meteorological services. The India Meteorological Department (1875) was established to follow tropical cyclone and monsoon. The Finnish Meteorological Central Office (1881) was formed from part of Magnetic Observatory of Helsinki University. Japan's Tokyo Meteorological Observatory, the forerunner of the Japan Meteorological Agency, began constructing surface weather maps in 1883. The United States Weather Bureau (1890) was established under the United States Department of Agriculture. The Australian Bureau of Meteorology (1906) was established by a Meteorology Act to unify existing state meteorological services.\n\nIn 1904, Norwegian scientist Vilhelm Bjerknes first argued in his paper \"Weather Forecasting as a Problem in Mechanics and Physics\" that it should be possible to forecast weather from calculations based upon natural laws.\n\nIt was not until later in the 20th century that advances in the understanding of atmospheric physics led to the foundation of modern numerical weather prediction. In 1922, Lewis Fry Richardson published \"Weather Prediction By Numerical Process\", after finding notes and derivations he worked on as an ambulance driver in World War I. He described how small terms in the prognostic fluid dynamics equations that govern atmospheric flow could be neglected, and a numerical calculation scheme that could be devised to allow predictions. Richardson envisioned a large auditorium of thousands of people performing the calculations. However, the sheer number of calculations required was too large to complete without electronic computers, and the size of the grid and time steps used in the calculations led to unrealistic results. Though numerical analysis later found that this was due to numerical instability.\n\nStarting in the 1950s, numerical forecasts with computers became feasible. The first weather forecasts derived this way used barotropic (single-vertical-level) models, and could successfully predict the large-scale movement of midlatitude Rossby waves, that is, the pattern of atmospheric lows and highs. In 1959, the UK Meteorological Office received its first computer, a Ferranti Mercury.\n\nIn the 1960s, the chaotic nature of the atmosphere was first observed and mathematically described by Edward Lorenz, founding the field of chaos theory. These advances have led to the current use of ensemble forecasting in most major forecasting centers, to take into account uncertainty arising from the chaotic nature of the atmosphere. Mathematical models used to predict the long term weather of the Earth (climate models), have been developed that have a resolution today that are as coarse as the older weather prediction models. These climate models are used to investigate long-term climate shifts, such as what effects might be caused by human emission of greenhouse gases.\n\nMeteorologists are scientists who study meteorology. The American Meteorological Society published and continually updates an authoritative electronic \"Meteorology Glossary\". Meteorologists work in government agencies, private consulting and research services, industrial enterprises, utilities, radio and television stations, and in education. In the United States, meteorologists held about 9,400 jobs in 2009.\n\nMeteorologists are best known by the public for weather forecasting. Some radio and television weather forecasters are professional meteorologists, while others are reporters (weather specialist, weatherman, etc.) with no formal meteorological training. The American Meteorological Society and National Weather Association issue \"Seals of Approval\" to weather broadcasters who meet certain requirements.\n\nEach science has its own unique sets of laboratory equipment. In the atmosphere, there are many things or qualities of the atmosphere that can be measured. Rain, which can be observed, or seen anywhere and anytime was one of the first atmospheric qualities measured historically. Also, two other accurately measured qualities are wind and humidity. Neither of these can be seen but can be felt. The devices to measure these three sprang up in the mid-15th century and were respectively the rain gauge, the anemometer, and the hygrometer. Many attempts had been made prior to the 15th century to construct adequate equipment to measure the many atmospheric variables. Many were faulty in some way or were simply not reliable. Even Aristotle noted this in some of his work as the difficulty to measure the air.\n\nSets of surface measurements are important data to meteorologists. They give a snapshot of a variety of weather conditions at one single location and are usually at a weather station, a ship or a weather buoy. The measurements taken at a weather station can include any number of atmospheric observables. Usually, temperature, pressure, wind measurements, and humidity are the variables that are measured by a thermometer, barometer, anemometer, and hygrometer, respectively. Professional stations may also include air quality sensors (carbon monoxide, carbon dioxide, methane, ozone, dust, and smoke), ceilometer (cloud ceiling), falling precipitation sensor, flood sensor, lightning sensor, microphone (explosions, sonic booms, thunder), pyranometer/pyrheliometer/spectroradiometer (IR/Vis/UV photodiodes), rain gauge/snow gauge, scintillation counter (background radiation, fallout, radon), seismometer (earthquakes and tremors), transmissometer (visibility), and a GPS clock for data logging. Upper air data are of crucial importance for weather forecasting. The most widely used technique is launches of radiosondes. Supplementing the radiosondes a network of aircraft collection is organized by the World Meteorological Organization.\n\nRemote sensing, as used in meteorology, is the concept of collecting data from remote weather events and subsequently producing weather information. The common types of remote sensing are Radar, Lidar, and satellites (or photogrammetry). Each collects data about the atmosphere from a remote location and, usually, stores the data where the instrument is located. Radar and Lidar are not passive because both use EM radiation to illuminate a specific portion of the atmosphere. Weather satellites along with more general-purpose Earth-observing satellites circling the earth at various altitudes have become an indispensable tool for studying a wide range of phenomena from forest fires to El Niño.\n\nThe study of the atmosphere can be divided into distinct areas that depend on both time and spatial scales. At one extreme of this scale is climatology. In the timescales of hours to days, meteorology separates into micro-, meso-, and synoptic scale meteorology. Respectively, the geospatial size of each of these three scales relates directly with the appropriate timescale.\n\nOther subclassifications are used to describe the unique, local, or broad effects within those subclasses.\nMicroscale meteorology is the study of atmospheric phenomena on a scale of about or less. Individual thunderstorms, clouds, and local turbulence caused by buildings and other obstacles (such as individual hills) are modeled on this scale.\n\nMesoscale meteorology is the study of atmospheric phenomena that has horizontal scales ranging from 1 km to 1000 km and a vertical scale that starts at the Earth's surface and includes the atmospheric boundary layer, troposphere, tropopause, and the lower section of the stratosphere. Mesoscale timescales last from less than a day to weeks. The events typically of interest are thunderstorms, squall lines, fronts, precipitation bands in tropical and extratropical cyclones, and topographically generated weather systems such as mountain waves and sea and land breezes.\n\nSynoptic scale meteorology predicts atmospheric changes at scales up to 1000 km and 10 sec (28 days), in time and space. At the synoptic scale, the Coriolis acceleration acting on moving air masses (outside of the tropics) plays a dominant role in predictions. The phenomena typically described by synoptic meteorology include events such as extratropical cyclones, baroclinic troughs and ridges, frontal zones, and to some extent jet streams. All of these are typically given on weather maps for a specific time. The minimum horizontal scale of synoptic phenomena is limited to the spacing between surface observation stations.\n\nGlobal scale meteorology is the study of weather patterns related to the transport of heat from the tropics to the poles. Very large scale oscillations are of importance at this scale. These oscillations have time periods typically on the order of months, such as the Madden–Julian oscillation, or years, such as the El Niño–Southern Oscillation and the Pacific decadal oscillation. Global scale meteorology pushes into the range of climatology. The traditional definition of climate is pushed into larger timescales and with the understanding of the longer time scale global oscillations, their effect on climate and weather disturbances can be included in the synoptic and mesoscale timescales predictions.\n\nNumerical Weather Prediction is a main focus in understanding air–sea interaction, tropical meteorology, atmospheric predictability, and tropospheric/stratospheric processes. The Naval Research Laboratory in Monterey, California, developed a global atmospheric model called Navy Operational Global Atmospheric Prediction System (NOGAPS). NOGAPS is run operationally at Fleet Numerical Meteorology and Oceanography Center for the United States Military. Many other global atmospheric models are run by national meteorological agencies.\n\nBoundary layer meteorology is the study of processes in the air layer directly above Earth's surface, known as the atmospheric boundary layer (ABL). The effects of the surface – heating, cooling, and friction – cause turbulent mixing within the air layer. Significant movement of heat, matter, or momentum on time scales of less than a day are caused by turbulent motions. Boundary layer meteorology includes the study of all types of surface–atmosphere boundary, including ocean, lake, urban land and non-urban land for the study of meteorology.\n\nDynamic meteorology generally focuses on the fluid dynamics of the atmosphere. The idea of air parcel is used to define the smallest element of the atmosphere, while ignoring the discrete molecular and chemical nature of the atmosphere. An air parcel is defined as a point in the fluid continuum of the atmosphere. The fundamental laws of fluid dynamics, thermodynamics, and motion are used to study the atmosphere. The physical quantities that characterize the state of the atmosphere are temperature, density, pressure, etc. These variables have unique values in the continuum.\n\nWeather forecasting is the application of science and technology to predict the state of the atmosphere at a future time and given location. Humans have attempted to predict the weather informally for millennia and formally since at least the 19th century. Weather forecasts are made by collecting quantitative data about the current state of the atmosphere and using scientific understanding of atmospheric processes to project how the atmosphere will evolve.\n\nOnce an all-human endeavor based mainly upon changes in barometric pressure, current weather conditions, and sky condition, forecast models are now used to determine future conditions. Human input is still required to pick the best possible forecast model to base the forecast upon, which involves pattern recognition skills, teleconnections, knowledge of model performance, and knowledge of model biases. The chaotic nature of the atmosphere, the massive computational power required to solve the equations that describe the atmosphere, error involved in measuring the initial conditions, and an incomplete understanding of atmospheric processes mean that forecasts become less accurate as the difference in current time and the time for which the forecast is being made (the \"range\" of the forecast) increases. The use of ensembles and model consensus help narrow the error and pick the most likely outcome.\n\nThere are a variety of end uses to weather forecasts. Weather warnings are important forecasts because they are used to protect life and property. Forecasts based on temperature and precipitation are important to agriculture, and therefore to commodity traders within stock markets. Temperature forecasts are used by utility companies to estimate demand over coming days. On an everyday basis, people use weather forecasts to determine what to wear. Since outdoor activities are severely curtailed by heavy rain, snow, and wind chill, forecasts can be used to plan activities around these events, and to plan ahead and survive them.\n\nAviation meteorology deals with the impact of weather on air traffic management. It is important for air crews to understand the implications of weather on their flight plan as well as their aircraft, as noted by the Aeronautical Information Manual:\n\"The effects of ice on aircraft are cumulative—thrust is reduced, drag increases, lift lessens, and weight increases. The results are an increase in stall speed and a deterioration of aircraft performance. In extreme cases, 2 to 3 inches of ice can form on the leading edge of the airfoil in less than 5 minutes. It takes but 1/2 inch of ice to reduce the lifting power of some aircraft by 50 percent and increases the frictional drag by an equal percentage.\"\n\nMeteorologists, soil scientists, agricultural hydrologists, and agronomists are people concerned with studying the effects of weather and climate on plant distribution, crop yield, water-use efficiency, phenology of plant and animal development, and the energy balance of managed and natural ecosystems. Conversely, they are interested in the role of vegetation on climate and weather.\n\nHydrometeorology is the branch of meteorology that deals with the hydrologic cycle, the water budget, and the rainfall statistics of storms. A hydrometeorologist prepares and issues forecasts of accumulating (quantitative) precipitation, heavy rain, heavy snow, and highlights areas with the potential for flash flooding. Typically the range of knowledge that is required overlaps with climatology, mesoscale and synoptic meteorology, and other geosciences.\n\nThe multidisciplinary nature of the branch can result in technical challenges, since tools and solutions from each of the individual disciplines involved may behave slightly differently, be optimized for different hard- and software platforms and use different data formats. There are some initiatives – such as the DRIHM project – that are trying to address this issue.\n\nNuclear meteorology investigates the distribution of radioactive aerosols and gases in the atmosphere.\n\nMaritime meteorology deals with air and wave forecasts for ships operating at sea. Organizations such as the Ocean Prediction Center, Honolulu National Weather Service forecast office, United Kingdom Met Office, and JMA prepare high seas forecasts for the world's oceans.\n\nMilitary meteorology is the research and application of meteorology for military purposes. In the United States, the United States Navy's Commander, Naval Meteorology and Oceanography Command oversees meteorological efforts for the Navy and Marine Corps while the United States Air Force's Air Force Weather Agency is responsible for the Air Force and Army.\n\nEnvironmental meteorology mainly analyzes industrial pollution dispersion physically and chemically based on meteorological parameters such as temperature, humidity, wind, and various weather conditions.\n\nMeteorology applications in renewable energy includes basic research, \"exploration\", and potential mapping of wind power and solar radiation for wind and solar energy.\n\n\n\n\"Please see weather forecasting for weather forecast sites.\"\n"}
{"id": "19908", "url": "https://en.wikipedia.org/wiki?curid=19908", "title": "Mount", "text": "Mount\n\nMount is often used as part of the name of specific mountains, e.g. Mount Everest.\n\nMount or Mounts may also refer to:\n\n\n\n\n\n\n"}
{"id": "19916", "url": "https://en.wikipedia.org/wiki?curid=19916", "title": "Meitnerium", "text": "Meitnerium\n\nMeitnerium is a synthetic chemical element with symbol Mt and atomic number 109. It is an extremely radioactive synthetic element (an element not found in nature, but can be created in a laboratory). The most stable known isotope, meitnerium-278, has a half-life of 7.6 seconds, although the unconfirmed meitnerium-282 may have a longer half-life of 67 seconds. The GSI Helmholtz Centre for Heavy Ion Research near Darmstadt, Germany, first created this element in 1982. It is named after Lise Meitner.\n\nIn the periodic table, meitnerium is a d-block transactinide element. It is a member of the 7th period and is placed in the group 9 elements, although no chemical experiments have yet been carried out to confirm that it behaves as the heavier homologue to iridium in group 9 as the seventh member of the 6d series of transition metals. Meitnerium is calculated to have similar properties to its lighter homologues, cobalt, rhodium, and iridium.\n\nMeitnerium was first synthesized on August 29, 1982 by a German research team led by Peter Armbruster and Gottfried Münzenberg at the Institute for Heavy Ion Research (Gesellschaft für Schwerionenforschung) in Darmstadt. The team bombarded a target of bismuth-209 with accelerated nuclei of iron-58 and detected a single atom of the isotope meitnerium-266:\n\nThis work was confirmed three years later at the Joint Institute for Nuclear Research at Dubna (then in the Soviet Union).\n\nUsing Mendeleev's nomenclature for unnamed and undiscovered elements, meitnerium should be known as \"eka-iridium\". In 1979, during the Transfermium Wars (but before the synthesis of meitnerium), IUPAC published recommendations according to which the element was to be called \"unnilennium\" (with the corresponding symbol of \"Une\"), a systematic element name as a placeholder, until the element was discovered (and the discovery then confirmed) and a permanent name was decided on. Although widely used in the chemical community on all levels, from chemistry classrooms to advanced textbooks, the recommendations were mostly ignored among scientists in the field, who either called it \"element 109\", with the symbol of \"E109\", \"(109)\" or even simply \"109\", or used the proposed name \"meitnerium\".\n\nThe naming of meitnerium was discussed in the element naming controversy regarding the names of elements 104 to 109, but \"meitnerium\" was the only proposal and thus was never disputed. The name \"meitnerium\" (Mt) was suggested by the GSI team in September 1992 in honor of the Austrian physicist Lise Meitner, a co-discoverer of protactinium (with Otto Hahn), and one of the discoverers of nuclear fission. In 1994 the name was recommended by IUPAC, and was officially adopted in 1997. It is thus the only element named specifically after a non-mythological woman (curium being named for both Pierre and Marie Curie).\n\nMeitnerium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Eight different isotopes of meitnerium have been reported with atomic masses 266, 268, 270, and 274–278, two of which, meitnerium-268 and meitnerium-270, have known but unconfirmed metastable states. A ninth isotope with atomic mass 282 is unconfirmed. Most of these decay predominantly through alpha decay, although some undergo spontaneous fission.\n\nAll meitnerium isotopes are extremely unstable and radioactive; in general, heavier isotopes are more stable than the lighter. The most stable known meitnerium isotope, Mt, is also the heaviest known; it has a half-life of 7.6 seconds. The unconfirmed Mt is even heavier and appears to have a longer half-life of 67 seconds. A metastable nuclear isomer, Mt, has been reported to also have a half-life of over a second. The isotopes Mt and Mt have half-lives of 0.72 and 0.44 seconds respectively. The remaining four isotopes have half-lives between 1 and 20 milliseconds.\n\nMeitnerium is the seventh member of the 6d series of transition metals. Since element 112 (copernicium) has been shown to be a group 12 metal, it is expected that all the elements from 104 to 111 would continue a fourth transition metal series, with meitnerium as part of the platinum group metals. Calculations on its ionization potentials and atomic and ionic radii are similar to that of its lighter homologue iridium, thus implying that meitnerium's basic properties will resemble those of the other group 9 elements, cobalt, rhodium, and iridium.\n\nPrediction of the probable chemical properties of meitnerium has not received much attention recently. Meitnerium is expected to be a noble metal. Based on the most stable oxidation states of the lighter group 9 elements, the most stable oxidation states of meitnerium are predicted to be the +6, +3, and +1 states, with the +3 state being the most stable in aqueous solutions. In comparison, rhodium and iridium show a maximum oxidation state of +6, while the most stable states are +4 and +3 for iridium and +3 for rhodium. The oxidation state +9, represented only by iridium in [IrO], might be possible for its congener meitnerium in the nonafluoride (MtF) and the [MtO] cation, although [IrO] is expected to be more stable than these meitnerium compounds. The tetrahalides of meitnerium have also been predicted to have similar stabilities to those of iridium, thus also allowing a stable +4 state. It is further expected that the maximum oxidation states of elements from bohrium (element 107) to darmstadtium (element 110) may be stable in the gas phase but not in aqueous solution.\n\nMeitnerium is expected to be a solid under normal conditions and assume a face-centered cubic crystal structure, similarly to its lighter congener iridium. It should be a very heavy metal with a density of around 37.4 g/cm, which would be the second-highest of any of the 118 known elements, second only to that predicted for its neighbor hassium (41 g/cm). In comparison, the densest known element that has had its density measured, osmium, has a density of only 22.61 g/cm. This results from meitnerium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough meitnerium to measure this quantity would be impractical, and the sample would quickly decay. Meitnerium is also predicted to be paramagnetic.\n\nTheoreticians have predicted the covalent radius of meitnerium to be 6 to 10 pm larger than that of iridium. The atomic radius of meitnerium is expected to be around 128 pm.\n\nMeitnerium is the first element on the periodic table whose chemistry has not yet been investigated. Unambiguous determination of the chemical characteristics of meitnerium has yet to have been established due to the short half-lives of meitnerium isotopes and a limited number of likely volatile compounds that could be studied on a very small scale. One of the few meitnerium compounds that are likely to be sufficiently volatile is meitnerium hexafluoride (), as its lighter homologue iridium hexafluoride () is volatile above 60 °C and therefore the analogous compound of meitnerium might also be sufficiently volatile; a volatile octafluoride () might also be possible. For chemical studies to be carried out on a transactinide, at least four atoms must be produced, the half-life of the isotope used must be at least 1 second, and the rate of production must be at least one atom per week. Even though the half-life of Mt, the most stable known meitnerium isotope, is 7.6 seconds, long enough to perform chemical studies, another obstacle is the need to increase the rate of production of meitnerium isotopes and allow experiments to carry on for weeks or months so that statistically significant results can be obtained. Separation and detection must be carried out continuously to separate out the meitnerium isotopes and have automated systems experiment on the gas-phase and solution chemistry of meitnerium, as the yields for heavier elements are predicted to be smaller than those for lighter elements; some of the separation techniques used for bohrium and hassium could be reused. However, the experimental chemistry of meitnerium has not received as much attention as that of the heavier elements from copernicium to livermorium.\n\nThe Lawrence Berkeley National Laboratory attempted to synthesize the isotope Mt in 2002–2003 for a possible chemical investigation of meitnerium because it was expected that it might be more stable than the isotopes around it as it has 162 neutrons, a magic number for deformed nuclei; its half-life was predicted to be a few seconds, long enough for a chemical investigation. However, no atoms of Mt were detected, and this isotope of meitnerium is currently unknown.\n\nAn experiment determining the chemical properties of a transactinide would need to compare a compound of that transactinide with analogous compounds of some of its lighter homologues: for example, in the chemical characterization of hassium, hassium tetroxide (HsO) was compared with the analogous osmium compound, osmium tetroxide (OsO). In a preliminary step towards determining the chemical properties of meitnerium, the GSI attempted sublimation of the rhodium compounds rhodium(III) oxide (RhO) and rhodium(III) chloride (RhCl). However, macroscopic amounts of the oxide would not sublimate until 1000 °C and the chloride would not until 780 °C, and then only in the presence of carbon aerosol particles: these temperatures are far too high for such procedures to be used on meitnerium, as most of the current methods used for the investigation of the chemistry of superheavy elements do not work above 500 °C.\n\nFollowing the 2014 successful synthesis of seaborgium hexacarbonyl, Sg(CO), studies were conducted with the stable transition metals of groups 7 through 9, suggesting that carbonyl formation could be extended to further probe the chemistries of the early 6d transition metals from rutherfordium to meitnerium inclusive. Nevertheless, the challenges of low half-lives and difficult production reactions make meitnerium difficult to access for radiochemists, though the isotopes Mt and Mt are long-lived enough for chemical research and may be produced in the decay chains of Ts and Mc respectively. Mt is likely more suitable, since producing tennessine requires a rare and rather short-lived berkelium target.\n\n"}
{"id": "19918", "url": "https://en.wikipedia.org/wiki?curid=19918", "title": "Megabyte", "text": "Megabyte\n\nThe megabyte is a multiple of the unit byte for digital information. Its recommended unit symbol is MB. The unit prefix \"mega\" is a multiplier of (10) in the International System of Units (SI). Therefore, one megabyte is one million bytes of information. This definition has been incorporated into the International System of Quantities.\n\nHowever, in the computer and information technology fields, several other definitions are used that arose for historical reasons of convenience. A common usage has been to designate one megabyte as (2 B), a measurement that conveniently expresses the binary multiples inherent in digital computer memory architectures. However, most standards bodies have deprecated this usage in favor of a set of binary prefixes, in which this quantity is designated by the unit mebibyte (MiB). Less common is a convention that used the megabyte to mean 1000×1024 () bytes.\n\nThe megabyte is commonly used to measure either 1000 bytes or 1024 bytes. The interpretation of using base 1024 originated as a compromise technical jargon for the byte multiples that needed to be expressed by the powers of 2 but lacked a convenient name. As 1024 (2) approximates 1000 (10), roughly corresponding to the SI prefix kilo-, it was a convenient term to denote the binary multiple. In 1998 the International Electrotechnical Commission (IEC) proposed standards for binary prefixes requiring the use of megabyte to strictly denote 1000 bytes and mebibyte to denote 1024 bytes. By the end of 2009, the IEC Standard had been adopted by the IEEE, EU, ISO and NIST. Nevertheless, the term megabyte continues to be widely used with different meanings:\n\nIn this convention, one thousand megabytes (1000 MB) is equal to one gigabyte (1 GB), where 1 GB is one billion bytes.\n\n\nIn this convention, one thousand and twenty-four megabytes (1024 MB) is equal to one gigabyte (1 GB), where 1 GB is 1024 bytes.\n\n\nSemiconductor memory doubles in size for each address lane added to an integrated circuit package, which favors counts that are powers of two. The capacity of a disk drive is the product of the sector size, number of sectors per track, number of tracks per side, and the number of disk platters in the drive. Changes in any of these factors would not usually double the size. Sector sizes were set as powers of two (most common 512 bytes or 4096 bytes) for convenience in processing. It was a natural extension to give the capacity of a disk drive in multiples of the sector size, giving a mix of decimal and binary multiples when expressing total disk capacity.\n\nDepending on compression methods and file format, a megabyte of data can roughly be:\n\nThe human genome consists of DNA representing 800 MB of data. The parts that differentiate one person from another can be compressed to 4 MB.\n\n\n"}
{"id": "19919", "url": "https://en.wikipedia.org/wiki?curid=19919", "title": "Monosaccharide", "text": "Monosaccharide\n\nMonosaccharides (from Greek \"monos\": single, \"sacchar\": sugar), also called simple sugars, are the simplest form of sugar and the most basic units of carbohydrates. They cannot be further hydrolyzed to simpler chemical compounds. The general formula is . They are usually colorless, water-soluble, and crystalline solids. Some monosaccharides have a sweet taste. \n\nExamples of monosaccharides include glucose (dextrose), fructose (levulose), and galactose. Monosaccharides are the building blocks of disaccharides (such as sucrose and lactose) and polysaccharides (such as cellulose and starch). Each carbon atom that supports a hydroxyl group (so, all of the carbons except for the primary and terminal carbon) is chiral, giving rise to a number of isomeric forms, all with the same chemical formula. For instance, galactose and glucose are both aldohexoses, but have different physical structures and chemical properties.\n\nWith few exceptions (e.g., deoxyribose), monosaccharides have this chemical formula: (CHO), where conventionally \"x\" ≥ 3. Monosaccharides can be classified by the number \"x\" of carbon atoms they contain: triose (3), tetrose (4), pentose (5), hexose (6), heptose (7), and so on.\n\nThe most important monosaccharide, glucose, is a hexose. Examples of heptoses include the ketoses, mannoheptulose and sedoheptulose. Monosaccharides with eight or more carbons are rarely observed as they are quite unstable. In aqueous solutions monosaccharides exist as rings if they have more than four carbons.\n\nSimple monosaccharides have a linear and unbranched carbon skeleton with one carbonyl (C=O) functional group, and one hydroxyl (OH) group on each of the remaining carbon atoms. Therefore, the molecular structure of a simple monosaccharide can be written as H(CHOH)(C=O)(CHOH)H, where \"n\" + 1 + \"m\" = \"x\"; so that its elemental formula is CHO.\n\nBy convention, the carbon atoms are numbered from 1 to \"x\" along the backbone, starting from the end that is closest to the C=O group. Monosaccharides are the simplest units of carbohydrates and the simplest form of sugar.\n\nIf the carbonyl is at position 1 (that is, \"n\" or \"m\" is zero), the molecule begins with a formyl group H(C=O)− and is technically an aldehyde. In that case, the compound is termed an aldose. Otherwise, the molecule has a keto group, a carbonyl −(C=O)− between two carbons; then it is formally a ketone, and is termed a ketose. Ketoses of biological interest usually have the carbonyl at position 2.\n\nThe various classifications above can be combined, resulting in names such as \"aldohexose\" and \"ketotriose\".\n\nA more general nomenclature for open-chain monosaccharides combines a Greek prefix to indicate the number of carbons (tri-, tetr-, pent-, hex-, etc.) with the suffixes \"-ose\" for aldoses and \"-ulose\" for ketoses. In the latter case, if the carbonyl is not at position 2, its position is then indicated by a numeric infix. So, for example, H(C=O)(CHOH)H is pentose, H(CHOH)(C=O)(CHOH)H is pentulose, and H(CHOH)(C=O)(CHOH)H is pent-3-ulose.\n\nTwo monosaccharides with equivalent molecular graphs (same chain length and same carbonyl position) may still be distinct stereoisomers, whose molecules differ in the three-dimensional arrangement of the bonds of certain atoms. This happens only if the molecule contains a stereogenic center, specifically a carbon atom that is chiral (connected to four distinct molecular sub-structures). Those four bonds can have any of two configurations in space distinguished by their handedness. In a simple open-chain monosaccharide, every carbon is chiral except the first and the last atoms of the chain, and (in ketoses) the carbon with the keto group.\n\nFor example, the triketose H(CHOH)(C=O)(CHOH)H (glycerone, dihydroxyacetone) has no stereogenic center, and therefore exists as a single stereoisomer. The other triose, the aldose H(C=O)(CHOH)H (glyceraldehyde), has one chiral carbon — the central one, number 2 — which is bonded to groups −H, −OH, −C(OH)H, and −(C=O)H. Therefore, it exists as two stereoisomers whose molecules are mirror images of each other (like a left and a right glove). Monosaccharides with four or more carbons may contain multiple chiral carbons, so they typically have more than two stereoisomers. The number of distinct stereoisomers with the same diagram is bounded by 2, where \"c\" is the total number of chiral carbons.\n\nThe Fischer projection is a systematic way of drawing the skeletal formula of an acyclic monosaccharide so that the handedness of each chiral carbon is well specified. Each stereoisomer of a simple open-chain monosaccharide can be identified by the positions (right or left) in the Fischer diagram of the chiral hydroxyls (the hydroxyls attached to the chiral carbons).\n\nMost stereoisomers are themselves chiral (distinct from their mirror images). In the Fischer projection, two mirror-image isomers differ by having the positions of all chiral hydroxyls reversed right-to-left. Mirror-image isomers are chemically identical in non-chiral environments, but usually have very different biochemical properties and occurrences in nature.\n\nWhile most stereoisomers can be arranged in pairs of mirror-image forms, there are some non-chiral stereoisomers that are identical to their mirror images, in spite of having chiral centers. This happens whenever the molecular graph is symmetrical, as in the 3-ketopentoses H(CHOH)(CO)(CHOH)H, and the two halves are mirror images of each other. In that case, mirroring is equivalent to a half-turn rotation. For this reason, there are only three distinct 3-ketopentose stereoisomers, even though the molecule has two chiral carbons.\n\nDistinct stereoisomers that are not mirror-images of each other usually have different chemical properties, even in non-chiral environments. Therefore, each mirror pair and each non-chiral stereoisomer may be given a specific monosaccharide name. For example, there are 16 distinct aldohexose stereoisomers, but the name \"glucose\" means a specific pair of mirror-image aldohexoses. In the Fischer projection, one of the two glucose isomers has the hydroxyl at left on C3, and at right on C4 and C5; while the other isomer has the reversed pattern. These specific monosaccharide names have conventional three-letter abbreviations, like \"Glu\" for glucose and \"Thr\" for threose.\n\nGenerally, a monosaccharide with \"n\" asymmetrical carbons has 2 stereoisomers. The number of open chain stereoisomers for an aldose monosaccharide is larger by one than that of a ketose monosaccharide of the same length. Every ketose will have 2 stereoisomers where \"n\" > 2 is the number of carbons. Every aldose will have 2 stereoisomers where \"n\" > 2 is the number of carbons.\nThese are also referred to as epimers which have the different arrangement of −OH and −H groups at the asymmetric or chiral carbon atoms (this does not apply to those carbons having the carbonyl functional group).\n\nLike many chiral molecules, the two stereoisomers of glyceraldehyde will gradually rotate the polarization direction of linearly polarized light as it passes through it, even in solution. The two stereoisomers are identified with the prefixes - and -, according to the sense of rotation: -glyceraldehyde is dextrorotatory (rotates the polarization axis clockwise), while -glyceraldehyde is levorotatory (rotates it counterclockwise).\nThe - and - prefixes are also used with other monosaccharides, to distinguish two particular stereoisomers that are mirror-images of each other. For this purpose, one considers the chiral carbon that is furthest removed from the C=O group. Its four bonds must connect to −H, −OH, −C(OH)H, and the rest of the molecule. If the molecule can be rotated in space so that the directions of those four groups match those of the analog groups in -glyceraldehyde's C2, then the isomer receives the - prefix. Otherwise, it receives the - prefix.\n\nIn the Fischer projection, the - and - prefixes specifies the configuration at the carbon atom that is second from bottom: - if the hydroxyl is on the right side, and - if it is on the left side.\n\nNote that the - and - prefixes do not indicate the direction of rotation of polarized light, which is a combined effect of the arrangement at all chiral centers. However, the two enantiomers will always rotate the light in opposite directions, by the same amount. See also .\n\nA monosaccharide often switches from the acyclic (open-chain) form to a cyclic form, through a nucleophilic addition reaction between the carbonyl group and one of the hydroxyls of the same molecule. The reaction creates a ring of carbon atoms closed by one bridging oxygen atom. The resulting molecule has an hemiacetal or hemiketal group, depending on whether the linear form was an aldose or a ketose. The reaction is easily reversed, yielding the original open-chain form.\n\nIn these cyclic forms, the ring usually has 5 or 6 atoms. These forms are called furanoses and pyranoses, respectively — by analogy with furan and pyran, the simplest compounds with the same carbon-oxygen ring (although they lack the double bonds of these two molecules). For example, the aldohexose glucose may form a hemiacetal linkage between the hydroxyl on carbon 1 and the oxygen on carbon 4, yielding a molecule with a 5-membered ring, called glucofuranose. The same reaction can take place between carbons 1 and 5 to form a molecule with a 6-membered ring, called glucopyranose. Cyclic forms with a 7-atom ring (the same of oxepane), rarely encountered, are called heptoses.\n\nFor many monosaccharides (including glucose), the cyclic forms predominate, in the solid state and in solutions, and therefore the same name commonly is used for the open- and closed-chain isomers. Thus, for example, the term \"glucose\" may signify glucofuranose, glucopyranose, the open-chain form, or a mixture of the three.\n\nCyclization creates a new stereogenic center at the carbonyl-bearing carbon. The −OH group that replaces the carbonyl's oxygen may end up in two distinct positions relative to the ring's midplane. Thus each open-chain monosaccharide yields two cyclic isomers (anomers), denoted by the prefixes α- and β-. The molecule can change between these two forms by a process called mutarotation, that consists in a reversal of the ring-forming reaction followed by another ring formation.\n\nThe stereochemical structure of a cyclic monosaccharide can be represented in a Haworth projection. In this diagram, the α-isomer for the pyranose form of a D-aldohexose has the -OH of the anomeric carbon below the plane of the carbon atoms, while the β-isomer has the -OH of the anomeric carbon above the plane. Pyranoses typically adopt a chair conformation, similar to that of cyclohexane. In this conformation, the α-isomer has the -OH of the anomeric carbon in an axial position, whereas the β-isomer has the OH- of the anomeric carbon in equatorial position (considering D-aldohexose sugars).\n\nA large number of biologically important modified monosaccharides exist:\n\n\n\n\n"}
{"id": "19924", "url": "https://en.wikipedia.org/wiki?curid=19924", "title": "Microscopium", "text": "Microscopium\n\nMicroscopium (\"the Microscope\") is a minor constellation in the southern celestial hemisphere, one of twelve created in the 18th century by French astronomer Nicolas-Louis de Lacaille and one of several depicting scientific instruments. The name is a Latinised form of the Greek word for microscope. Its stars are faint and hardly visible from most of the non-tropical Northern Hemisphere.\n\nThe constellation's brightest star is Gamma Microscopii of apparent magnitude 4.68, a yellow giant 2.5 times the Sun's mass located 223 ± 8 light-years distant. It passed within 1.14 and 3.45 light-years of the Sun some 3.9 million years ago, possibly disturbing the outer Solar System. Two star systems—WASP-7 and HD 205739—have been determined to have planets, while two others—the young red dwarf star AU Microscopii and the sunlike HD 202628—have debris disks. AU Microscopii and the binary red dwarf system AT Microscopii are probably a wide triple system and members of the Beta Pictoris moving group. Nicknamed \"Speedy Mic\", BO Microscopii is a star with an extremely fast rotation period of 9 hours, 7 minutes.\n\nMicroscopium is a small constellation bordered by Capricornus to the north, Piscis Austrinus and Grus to the east, Sagittarius to the west, and Indus to the south, touching on Telescopium to the southwest. The recommended three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'Mic'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of four segments (\"illustrated in infobox\"). In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between −27.45° and −45.09°. The whole constellation is visible to observers south of latitude 45°N. Given that its brightest stars are of fifth magnitude, the constellation is invisible to the naked eye in areas with polluted skies.\n\nFrench astronomer Nicolas-Louis de Lacaille charted and designated ten stars with the Bayer designations Alpha through to Iota in 1756. A star in neighbouring Indus that Lacaille had labelled Nu Indi turned out to be in Microscopium, so Gould renamed it Nu Microscopii. Francis Baily considered Gamma and Epsilon Microscopii to belong to the neighbouring constellation Piscis Austrinus, but subsequent cartographers did not follow this. In his 1725 \"Catalogus Britannicus\", John Flamsteed labelled the stars 1, 2, 3 and 4 Piscis Austrini, which became Gamma Microscopii, HR 8076, HR 8110 and Epsilon Microscopii respectively. Within the constellation's borders, there are 43 stars brighter than or equal to apparent magnitude 6.5.\n\nDepicting the eyepiece of the microscope is Gamma Microscopii, which—at magnitude of 4.68—is the brightest star in the constellation. Having spent much of its 620-million-year lifespan as a blue-white main sequence star, it has swollen and cooled to become a yellow giant of spectral type G6III, with a diameter ten times that of the Sun. Measurement of its parallax yields a distance of 223 ± 8 light years from Earth. At around 2.5 times the mass of the Sun, it likely passed within 1.14 and 3.45 light-years of the Sun some 3.9 million years ago, possibly massive enough and close enough to disturb the Oort cloud. Alpha Microscopii is also an ageing yellow giant star of spectral type G7III with an apparent magnitude of 4.90. Located 400 ± 30 light-years away from Earth, it has swollen to 17.5 times the diameter of the Sun. Alpha has a 10th magnitude companion, visible in 7.5 cm telescopes, though this is a coincidental closeness rather than a true binary system. Epsilon Microscopii lies 166 ± 5 light-years away, and is a white star of apparent magnitude 4.7, and spectral type A1V. Theta and Theta Microscopii make up a wide double whose components are splittable to the naked eye. Both are white A-class magnetic spectrum variable stars with strong metallic lines, similar to Cor Caroli. They mark the constellation's specimen slide.\n\nMany notable objects are too faint to be seen with the naked eye. AX Microscopii, better known as Lacaille 8760, is a red dwarf which lies only 12.9 light-years from the Solar System. At magnitude 6.68, it is the brightest red dwarf in the sky. BO Microscopii is a rapidly rotating star that has 80% the diameter of the Sun. Nicknamed \"Speedy Mic\", it has a rotation period of 9 hours 7 minutes. An active star, it has prominent stellar flares that average 100 times stronger than those of the Sun, and are emitting energy mainly in the X-ray and ultraviolet bands of the spectrum. It lies 218 ± 4 light-years away from the Sun. AT Microscopii is a binary star system, both members of which are flare star red dwarfs. The system lies close to and may form a very wide triple system with AU Microscopii, a young star which appears to be a planetary system in the making with a debris disk. The three stars are candidate members of the Beta Pictoris moving group, one of the nearest associations of stars that share a common motion through space.\n\nThe Astronomical Society of Southern Africa in 2003 reported that observations of four of the Mira variables in Microscopium were very urgently needed as data on their light curves was incomplete. Two of them—R and S Microscopii—are challenging stars for novice amateur astronomers, and the other two, U and RY Microscopii, are more difficult still. Another red giant, T Microscopii, is a semiregular variable that ranges between magnitudes 7.7 and 9.6 over 344 days. Of apparent magnitude 11, DD Microscopii is a symbiotic star system composed of an orange giant of spectral type K2III and white dwarf in close orbit, with the smaller star ionizing the stellar wind of the larger star. The system has a low metallicity. Combined with its high galactic latitude, this indicates that the star system has its origin in the galactic halo of the Milky Way.\n\nHD 205739 is a yellow-white main sequence star of spectral type F7V that is around 1.22 times as massive and 2.3 times as luminous as the Sun. It has a Jupiter-sized planet with an orbital period of 280 days that was discovered by the radial velocity method. WASP-7 is a star of spectral type F5V with an apparent magnitude of 9.54, about 1.28 times as massive as the Sun. Its hot Jupiter planet—WASP-7b—was discovered by transit method and found to orbit the star every 4.95 days. HD 202628 is a sunlike star of spectral type G2V with a debris disk that ranges from 158 to 220 AU distant. Its inner edge is sharply defined, indicating a probable planet orbiting between 86 and 158 AU from the star.\n\nDescribing Microscopium as \"totally unremarkable\", astronomer Patrick Moore concluded there was nothing of interest for amateur observers. NGC 6925 is a barred spiral galaxy of apparent magnitude 11.3 which is lens-shaped, as it lies almost edge-on to observers on Earth, 3.7 degrees west-northwest of Alpha Microscopii. SN 2011ei, a Type II Supernova in NGC 6925, was discovered by Stu Parker in New Zealand in July 2011. NGC 6923 lies nearby and is a magnitude fainter still. The Microscopium Void is a roughly rectangular region of relatively empty space, bounded by incomplete sheets of galaxies from other voids. The Microscopium Supercluster is an overdensity of galaxy clusters that was first noticed in the early 1990s. The component Abell clusters 3695 and 3696 are likely to be gravitationally bound, while the relations of Abell clusters 3693 and 3705 in the same field are unclear.\n\nThe Microscopids are a minor meteor shower that appear from June to mid-July.\n\nThe stars that comprise Microscopium are in a region previously considered the hind feet of Sagittarius, a neighbouring constellation. John Ellard Gore wrote that al-Sufi seems to have reported that Ptolemy had seen the stars but he (Al Sufi) did not pinpoint their positions. Microscopium itself was introduced in 1751–52 by Lacaille with the French name \"le Microscope\", after he had observed and catalogued 10,000 southern stars during a two-year stay at the Cape of Good Hope. He devised fourteen new constellations in uncharted regions of the Southern Celestial Hemisphere not visible from Europe. All but one honoured instruments that symbolised the Age of Enlightenment. Commemorating the compound microscope, the Microscope's name had been Latinised by Lacaille to \"Microscopium\" by 1763.\n"}
{"id": "19925", "url": "https://en.wikipedia.org/wiki?curid=19925", "title": "IC 342/Maffei Group", "text": "IC 342/Maffei Group\n\nThe IC 342/Maffei Group (also known as the IC 342 Group or the Maffei 1 Group) is the nearest group of galaxies to the Local Group. The group can be described as a binary group; the member galaxies are mostly concentrated around either IC 342 or Maffei 1, both of which are the brightest galaxies within the group. The group is part of the Virgo Supercluster.\n\nThe table below lists galaxies that have been identified as associated with the IC342/Maffei 1 Group by I. D. Karachentsev. Note that Karachentsev divides this group into two subgroups centered around IC 342 and Maffei 1.\n\nAdditionally, KKH 37 is listed as possibly being a member of the IC 342 Subgroup, and KKH 6 is listed as possibly being a member of the Maffei 1 Subgroup.\n\nAs seen from Earth, the group lies near the plane of the Milky Way (a region sometimes called the Zone of Avoidance). Consequently, the light from many of the galaxies is severely affected by dust obscuration within the Milky Way. This complicates observational studies of the group, as uncertainties in the dust obscuration also affect measurements of the galaxies' luminosities and distances as well as other related quantities.\n\nMoreover, the galaxies within the group have historically been difficult to identify. Many galaxies have only been discovered using late 20th century astronomical instrumentation. For example, while many fainter, more distant galaxies, such as the galaxies in the New General Catalogue, were already identified visually by the end of the nineteenth century, Maffei 1 and Maffei 2 were only discovered in 1968 using infrared photographic images of the region. Furthermore, it is difficult to determine whether some objects near IC 342 or Maffei 1 are galaxies associated with the IC 342/Maffei Group or diffuse foreground objects within the Milky Way that merely look like galaxies. For example, the objects MB 2 and Camelopardalis C were once thought to be dwarf galaxies in the IC 342/Maffei Group but are now known to be objects within the Milky Way.\n\nSince the IC 342/Maffei Group and the Local Group are located physically close to each other, the two groups may have influenced each other's evolution during the early stages of galaxy formation. An analysis of the velocities and distances to the IC 342/Maffei Group as measured by M. J. Valtonen and collaborators suggested that IC 342 and Maffei 1 were moving faster than what could be accounted for in the expansion of the universe. They therefore suggested that IC 342 and Maffei 1 were ejected from the Local Group after a violent gravitational interaction with the Andromeda Galaxy during the early stages of the formation of the two groups.\n\nHowever, this interpretation is dependent on the distances measured to the galaxies in the group, which in turn is dependent on accurately measuring the degree to which interstellar dust in the Milky Way obscures the group. More recent observations have demonstrated that the dust obscuration may have been previously overestimated, so the distances may have been underestimated. If these new distance measurements are correct, then the galaxies in the IC 342/Maffei Group appear to be moving at the rate expected from the expansion of the universe, and the scenario of a collision between the IC 342/Maffei Group and the Local Group would be implausible.\n"}
{"id": "19926", "url": "https://en.wikipedia.org/wiki?curid=19926", "title": "M81 Group", "text": "M81 Group\n\nThe M81 Group is a galaxy group in the constellations Ursa Major and Camelopardalis that includes the galaxies Messier 81 and Messier 82, as well as several other galaxies with high apparent brightnesses. The approximate center of the group is located at a distance of 3.6 Mpc, making it one of the nearest groups to the Local Group. The group is estimated to have a total mass of (1.03 ± 0.17).\nThe M81 Group, the Local Group, and other nearby groups all lie within the Virgo Supercluster (i.e. the Local Supercluster).\n\nThe table below lists galaxies that have been identified as associated with the M81 Group by I. D. Karachentsev.\n\nNote that the object names used in the above table differ from the names used by Karachentsev. NGC, IC, UGC, and PGC numbers have been used in many cases to allow for easier referencing.\n\nMessier 81, Messier 82, and NGC 3077 are all strongly interacting with each other. The gravitational interactions have stripped some hydrogen gas away from all three galaxies, leading to the formation of filamentary gas structures within the group. Moreover, the interactions have also caused some interstellar gas to fall into the centers of Messier 82 and NGC 3077, which has led to strong starburst activity (or the formation of many stars) within the centers of these two galaxies.\n\n"}
{"id": "19929", "url": "https://en.wikipedia.org/wiki?curid=19929", "title": "Mensa", "text": "Mensa\n\nMensa may refer to:\n"}
{"id": "19930", "url": "https://en.wikipedia.org/wiki?curid=19930", "title": "Metre (poetry)", "text": "Metre (poetry)\n\nIn poetry, metre (British) or meter (American; see spelling differences) is the basic rhythmic structure of a verse or lines in verse. Many traditional verse forms prescribe a specific verse metre, or a certain set of metres alternating in a particular order. The study and the actual use of metres and forms of versification are both known as prosody. (Within linguistics, \"prosody\" is used in a more general sense that includes not only poetic metre but also the rhythmic aspects of prose, whether formal or informal, that vary from language to language, and sometimes between poetic traditions.)\n\nAn assortment of features can be identified when classifying poetry and its metre.\n\nThe metre of most poetry of the Western world and elsewhere is based on patterns of syllables of particular types. The familiar type of metre in English-language poetry is called qualitative metre, with stressed syllables coming at regular intervals (e.g. in iambic pentameters, usually every even-numbered syllable). Many Romance languages use a scheme that is somewhat similar but where the position of only one particular stressed syllable (e.g. the last) needs to be fixed. The metre of the old Germanic poetry of languages such as Old Norse and Old English was radically different, but was still based on stress patterns.\n\nSome classical languages, in contrast, used a different scheme known as quantitative metre, where patterns were based on syllable weight rather than stress. In the dactylic hexameters of Classical Latin and Classical Greek, for example, each of the six feet making up the line was either a dactyl (long-short-short) or a spondee (long-long): a \"long syllable\" was literally one that took longer to pronounce than a short syllable: specifically, a syllable consisting of a long vowel or diphthong or followed by two consonants. The stress pattern of the words made no difference to the metre. A number of other ancient languages also used quantitative metre, such as Sanskrit and Classical Arabic (but not Biblical Hebrew).\n\nFinally, non-stressed languages that have little or no differentiation of syllable length, such as French or Chinese, base their verses on the number of syllables only. The most common form in French is the Alexandrine, with twelve syllables a verse, and in classical Chinese five characters, and thus five syllables. But since each Chinese character is pronounced using one syllable in a certain tone, classical Chinese poetry also had more strictly defined rules, such as parallelism or antithesis between lines.\n\nIn many Western classical poetic traditions, the metre of a verse can be described as a sequence of \"feet\", each foot being a specific sequence of syllable types — such as relatively unstressed/stressed (the norm for English poetry) or long/short (as in most classical Latin and Greek poetry).\n\nIambic pentameter, a common metre in English poetry, is based on a sequence of five \"iambic feet\" or \"iambs\", each consisting of a relatively unstressed syllable (here represented with \"×\" above the syllable) followed by a relatively stressed one (here represented with \"/\" above the syllable) — \"da-DUM\" = \"× /\" :\n\nThis approach to analyzing and classifying metres originates from Ancient Greek tragedians and poets such as Homer, Pindar, Hesiod, and Sappho.\n\nHowever some metres have an overall rhythmic pattern to the line that cannot easily be described using feet. This occurs in Sanskrit poetry; see Vedic metre and Sanskrit metre. (Although this poetry is in fact specified using feet, each \"foot\" is more or less equivalent to an entire line.) It also occurs in some Western metres, such as the hendecasyllable favoured by Catullus and Martial, which can be described as:\n\nx x — ∪ ∪ — ∪ — ∪ — —\n\nIf the line has only one foot, it is called a \"monometer\"; two feet, \"dimeter\"; three is \"trimeter\"; four is \"tetrameter\"; five is \"pentameter\"; six is \"hexameter\", seven is \"heptameter\" and eight is \"octameter\". For example, if the feet are iambs, and if there are five feet to a line, then it is called a iambic pentameter. If the feet are primarily \"dactyls\" and there are six to a line, then it is a dactylic hexameter.\n\nSometimes a natural pause occurs in the middle of a line rather than at a line-break. This is a caesura (cut). A good example is from \"The Winter's Tale\" by William Shakespeare; the caesurae are indicated by '/':\nIn Latin and Greek poetry, a caesura is a break within a foot caused by the end of a word.\n\nEach line of traditional Germanic alliterative verse is divided into two half-lines by a caesura. This can be seen in Piers Plowman:\n\nBy contrast with caesura, enjambment is incomplete syntax at the end of a line; the meaning runs over from one poetic line to the next, without terminal punctuation. Also from Shakespeare's \"The Winter's Tale\":\nPoems with a well-defined overall metric pattern often have a few lines that violate that pattern. A common variation is the \"inversion\" of a foot, which turns an iamb (\"da-DUM\") into a trochee (\"DUM-da\"). A second variation is a \"headless\" verse, which lacks the first syllable of the first foot. A third variation is catalexis, where the end of a line is shortened by a foot, or two or part thereof - an example of this is at the end of each verse in Keats' 'La Belle Dame sans Merci':\n\nVersification in Classical Sanskrit poetry is of three kinds.\n\n\nStandard traditional works on metre are Pingala's Chandaḥśāstra and Kedāra's Vṛttaratnākara. The most exhaustive compilations, such as the modern ones by Patwardhan and Velankar contain over 600 metres. This is a substantially larger repertoire than in any other metrical tradition.\n\nThe metrical \"feet\" in the classical languages were based on the length of time taken to pronounce each syllable, which were categorized according to their weight as either \"long\" syllables or \"short\" syllables (indicated as \"dum\" and \"di\" below). These are also called \"heavy\" and \"light\" syllables, respectively, to distinguish from long and short vowels. The foot is often compared to a musical measure and the long and short syllables to whole notes and half notes. In English poetry, feet are determined by emphasis rather than length, with stressed and unstressed syllables serving the same function as long and short syllables in classical metre.\n\nThe basic unit in Greek and Latin prosody is a mora, which is defined as a single short syllable. A long syllable is equivalent to two morae. A long syllable contains either a long vowel, a diphthong, or a short vowel followed by two or more consonants. Various rules of elision sometimes prevent a grammatical syllable from making a full syllable, and certain other lengthening and shortening rules (such as correption) can create long or short syllables in contexts where one would expect the opposite.\n\nThe most important Classical metre is the dactylic hexameter, the metre of Homer and Virgil. This form uses verses of six feet. The word \"dactyl\" comes from the Greek word \"daktylos\" meaning \"finger\", since there is one long part followed by two short stretches. The first four feet are dactyls (\"daa-duh-duh\"), but can be spondees (\"daa-daa\"). The fifth foot is almost always a dactyl. The sixth foot is either a spondee or a trochee (\"daa-duh\"). The initial syllable of either foot is called the \"ictus\", the basic \"beat\" of the verse. There is usually a caesura after the ictus of the third foot. The opening line of the \"Æneid\" is a typical line of dactylic hexameter:\n\nIn this example, the first and second feet are dactyls; their first syllables, \"Ar\" and \"rum\" respectively, contain short vowels, but count as long because the vowels are both followed by two consonants. The third and fourth feet are spondees, the first of which is divided by the main caesura of the verse. The fifth foot is a dactyl, as is nearly always the case. The final foot is a spondee.\n\nThe dactylic hexameter was imitated in English by Henry Wadsworth Longfellow in his poem \"Evangeline\":\nNotice how the first line:\n\nFollows this pattern:\n\nAlso important in Greek and Latin poetry is the dactylic pentameter. This was a line of verse, made up of two equal parts, each of which contains two dactyls followed by a long syllable, which counts as a half foot. In this way, the number of feet amounts to five in total. Spondees can take the place of the dactyls in the first half, but never in the second. The long syllable at the close of the first half of the verse always ends a word, giving rise to a caesura.\n\nDactylic pentameter is never used in isolation. Rather, a line of dactylic pentameter follows a line of dactylic hexameter in the elegiac distich or elegiac couplet, a form of verse that was used for the composition of elegies and other tragic and solemn verse in the Greek and Latin world, as well as love poetry that was sometimes light and cheerful. An example from Ovid's \"Tristia\":\n\nThe Greeks and Romans also used a number of lyric metres, which were typically used for shorter poems than elegiacs or hexameter. In Aeolic verse, one important line was called the hendecasyllabic, a line of eleven syllables. This metre was used most often in the Sapphic stanza, named after the Greek poet Sappho, who wrote many of her poems in the form. A hendecasyllabic is a line with a never-varying structure: two trochees, followed by a dactyl, then two more trochees. In the Sapphic stanza, three hendecasyllabics are followed by an \"Adonic\" line, made up of a dactyl and a trochee. This is the form of Catullus 51 (itself an homage to Sappho 31):\n\nThe Sapphic stanza was imitated in English by Algernon Charles Swinburne in a poem he simply called \"Sapphics\":\n\nThe metrical system of Classical Arabic poetry, like those of classical Greek and Latin, is based on the weight of syllables classified as either \"long\" or \"short\". The basic principles of Arabic poetic metre \"Arūḍ\" or Arud ( ') Science of Poetry ( '), were put forward by Al-Farahidi (786 - 718 CE) who did so after noticing that poems consisted of repeated syllables in each verse. In his first book, \"Al-Ard\" ( \"\"), he described 15 types of verse. Al-Akhfash described one extra, the 16th.\n\nA short syllable contains a short vowel with no following consonants. For example, the word \"kataba,\" which syllabifies as \"ka-ta-ba\", contains three short vowels and is made up of three short syllables. A long syllable contains either a long vowel or a short vowel followed by a consonant as is the case in the word \"maktūbun\" which syllabifies as \"mak-tū-bun\". These are the only syllable types possible in Classical Arabic phonology which, by and large, does not allow a syllable to end in more than one consonant or a consonant to occur in the same syllable after a long vowel. In other words, syllables of the type \"-āk-\" or \"-akr-\" are not found in classical Arabic.\n\nEach verse consists of a certain number of metrical feet (\"tafāʿīl\" or \"ʾaǧzāʾ\") and a certain combination of possible feet constitutes a metre (\"baḥr\").\n\nThe traditional Arabic practice for writing out a poem's metre is to use a concatenation of various derivations of the verbal root \"F-ʿ-L\" (فعل). Thus, the following hemistich\n\nقفا نبك من ذكرى حبيبٍ ومنزلِ\n\nWould be traditionally scanned as:\n\nفعولن مفاعيلن فعولن مفاعلن\n\nThat is, Romanized and with traditional Western scansion:\n\nClassical Arabic has sixteen established metres. Though each of them allows for a certain amount of variation, their basic patterns are as follows, using:\n\nThe terminology for metrical system used in classical and classical-style Persian poetry is the same as that of Classical Arabic, even though these are quite different in both origin and structure. This has led to serious confusion among prosodists, both ancient and modern, as to the true source and nature of the Persian metres, the most obvious error being the assumption that they were copied from Arabic.\n\nPersian poetry is quantitative, and the metrical patterns are made of long and short syllables, much as in Classical Greek, Latin and Arabic. \"Anceps\" positions in the line, however, that is places where either a long or short syllable can be used (marked \"x\" in the schemes below), are not found in Persian verse except in some metres at the beginning of a line.\n\nPersian poetry is written in couplets, with each half-line (hemistich) being 10-14 syllables long. Except in the ruba'i (quatrain), where either of two very similar metres may be used, the same metre is used for every line in the poem. Rhyme is always used, sometimes with double rhyme or internal rhymes in addition. In some poems, known as masnavi, the two halves of each couplet rhyme, with a scheme \"aa\", \"bb\", \"cc\" and so on. In lyric poetry, the same rhyme is used throughout the poem at the end of each couplet, but except in the opening couplet, the two halves of each couplet do not rhyme; hence the scheme is \"aa\", \"ba\", \"ca\", \"da\". A \"ruba'i\" (quatrain) also usually has the rhyme \"aa, ba\".\n\nA particular feature of classical Persian prosody, not found in Latin, Greek or Arabic, is that instead of two lengths of syllables (long and short), there are three lengths (short, long, and overlong). Overlong syllables can be used anywhere in the line in place of a long + a short, or in the final position in a line or half line. When a metre has a pair of short syllables (u u), it is common for a long syllable to be substituted, especially at the end of a line or half-line.\n\nAbout 30 different metres are commonly used in Persian. 70% of lyric poems are written in one of the following seven metres:\n\n\"Masnavi\" poems (that is, long poems in rhyming couplets) are always written in one of the shorter 11 or 10-syllable metres (traditionally seven in number) such as the following:\n\nThe two metres used for \"ruba'iyat\" (quatrains), which are only used for this, are the following, of which the second is a variant of the first:\n\nClassical Chinese poetic metric may be divided into fixed and variable length line types, although the actual scansion of the metre is complicated by various factors, including linguistic changes and variations encountered in dealing with a tradition extending over a geographically extensive regional area for a continuous time period of over some two-and-a-half millennia. Beginning with the earlier recorded forms: the Classic of Poetry tends toward couplets of four-character lines, grouped in rhymed quatrains; and, the Chuci follows this to some extent, but moves toward variations in line length. Han Dynasty poetry tended towards the variable line-length forms of the folk ballads and the Music Bureau yuefu. Jian'an poetry, Six Dynasties poetry, and Tang Dynasty poetry tend towards a poetic metre based on fixed-length lines of five, seven, (or, more rarely six) characters/verbal units tended to predominate, generally in couplet/quatrain-based forms, of various total verse lengths. The Song poetry is specially known for its use of the \"ci\", using variable line lengths which follow the specific pattern of a certain musical song's lyrics, thus \"ci\" are sometimes referred to as \"fixed-rhythm\" forms. Yuan poetry metres continued this practice with their \"qu\" forms, similarly fixed-rhythm forms based on now obscure or perhaps completely lost original examples (or, ur-types). Not that Classical Chinese poetry ever lost the use of the \"shi\" forms, with their metrical patterns found in the \"old style poetry\" (\"gushi\") and the regulated verse forms of (\"lüshi\" or \"jintishi\"). The regulated verse forms also prescribed patterns based upon linguistic tonality. The use of caesura is important in regard to the metrical analysis of Classical Chinese poetry forms.\n\nThe metric system of Old English poetry was different from that of modern English, and related more to the verse forms of most of the older Germanic languages such as Old Norse. It used alliterative verse, a metrical pattern involving varied numbers of syllables but a fixed number (usually four) of strong stresses in each line. The unstressed syllables were relatively unimportant, but the caesurae (breaks between the half-lines) played a major role in Old English poetry.\n\nIn place of using feet, alliterative verse divided each line into two half-lines. Each half-line had to follow one of five or so patterns, each of which defined a sequence of stressed and unstressed syllables, typically with two stressed syllables per half line. Unlike typical Western poetry, however, the number of unstressed syllables could vary somewhat. For example, the common pattern \"DUM-da-DUM-da\" could allow between one and five unstressed syllables between the two stresses.\n\nThe following is a famous example, taken from The Battle of Maldon, a poem written shortly after the date of that battle (AD 991):\n\n<poem style=\"margin-left: 2em\">\n\"Hige sceal þe heardra,\" || \"heorte þe cēnre,\"\n\"mōd sceal þe māre,\" || \"swā ūre mægen lȳtlað\"\n\n(\"Will must be the harder, courage the bolder,\nspirit must be the more, as our might lessens.\")\n</poem>\n\nIn the quoted section, the stressed syllables have been underlined. (Normally, the stressed syllable must be long if followed by another syllable in a word. However, by a rule known as \"syllable resolution\", two short syllables in a single word are considered equal to a single long syllable. Hence, sometimes two syllables have been underlined, as in \"hige\" and \"mægen\".) The German philologist Eduard Sievers (died 1932) identified five different patterns of half-line in Anglo-Saxon alliterative poetry. The first three half-lines have the type A pattern \"DUM-da-(da-)DUM-da\", while the last one has the type C pattern \"da-(da-da-)DUM-DUM-da\", with parentheses indicating optional unstressed syllables that have been inserted. Note also the pervasive pattern of alliteration, where the first and/or second stressed syllables alliterate with the third, but not with the fourth.\n\nMost English metre is classified according to the same system as Classical metre with an important difference. English is an accentual language, and therefore beats and offbeats (stressed and unstressed syllables) take the place of the long and short syllables of classical systems. In most English verse, the metre can be considered as a sort of back beat, against which natural speech rhythms vary expressively. The most common characteristic feet of English verse are the iamb in two syllables and the anapest in three. (See Foot (prosody) for a complete list of the metrical feet and their names.)\n\nThe number of metrical systems in English is not agreed upon. The four major types are: accentual verse, accentual-syllabic verse, syllabic verse and quantitative verse. The alliterative verse of Old English could also be added to this list, or included as a special type of accentual verse. Accentual verse focuses on the number of stresses in a line, while ignoring the number of offbeats and syllables; accentual-syllabic verse focuses on regulating both the number of stresses and the total number of syllables in a line; syllabic verse only counts the number of syllables in a line; quantitative verse regulates the patterns of long and short syllables (this sort of verse is often considered alien to English). It is to be noted, however, that the use of foreign metres in English is all but exceptional.\n\nThe most frequently encountered metre of English verse is the iambic pentameter, in which the metrical norm is five iambic feet per line, though metrical substitution is common and rhythmic variations practically inexhaustible. John Milton's \"Paradise Lost\", most sonnets, and much else besides in English are written in iambic pentameter. Lines of unrhymed iambic pentameter are commonly known as blank verse. Blank verse in the English language is most famously represented in the plays of William Shakespeare and the great works of Milton, though Tennyson (\"Ulysses\", \"The Princess\") and Wordsworth (\"The Prelude\") also make notable use of it.\n\nA rhymed pair of lines of iambic pentameter make a heroic couplet, a verse form which was used so often in the 18th century that it is now used mostly for humorous effect (although see Pale Fire for a non-trivial case). The most famous writers of heroic couplets are Dryden and Pope.\n\nAnother important metre in English is the ballad metre, also called the \"common metre\", which is a four-line stanza, with two pairs of a line of iambic tetrameter followed by a line of iambic trimeter; the rhymes usually fall on the lines of trimeter, although in many instances the tetrameter also rhymes. This is the metre of most of the Border and Scots or English ballads. In hymnody it is called the \"common metre\", as it is the most common of the named hymn metres used to pair many hymn lyrics with melodies, such as \"Amazing Grace\":\n\nEmily Dickinson is famous for her frequent use of ballad metre:\n\nIn French poetry, metre is determined solely by the number of syllables in a line. A silent 'e' counts as a syllable before a consonant, but is elided before a vowel (where \"h aspiré\" counts as a consonant). At the end of a line, the \"e\" remains unelided but is hypermetrical (outside the count of syllables, like a feminine ending in English verse), in that case, the rhyme is also called \"feminine\", whereas it is called \"masculine\" in the other cases.\n\nThe most frequently encountered metre in Classical French poetry is the alexandrine, composed of two hemistiches of six syllables each. Two famous alexandrines are\n\n(the daughter of Minos and Pasiphae), and\n\nClassical French poetry also had a complex set of rules for rhymes that goes beyond how words merely sound. These are usually taken into account when describing the metre of a poem.\n\nIn Spanish poetry the metre is determined by the number of syllables the verse has. Still it is the phonetic accent in the last word of the verse that decides the final count of the line. If the accent of the final word is at the last syllable, then the poetic rule states that one syllable shall be added to the actual count of syllables in the said line, thus having a higher number of poetic syllables than the number of grammatical syllables. If the accent lies on the second to last syllable of the last word in the verse, then the final count of poetic syllables will be the same as the grammatical number of syllables. Furthermore, if the accent lies on the third to last syllable, then one syllable is subtracted from the actual count, having then less poetic syllables than grammatical syllables.\n\nSpanish poetry uses poetic licenses, unique to Romance languages, to change the number of syllables by manipulating mainly the vowels in the line.\n\nRegarding these poetic licenses one must consider three kinds of phenomena: (1) syneresis, (2) dieresis and (3) hiatus\n\nThere are many types of licenses, used either to add or subtract syllables, that may be applied when needed after taking in consideration the poetic rules of the last word. Yet all have in common that they only manipulate vowels that are close to each other and not interrupted by consonants.\n\nSome common metres in Spanish verse are:\n\nIn Italian poetry, metre is determined solely by the position of the last accent in a line, the position of the other accents being however important for verse equilibrium. Syllables are enumerated with respect to a verse which ends with a paroxytone, so that a Septenary (having seven syllables) is defined as a verse whose last accent falls on the sixth syllable: it may so contain eight syllables (\"Ei fu. Siccome immobile\") or just six (\"la terra al nunzio sta\"). Moreover, when a word ends with a vowel and the next one starts with a vowel, they are considered to be in the same syllable (synalepha): so \"Gli anni e i giorni\" consists of only four syllables (\"Gli an\" \"ni e i\" \"gior\" \"ni\"). Even-syllabic verses have a fixed stress pattern. Because of the mostly trochaic nature of the Italian language, verses with an even number of syllables are far easier to compose, and the Novenary is usually regarded as the most difficult verse.\n\nSome common metres in Italian verse are:\n\nApart from Ottoman poetry, which was heavily influenced by Persian traditions and created a unique Ottoman style, traditional Turkish poetry features a system in which the number of syllables in each verse must be the same, most frequently 7, 8, 11, 14 syllables. These verses are then divided into syllable groups depending on the number of total syllables in a verse: 4+3 for 7 syllables, 4+4 or 5+3 for 8, 4+4+3 or 6+5 for 11 syllables. The end of each group in a verse is called a \"durak\" (stop), and must coincide with the last syllable of a word.\n\nThe following example is by Faruk Nafiz Çamlıbel (died 1973), one of the most devoted users of traditional Turkish metre:\n\nIn this poem the 6+5 metre is used, so that there is a word-break (\"durak\" = \"stop\" or caesura) after the sixth syllable of every line, as well as at the end of each line.\n\nIn the Ottoman Turkish language, the structures of the poetic foot (تفعل \"tef'ile\") and of poetic metre (وزن \"vezin\") were imitated from Persian poetry. About twelve of the commonest Persian metres were used for writing Turkish poetry. As was the case with Persian, no use at all was made of the commonest metres of Arabic poetry (the \"tawīl\", \"basīt\", \"kāmil\", and \"wāfir\"). However, the terminology used to described the metres was indirectly borrowed from the Arabic poetic tradition through the medium of the Persian language.\n\nAs a result, Ottoman poetry, also known as Dîvân poetry, was generally written in quantitative, mora-timed metre. The moras, or syllables, are divided into three basic types:\n\n\nIn writing out a poem's poetic metre, open syllables are symbolized by \".\" and closed syllables are symbolized by \"–\". From the different syllable types, a total of sixteen different types of poetic foot—the majority of which are either three or four syllables in length—are constructed, which are named and scanned as follows:\n\nThese individual poetic feet are then combined in a number of different ways, most often with four feet per line, so as to give the poetic metre for a line of verse. Some of the most commonly used metres are the following:\n\n\nPortuguese poetry uses a syllabic metre in which the verse is classified according to the last stressed syllable. The Portuguese system is quite similar to those of Spanish and Italian, as they are closely related languages. The most commonly used verses are:\n\nMetrical texts are first attested in early Indo-European languages. The earliest known unambiguously metrical texts, and at the same time the only metrical texts with a claim of dating to the Late Bronze Age, are the hymns of the Rigveda. That the texts of the Ancient Near East (Sumerian, Egyptian or Semitic) should not exhibit metre is surprising, and may be partly due to the nature of Bronze Age writing. There were, in fact, attempts to reconstruct metrical qualities of the poetic portions of the Hebrew Bible, e.g. by Gustav Bickell or Julius Ley, but they remained inconclusive (see Biblical poetry). Early Iron Age metrical poetry is found in the Iranian Avesta and in the Greek works attributed to Homer and Hesiod.\nLatin verse survives from the Old Latin period (c. 2nd century BC), in the Saturnian metre. Persian poetry arises in the Sassanid era. Tamil poetry of the early centuries AD may be the earliest known non-Indo-European\n\nMedieval poetry was metrical without exception, spanning traditions as diverse as European Minnesang, Trouvère or Bardic poetry, Classical Persian and Sanskrit poetry, Tang dynasty Chinese poetry or the Japanese Nara period \"Man'yōshū\". Renaissance and Early Modern poetry in Europe is characterized by a return to templates of Classical Antiquity, a tradition begun by Petrarca's generation and continued into the time of Shakespeare and Milton.\n\nNot all poets accept the idea that metre is a fundamental part of poetry. 20th-century American poets Marianne Moore, William Carlos Williams and Robinson Jeffers believed that metre was an artificial construct imposed upon poetry rather than being innate to poetry. In an essay titled \"Robinson Jeffers, & The Metric Fallacy\" Dan Schneider echoes Jeffers' sentiments: \"What if someone actually said to you that all music was composed of just 2 notes? Or if someone claimed that there were just 2 colors in creation? Now, ponder if such a thing were true. Imagine the clunkiness & mechanicality of such music. Think of the visual arts devoid of not just color, but sepia tones, & even shades of gray.\" Jeffers called his technique \"rolling stresses\".\n\nMoore went further than Jeffers, openly declaring her poetry was written in syllabic form, and wholly denying metre. These syllabic lines from her famous poem illustrate her contempt for metre and other poetic tools. Even the syllabic pattern of this poem does not remain perfectly consistent:\n\nWilliams tried to form poetry whose subject matter was centered on the lives of common people. He came up with the concept of the variable foot. Williams spurned traditional metre in most of his poems, preferring what he called \"colloquial idioms.\" Another poet that turned his back on traditional concepts of metre was Britain's Gerard Manley Hopkins. Hopkins' major innovation was what he called sprung rhythm. He claimed most poetry was written in this older rhythmic structure inherited from the Norman side of the English literary heritage, based on repeating groups of two or three syllables, with the stressed syllable falling in the same place on each repetition. Sprung rhythm is structured around feet with a variable number of syllables, generally between one and four syllables per foot, with the stress always falling on the first syllable in a foot.\n\n\n"}
{"id": "19932", "url": "https://en.wikipedia.org/wiki?curid=19932", "title": "Majed Moqed", "text": "Majed Moqed\n\nA former law student, Majed Mashaan Ghanem Moqed (, ; also transliterated as Moqued) (June 18, 1977 – September 11, 2001) was one of five hijackers of American Airlines Flight 77 as part of the September 11 attacks.\n\nA Saudi, Moqed was studying law at a university in Saudi Arabia before joining Al-Qaeda in 1999 and being chosen to participate in the 9/11 attacks. He arrived in the United States in May 2001 and helped with the planning of how the attacks would be carried out.\n\nOn September 11, 2001, Moqed boarded American Airlines Flight 77 and assisted in the hijacking of the plane so that it could be crashed into the Pentagon.\n\nMoqed was a law student from the small town of Al-Nakhil, Saudi Arabia (west of Medina), studying at King Fahd University's Faculty of Administration and Economics. Before he dropped out, he was apparently recruited into al-Qaeda in 1999 along with friend Satam al-Suqami, with whom he had earlier shared a college room. \n\nThe two trained at Khalden, a large training facility near Kabul that was run by Ibn al-Shaykh al-Libi. A friend in Saudi Arabia claimed he was last seen there in 2000, before leaving to study English in the United States. In November 2000, Moqed and Suqami flew into Iran from Bahrain together.\n\nSome time late in 2000, Moqed traveled to the United Arab Emirates, where he purchased traveler's cheques presumed to have been paid for by 9/11 financier Mustafa Ahmed al-Hawsawi. Five other hijackers also passed through the UAE and purchased travellers cheques, including Wail al-Shehri, Saeed al-Ghamdi, Hamza al-Ghamdi, Ahmed al-Haznawi and Ahmed al-Nami.\n\nKnown as \"al-Ahlaf\" during the preparations, Moqed then moved in with hijackers Salem al-Hazmi, Abdulaziz al-Omari and Khalid al-Mihdhar in an apartment in Paterson, New Jersey.\n\nAccording to the FBI, Moqed first arrived in the United States on May 2, 2001.\n\nIn March 2001, Moqed, Hani Hanjour, Hazmi and Ahmed al-Ghamdi rented a minivan and travelled to Fairfield, Connecticut. There they met a contact in the parking lot of a local convenience store who provided them with false IDs. (This was possibly Eyad Alrababah, a Jordanian charged with document fraud).\n\nMoqed was one of the five hijackers who asked for a state identity card on August 2, 2001. On August 24, both Mihdhar and Moqed tried to purchase flight tickets from the American Airlines online ticket-merchant, but had technical difficulties resolving their address and gave up.\n\nEmployees at Advance Travel Service in Totowa, New Jersey later claimed that Moqed and Hanjour had both purchased tickets there. They claimed that Hani Hanjour spoke very little English, and Moqed did most of the speaking. Hanjour requested a seat in the front row of the airplane. Their credit card failed to authorize, and after being told the agency did not accept personal cheques, the pair left to withdraw cash. They returned shortly afterwards and paid $1842.25 total in cash. \nDuring this time, Moqed was staying in Room 343 of the \"Valencia Motel\". On September 2, Moqed paid cash for a $30 weekly membership at Gold's Gym in Greenbelt, Maryland.\n\nThree days later he was seen on an ATM camera with Hani Hanjour. After the attacks, employees at an adult video store, \"Adult Lingerie Center\", in Beltsville claimed that Moqed had been in the store three times, although there were no transactions slips that confirmed this.\n\nOn September 11, 2001, Moqed arrived at Washington Dulles International Airport.\n\nAccording to the 9/11 Commission Report, Moqed set off the metal detector at the airport and was screened with a hand-wand. He passed the cursory inspection, and was able board his flight at 7:50. He was seated in 12A, adjacent to Mihdhar who was in 12B. Moqed helped to hijack the plane and assisted Hani Hanjour in crashing the plane into the Pentagon at 9:37 A.M., killing 189 people (64 on the plane and 125 on the ground).\n\nThe flight was scheduled to depart at 08:10, but ended up departing 10 minutes late from Gate D26 at Dulles. The last normal radio communications from the aircraft to air traffic control occurred at 08:50:51. At 08:54, Flight 77 began to deviate from its normal, assigned flight path and turned south, and then hijackers set the flight's autopilot heading for Washington, D.C. Passenger Barbara Olson called her husband, United States Solicitor General Theodore Olson, and reported that the plane had been hijacked and that the assailants had box cutters and knives. At 09:37, American Airlines Flight 77 crashed into the west facade of the Pentagon, killing all 64 aboard (including the hijackers), along with 125 on the ground in the Pentagon. In the recovery process at the Pentagon, remains of all five Flight 77 hijackers were identified through a process of elimination, as not matching any DNA samples for the victims, and put into custody of the FBI.\n\nAfter the attacks his family told Arab News that Moqed had been a fan of sports, and enjoyed travelling. Additionally, the U.S. announced it had found a \"Kingdom of Saudi Arabia Student Identity Card\" bearing Moqed's name in the rubble surrounding the Pentagon. They also stated that it appeared to have been a forgery.\n\n\n"}
{"id": "19933", "url": "https://en.wikipedia.org/wiki?curid=19933", "title": "Matthew Perry (disambiguation)", "text": "Matthew Perry (disambiguation)\n\nMatthew Perry (born 1969) is Canadian-American television and film actor.\n\nMatthew Perry or Matt Perry may also refer to:\n\n\n"}
{"id": "19935", "url": "https://en.wikipedia.org/wiki?curid=19935", "title": "Mimeograph", "text": "Mimeograph\n\nThe stencil duplicator or mimeograph machine (often abbreviated to mimeo) is a low-cost duplicating machine that works by forcing ink through a stencil onto paper. The mimeograph process should not be confused with the spirit duplicator process.\n\nMimeographs, along with spirit duplicators and hectographs, were a common technology in printing small quantities, as in office work, classroom materials, and church bulletins. Early fanzines were printed with this technology, because it was widespread and cheap. In the late 1960s, mimeographs, spirit duplicators, and hectographs began to be gradually displaced by photocopying.\n\nUse of stencils is an ancient art, butthrough chemistry, papers, and pressestechniques advanced rapidly in the late nineteenth century:\n\nA description of the Papyrograph method of duplication was published by David Owen: \n\nA major beneficiary of the invention of synthetic dyes was a document reproduction technique known as stencil duplicating. Its earliest form was invented in 1874 by Eugenio de Zuccato, a young Italian studying law in London, who called his device the Papyrograph. Zuccato’s system involved writing on a sheet of varnished paper with caustic ink, which ate through the varnish and paper fibers, leaving holes where the writing had been. This sheet – which had now become a stencil – was placed on a blank sheet of paper, and ink rolled over it so that the ink oozed through the holes, creating a duplicate on the second sheet.\n\nThe process was commercialized and Zuccato applied for a patent in 1895 having stencils prepared by typewriting.\n\nThomas Edison received US patent 180,857 for Autographic Printing on August 8, 1876. The patent covered the electric pen, used for making the stencil, and the flatbed duplicating press. In 1880 Edison obtained a further patent, US 224,665: \"Method of Preparing Autographic Stencils for Printing,\" which covered the making of stencils using a file plate, a grooved metal plate on which the stencil was placed which perforated the stencil when written on with a blunt metal stylus.\n\nThe word mimeograph was first used by Albert Blake Dick when he licensed Edison's patents in 1887.\n\nDick received Trademark Registration no. 0356815 for the term \"Mimeograph\" in the US Patent Office. It is currently listed as a dead entry, but shows the A.B. Dick Company of Chicago as the owner of the name.\n\nOver time, the term became generic and is now an example of a genericized trademark. (\"Roneograph,\" also \"Roneo machine,\" was another trademark used for mimeograph machines, the name being a contraction of Rotary Neostyle.)\n\nIn 1891, David Gestetner patented his Automatic Cyclostyle. This was one of the first rotary machines that retained the flatbed, which passed back and forth under inked rollers. This invention provided for more automated, faster reproductions since the pages were produced and moved by rollers instead of pressing one single sheet at a time.\n\nBy 1900, two primary types of mimeographs had come into use: a single-drum machine and a dual-drum machine. The single-drum machine used a single drum for ink transfer to the stencil, and the dual-drum machine used two drums and silk-screens to transfer the ink to the stencils. The single drum (example Roneo) machine could be easily used for multi-color work by changing the drum - each of which contained ink of a different color. This was spot color for mastheads. Colors could not be mixed.\n\nThe mimeograph became popular because it was much cheaper than traditional print - there was neither typesetting nor skilled labor involved. One individual with a typewriter and the necessary equipment became their own printing factory, allowing for greater circulation of printed material.\n\nThe image transfer medium was originally a stencil made from waxed mulberry paper. Later this became an immersion-coated long-fibre paper, with the coating being a plasticized nitrocellulose. This flexible waxed or coated sheet is backed by a sheet of stiff card stock, with the two sheets bound at the top.\n\nOnce prepared, the stencil is wrapped around the ink-filled drum of the rotary machine. When a blank sheet of paper is drawn between the rotating drum and a pressure roller, ink is forced through the holes on the stencil onto the paper. Early flatbed machines used a kind of squeegee. The ink originally had a lanolin base. and later became an oil in water emulsion. This emulsion commonly used Turkey-Red Oil (Sulfated Castor Oil) which gives it a distinctive and heavy scent.\n\nFor printed copy, a stencil assemblage is placed in a typewriter. The part of the mechanism which lifts the ribbon must be disabled so that the bare, sharp type element strikes the stencil directly. The impact of the type element displaces the coating, making the tissue paper permeable to the oil-based ink. This is called \"cutting a stencil.\"\n\nA variety of specialized styluses were used on the stencil to render lettering, illustrations, or other artistic features by hand against a textured plastic backing plate.\n\nMistakes can be corrected by brushing them out with a specially formulated correction fluid, and retyping once it has dried. (\"Obliterine\" was a popular brand of correction fluid in Australia and the United Kingdom.)\n\nStencils were also made with a thermal process; an infrared method similar to that used by early photocopiers. The common machine was a Thermofax.\n\nAnother device, called an electrostencil machine, sometimes was used to make mimeo stencils from a typed or printed original. It worked by scanning the original on a rotating drum with a moving optical head and burning through the blank stencil with an electric spark in the places where the optical head detected ink. It was slow and produced ozone. Text from electrostencils had lower resolution than that from typed stencils, although the process was good for reproducing illustrations. A skilled mimeo operator using an electrostencil and a very coarse halftone screen could make acceptable printed copies of a photograph.\n\nDuring the declining years of the mimeograph, some people made stencils with early computers and dot-matrix impact printers.\n\nUnlike spirit duplicators (where the only ink available is depleted from the master image), mimeograph technology works by forcing a replenishable supply of ink through the stencil master. In theory, the mimeography process could be continued indefinitely, especially if a durable stencil master were used (e.g. a thin metal foil). In practice, most low-cost mimeo stencils gradually wear out over the course of producing several hundred copies. Typically the stencil deteriorates gradually, producing a characteristic degraded image quality until the stencil tears, abruptly ending the print run. If further copies are desired at this point, another stencil must be made.\n\nOften, the stencil material covering the interiors of closed letterforms (e.g. \"a\", \"b\", \"d\", \"e\", \"g\", etc.) would fall away during continued printing, causing ink-filled letters in the copies. The stencil would gradually stretch, starting near the top where the mechanical forces were greatest, causing a characteristic \"mid-line sag\" in the textual lines of the copies, that would progress until the stencil failed completely. The Gestetner Company (and others) devised various methods to make mimeo stencils more durable.\n\nCompared to spirit duplication, mimeography produced a darker, more legible image. Spirit duplicated images were usually tinted a light purple or lavender, which gradually became lighter over the course of some dozens of copies. Mimeography was often considered \"the next step up\" in quality, capable of producing hundreds of copies. Print runs beyond that level were usually produced by professional printers or, as the technology became available, xerographic copiers.\n\nMimeographed images generally have much better durability than spirit-duplicated images, since the inks are more resistant to ultraviolet light. The primary preservation challenge is the low-quality paper often used, which would yellow and degrade due to residual acid in the treated pulp from which the paper was made. In the worst case, old copies can crumble into small particles when handled. Mimeographed copies have moderate durability when acid-free paper is used.\n\nGestetner, Risograph, and other companies still make and sell highly automated mimeograph-like machines that are externally similar to photocopiers. The modern version of a mimeograph, called a digital duplicator, or copyprinter, contains a scanner, a thermal head for stencil cutting, and a large roll of stencil material entirely inside the unit. The stencil material consists of a very thin polymer film laminated to a long-fibre non-woven tissue. It makes the stencils and mounts and unmounts them from the print drum automatically, making it almost as easy to operate as a photocopier. The Risograph is the best known of these machines.\n\nAlthough mimeographs remain more economical and energy-efficient in mid-range quantities, easier-to-use photocopying and offset printing have replaced mimeography almost entirely in developed countries. Mimeograph machines continue to be used in developing countries because it is a simple, cheap, and robust technology. Many mimeographs can be hand-cranked, requiring no electricity.\n\nMimeographs and the closely related but distinctly different spirit duplicator process were both used extensively in schools to copy homework assignments and tests. They were also commonly used for low-budget amateur publishing, including club newsletters and church bulletins. They were especially popular with science fiction fans, who used them extensively in the production of fanzines in the middle 20th century, before photocopying became inexpensive.\n\nLetters and typographical symbols were sometimes used to create illustrations, in a precursor to ASCII art. Because changing ink color in a mimeograph could be a laborious process, involving extensively cleaning the machine or, on newer models, replacing the drum or rollers, and then running the paper through the machine a second time, some fanzine publishers experimented with techniques for painting several colors on the pad, notably Shelby Vick, who created a kind of plaid \"Vicolor\".\n\n\n"}
{"id": "19937", "url": "https://en.wikipedia.org/wiki?curid=19937", "title": "Meteorite", "text": "Meteorite\n\nA meteorite is a solid piece of debris from an object, such as a comet, asteroid, or meteoroid, that originates in outer space and survives its passage through the atmosphere to reach the surface of a planet or moon. When the object enters the atmosphere, various factors such as friction, pressure, and chemical interactions with the atmospheric gases cause it to heat up and radiate that energy. It then becomes a meteor and forms a fireball, also known as a shooting star or falling star; astronomers call the brightest examples \"bolides\". Meteorites vary greatly in size. For geologists, a bolide is a meteorite large enough to create an impact crater.\n\nMeteorites that are recovered after being observed as they transit the atmosphere and impact the Earth are called meteorite falls. All others are known as meteorite finds. , there were about 1,412 witnessed falls that have specimens in the world's collections. , there are more than 59,200 well-documented meteorite finds.\n\nMeteorites have traditionally been divided into three broad categories: stony meteorites that are rocks, mainly composed of silicate minerals; iron meteorites that are largely composed of metallic iron-nickel; and stony-iron meteorites that contain large amounts of both metallic and rocky material. Modern classification schemes divide meteorites into groups according to their structure, chemical and isotopic composition and mineralogy. Meteorites smaller than 2 mm are classified as micrometeorites. Extraterrestrial meteorites are such objects that have impacted other celestial bodies, whether or not they have passed through an atmosphere. They have been found on the Moon and Mars.\n\nMeteorites are always named for the places they were found, usually a nearby town or geographic feature. In cases where many meteorites were found in one place, the name may be followed by a number or letter (e.g., Allan Hills 84001 or Dimmitt (b)). The name designated by the Meteoritical Society is used by scientists, catalogers, and most collectors.\n\nMost meteoroids disintegrate when entering the Earth's atmosphere. Usually, five to ten a year are observed to fall and are subsequently recovered and made known to scientists. Few meteorites are large enough to create large impact craters. Instead, they typically arrive at the surface at their terminal velocity and, at most, create a small pit.\n\nLarge meteoroids may strike the earth with a significant fraction of their escape velocity (second cosmic velocity), leaving behind a hypervelocity impact crater. The kind of crater will depend on the size, composition, degree of fragmentation, and incoming angle of the impactor. The force of such collisions has the potential to cause widespread destruction. The most frequent hypervelocity cratering events on the Earth are caused by iron meteoroids, which are most easily able to transit the atmosphere intact. Examples of craters caused by iron meteoroids include Barringer Meteor Crater, Odessa Meteor Crater, Wabar craters, and Wolfe Creek crater; iron meteorites are found in association with all of these craters. In contrast, even relatively large stony or icy bodies like small comets or asteroids, up to millions of tons, are disrupted in the atmosphere, and do not make impact craters. Although such disruption events are uncommon, they can cause a considerable concussion to occur; the famed Tunguska event probably resulted from such an incident. Very large stony objects, hundreds of meters in diameter or more, weighing tens of millions of tons or more, can reach the surface and cause large craters, but are very rare. Such events are generally so energetic that the impactor is completely destroyed, leaving no meteorites. (The very first example of a stony meteorite found in association with a large impact crater, the Morokweng crater in South Africa, was reported in May 2006.)\nSeveral phenomena are well documented during witnessed meteorite falls too small to produce hypervelocity craters. The fireball that occurs as the meteoroid passes through the atmosphere can appear to be very bright, rivaling the sun in intensity, although most are far dimmer and may not even be noticed during daytime. Various colors have been reported, including yellow, green, and red. Flashes and bursts of light can occur as the object breaks up. Explosions, detonations, and rumblings are often heard during meteorite falls, which can be caused by sonic booms as well as shock waves resulting from major fragmentation events. These sounds can be heard over wide areas, with a radius of a hundred or more kilometers. Whistling and hissing sounds are also sometimes heard, but are poorly understood. Following passage of the fireball, it is not unusual for a dust trail to linger in the atmosphere for several minutes.\nAs meteoroids are heated during atmospheric entry, their surfaces melt and experience ablation. They can be sculpted into various shapes during this process, sometimes resulting in shallow thumbprint-like indentations on their surfaces called regmaglypts. If the meteoroid maintains a fixed orientation for some time, without tumbling, it may develop a conical \"nose cone\" or \"heat shield\" shape. As it decelerates, eventually the molten surface layer solidifies into a thin fusion crust, which on most meteorites is black (on some achondrites, the fusion crust may be very light colored). On stony meteorites, the heat-affected zone is at most a few mm deep; in iron meteorites, which are more thermally conductive, the structure of the metal may be affected by heat up to below the surface. Reports vary; some meteorites are reported to be \"burning hot to the touch\" upon landing, while others are alleged to have been cold enough to condense water and form a frost. Meteorites from multiple falls, such as Bjurbole, Tagish Lake, and Buzzard Coulee, have been found having fallen on lake and sea ice, perhaps suggesting that they were not hot when they fell.\n\nMeteoroids that experience disruption in the atmosphere may fall as meteorite showers, which can range from only a few up to thousands of separate individuals. The area over which a meteorite shower falls is known as its strewn field. Strewn fields are commonly elliptical in shape, with the major axis parallel to the direction of flight. In most cases, the largest meteorites in a shower are found farthest down-range in the strewn field.\n\nMost meteorites are stony meteorites, classed as chondrites and achondrites. Only about 6% of meteorites are iron meteorites or a blend of rock and metal, the stony-iron meteorites. Modern classification of meteorites is complex. The review paper of Krot et al. (2007) summarizes modern meteorite taxonomy.\n\nAbout 86% of the meteorites are chondrites, which are named for the small, round particles they contain. These particles, or chondrules, are composed mostly of silicate minerals that appear to have been melted while they were free-floating objects in space. Certain types of chondrites also contain small amounts of organic matter, including amino acids, and presolar grains. Chondrites are typically about 4.55 billion years old and are thought to represent material from the asteroid belt that never coalesced into large bodies. Like comets, chondritic asteroids are some of the oldest and most primitive materials in the solar system. Chondrites are often considered to be \"the building blocks of the planets\".\n\nAbout 8% of the meteorites are achondrites (meaning they do not contain chondrules), some of which are similar to terrestrial igneous rocks. Most achondrites are also ancient rocks, and are thought to represent crustal material of differentiated planetesimals. One large family of achondrites (the HED meteorites) may have originated on the parent body of the Vesta Family, although this claim is disputed. Others derive from unidentified asteroids. Two small groups of achondrites are special, as they are younger and do not appear to come from the asteroid belt. One of these groups comes from the Moon, and includes rocks similar to those brought back to Earth by Apollo and Luna programs. The other group is almost certainly from Mars and constitutes the only materials from other planets ever recovered by humans.\n\nAbout 5% of meteorites that have been seen to fall are iron meteorites composed of iron-nickel alloys, such as kamacite and/or taenite. Most iron meteorites are thought to come from the cores of planetesimals that were once molten. As with the Earth, the denser metal separated from silicate material and sank toward the center of the planetesimal, forming its core. After the planetesimal solidified, it broke up in a collision with another planetesimal. Due to the low abundance of iron meteorites in collection areas such as Antarctica, where most of the meteoric material that has fallen can be recovered, it is possible that the percentage of iron-meteorite falls is lower than 5%. This would be explained by a recovery bias; laypeople are more likely to notice and recover solid masses of metal than most other meteorite types. The abundance of iron meteorites relative to total Antarctic finds is 0.4% \n\nStony-iron meteorites constitute the remaining 1%. They are a mixture of iron-nickel metal and silicate minerals. One type, called pallasites, is thought to have originated in the boundary zone above the core regions where iron meteorites originated. The other major type of stony-iron meteorites is the mesosiderites.\n\nTektites (from Greek \"tektos\", molten) are not themselves meteorites, but are rather natural glass objects up to a few centimeters in size that were formed—according to most scientists—by the impacts of large meteorites on Earth's surface. A few researchers have favored tektites originating from the Moon as volcanic ejecta, but this theory has lost much of its support over the last few decades.\n\nIn March 2015, NASA scientists reported that, for the first time, complex organic compounds found in DNA and RNA, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine and polycyclic aromatic hydrocarbons (PAHs) may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.\n\nIn January 2018, researchers found that 4.5 billion-year-old meteorites found on Earth contained liquid water along with prebiotic complex organic substances that may be ingredients for life.\n\nMost meteorite falls are recovered on the basis of eyewitness accounts of the fireball or the impact of the object on the ground, or both. Therefore, despite the fact that meteorites fall with virtually equal probability everywhere on Earth, verified meteorite falls tend to be concentrated in areas with high human population densities such as Europe, Japan, and northern India.\n\nA small number of meteorite falls have been observed with automated cameras and recovered following calculation of the impact point. The first of these was the Přibram meteorite, which fell in Czechoslovakia (now the Czech Republic) in 1959. In this case, two cameras used to photograph meteors captured images of the fireball. The images were used both to determine the location of the stones on the ground and, more significantly, to calculate for the first time an accurate orbit for a recovered meteorite.\n\nFollowing the Pribram fall, other nations established automated observing programs aimed at studying infalling meteorites. One of these was the \"Prairie Network\", operated by the Smithsonian Astrophysical Observatory from 1963 to 1975 in the midwestern US. This program also observed a meteorite fall, the \"Lost City\" chondrite, allowing its recovery and a calculation of its orbit. Another program in Canada, the Meteorite Observation and Recovery Project, ran from 1971 to 1985. It too recovered a single meteorite, \"Innisfree\", in 1977. Finally, observations by the European Fireball Network, a descendant of the original Czech program that recovered Pribram, led to the discovery and orbit calculations for the \"Neuschwanstein\" meteorite in 2002.\nNASA has an automated system that detects meteors and calculates the orbit, magnitude, ground track, and other parameters over the southeast USA, which often detects a number of events each night.\n\nUntil the twentieth century, only a few hundred meteorite finds had ever been discovered. More than 80% of these were iron and stony-iron meteorites, which are easily distinguished from local rocks. To this day, few stony meteorites are reported each year that can be considered to be \"accidental\" finds. The reason there are now more than 30,000 meteorite finds in the world's collections started with the discovery by Harvey H. Nininger that meteorites are much more common on the surface of the Earth than was previously thought.\n\nNininger's strategy was to search for meteorites in the Great Plains of the United States, where the land was largely cultivated and the soil contained few rocks. Between the late 1920s and the 1950s, he traveled across the region, educating local people about what meteorites looked like and what to do if they thought they had found one, for example, in the course of clearing a field. The result was the discovery of over 200 new meteorites, mostly stony types.\n\nIn the late 1960s, Roosevelt County, New Mexico in the Great Plains was found to be a particularly good place to find meteorites. After the discovery of a few meteorites in 1967, a public awareness campaign resulted in the finding of nearly 100 new specimens in the next few years, with many being by a single person, Ivan Wilson. In total, nearly 140 meteorites were found in the region since 1967. In the area of the finds, the ground was originally covered by a shallow, loose soil sitting atop a hardpan layer. During the dustbowl era, the loose soil was blown off, leaving any rocks and meteorites that were present stranded on the exposed surface.\n\nA few meteorites were found in Antarctica between 1912 and 1964. In 1969, the 10th Japanese Antarctic Research Expedition found nine meteorites on a blue ice field near the Yamato Mountains. With this discovery, came the realization that movement of ice sheets might act to concentrate meteorites in certain areas. After a dozen other specimens were found in the same place in 1973, a Japanese expedition was launched in 1974 dedicated to the search for meteorites. This team recovered nearly 700 meteorites.\n\nShortly thereafter, the United States began its own program to search for Antarctic meteorites, operating along the Transantarctic Mountains on the other side of the continent: the Antarctic Search for Meteorites (ANSMET) program. European teams, starting with a consortium called \"EUROMET\" in the late 1980s, and continuing with a program by the Italian Programma Nazionale di Ricerche in Antartide have also conducted systematic searches for Antarctic meteorites.\n\nThe Antarctic Scientific Exploration of China has conducted successful meteorite searches since 2000. A Korean program (KOREAMET) was launched in 2007 and has collected a few meteorites. The combined efforts of all of these expeditions have produced more than 23,000 classified meteorite specimens since 1974, with thousands more that have not yet been classified. For more information see the article by Harvey (2003).\n\nAt about the same time as meteorite concentrations were being discovered in the cold desert of Antarctica, collectors discovered that many meteorites could also be found in the hot deserts of Australia. Several dozen meteorites had already been found in the Nullarbor region of Western and South Australia. Systematic searches between about 1971 and the present recovered more than 500 others, ~300 of which are currently well characterized. The meteorites can be found in this region because the land presents a flat, featureless, plain covered by limestone. In the extremely arid climate, there has been relatively little weathering or sedimentation on the surface for tens of thousands of years, allowing meteorites to accumulate without being buried or destroyed. The dark colored meteorites can then be recognized among the very different looking limestone pebbles and rocks.\n\nIn 1986–87, a German team installing a network of seismic stations while prospecting for oil discovered about 65 meteorites on a flat, desert plain about southeast of Dirj (Daraj), Libya. A few years later, a desert enthusiast saw photographs of meteorites being recovered by scientists in Antarctica, and thought that he had seen similar occurrences in northern Africa. In 1989, he recovered about 100 meteorites from several distinct locations in Libya and Algeria. Over the next several years, he and others who followed found at least 400 more meteorites. The find locations were generally in regions known as regs or hamadas: flat, featureless areas covered only by small pebbles and minor amounts of sand. Dark-colored meteorites can be easily spotted in these places. In the case of several meteorite fields, such as Dar el Gani, Dhofar, and others, favorable light-colored geology consisting of basic rocks (clays, dolomites, and limestones) makes meteorites particularly easy to identify.\n\nAlthough meteorites had been sold commercially and collected by hobbyists for many decades, up to the time of the Saharan finds of the late 1980s and early 1990s, most meteorites were deposited in or purchased by museums and similar institutions where they were exhibited and made available for scientific research. The sudden availability of large numbers of meteorites that could be found with relative ease in places that were readily accessible (especially compared to Antarctica), led to a rapid rise in commercial collection of meteorites. This process was accelerated when, in 1997, meteorites coming from both the Moon and Mars were found in Libya. By the late 1990s, private meteorite-collecting expeditions had been launched throughout the Sahara. Specimens of the meteorites recovered in this way are still deposited in research collections, but most of the material is sold to private collectors. These expeditions have now brought the total number of well-described meteorites found in Algeria and Libya to more than 500.\n\nMeteorite markets came into existence in the late 1990s, especially in Morocco. This trade was driven by Western commercialization and an increasing number of collectors. The meteorites were supplied by nomads and local people who combed the deserts looking for specimens to sell. Many thousands of meteorites have been distributed in this way, most of which lack any information about how, when, or where they were discovered. These are the so-called \"Northwest Africa\" meteorites. When they get classified, they are named \"Northwest Africa\" (abbreviated NWA) followed by a number. It is generally accepted that NWA meteorites originate in Morocco, Algeria, Western Sahara, Mali, and possibly even further afield. Nearly all of these meteorites leave Africa through Morocco. Scores of important meteorites, including Lunar and Martian ones, have been discovered and made available to science via this route. A few of the more notable meteorites recovered include Tissint and Northwest Africa 7034. Tissint was the first witnessed Martian meteorite fall in over fifty years; NWA 7034 is the oldest meteorite known to come from Mars, and is a unique water-bearing regolith breccia.\n\nIn 1999, meteorite hunters discovered that the desert in southern and central Oman were also favorable for the collection of many specimens. The gravel plains in the Dhofar and Al Wusta regions of Oman, south of the sandy deserts of the Rub' al Khali, had yielded about 5,000 meteorites as of mid-2009. Included among these are a large number of lunar and Martian meteorites, making Oman a particularly important area both for scientists and collectors. Early expeditions to Oman were mainly done by commercial meteorite dealers, however international teams of Omani and European scientists have also now collected specimens.\n\nThe recovery of meteorites from Oman is currently prohibited by national law, but a number of international hunters continue to remove specimens now deemed national treasures. This new law provoked a small international incident, as its implementation preceded any public notification of such a law, resulting in the prolonged imprisonment of a large group of meteorite hunters, primarily from Russia, but whose party also consisted of members from the US as well as several other European countries.\n\nBeginning in the mid-1960s, amateur meteorite hunters began scouring the arid areas of the southwestern United States. To date, meteorites numbering possibly into the thousands have been recovered from the Mojave, Sonoran, Great Basin, and Chihuahuan Deserts, with many being recovered on dry lake beds. Significant finds include the three tonne Old Woman meteorite, currently on display at the Desert Discovery Center in Barstow, California. Other rare finds include the Los Angeles meteorite, a Martian meteorite, Superior Valley 014 Acapulcoite, one of two of its type found within the United States, and the Blue Eagle meteorite, the first Rumuruti-type chondrite yet found in the Americas.\nA number of finds from the American Southwest have yet to be formally submitted to the Meteorite Nomenclature Committee, as many finders think it is unwise to publicly state the coordinates of their discoveries for fear of confiscation by the federal government and competition with other hunters at published find sites. \nSeveral of the meteorites found recently are currently on display in the Griffith Observatory in Los Angeles.\n\nMeteorite falls may have been the source of cultish worship. The cult in the Temple of Artemis at Ephesus, one of the Seven Wonders of the Ancient World, possibly originated with the observation and recovery of a meteorite that was understood by contemporaries to have fallen to the earth from Jupiter, the principal Roman deity.\nThere are reports that a sacred stone was enshrined at the temple that may have been a meteorite. The Black Stone set into the wall of the Kaaba has often been presumed to be a meteorite, but the little available evidence for this is inconclusive. Although the use of the metal found in meteorites is also recorded in myths of many countries and cultures where the celestial source was often acknowledged, scientific documentation only began in the last few centuries.\n\nThe oldest known iron artifacts are nine small beads hammered from meteoritic iron. They were found in northern Egypt and have been securely dated to 3200 BC.\n\nIn the 1970s, a stone meteorite was uncovered during an archaeological dig at Danebury Iron Age hillfort, Danebury England. It was found deposited part way down in an Iron Age pit (c. 1200 BC). Since it must have been deliberately placed there, this could indicate one of the first (known) human finds of a meteorite in Europe.\n\nSome Native Americans treated meteorites as ceremonial objects. In 1915, a iron meteorite was found in a Sinagua (c. 1100–1200 AD) burial cyst near Camp Verde, Arizona, respectfully wrapped in a feather cloth. A small pallasite was found in a pottery jar in an old burial found at Pojoaque Pueblo, New Mexico. Nininger reports several other such instances, in the Southwest US and elsewhere, such as the discovery of Native American beads of meteoric iron found in Hopewell burial mounds, and the discovery of the Winona meteorite in a Native American stone-walled crypt.\nIndigenous peoples often prized iron-nickel meteorites as an easy, if limited, source of iron metal. For example, the Inuit used chips of the Cape York meteorite to form cutting edges for tools and spear tips.\n\nTwo of the oldest recorded meteorite falls in Europe are the Elbogen (1400) and Ensisheim (1492) meteorites. The German physicist, Ernst Florens Chladni, was the first to publish (in 1794) the then audacious idea that meteorites were rocks from space. His booklet was \"\"On the Origin of the Iron Masses Found by Pallas and Others Similar to it, and on Some Associated Natural Phenomena\"\". In this he compiled all available data on several meteorite finds and falls concluded that they must have their origins in outer space. The scientific community of the time responded with resistance and mockery. It took nearly ten years before a general acceptance of the origin of meteorites was achieved through the work of the French scientist Jean-Baptiste Biot and the British chemist, Edward Howard. Biot's study, initiated by the French Academy of Sciences, was compelled by a fall of thousands of meteorites on 26 April 1803 from the skies of L'Aigle, France.\n\nOne of the leading theories for the cause of the Cretaceous–Paleogene extinction event that included the dinosaurs is a large meteorite impact. The Chicxulub Crater has been identified as the site of this impact. There has been a lively scientific debate as to whether other major extinctions, including the ones at the end of the Permian and Triassic periods might also have been the result of large impact events, but the evidence is much less compelling than for the end Cretaceous extinction.\n\nThere are several reported instances of falling meteorites having killed people and livestock, but a few of these appear more credible than others. The most infamous reported fatality from a meteorite impact is that of an Egyptian dog that was killed in 1911, although this report is highly disputed. This meteorite fall was identified in the 1980s as Martian in origin. There is substantial evidence that the meteorite known as Valera (Venezuela 1972, see Meteorite fall) hit and killed a cow upon impact, nearly dividing the animal in two, and similar unsubstantiated reports of a horse being struck and killed by a stone of the New Concord fall also abound. Throughout history, many first- and second-hand reports of meteorites falling on and killing both humans and other animals abound. One example is from 1490 AD in China, which purportedly killed thousands of people. John Lewis has compiled some of these reports, and summarizes, \"No one in recorded history has ever been killed by a meteorite in the presence of a meteoriticist and a medical doctor\" and \"reviewers who make sweeping negative conclusions usually do not cite any of the primary publications in which the eyewitnesses describe their experiences, and give no evidence of having read them\".\n\nThe first known modern case of a human hit by a space rock occurred on 30 November 1954 in Sylacauga, Alabama. A stone chondrite crashed through a roof and hit Ann Hodges in her living room after it bounced off her radio. She was badly bruised. The Hodges meteorite, or Sylacauga meteorite, is currently on exhibit at the Alabama Museum of Natural History.\n\nAnother claim was put forth by a young boy who stated that he had been hit by a small (~3-gram) stone of the Mbale meteorite fall from Uganda, and who stood to gain nothing from this assertion. The stone reportedly fell through banana leaves before striking the boy on the head, causing little to no pain, as it was small enough to have been slowed by both friction with the atmosphere as well as that with banana leaves, before striking the boy.\n\nSeveral persons have since claimed to have been struck by \"meteorites\" but no verifiable meteorites have resulted.\n\nMost meteorites date from the oldest times in the solar system and are by far the oldest material available on the planet. Despite their age, they are fairly vulnerable to terrestrial environment: water, salt, and oxygen attack the meteorites as soon they reach the ground.\n\nThe terrestrial alteration of meteorites is called weathering. In order to quantify the degree of alteration that a meteorite experienced, several qualitative weathering indices have been applied to Antarctic and desertic samples.\n\nThe most known weathering scale, used for ordinary chondrites, ranges from W0 (pristine state) to W6 (heavy alteration).\n\n\"Fossil\" meteorites are sometimes discovered by geologists. They represent the deeply weathered remains of meteorites that fell to Earth in the remote past and were preserved in sedimentary deposits sufficiently well that they can be recognized through mineralogical and geochemical studies. One limestone quarry in Sweden has produced an anomalously large number (more than a hundred) fossil meteorites from the Ordovician, nearly all of which are deeply weathered L-chondrites that still resemble the original meteorite under a petrographic microscope, but which have had their original material almost entirely replaced by terrestrial secondary mineralization. The extraterrestrial provenance was demonstrated in part through isotopic analysis of relict spinel grains, a mineral that is common in meteorites, is insoluble in water, and is able to persist chemically unchanged in the terrestrial weathering environment. One of these fossil meteorites, dubbed Österplana 065, appears to represent a distinct type of meteorite that is \"extinct\" in the sense that it is no longer falling to Earth, the parent body having already been completely depleted from reservoir of Near Earth Objects.\n\n\nApart from meteorites fallen onto the Earth, two tiny fragments of asteroids were found among the samples collected on the Moon; these were the Bench Crater meteorite (Apollo 12, 1969) and the Hadley Rille meteorite (Apollo 15, 1971).\n\n\n"}
{"id": "19938", "url": "https://en.wikipedia.org/wiki?curid=19938", "title": "Mega-", "text": "Mega-\n\nMega is a unit prefix in metric systems of units denoting a factor of one million (10 or ). It has the unit symbol M. It was confirmed for use in the International System of Units (SI) in 1960. \"Mega\" comes from .\n\n\nWhen units occur in exponentiation, such as in square and cubic forms, any multiples-prefix is considered part of the unit, and thus included in the exponentiation.\n\nIn some fields of computing, \"mega\" may sometimes denote 1,048,576 (2) of information units, for example, a megabyte, a megaword, but denotes (10) units of other quantities, for example, transfer rates: = . The prefix \"mebi-\" has been suggested as a prefix for 2 to avoid ambiguity.\n\n\n"}
{"id": "19940", "url": "https://en.wikipedia.org/wiki?curid=19940", "title": "Maciej Płażyński", "text": "Maciej Płażyński\n\nMaciej Płażyński (; 10 February 1958 – 10 April 2010) was a Polish liberal-conservative politician.\n\nPłażyński was born in Młynary. He began his political career in 1980 / 1981 as one of the leaders of the Students' Solidarity; he was governor of the Gdańsk Voivodship from August 1990 to July 1996, and was elected to the Sejm (the lower house of the Polish parliament) in September 1997. To date he is longest serving Marshal of the Sejm of the Third Republic of Poland\n\nIn January 2001, he founded the Civic Platform political party with Donald Tusk and Andrzej Olechowski. He left Civic Platform for personal reasons and at the time of his death was an independent MP. He was member of Kashubian-Pomeranian Association. He was later chosen as a chairman of the Association \"Polish Community\".\n\nMaciej Płażyński was married to Elżbieta Płażyńska and together they had three children: Jakub, Katarzyna, and Kacper.\n\nHe was listed on the flight manifest of the Tupolev Tu-154 of the 36th Special Aviation Regiment carrying the President of Poland Lech Kaczyński which crashed while landing at Smolensk-North airport near Pechersk near Smolensk, Russia, on 10 April 2010, killing all aboard.\n\nIn 2000, Płażyński was awarded the Order of Merit of the Italian Republic, First Class. He received the titles of honorary citizen of Młynary, Puck, Pionki and Lidzbark Warmiński.\n\nOn 16 April 2010 he was posthumously awarded the Grand Cross of the Order of Polonia Restituta. He was also awarded a Gold Medal of Gloria Artis.\n\n\n"}
{"id": "19941", "url": "https://en.wikipedia.org/wiki?curid=19941", "title": "Mark Bingham", "text": "Mark Bingham\n\nMark Kendall Bingham (May 22, 1970 – September 11, 2001) was an American public relations executive who founded his own company, the Bingham Group. During the September 11 attacks in 2001, he was a passenger on board United Airlines Flight 93. Bingham was among the passengers who, along with Todd Beamer, Tom Burnett and Jeremy Glick, formed the plan to retake the plane from the hijackers, and led the effort that resulted in the crash of the plane into a field near Shanksville, Pennsylvania, thwarting the hijackers’ plan to crash the plane into a building in Washington, D.C., most likely either the U.S. Capitol Building or the White House.\n\nBoth for his presence on United 93, as well as his athletic physique, Bingham has been widely honored posthumously for having \"smashed the gay stereotype mold and really opened the door to many others who came after him.\"\n\nMark Bingham was born in 1970, the only child of mother Alice Hoagland and father Gerald Bingham. When Mark was two years old, his parents divorced. Raised by his mother and her family, Mark grew up in Miami, Florida, and Southern California before moving to the San Jose area in 1983. Bingham was an aspiring filmmaker growing up, and began using a video camera as a teenager as a personal diary through which he expressed himself and documented his life and the lives of his family and friends. He accumulated hundreds of hours of video documenting the final decade and a half of his life. He graduated from Los Gatos High School as a two-year captain of his rugby team in 1988. As an undergraduate at the University of California, Berkeley, Bingham played on two of Coach Jack Clark's national-championship-winning rugby teams in the early 1990s. He also joined the Chi Psi fraternity, eventually becoming its president. Upon graduation at the age of twenty-one, Bingham came out as gay to his family and friends.\n\nA large athlete at and , Bingham also played for the gay-inclusive rugby union team San Francisco Fog RFC. Bingham played No. 8 in their first two friendly matches. He played in their first tournament, and taught his teammates his favorite rugby songs.\n\nBingham had recently opened a satellite office of his public relations firm in New York City, and was spending more time on the East Coast. He discussed plans with his friend Scott Glaessgen to form a New York City rugby team, the Gotham Knights.\n\nOn the morning of September 11, Bingham overslept and nearly missed his flight, on his way to San Francisco to be an usher in his fraternity brother Joseph Salama's wedding. He arrived at the Terminal A at 7:40am, ran to Gate 17, and was the last passenger to board United Airlines Flight 93, taking seat 4D, next to passenger Tom Burnett.\n\nUnited Flight 93 was scheduled to depart at 8:00am, but the Boeing 757 did not depart until 42 minutes later due to runway traffic delays. Four minutes later, American Airlines Flight 11 crashed into the World Trade Center's North Tower. Fifteen minutes later, at 9:03 am, as United Flight 175 crashed into the South Tower, United 93 was climbing to cruising altitude, heading west over New Jersey and into Pennsylvania. At 9:25 am, Flight 93 was above eastern Ohio, and pilots Jason Dahl and LeRoy Homer received an alert, \"beware of cockpit intrusion,\" on the cockpit computer device ACARS (Aircraft Communications and Reporting System). Three minutes later, Cleveland controllers could hear screams over the cockpit's open microphone. Moments later, the hijackers, led by the Lebanese Ziad Samir Jarrah, took over the plane's controls and told passengers, \"Keep remaining sitting. We have a bomb on board\". Bingham and the other passengers were herded into the back of the plane. Within six minutes, the plane changed course and was heading for Washington, D.C. Several of the passengers made phone calls to loved ones, who informed them about the two planes that had crashed into the World Trade Center.\n\nAfter the hijackers veered the plane sharply south, the passengers decided to act. Bingham, along with Todd Beamer, Tom Burnett and Jeremy Glick, formed a plan to take the plane back from the hijackers. They related this to their loved ones and the authorities via telephone. Bingham got through to his aunt's home in California. Bingham stated, \"This is Mark. I want to let you guys know that I love you, in case I don't see you again...I'm on United Airlines, Flight 93. It's being hijacked.\" according to \"The Week\", Hoagland formed the impression that her son was talking \"confidentially\" with a fellow passenger, to form a plan to retake the plane. According to ABC News, the call was cut off after about three minutes. Hoagland, after seeing news reports of the plane's hijacking, called him back and left two messages for him, calmly saying, \"Mark, this is your mom. The news is that it's been hijacked by terrorists. They are planning to probably use the plane as a target to hit some site on the ground. I would say go ahead and do everything you can to overpower them, because they are hellbent. Try to call me back if you can.\" Bingham, Burnett, and Glick were each more than 6 feet tall, well-built and fit. As they made their decision to retake the plane, Glick related this over the phone to his wife, Lyz. Fellow passenger Todd Beamer, speaking to GTE-Verizon Lisa Jefferson and the FBI, related that he too was part of this group. They were joined by other passengers, including Lou Nacke, Rich Guadagno, Alan Beaven, Honor Elizabeth Wainio, Linda Gronlund, and William Cashman, along with flight attendants Sandra Bradshaw and Cee Cee Ross-Lyles, in discussing their options and voting on a course of action, ultimately deciding to storm the cockpit and take over the plane.\n\nAccording to the \"9/11 Commission Report\", after the plane's voice data recorder was recovered, it revealed pounding and crashing sounds against the cockpit door and shouts and screams in English. \"Let's get them!\" a passenger cries. A hijacker shouts, \"Allah akbar!\" (\"God is great\"). Jarrah repeatedly pitched the plane to knock passengers off their feet, but the passengers apparently managed to invade the cockpit, where one was heard shouting, \"In the cockpit. If we don't, we'll die.\" At 10:02 am, a hijacker ordered, \"Pull it down! Pull it down!\" The 9/11 Commission later reported that the plane's control wheel was turned hard to the right, causing it to roll on its back and plow into an empty field in Shanksville, Pennsylvania at 580 miles an hour, killing everyone on board. The plane was twenty minutes of flying time away from its suspected target, the White House or the U.S. Capitol Building in Washington, D.C. According to Vice President Dick Cheney, President George W. Bush had given the order to shoot the plane down had it continued its path to Washington.\n\nBingham was survived by his parents, and Hoagland family members who played a part in his upbringing: his grandparents Betty and Herbert Hoagland, his aunt Candyce Hoagland, his uncles Lee, Linden and Vaughn Hoagland. He is survived as well by his stepmother and various stepsiblings. and his former partner of six years, Paul Holm, who said Bingham had risked his life to protect the lives of others on occasions prior to 9/11, having twice successfully protected Holm from attempted muggings, one at gunpoint. Holm described Bingham as a brave, competitive man, saying, \"He hated to lose—at anything.\" He was known to proudly display a scar he received after being gored at the Running of the Bulls in Pamplona, Spain. He is buried at Madronia Cemetery, Saratoga, California.\nU.S. Senators John McCain and Barbara Boxer honored Bingham on September 17, 2001, in a ceremony for San Francisco Bay Area victims of the attacks, presenting a folded American flag to Paul Holm.\n\nThe Mark Kendall Bingham Memorial Tournament (referred to as the Bingham Cup), a biennial international rugby union competition predominantly for gay and bisexual men, was established in 2002 in his memory.\n\nBingham, along with the other passengers on Flight 93, was posthumously awarded the Arthur Ashe Courage Award in 2002.\n\nThe Eureka Valley Recreation Center's Gymnasium in San Francisco was renamed the Mark Bingham Gymnasium in August 2002.\n\nSinger Melissa Etheridge dedicated the song \"Tuesday Morning\" in 2004 to his memory.\n\nBeginning in 2005, the Mark Bingham Award for Excellence in Achievement has been awarded by the California Alumni Association of the University of California, Berkeley to a young alumnus or alumna at its annual Charter Gala.\n\nAt the National 9/11 Memorial, Bingham and other passengers from Flight 93 are memorialized at the South Pool, on Panel S-67.\n\nAt the Flight 93 National Memorial in Pennsylvania, Bingham's name is located on one of the 40 8-foot-tall panels of polished, 3-inch thick granite that comprise the Memorial's Wall of Names.\n\nThe 2013 feature-length documentary \"The Rugby Player\" focuses on Bingham and the bond he had with his mother, Alice Hoglan, a former United Airlines flight attendant who, following his death, became a nationally known authority on airline safety and a champion of LGBT rights. Directed by Scott Gracheff, the film relies on the vast amount of video footage Bingham himself shot beginning in his teens until weeks before his death. The film's alternate title, \"With You\", is a popular rugby term, and one of Bingham's favorite expressions.\n\n\n\n"}
{"id": "19942", "url": "https://en.wikipedia.org/wiki?curid=19942", "title": "Manner of articulation", "text": "Manner of articulation\n\nIn articulatory phonetics, the manner of articulation is the configuration and interaction of the articulators (speech organs such as the tongue, lips, and palate) when making a speech sound. One parameter of manner is \"stricture,\" that is, how closely the speech organs approach one another. Others include those involved in the r-like sounds (taps and trills), and the sibilancy of fricatives.\n\nThe concept of manner is mainly used in the discussion of consonants, although the movement of the articulators will also greatly alter the resonant properties of the vocal tract, thereby changing the formant structure of speech sounds that is crucial for the identification of vowels. For consonants, the place of articulation and the degree of phonation of voicing are considered separately from manner, as being independent parameters. Homorganic consonants, which have the same place of articulation, may have different manners of articulation. Often nasality and laterality are included in manner, but some phoneticians, such as Peter Ladefoged, consider them to be independent.\n\nManners of articulation with substantial obstruction of the airflow (stops, fricatives, affricates) are called obstruents. These are prototypically voiceless, but voiced obstruents are extremely common as well. Manners without such obstruction (nasals, liquids, approximants, and also vowels) are called sonorants because they are nearly always voiced. Voiceless sonorants are uncommon, but are found in Welsh and Classical Greek (the spelling \"rh\"), in Standard Tibetan (the \"lh\" of Lhasa), and the \"wh\" in those dialects of English that distinguish \"which\" from \"witch\".\n\nSonorants may also be called resonants, and some linguists prefer that term, restricting the word 'sonorant' to non-vocoid resonants (that is, nasals and liquids, but not vowels or semi-vowels). Another common distinction is between occlusives (stops, nasals and affricates) and continuants (all else).\n\nFrom greatest to least stricture, speech sounds may be classified along a cline as stop consonants (with \"occlusion\", or blocked airflow), fricative consonants (with partially blocked and therefore strongly turbulent airflow), approximants (with only slight turbulence), and vowels (with full unimpeded airflow). Affricates often behave as if they were intermediate between stops and fricatives, but phonetically they are sequences of a stop and fricative.\n\nOver time, sounds in a language may move along this cline toward less stricture in a process called lenition, or towards more stricture in a process called fortition.\n\nSibilants are distinguished from other fricatives by the shape of the tongue and how the airflow is directed over the teeth. Fricatives at coronal places of articulation may be sibilant or non-sibilant, sibilants being the more common.\n\nFlaps (also called taps) are similar to very brief stops. However, their articulation and behavior are distinct enough to be considered a separate manner, rather than just length. The main articulatory difference between flaps and stops is that, due to the greater length of stops compared to flaps, a build-up of air pressure occurs behind a stop which does not occur behind a flap. This means that when the stop is released, there is a burst of air as the pressure is relieved, while for flaps there is no such burst.\n\nTrills involve the vibration of one of the speech organs. Since trilling is a separate parameter from stricture, the two may be combined. Increasing the stricture of a typical trill results in a trilled fricative. Trilled affricates are also known.\n\nNasal airflow may be added as an independent parameter to any speech sound. It is most commonly found in nasal occlusives and nasal vowels, but nasalized fricatives, taps, and approximants are also found. When a sound is not nasal, it is called \"oral.\"\n\nLaterality is the release of airflow at the side of the tongue. This can be combined with other manners, resulting in lateral approximants (such as the pronunciation of the letter L in the English word \"let\"), lateral flaps, and lateral fricatives and affricates.\n\n\n\nAll of these manners of articulation are pronounced with an airstream mechanism called pulmonic egressive, meaning that the air flows outward, and is powered by the lungs (actually the ribs and diaphragm). Other airstream mechanisms are possible. Sounds that rely on some of these include:\n\n\n\n"}
{"id": "19943", "url": "https://en.wikipedia.org/wiki?curid=19943", "title": "Mostaganem Province", "text": "Mostaganem Province\n\nMostaganem () is a province (\"wilaya\") of Algeria. The capital is Mostaganem. Other localities include Ain Nouissi, Ain Tadles, Tazgait and Stidia.\n\nThe province is divided into 10 districts (\"daïras\"), which are further divided into 32 \"communes\" or municipalities.\n\n"}
{"id": "19945", "url": "https://en.wikipedia.org/wiki?curid=19945", "title": "Motherboard", "text": "Motherboard\n\nA motherboard (sometimes alternatively known as the main circuit board, system board, baseboard, planar board or logic board, or colloquially, a mobo) is the main printed circuit board (PCB) found in general purpose microcomputers and other expandable systems. It holds and allows communication between many of the crucial electronic components of a system, such as the central processing unit (CPU) and memory, and provides connectors for other peripherals. Unlike a backplane, a motherboard usually contains significant sub-systems such as the central processor, the chipset's input/output and memory controllers, interface connectors, and other components integrated for general purpose use and applications.\n\n\"Motherboard\" specifically refers to a PCB with expansion capability and as the name suggests, this board is often referred to as the \"mother\" of all components attached to it, which often include peripherals, interface cards, and daughtercards: sound cards, video cards, network cards, hard drives, or other forms of persistent storage; TV tuner cards, cards providing extra USB or FireWire slots and a variety of other custom components.\n\nSimilarly, the term \"mainboard\" is applied to devices with a single board and no additional expansions or capability, such as controlling boards in laser printers, televisions, washing machines and other embedded systems with limited expansion abilities.\n\nPrior to the invention of the microprocessor, the digital computer consisted of multiple printed circuit boards in a card-cage case with components connected by a backplane, a set of interconnected sockets. In very old designs, copper wires were the discrete connections between card connector pins, but printed circuit boards soon became the standard practice. The Central Processing Unit (CPU), memory, and peripherals were housed on individual printed circuit boards, which were plugged into the backplane. The ubiquitous S-100 bus of the 1970s is an example of this type of backplane system.\n\nThe most popular computers of the 1980s such as the Apple II and IBM PC had published schematic diagrams and other documentation which permitted rapid reverse-engineering and third-party replacement motherboards. Usually intended for building new computers compatible with the exemplars, many motherboards offered additional performance or other features and were used to upgrade the manufacturer's original equipment.\n\nDuring the late 1981s and early 1990s, it became economical to move an increasing number of peripheral functions onto the motherboard. In the late 1980s, personal computer motherboards began to include single ICs (also called Super I/O chips) capable of supporting a set of low-speed peripherals: keyboard, mouse, floppy disk drive, serial ports, and parallel ports. By the late 1990s, many personal computer motherboards included consumer-grade embedded audio, video, storage, and networking functions without the need for any expansion cards at all; higher-end systems for 3D gaming and computer graphics typically retained only the graphics card as a separate component. Business PCs, workstations, and servers were more likely to need expansion cards, either for more robust functions, or for higher speeds; those systems often had fewer embedded components.\n\nLaptop and notebook computers that were developed in the 1990s integrated the most common peripherals. This even included motherboards with no upgradeable components, a trend that would continue as smaller systems were introduced after the turn of the century (like the tablet computer and the netbook). Memory, processors, network controllers, power source, and storage would be integrated into some systems.\n\nA motherboard provides the electrical connections by which the other components of the system communicate. Unlike a backplane, it also contains the central processing unit and hosts other subsystems and devices.\n\nA typical desktop computer has its microprocessor, main memory, and other essential components connected to the motherboard. Other components such as external storage, controllers for video display and sound, and peripheral devices may be attached to the motherboard as plug-in cards or via cables; in modern microcomputers it is increasingly common to integrate some of these peripherals into the motherboard itself.\n\nAn important component of a motherboard is the microprocessor's supporting chipset, which provides the supporting interfaces between the CPU and the various buses and external components. This chipset determines, to an extent, the features and capabilities of the motherboard.\n\nModern motherboards include:\n\nAdditionally, nearly all motherboards include logic and connectors to support commonly used input devices, such as USB for mouse devices and keyboards. Early personal computers such as the Apple II or IBM PC included only this minimal peripheral support on the motherboard. Occasionally video interface hardware was also integrated into the motherboard; for example, on the Apple II and rarely on IBM-compatible computers such as the IBM PC Jr. Additional peripherals such as disk controllers and serial ports were provided as expansion cards.\n\nGiven the high thermal design power of high-speed computer CPUs and components, modern motherboards nearly always include heat sinks and mounting points for fans to dissipate excess heat.\n\nMotherboards are produced in a variety of sizes and shape called computer form factor, some of which are specific to individual computer manufacturers. However, the motherboards used in IBM-compatible systems are designed to fit various case sizes. , most desktop computer motherboards use the ATX standard form factor — even those found in Macintosh and Sun computers, which have not been built from commodity components. A case's motherboard and power supply unit (PSU) form factor must all match, though some smaller form factor motherboards of the same family will fit larger cases. For example, an ATX case will usually accommodate a microATX motherboard.\n\nLaptop computers generally use highly integrated, miniaturized and customized motherboards. This is one of the reasons that laptop computers are difficult to upgrade and expensive to repair. Often the failure of one laptop component requires the replacement of the entire motherboard, which is usually more expensive than a desktop motherboard due to a large number of integrated components and their custom shape and size. The motherboard layout for laptops depends on the laptop case.\n\nA CPU socket (central processing unit) or slot is an electrical component that attaches to a Printed Circuit Board (PCB) and is designed to house a CPU (also called a microprocessor). It is a special type of integrated circuit socket designed for very high pin counts. A CPU socket provides many functions, including a physical structure to support the CPU, support for a heat sink, facilitating replacement (as well as reducing cost), and most importantly, forming an electrical interface both with the CPU and the PCB. CPU sockets on the motherboard can most often be found in most desktop and server computers (laptops typically use surface mount CPUs), particularly those based on the Intel x86 architecture. A CPU socket type and motherboard chipset must support the CPU series and speed.\n\nWith the steadily declining costs and size of integrated circuits, it is now possible to include support for many peripherals on the motherboard. By combining many functions on one PCB, the physical size and total cost of the system may be reduced; highly integrated motherboards are thus especially popular in small form factor and budget computers.\n\nA typical motherboard will have a different number of connections depending on its standard and form factor.\n\nA standard, modern ATX motherboard will typically have two or three PCI-Express 16x connection for a graphics card, one or two legacy PCI slots for various expansion cards, and one or two PCI-E 1x (which has superseded PCI). A standard EATX motherboard will have two to four PCI-E 16x connection for graphics cards, and a varying number of PCI and PCI-E 1x slots. It can sometimes also have a PCI-E 4x slot (will vary between brands and models).\n\nSome motherboards have two or more PCI-E 16x slots, to allow more than 2 monitors without special hardware, or use a special graphics technology called SLI (for Nvidia) and Crossfire (for AMD). These allow 2 to 4 graphics cards to be linked together, to allow better performance in intensive graphical computing tasks, such as gaming, video editing, etc.\n\nMotherboards are generally air cooled with heat sinks often mounted on larger chips, such as the Northbridge, in modern motherboards. Insufficient or improper cooling can cause damage to the internal components of the computer, or cause it to crash. Passive cooling, or a single fan mounted on the power supply, was sufficient for many desktop computer CPU's until the late 1990s; since then, most have required CPU fans mounted on their heat sinks, due to rising clock speeds and power consumption. Most motherboards have connectors for additional case fans and integrated temperature sensors to detect motherboard and CPU temperatures and controllable fan connectors which the BIOS or operating system can use to regulate fan speed. Alternatively computers can use a water cooling system instead of many fans.\n\nSome small form factor computers and home theater PCs designed for quiet and energy-efficient operation boast fan-less designs. This typically requires the use of a low-power CPU, as well as a careful layout of the motherboard and other component, s to allow for heat sink placement.\n\nA 2003 study found that some spurious computer crashes and general reliability issues, ranging from screen image distortions to I/O read/write errors, can be attributed not to software or peripheral hardware but to aging capacitors on PC motherboards. Ultimately this was shown to be the result of a faulty electrolyte formulation, an issue termed capacitor plague.\n\nStandard motherboards use electrolytic capacitors to filter the DC power distributed around the board. These capacitors age at a temperature-dependent rate, as their water based electrolytes slowly evaporate. This can lead to loss of capacitance and subsequent motherboard malfunctions due to voltage instabilities. While most capacitors are rated for 2000 hours of operation at , their expected design life roughly doubles for every below this. At a lifetime of 3 to 4 years can be expected. However, many manufacturers deliver substandard capacitors, which significantly reduce life expectancy. Inadequate case cooling and elevated temperatures around the CPU socket exacerbate this problem. With top blowers, the motherboard components can be kept under , effectively doubling the motherboard lifetime.\n\nMid-range and high-end motherboards, on the other hand, use solid capacitors exclusively. For every 10 °C less, their average lifespan is multiplied approximately by three, resulting in a 6-times higher lifetime expectancy at . These capacitors may be rated for 5000, 10000 or 12000 hours of operation at , extending the projected lifetime in comparison with standard solid capacitors.\n\nHigh rates of motherboard failures in China and India appear to be due to \"sulfurous air pollution produced by coal\" burned to generate electricity. Air pollution corrodes the circuitry, according to Intel researchers.\n\nMotherboards contain some non-volatile memory to initialize the system and load some startup software, usually an operating system, from some external peripheral device. Microcomputers such as the Apple II and IBM PC used ROM chips mounted in sockets on the motherboard. At power-up, the central processor would load its program counter with the address of the boot ROM and start executing instructions from the ROM. These instructions initialized and tested the system hardware displayed system information on the screen, performed RAM checks, and then loaded an initial program from a peripheral device. If none was available, then the computer would perform tasks from other memory stores or display an error message, depending on the model and design of the computer and the ROM version. For example, both the Apple II and the original IBM PC had Microsoft Cassette BASIC in ROM and would start that if no program could be loaded from disk.\n\nMost modern motherboard designs use a BIOS, stored in an EEPROM chip soldered to or socketed on the motherboard, to boot an operating system. Non-operating system boot programs are still supported on modern IBM PC-descended machines, but nowadays it is assumed that the boot program will be a complex operating system such as Microsoft Windows or Linux. When power is first supplied to the motherboard, the BIOS firmware tests and configures memory, circuitry, and peripherals. This Power-On Self Test (POST) may include testing some of the following things:\n\nOn recent motherboards, the BIOS may also patch the central processor microcode if the BIOS detects that the installed CPU is one for which errata have been published.\n\nMany motherboards now use an update to BIOS called UEFI.\n\n"}
{"id": "19947", "url": "https://en.wikipedia.org/wiki?curid=19947", "title": "Mannerism", "text": "Mannerism\n\nMannerism, also known as Late Renaissance, is a style in European art that emerged in the later years of the Italian High Renaissance around 1520, spreading by about 1530 and lasting until about the end of the 16th century in Italy, when the Baroque style largely replaced it. Northern Mannerism continued into the early 17th century.\n\nStylistically, Mannerism encompasses a variety of approaches influenced by, and reacting to, the harmonious ideals associated with artists such as Leonardo da Vinci, Raphael, and early Michelangelo. Where High Renaissance art emphasizes proportion, balance, and ideal beauty, Mannerism exaggerates such qualities, often resulting in compositions that are asymmetrical or unnaturally elegant. The style is notable for its intellectual sophistication as well as its artificial (as opposed to naturalistic) qualities. It favors compositional tension and instability rather than the balance and clarity of earlier Renaissance painting. Mannerism in literature and music is notable for its highly florid style and intellectual sophistication.\n\nThe definition of Mannerism and the phases within it continue to be a subject of debate among art historians. For example, some scholars have applied the label to certain early modern forms of literature (especially poetry) and music of the 16th and 17th centuries. The term is also used to refer to some late Gothic painters working in northern Europe from about 1500 to 1530, especially the Antwerp Mannerists—a group unrelated to the Italian movement. Mannerism has also been applied by analogy to the Silver Age of Latin literature.\n\nThe word \"mannerism\" derives from the Italian \"maniera\", meaning \"style\" or \"manner\". Like the English word \"style\", \"maniera\" can either indicate a specific type of style (a beautiful style, an abrasive style) or indicate an absolute that needs no qualification (someone \"has style\"). In the second edition of his \"Lives of the Most Excellent Painters, Sculptors, and Architects\" (1568), Giorgio Vasari used \"maniera\" in three different contexts: to discuss an artist's manner or method of working; to describe a personal or group style, such as the term \"maniera greca\" to refer to the Byzantine style or simply to the \"maniera\" of Michelangelo; and to affirm a positive judgment of artistic quality. Vasari was also a Mannerist artist, and he described the period in which he worked as \"la maniera moderna\", or the \"modern style\". James V. Mirollo describes how \"bella maniera\" poets attempted to surpass in virtuosity the sonnets of Petrarch. This notion of \"bella maniera\" suggests that artists who were thus inspired looked to copying and bettering their predecessors, rather than confronting nature directly. In essence, \"bella maniera\" utilized the best from a number of source materials, synthesizing it into something new.\n\nAs a stylistic label, \"Mannerism\" is not easily defined. It was used by Swiss historian Jacob Burckhardt and popularized by German art historians in the early 20th century to categorize the seemingly uncategorizable art of the Italian 16th century — art that was no longer found to exhibit the harmonious and rational approaches associated with the High Renaissance. “High Renaissance” connoted a period distinguished by harmony, grandeur and the revival of classical antiquity. The term Mannerist was redefined in 1967 by John Shearman following the exhibition of Mannerist paintings organised by Fritz Grossmann at Manchester City Art Gallery in 1965. The label “Mannerism” was used during the 16th century to comment on social behaviour and to convey a refined virtuoso quality or to signify a certain technique.\nHowever, for later writers, such as the 17th-century Gian Pietro Bellori, \"la maniera\" was a derogatory term for the perceived decline of art after Raphael, especially in the 1530s and 1540s. From the late 19th century on, art historians have commonly used the term to describe art that follows Renaissance classicism and precedes the Baroque.\n\nYet historians differ as to whether Mannerism is a style, a movement, or a period; and while the term remains controversial it is still commonly used to identify European art and culture of the 16th century.\n\nBy the end of the High Renaissance, young artists experienced a crisis: it seemed that everything that could be achieved was already achieved. No more difficulties, technical or otherwise, remained to be solved. The detailed knowledge of anatomy, light, physiognomy and the way in which humans register emotion in expression and gesture, the innovative use of the human form in figurative composition, the use of the subtle gradation of tone, all had reached near perfection. The young artists needed to find a new goal, and they sought new approaches. At this point Mannerism started to emerge. The new style developed between 1510 and 1520 either in Florence, or in Rome, or in both cities simultaneously.\n\nThis period has been described as a \"natural extension\" of the art of Andrea del Sarto, Michelangelo, and Raphael. Michelangelo developed his own style at an early age, a deeply original one which was greatly admired at first, then often copied and imitated by other artists of the era. One of the qualities most admired by his contemporaries was his \"terribilità\", a sense of awe-inspiring grandeur, and subsequent artists attempted to imitate it. Other artists learned Michelangelo's impassioned and highly personal style by copying the works of the master, a standard way that students learned to paint and sculpt. His Sistine Chapel ceiling provided examples for them to follow, in particular his representation of collected figures often called \"ignudi\" and of the Libyan Sibyl, his vestibule to the Laurentian Library, the figures on his Medici tombs, and above all his \"Last Judgment\". The later Michelangelo was one of the great role models of Mannerism. Young artists broke in to his house and stole drawings from him. In his book \"Lives of the Most Eminent Painters, Sculptors, and Architects\", Giorgio Vasari noted that Michelangelo stated once: \"Those who are followers can never pass by whom they follow\".\n\nThe competitive spirit was cultivated by patrons who encouraged sponsored artists to emphasize virtuosic technique and to compete with one another for commissions. It drove artists to look for new approaches and dramatically illuminated scenes, elaborate clothes and compositions, elongated proportions, highly stylized poses, and a lack of clear perspective. Leonardo da Vinci and Michelangelo were each given a commission by Gonfaloniere Piero Soderini to decorate a wall in the Hall of Five Hundred in Florence. These two artists were set to paint side by side and compete against each other, fueling the incentive to be as innovative as possible.\n\nThe early Mannerists in Florence—especially the students of Andrea del Sarto such as Jacopo da Pontormo and Rosso Fiorentino who are notable for elongated forms, precariously balanced poses, a collapsed perspective, irrational settings, and theatrical lighting. Parmigianino (a student of Correggio) and Giulio Romano (Raphael’s head assistant) were moving in similarly stylized aesthetic directions in Rome. These artists had matured under the influence of the High Renaissance, and their style has been characterized as a reaction to or exaggerated extension of it. Instead of studying nature directly, younger artists began studying Hellenistic sculpture and paintings of masters past. Therefore, this style is often identified as \"anti-classical”, yet at the time it was considered a natural progression from the High Renaissance. The earliest experimental phase of Mannerism, known for its \"anti-classical\" forms, lasted until about 1540 or 1550. Marcia B. Hall, professor of art history at Temple University, notes in her book \"After Raphael\" that Raphael's premature death marked the beginning of Mannerism in Rome.\n\nIn past analyses, it has been noted that mannerism arose in the early 16th century contemporaneously with a number of other social, scientific, religious and political movements such as the Copernican model, the Sack of Rome, and the Protestant Reformation's increasing challenge to the power of the Catholic Church. Because of this, the style's elongated forms and distorted forms were once interpreted as a reaction to the idealized compositions prevalent in High Renaissance art. This explanation for the radical stylistic shift c. 1520 has fallen out of scholarly favor, though early Mannerist art is still sharply contrasted with High Renaissance conventions; the accessibility and balance achieved by Raphael's \"School of Athens\" no longer seemed to interest young artists.\n\nThe second period of Mannerism is commonly differentiated from the earlier, so-called \"anti-classical\" phase.\nSubsequent mannerists stressed intellectual conceits and artistic virtuosity, features that have led later critics to accuse them of working in an unnatural and affected \"manner\" (\"maniera\"). Maniera artists looked to their older contemporary Michelangelo as their principal model; theirs was an art imitating art, rather than an art imitating nature. Art historian Sydney Joseph Freedberg argues that the intellectualizing aspect of maniera art involves expecting its audience to notice and appreciate this visual reference—a familiar figure in an unfamiliar setting enclosed between \"unseen, but felt, quotation marks\". The height of artifice is the Maniera painter's penchant for deliberately misappropriating a quotation. Agnolo Bronzino and Giorgio Vasari exemplify this strain of Maniera that lasted from about 1530 to 1580. Based largely at courts and in intellectual circles around Europe, Maniera art couples exaggerated elegance with exquisite attention to surface and detail: porcelain-skinned figures recline in an even, tempered light, acknowledging the viewer with a cool glance, if they make eye contact at all. The Maniera subject rarely displays much emotion, and for this reason works exemplifying this trend are often called 'cold' or 'aloof.' This is typical of the so-called \"stylish style\" or \"Maniera\" in its maturity.\n\nThe cities Rome, Florence, and Mantua were Mannerist centers in Italy. Venetian painting pursued a different course, represented by Titian in his long career. A number of the earliest Mannerist artists who had been working in Rome during the 1520s fled the city after the Sack of Rome in 1527. As they spread out across the continent in search of employment, their style was disseminated throughout Italy and Northern Europe. The result was the first international artistic style since the Gothic. Other parts of Northern Europe did not have the advantage of such direct contact with Italian artists, but the Mannerist style made its presence felt through prints and illustrated books. European rulers, among others, purchased Italian works, while northern European artists continued to travel to Italy, helping to spread the Mannerist style. Individual Italian artists working in the North gave birth to a movement known as the Northern Mannerism. Francis I of France, for example, was presented with Bronzino's \"Venus, Cupid, Folly and Time\". The style waned in Italy after 1580, as a new generation of artists, including the Carracci brothers, Caravaggio and Cigoli, revived naturalism. Walter Friedlaender identified this period as \"anti-mannerism\", just as the early mannerists were \"anti-classical\" in their reaction away from the aesthetic values of the High Renaissance and today the Carracci brothers and Caravaggio are agreed to have begun the transition to Baroque-style painting which was dominant by 1600.\n\nOutside of Italy, however, Mannerism continued into the 17th century. In France, where Rosso traveled to work for the court at Fontainebleau, it is known as the \"Henry II style\" and had a particular impact on architecture. Other important continental centers of Northern Mannerism include the court of Rudolf II in Prague, as well as Haarlem and Antwerp. Mannerism as a stylistic category is less frequently applied to English visual and decorative arts, where native labels such as \"Elizabethan\" and \"Jacobean\" are more commonly applied. Seventeenth-century Artisan Mannerism is one exception, applied to architecture that relies on pattern books rather than on existing precedents in Continental Europe.\n\nOf particular note is the Flemish influence at Fontainebleau that combined the eroticism of the French style with an early version of the vanitas tradition that would dominate seventeenth-century Dutch and Flemish painting. Prevalent at this time was the \"pittore vago,\" a description of painters from the north who entered the workshops in France and Italy to create a truly international style.\n\nAs in painting, early Italian Mannerist sculpture was very largely an attempt to find an original style that would top the achievement of the High Renaissance, which in sculpture essentially meant Michelangelo, and much of the struggle to achieve this was played out in commissions to fill other places in the Piazza della Signoria in Florence, next to Michelangelo's \"David\". Baccio Bandinelli took over the project of \"Hercules and Cacus\" from the master himself, but it was little more popular then than it is now, and maliciously compared by Benvenuto Cellini to \"a sack of melons\", though it had a long-lasting effect in apparently introducing relief panels on the pedestal of statues. Like other works of his and other Mannerists it removes far more of the original block than Michelangelo would have done. Cellini's bronze \"Perseus with the head of Medusa\" is certainly a masterpiece, designed with eight angles of view, another Mannerist characteristic, and artificially stylized in comparison with the \"David\"s of Michelangelo and Donatello. Originally a goldsmith, his famous gold and enamel Salt Cellar (1543) was his first sculpture, and shows his talent at its best.\n\nSmall bronze figures for collector's cabinets, often mythological subjects with nudes, were a popular Renaissance form at which Giambologna, originally Flemish but based in Florence, excelled in the later part of the century. He also created life-size sculptures, of which two entered the collection in the Piazza della Signoria. He and his followers devised elegant elongated examples of the \"figura serpentinata\", often of two intertwined figures, that were interesting from all angles.\n\nGiorgio Vasari's opinions about the art of painting emerge in the praise he bestows on fellow artists in his multi-volume \"Lives of the Artists\": he believed that excellence in painting demanded refinement, richness of invention (\"invenzione\"), expressed through virtuoso technique (\"maniera\"), and wit and study that appeared in the finished work, all criteria that emphasized the artist's intellect and the patron's sensibility. The artist was now no longer just a trained member of a local Guild of St Luke. Now he took his place at court alongside scholars, poets, and humanists, in a climate that fostered an appreciation for elegance and complexity. The coat-of-arms of Vasari's Medici patrons appears at the top of his portrait, quite as if it were the artist's own. The framing of the woodcut image of Vasari's \"Lives of the Artists\" would be called \"Jacobean\" in an English-speaking milieu. In it, Michelangelo's Medici tombs inspire the anti-architectural \"architectural\" features at the top, the papery pierced frame, the satyr nudes at the base. As a mere frame it is extravagant: Mannerist, in short.\n\nAnother literary figure from the period is Gian Paolo Lomazzo, who produced two works—one practical and one metaphysical—that helped define the Mannerist artist's self-conscious relation to his art. His \"Trattato dell'arte della pittura, scoltura et architettura\" (Milan, 1584) is in part a guide to contemporary concepts of decorum, which the Renaissance inherited in part from Antiquity but Mannerism elaborated upon. Lomazzo's systematic codification of aesthetics, which typifies the more formalized and academic approaches typical of the later 16th century, emphasized a consonance between the functions of interiors and the kinds of painted and sculpted decors that would be suitable. Iconography, often convoluted and abstruse, is a more prominent element in the Mannerist styles. His less practical and more metaphysical \"Idea del tempio della pittura\" (\"The ideal temple of painting\", Milan, 1590) offers a description along the lines of the \"four temperaments\" theory of human nature and personality, defining the role of individuality in judgment and artistic invention.\n\nJacopo da Pontormo's \"Joseph in Egypt\" features what would in the Renaissance have been considered incongruous colors and an incoherent handling of time and space.\n\nRosso Fiorentino, who had been a fellow pupil of Pontormo in the studio of Andrea del Sarto, in 1530 brought Florentine mannerism to Fontainebleau, where he became one of the founders of French 16th-century Mannerism, popularly known as the \"School of Fontainebleau\".\n\nThe examples of a rich and hectic decorative style at Fontainebleau further disseminated the Italian style through the medium of engravings, to Antwerp and from there throughout Northern Europe from London to Poland. Mannerist design was extended to luxury goods like silver and carved furniture. A sense of tense, controlled emotion expressed in elaborate symbolism and allegory, and an ideal of female beauty characterized by elongated proportions are features of this style.\n\nMannerist portraits by Agnolo Bronzino are distinguished by a serene elegance and meticulous attention to detail. As a result, Bronzino's sitters have been said to project an aloofness and marked emotional distance from the viewer. There is also a virtuosic concentration on capturing the precise pattern and sheen of rich textiles.\n\nAlessandro Allori's (1535–1607) \"Susanna and the Elders\" (\"below\") is distinguished by latent eroticism and consciously brilliant still life detail, in a crowded, contorted composition.\n\nTintoretto's \"Last Supper\" (below) focuses on light and motion, bringing the image to dramatic life. Unlike more traditional views of the Last Supper, Tintoretto depicts Heaven opening up into the room, and the angels looking on in awe, in line with the old Catholic maxim that \"If the angels were capable of envy, they would envy the Eucharist.\"\n\nEl Greco attempted to express religious emotion with exaggerated traits. After the realistic depiction of the human form and the mastery of perspective achieved in high Renaissance Classicism, some artists started to deliberately distort proportions in disjointed, irrational space for emotional and artistic effect. El Greco still is a deeply original artist. El Greco has been characterized by modern scholars as an artist so individual that he belongs to no conventional school. Key aspects of Mannerism in El Greco include the jarring \"acid\" palette, elongated and tortured anatomy, irrational perspective and light, and obscure and troubling iconography.\n\nBenvenuto Cellini created the Cellini Salt Cellar of gold and enamel in 1540 featuring Poseidon and Amphitrite (water and earth) placed in uncomfortable positions and with elongated proportions. It is considered a masterpiece of Mannerist sculpture.\n\nJoachim Wtewael (1566–1638) continued to paint in a Northern Mannerist style until the end of his life, ignoring the arrival of the Baroque, and making him perhaps the last significant Mannerist artist still to be working. His subjects included large scenes with still life in the manner of Pieter Aertsen, and mythological scenes, many small cabinet paintings beautifully executed on copper, and most featuring nudity.\n\nGiuseppe Arcimboldo (also spelled \"Arcimboldi\") is known for his portraits contrived from a still life composition\n\nMannerist architecture was characterized by visual trickery and unexpected elements that challenged the renaissance norms. Flemish artists, many of whom had traveled to Italy and were influenced by Mannerist developments there, were responsible for the spread of Mannerist trends into Europe north of the Alps, including into the realm of architecture. During the period, architects experimented with using architectural forms to emphasize solid and spatial relationships. The Renaissance ideal of harmony gave way to freer and more imaginative rhythms. The best known architect associated with the Mannerist style, and a pioneer at the Laurentian Library, was Michelangelo (1475–1564). He is credited with inventing the giant order, a large pilaster that stretches from the bottom to the top of a façade. He used this in his design for the Campidoglio in Rome.\n\nPrior to the 20th century, the term \"Mannerism\" had negative connotations, but it is now used to describe the historical period in more general non-judgmental terms. Mannerist architecture has also been used to describe a trend in the 1960s and 1970s that involved breaking the norms of modernist architecture while at the same time recognizing their existence. Defining mannerist in this context, architect and author Robert Venturi wrote \"Mannerism for architecture of our time that acknowledges conventional order rather than original expression but breaks the conventional order to accommodate complexity and contradiction and thereby engages ambiguity unambiguously.\" \n\nAn example of mannerist architecture is the Villa Farnese at Caprarola. in the rugged country side outside of Rome. The proliferation of engravers during the 16th century spread Mannerist styles more quickly than any previous styles.\n\nDense with ornament of \"Roman\" detailing, the display doorway at Colditz Castle exemplifies this northern style, characteristically applied as an isolated \"set piece\" against unpretentious vernacular walling.\n\nFrom the late 1560s onwards, many buildings in Valletta, the new capital city of Malta, were designed by the architect Girolamo Cassar in the Mannerist style. Such buildings include St. John's Co-Cathedral, the Grandmaster's Palace and the seven original auberges. Many of Cassar's buildings were modified over the years, especially in the Baroque period. However, a few buildings, such as Auberge d'Aragon and the exterior of St. John's Co-Cathedral, still retain most of Cassar's original Mannerist design.\n\nIn English literature, Mannerism is commonly identified with the qualities of the \"Metaphysical\" poets of whom the most famous is John Donne. The witty sally of a Baroque writer, John Dryden, against the verse of Donne in the previous generation, affords a concise contrast between Baroque and Mannerist aims in the arts:\n\nThe rich musical possibilities in the poetry of the late 16th and early 17th centuries provided an attractive basis for the madrigal, which quickly rose to prominence as the pre-eminent musical form in Italian musical culture, as discussed by Tim Carter:\n\nThe word Mannerism has also been used to describe the style of highly florid and contrapuntally complex polyphonic music made in France in the late 14th century. This period is now usually referred to as the \"ars subtilior\".\n\n\"The Early Commedia dell'Arte (1550–1621): The Mannerist Context\" by Paul Castagno discusses Mannerism's effect on the contemporary professional theatre. Castagno's was the first study to define a theatrical form as Mannerist, employing the vocabulary of Mannerism and maniera to discuss the typification, exaggerated, and effetto meraviglioso of the comici dell'arte. See Part II of the above book for a full discussion of Mannerist characteristics in the commedia dell'arte. The study is largely iconographic, presenting a pictorial evidence that many of the artists who painted or printed commedia images were in fact, coming from the workshops of the day, heavily ensconced in the maniera tradition.\n\nThe preciosity in Jacques Callot's minute engravings seem to belie a much larger scale of action. Callot's \"Balli di Sfessania\" (literally, dance of the buttocks) celebrates the commedia's blatant eroticism, with protruding phalli, spears posed with the anticipation of a comic ream, and grossly exaggerated masks that mix the bestial with human. The eroticism of the innamorate (lovers) including the baring of breasts, or excessive veiling, was quite in vogue in the paintings and engravings from the second school at Fontainebleau, particularly those that detect a Franco-Flemish influence. Castagno demonstrates iconographic linkages between genre painting and the figures of the commedia dell'arte that demonstrate how this theatrical form was embedded within the cultural traditions of the late cinquecento.\n\nImportant corollaries exist between the \"disegno interno\", which substituted for the \"disegno esterno\" (external design) in mannerist painting. This notion of projecting a deeply subjective view as superseding nature or established principles (perspective, for example), in essence, the emphasis away from the object to its subject, now emphasizing execution, displays of virtuosity, or unique techniques. This inner vision is at the heart of commedia performance. For example, in the moment of improvisation the actor expresses his virtuosity without heed to formal boundaries, decorum, unity, or text. Arlecchino became emblematic of the mannerist \"discordia concors\" (the union of opposites), at one moment he would be gentle and kind, then, on a dime, become a thief violently acting out with his battle. Arlecchino could be graceful in movement, only in the next beat, to clumsily trip over his feet. Freed from the external rules, the actor celebrated the evanescence of the moment; much the way Cellini would dazzle his patrons by draping his sculptures, unveiling them with lighting effects and a sense of the marvelous. The presentation of the object became as important as the object itself.\n\nAccording to art critic Jerry Saltz, \"Neo-Mannerism\" (new Mannerism) is among several clichés that are \"squeezing the life out of the art world\". Neo-Mannerism describes art of the 21st century that is turned out by students whose academic teachers \"have scared [them] into being pleasingly meek, imitative, and ordinary\".\n\n\n"}
{"id": "19948", "url": "https://en.wikipedia.org/wiki?curid=19948", "title": "Monica Lewinsky", "text": "Monica Lewinsky\n\nMonica Samille Lewinsky (born July 23, 1973) is an American activist, television personality, fashion designer, and former White House intern.\n\nPresident Bill Clinton admitted to having had what he called an \"inappropriate relationship\" with Lewinsky while she worked at the White House in 1995–1996. The affair and its repercussions (which included Clinton's impeachment) became known later as the Clinton–Lewinsky scandal.\n\nAs a result of the public coverage of the political scandal, Lewinsky gained international celebrity status; she subsequently engaged in a variety of ventures that included designing a line of handbags under her name, being an advertising spokesperson for a diet plan, and working as a television personality.\n\nLewinsky then decided to leave the public spotlight to pursue a master's degree in psychology in London. In 2014, she returned to public view as a social activist speaking out against cyberbullying, from which she personally suffered when publicly ridiculed on the Internet regarding the scandal.\n\nLewinsky was born in San Francisco, California, and grew up in an affluent family in Southern California in the Westside Brentwood area of Los Angeles and in Beverly Hills. Her father is Bernard Lewinsky, an oncologist, who is the son of German Jews who escaped from Nazi Germany and moved to El Salvador and then to the United States when he was 14. Her mother, born Marcia Kay Vilensky, is an author who uses the name Marcia Lewis. In 1996, she wrote her only book, the gossip biography, \"The Private Lives of the Three Tenors\". During the Lewinsky scandal, the press compared Lewis' unproven \"hints\" that she had an affair with opera star Plácido Domingo to her daughter's sexual relationship with Clinton. Monica's maternal grandfather, Samuel M. Vilensky, was a Lithuanian Jew, and Monica's maternal grandmother, Bronia Poleshuk, was born in the British Concession of Tianjin, China, to a Russian Jewish family. Monica's parents' acrimonious separation and divorce during 1987 and 1988 had a significant effect on her. Her father later married his current wife, Barbara; her mother later married R. Peter Straus, a media executive and former director of the Voice of America under President Jimmy Carter.\n\nThe family attended Sinai Temple in Los Angeles and Monica attended Sinai Akiba Academy, its religious school. For her primary education she attended the John Thomas Dye School in Bel-Air. She then attended Beverly Hills High School, but for her senior year transferred to, and graduated from, Bel Air Prep (later known as Pacific Hills School) in 1991.\n\nFollowing high school graduation, Lewinsky attended Santa Monica College, a two-year community college, and worked for the drama department at Beverly Hills High School and at a tie shop. In 1992, she allegedly began a five-year affair with Andy Bleiler, her married former high school drama instructor. In 1993, she enrolled at Lewis & Clark College in Portland, Oregon, graduating with a bachelor's degree in psychology in 1995.\n\nWith the assistance of a family connection, Lewinsky got an unpaid summer White House internship in the office of White House Chief of Staff Leon Panetta. Lewinsky moved to Washington, D.C. and took up the position in July 1995. She moved to a paid position in the White House Office of Legislative Affairs in December 1995.\n\nLewinsky stated that between November 1995 and March 1997, she had nine sexual encounters in the Oval Office with then-President Bill Clinton. According to her testimony, these involved fellatio and other sexual acts, but not sexual intercourse.\n\nClinton had previously been confronted with allegations of sexual misconduct during his time as Governor of Arkansas. Former Arkansas state employee Paula Jones filed a civil lawsuit against him; she alleged that he had sexually harassed her. Lewinsky's name surfaced during the discovery phase of Jones' case, when Jones' lawyers sought to show a pattern of behavior by Clinton that involved inappropriate sexual relationships with other government employees.\n\nIn April 1996, Lewinsky's superiors transferred her from the White House to the Pentagon because they felt she was spending too much time around Clinton. At the Pentagon, she worked as an assistant to chief Pentagon spokesperson Kenneth Bacon. Lewinsky told co-worker Linda Tripp about her relationship with the President. Beginning in September 1997, Tripp began secretly recording their telephone conversations regarding the affair with Clinton. In December 1997, Lewinsky left the Pentagon position. In January 1998, after Lewinsky had submitted an affidavit in the Paula Jones case denying any physical relationship with Clinton, and had attempted to persuade Tripp to lie under oath in that case, Tripp gave the tapes to Independent Counsel Kenneth Starr, adding to his ongoing investigation into the Whitewater controversy. Starr then broadened his investigation beyond the Arkansas land use deal to include Lewinsky, Clinton, and others for possible perjury and subornation of perjury in the Jones case. Tripp reported the taped conversations to literary agent Lucianne Goldberg. She also convinced Lewinsky to save the gifts that Clinton had given her during their relationship, and not to dry clean what would later become known as \"the blue dress\". Under oath, Clinton denied having had \"a sexual affair\", \"sexual relations\", or \"a sexual relationship\" with Lewinsky.\n\nNews of the Clinton–Lewinsky relationship broke in January 1998. On January 26, 1998, Clinton stated, \"I did not have sexual relations with that woman, Miss Lewinsky\" in a nationally televised White House news conference. The matter instantly occupied the news media, and Lewinsky spent the next weeks hiding from public attention in her mother's residence at the Watergate complex. News of Lewinsky's affair with Bleiler also came to light, and he turned over to Starr various souvenirs, photographs, and documents that Lewinsky had sent him and his wife during the time she was in the White House.\n\nClinton had also said, \"there is not a sexual relationship, an improper sexual relationship or any other kind of improper relationship\" which he defended as truthful on August 17, 1998 because of his use of the present tense, famously arguing \"it depends on what the meaning of the word 'is' is\" (i.e., he was not, at the time he made that statement, still in a sexual relationship with Lewinsky). Under pressure from Starr, who had obtained from Lewinsky a blue dress with Clinton's semen stain, as well as testimony from Lewinsky that the President had inserted a cigar into her vagina, Clinton stated, \"I did have a relationship with Miss Lewinsky that was not appropriate.\" Clinton denied having committed perjury because, according to Clinton, the legal definition of oral sex was not encompassed by \"sex\" \"per se\". In addition, relying upon the definition of \"sexual relations\" as proposed by the prosecution and agreed by the defense and by Judge Susan Webber Wright, who was hearing the Paula Jones case, Clinton claimed that because certain acts were performed on him, not by him, he did not engage in sexual relations. Lewinsky's testimony to the Starr Commission, however, contradicted Clinton's claim of being totally passive in their encounters.\n\nClinton and Lewinsky were both called before a grand jury; Clinton testified via closed-circuit television, Lewinsky in person. She was granted transactional immunity by the United States Office of the Independent Counsel, in exchange for her testimony.\n\nThe affair led to pop culture celebrity for Lewinsky, as she had become the focus of a political storm. Her immunity agreement restricted what she could talk about publicly, but she was able to cooperate with Andrew Morton in his writing of \"Monica's Story\", her biography which included her side of the Clinton affair. The book was published in March 1999; it was also excerpted as a cover story in \"Time\" magazine. On March 3, 1999, Barbara Walters interviewed Lewinsky on ABC's \"20/20\". The program was watched by 70 million Americans, which ABC said was a record for a news show. Lewinsky made about $500,000 from her participation in the book and another $1 million from international rights to the Walters interview, but was still beset by high legal bills and living costs.\n\nIn June 1999, \"Ms.\" magazine published a series of articles by writer Susan Jane Gilman, sexologist Susie Bright, and author-host Abiola Abrams arguing from three generations of women whether Lewinsky's behavior had any meaning for feminism. Also in 1999, Lewinsky declined to sign an autograph in an airport, saying, \"I'm kind of known for something that's not so great to be known for.\" She made a cameo appearance as herself in two sketches during the May 8, 1999, episode of NBC's \"Saturday Night Live\", a program that had lampooned her relationship with Clinton over the prior 16 months.\n\nBy her own account, Lewinsky had survived the intense media attention during the scandal period by knitting. In September 1999, she took this interest further by beginning to sell a line of handbags bearing her name, under the company name The Real Monica, Inc. They were sold online as well as at Henri Bendel in New York, Fred Segal in California, and The Cross in London. Lewinsky designed the bags—described by \"New York\" magazine as \"hippie-ish, reversible totes\"—and traveled frequently to supervise their manufacture in Louisiana.\n\nAt the start of 2000, Lewinsky began appearing in television commercials for the diet company Jenny Craig, Inc. The $1 million endorsement deal, which required Lewinsky to lose 40 or more pounds in six months, gained considerable publicity at the time. Lewinsky said that despite her desire to return to a more private life, she needed the money to pay off legal fees, and she believed in the product. A Jenny Craig spokesperson said of Lewinsky, \"She represents a busy active woman of today with a hectic lifestyle. And she has had weight issues and weight struggles for a long time. That represents a lot of women in America.\" The choice of Lewinsky as a role model proved controversial for Jenny Craig, and some of its private franchises switched to an older advertising campaign. The company stopped running the Lewinsky ads in February 2000, concluded her campaign entirely in April 2000, and paid her only $300,000 of the $1 million contracted for her involvement.\n\nAlso at the start of 2000, Lewinsky moved to New York City, lived in the West Village, and became an A-list guest in the Manhattan social scene. In February 2000, she appeared on MTV's \"The Tom Green Show\", in an episode in which the host took her to his parents' home in Ottawa in search of fabric for her new handbag business. Later in 2000, Lewinsky worked as a correspondent for Channel 5 in the UK, on the show \"Monica's Postcards\", reporting on U.S. culture and trends from a variety of locations.\n\nIn March 2002, Lewinsky, no longer bound by the terms of her immunity agreement, appeared in the HBO special, \"Monica in Black and White\", part of the \"America Undercover\" series. In it she answered a studio audience's questions about her life and the Clinton affair.\n\nLewinsky hosted the reality television dating program, \"Mr. Personality\", on Fox Television Network in 2003, where she advised young women contestants who were picking men hidden by masks. Some Americans tried to organize a boycott of advertisers on the show, to protest Lewinsky's capitalizing on her notoriety. Nevertheless, the show debuted to very high ratings, and Alessandra Stanley wrote in \"The New York Times\": \"after years of trying to cash in on her fame by designing handbags and other self-marketing schemes, Ms. Lewinsky has finally found a fitting niche on television.\" The ratings, however, slid downward each successive week, and after the show completed its initial limited run, it did not reappear. The same year she appeared as a guest on the programs \"V Graham Norton\" in the UK, \"High Chaparall\" in Sweden, and \"The View\" and \"Jimmy Kimmel Live!\" in the U.S.\n\nAfter Clinton's autobiography, \"My Life\", appeared in 2004, Lewinsky said in an interview with the British tabloid \"Daily Mail\":\nBy 2005, Lewinsky found that she could not escape the spotlight in the U.S., which made both her professional and personal life difficult. She stopped selling her handbag line and moved to London to study social psychology at the London School of Economics. In December 2006, Lewinsky graduated with a Master of Science degree. Her thesis was titled, \"In Search of the Impartial Juror: An Exploration of the Third-Person Effect and Pre-Trial Publicity.\" For the next decade she tried to avoid publicity.\n\nLewinsky did correspond in 2009 with scholar Ken Gormley, who was writing an in-depth study of the Clinton scandals, maintaining that Clinton had lied under oath when asked detailed and specific questions about his relationship with her. In 2013, the items associated with Lewinsky that Bleiler had turned over to Starr were put up for auction by Bleiler's ex-wife, who had come into possession of them.\n\nDuring her decade out of the public eye, Lewinsky lived in London, Los Angeles, New York, and Portland but, due to her notoriety, had trouble finding employment in the communications and marketing jobs for nonprofit organizations where she had been interviewed.\n\nIn May 2014, Lewinsky wrote an essay for \"Vanity Fair\" magazine titled \"Shame and Survival\", wherein she discussed her life and the scandal. She continued to maintain that the relationship was mutual and wrote that while Clinton took advantage of her, it was a consensual relationship. She added: \"I, myself, deeply regret what happened between me and President Clinton. Let me say it again: I. Myself. Deeply. Regret. What. Happened.\" However, she said it was now time to \"stick my head above the parapet so that I can take back my narrative and give a purpose to my past.\" The magazine later announced her as a \"Vanity Fair\" contributor, stating she would \"contribute to their website on an ongoing basis, on the lookout for relevant topics of interest\".\nIn July 2014, Lewinsky was interviewed in a three-part television special for the National Geographic Channel, titled \"The 90s: The Last Great Decade\". The series looked at various events of the 1990s, including the scandal that brought Lewinsky into the national spotlight. This was Lewinsky's first such interview in more than ten years.\n\nIn October 2014, she took a public stand against cyberbullying, calling herself \"patient zero\" of online harassment. Speaking at a \"Forbes\" magazine \"30 Under 30\" summit about her experiences in the aftermath of the scandal, she said, \"Having survived myself, what I want to do now is help other victims of the shame game survive, too.\" She said she was influenced by reading about the suicide of Tyler Clementi, a Rutgers University freshman, involving cyberbullying and joined Twitter to facilitate her efforts. In March 2015, Lewinsky continued to speak out publicly against cyberbullying, delivering a TED talk calling for a more compassionate Internet. In June 2015, she became an ambassador and strategic advisor for anti-bullying organization Bystander Revolution. The same month, she gave an anti-cyberbullying speech at the Cannes Lions International Festival of Creativity. In September 2015, Lewinsky was interviewed by Amy Robach on \"Good Morning America\", about Bystander Revolution's Month of Action campaign for National Bullying Prevention Month. Lewinsky wrote the foreword to an October 2017 book by Sue Scheff and Melissa Schorr, \"Shame Nation: The Global Epidemic of Online Hate\".\n\nIn October 2017, Lewinsky tweeted the #MeToo hashtag to indicate that she was a victim of sexual harassment and/or sexual assault, but did not provide details. She wrote an essay in the March 2018 issue of \"Vanity Fair\" in which she did not directly explain why she used the #MeToo hashtag in October, but she did write that although her relationship with Bill Clinton was consensual, because he was 27 years older than she was and in a position with a lot more power than she had, in her opinion now the relationship constituted an \"abuse of power\" on Clinton's part. She added that she had been diagnosed with post-traumatic stress disorder due to the experiences involved after the relationship was disclosed. In May 2018, Lewinsky was disinvited from an event hosted by \"Town & Country\" when Bill Clinton accepted an invitation to the event.\n\nIn September 2018, Lewinsky spoke at a conference in Jerusalem. Following her speech, she sat for a Q&A session with the host, journalist Yonit Levi. The first question Levi asked was whether Lewinsky thinks that Clinton owes her a private apology. Lewinsky refused to answer the question, and walked off the stage. She later tweeted that the question was posed in a pre-event meeting with Levi, and Lewinsky told her that such a question was off limits. A spokesman for the Israel Television News Company, which hosted the conference and is Levi's employer, responded that Levi had kept all the agreements she made with Lewinsky and honored her requests.\n\n\n"}
{"id": "19951", "url": "https://en.wikipedia.org/wiki?curid=19951", "title": "Pressure measurement", "text": "Pressure measurement\n\nPressure measurement is the analysis of an applied force by a fluid (liquid or gas) on a surface. Pressure is typically measured in units of force per unit of surface area. Many techniques have been developed for the measurement of pressure and vacuum. Instruments used to measure and display pressure in an integral unit are called pressure gauges or vacuum gauges. A manometer (not to be confused with nanometer) is a good example, as it uses a column of liquid to both measure and indicate pressure. Likewise the widely used Bourdon gauge is a mechanical device, which both measures and indicates and is probably the best known type of gauge.\n\nA vacuum gauge is a pressure gauge used to measure pressures lower than the ambient atmospheric pressure, which is set as the zero point, in negative values (e.g.: −15 psig or −760 mmHg equals total vacuum). Most gauges measure pressure relative to atmospheric pressure as the zero point, so this form of reading is simply referred to as \"gauge pressure\". However, anything greater than total vacuum is technically a form of pressure. For very accurate readings, especially at very low pressures, a gauge that uses total vacuum as the zero point may be used, giving pressure readings in an absolute scale.\n\nOther methods of pressure measurement involve sensors that can transmit the pressure reading to a remote indicator or control system (telemetry).\n\nEveryday pressure measurements, such as for vehicle tire pressure, are usually made relative to ambient air pressure. In other cases measurements are made relative to a vacuum or to some other specific reference. When distinguishing between these zero references, the following terms are used:\n\nThe zero reference in use is usually implied by context, and these words are added only when clarification is needed. Tire pressure and blood pressure are gauge pressures by convention, while atmospheric pressures, deep vacuum pressures, and altimeter pressures must be absolute.\n\nFor most working fluids where a fluid exists in a closed system, gauge pressure measurement prevails. Pressure instruments connected to the system will indicate pressures relative to the current atmospheric pressure. The situation changes when extreme vacuum pressures are measured, then absolute pressures are typically used instead.\n\nDifferential pressures are commonly used in industrial process systems. Differential pressure gauges have two inlet ports, each connected to one of the volumes whose pressure is to be monitored. In effect, such a gauge performs the mathematical operation of subtraction through mechanical means, obviating the need for an operator or control system to watch two separate gauges and determine the difference in readings.\n\nModerate vacuum pressure readings can be ambiguous without the proper context, as they may represent absolute pressure or gauge pressure without a negative sign. Thus a vacuum of 26 inHg gauge is equivalent to an absolute pressure of 30 inHg (typical atmospheric pressure) − 26 inHg = 4 inHg.\n\nAtmospheric pressure is typically about 100 kPa at sea level, but is variable with altitude and weather. If the absolute pressure of a fluid stays constant, the gauge pressure of the same fluid will vary as atmospheric pressure changes. For example, when a car drives up a mountain, the (gauge) tire pressure goes up because atmospheric pressure goes down. The absolute pressure in the tire is essentially unchanged.\n\nUsing atmospheric pressure as reference is usually signified by a \"g\" for gauge after the pressure unit, e.g. 70 psig, which means that the pressure measured is the total pressure minus atmospheric pressure. There are two types of gauge reference pressure: vented gauge (vg) and sealed gauge (sg).\n\nA vented-gauge pressure transmitter, for example, allows the outside air pressure to be exposed to the negative side of the pressure-sensing diaphragm, through a vented cable or a hole on the side of the device, so that it always measures the pressure referred to ambient barometric pressure. Thus a vented-gauge reference pressure sensor should always read zero pressure when the process pressure connection is held open to the air.\n\nA sealed gauge reference is very similar, except that atmospheric pressure is sealed on the negative side of the diaphragm. This is usually adopted on high pressure ranges, such as hydraulics, where atmospheric pressure changes will have a negligible effect on the accuracy of the reading, so venting is not necessary. This also allows some manufacturers to provide secondary pressure containment as an extra precaution for pressure equipment safety if the burst pressure of the primary pressure sensing diaphragm is exceeded.\n\nThere is another way of creating a sealed gauge reference, and this is to seal a high vacuum on the reverse side of the sensing diaphragm. Then the output signal is offset, so the pressure sensor reads close to zero when measuring atmospheric pressure.\n\nA sealed gauge reference pressure transducer will never read exactly zero because atmospheric pressure is always changing and the reference in this case is fixed at 1 bar.\n\nTo produce an absolute pressure sensor, the manufacturer seals a high vacuum behind the sensing diaphragm. If the process-pressure connection of an absolute-pressure transmitter is open to the air, it will read the actual barometric pressure.\n\nThe SI unit for pressure is the pascal (Pa), equal to one newton per square metre (N·m or kg·m·s). This special name for the unit was added in 1971; before that, pressure in SI was expressed in units such as N·m. When indicated, the zero reference is stated in parenthesis following the unit, for example 101 kPa (abs). The pound per square inch (psi) is still in widespread use in the US and Canada, for measuring, for instance, tire pressure. A letter is often appended to the psi unit to indicate the measurement's zero reference; psia for absolute, psig for gauge, psid for differential, although this practice is discouraged by the NIST.\n\nBecause pressure was once commonly measured by its ability to displace a column of liquid in a manometer, pressures are often expressed as a depth of a particular fluid (\"e.g.,\" inches of water). Manometric measurement is the subject of pressure head calculations. The most common choices for a manometer's fluid are mercury (Hg) and water; water is nontoxic and readily available, while mercury's density allows for a shorter column (and so a smaller manometer) to measure a given pressure. The abbreviation \"W.C.\" or the words \"water column\" are often printed on gauges and measurements that use water for the manometer.\n\nFluid density and local gravity can vary from one reading to another depending on local factors, so the height of a fluid column does not define pressure precisely. So measurements in \"millimetres of mercury\" or \"inches of mercury\" can be converted to SI units as long as attention is paid to the local factors of fluid density and gravity. Temperature fluctuations change the value of fluid density, while location can affect gravity.\n\nAlthough no longer preferred, these manometric units are still encountered in many fields. Blood pressure is measured in millimetres of mercury (see torr) in most of the world, central venous pressure and lung pressures in centimeters of water are still common, as in settings for CPAP machines. Natural gas pipeline pressures are measured in inches of water, expressed as \"inches W.C.\" Scuba divers often use a manometric rule of thumb: the pressure exerted by ten meters depth of sea water (\"10 msw\") is approximately equal to one atmosphere. In vacuum systems, the units torr (millimeter of mercury), micron (micrometer of mercury), and inch of mercury (inHg) are most commonly used. Torr and micron usually indicates an absolute pressure, while inHg usually indicates a gauge pressure.\n\nAtmospheric pressures are usually stated using hectopascal (hPa), kilopascal (kPa), millibar (mbar) or atmospheres (atm). In American and Canadian engineering, stress is often measured in kip. Note that stress is not a true pressure since it is not scalar. In the cgs system the unit of pressure was the barye (ba), equal to 1 dyn·cm. In the mts system, the unit of pressure was the pieze, equal to 1 sthene per square metre.\n\nMany other hybrid units are used such as mmHg/cm or grams-force/cm (sometimes as kg/cm without properly identifying the force units). Using the names kilogram, gram, kilogram-force, or gram-force (or their symbols) as a unit of force is prohibited in SI; the unit of force in SI is the newton (N).\n\nStatic pressure is uniform in all directions, so pressure measurements are independent of direction in an immovable (static) fluid. Flow, however, applies additional pressure on surfaces perpendicular to the flow direction, while having little impact on surfaces parallel to the flow direction. This directional component of pressure in a moving (dynamic) fluid is called dynamic pressure. An instrument facing the flow direction measures the sum of the static and dynamic pressures; this measurement is called the total pressure or stagnation pressure. Since dynamic pressure is referenced to static pressure, it is neither gauge nor absolute; it is a differential pressure.\n\nWhile static gauge pressure is of primary importance to determining net loads on pipe walls, dynamic pressure is used to measure flow rates and airspeed. Dynamic pressure can be measured by taking the differential pressure between instruments parallel and perpendicular to the flow. Pitot-static tubes, for example perform this measurement on airplanes to determine airspeed. The presence of the measuring instrument inevitably acts to divert flow and create turbulence, so its shape is critical to accuracy and the calibration curves are often non-linear.\n\n\nMany instruments have been invented to measure pressure, with different advantages and disadvantages. Pressure range, sensitivity, dynamic response and cost all vary by several orders of magnitude from one instrument design to the next. The oldest type is the liquid column (a vertical tube filled with mercury) manometer invented by Evangelista Torricelli in 1643. The U-Tube was invented by Christiaan Huygens in 1661.\n\nHydrostatic gauges (such as the mercury column manometer) compare pressure to the hydrostatic force per unit area at the base of a column of fluid. Hydrostatic gauge measurements are independent of the type of gas being measured, and can be designed to have a very linear calibration. They have poor dynamic response.\n\nPiston-type gauges counterbalance the pressure of a fluid with a spring (for example tire-pressure gauges of comparatively low accuracy) or a solid weight, in which case it is known as a deadweight tester and may be used for calibration of other gauges.\n\nLiquid-column gauges consist of a column of liquid in a tube whose ends are exposed to different pressures. The column will rise or fall until its weight (a force applied due to gravity) is in equilibrium with the pressure differential between the two ends of the tube (a force applied due to fluid pressure). A very simple version is a U-shaped tube half-full of liquid, one side of which is connected to the region of interest while the reference pressure (which might be the atmospheric pressure or a vacuum) is applied to the other. The difference in liquid levels represents the applied pressure. The pressure exerted by a column of fluid of height \"h\" and density \"ρ\" is given by the hydrostatic pressure equation, \"P\" = \"hgρ\". Therefore, the pressure difference between the applied pressure \"P\" and the reference pressure \"P\" in a U-tube manometer can be found by solving . In other words, the pressure on either end of the liquid (shown in blue in the figure) must be balanced (since the liquid is static), and so .\n\nIn most liquid-column measurements, the result of the measurement is the height \"h\", expressed typically in mm, cm, or inches. The \"h\" is also known as the pressure head. When expressed as a pressure head, pressure is specified in units of length and the measurement fluid must be specified. When accuracy is critical, the temperature of the measurement fluid must likewise be specified, because liquid density is a function of temperature. So, for example, pressure head might be written \"742.2 mm\" or \"4.2 in at 59 °F\" for measurements taken with mercury or water as the manometric fluid respectively. The word \"gauge\" or \"vacuum\" may be added to such a measurement to distinguish between a pressure above or below the atmospheric pressure. Both mm of mercury and inches of water are common pressure heads, which can be converted to S.I. units of pressure using unit conversion and the above formulas.\n\nIf the fluid being measured is significantly dense, hydrostatic corrections may have to be made for the height between the moving surface of the manometer working fluid and the location where the pressure measurement is desired, except when measuring differential pressure of a fluid (for example, across an orifice plate or venturi), in which case the density ρ should be corrected by subtracting the density of the fluid being measured.\n\nAlthough any fluid can be used, mercury is preferred for its high density (13.534 g/cm) and low vapour pressure. For low pressure differences, light oil or water are commonly used (the latter giving rise to units of measurement such as inches water gauge and millimetres HO. Liquid-column pressure gauges have a highly linear calibration. They have poor dynamic response because the fluid in the column may react slowly to a pressure change.\n\nWhen measuring vacuum, the working liquid may evaporate and contaminate the vacuum if its vapor pressure is too high. When measuring liquid pressure, a loop filled with gas or a light fluid can isolate the liquids to prevent them from mixing, but this can be unnecessary, for example, when mercury is used as the manometer fluid to measure differential pressure of a fluid such as water. Simple hydrostatic gauges can measure pressures ranging from a few torrs (a few 100 Pa) to a few atmospheres (approximately ).\n\nA single-limb liquid-column manometer has a larger reservoir instead of one side of the U-tube and has a scale beside the narrower column. The column may be inclined to further amplify the liquid movement. Based on the use and structure, following types of manometers are used\n\nA McLeod gauge isolates a sample of gas and compresses it in a modified mercury manometer until the pressure is a few millimetres of mercury. The technique is very slow and unsuited to continual monitoring, but is capable of good accuracy. Unlike other manometer gauges, the McLeod gauge reading is dependent on the composition of the gas, since the interpretation relies on the sample compressing as an ideal gas. Due to the compression process, the McLeod gauge completely ignores partial pressures from non-ideal vapors that condense, such as pump oils, mercury, and even water if compressed enough.\n\n0.1 mPa is the lowest direct measurement of pressure that is possible with current technology. Other vacuum gauges can measure lower pressures, but only indirectly by measurement of other pressure-dependent properties. These indirect measurements must be calibrated to SI units by a direct measurement, most commonly a McLeod gauge.\n\nAneroid gauges are based on a metallic pressure-sensing element that flexes elastically under the effect of a pressure difference across the element. \"Aneroid\" means \"without fluid\", and the term originally distinguished these gauges from the hydrostatic gauges described above. However, aneroid gauges can be used to measure the pressure of a liquid as well as a gas, and they are not the only type of gauge that can operate without fluid. For this reason, they are often called mechanical gauges in modern language. Aneroid gauges are not dependent on the type of gas being measured, unlike thermal and ionization gauges, and are less likely to contaminate the system than hydrostatic gauges. The pressure sensing element may be a Bourdon tube, a diaphragm, a capsule, or a set of bellows, which will change shape in response to the pressure of the region in question. The deflection of the pressure sensing element may be read by a linkage connected to a needle, or it may be read by a secondary transducer. The most common secondary transducers in modern vacuum gauges measure a change in capacitance due to the mechanical deflection. Gauges that rely on a change in capacitance are often referred to as capacitance manometers.\n\nThe Bourdon pressure gauge uses the principle that a flattened tube tends to straighten or regain its circular form in cross-section when pressurized. This change in cross-section may be hardly noticeable, involving moderate stresses within the elastic range of easily workable materials. The strain of the material of the tube is magnified by forming the tube into a C shape or even a helix, such that the entire tube tends to straighten out or uncoil elastically as it is pressurized. Eugène Bourdon patented his gauge in France in 1849, and it was widely adopted because of its superior sensitivity, linearity, and accuracy; Edward Ashcroft purchased Bourdon's American patent rights in 1852 and became a major manufacturer of gauges. Also in 1849, Bernard Schaeffer in Magdeburg, Germany patented a successful diaphragm (see below) pressure gauge, which, together with the Bourdon gauge, revolutionized pressure measurement in industry. But in 1875 after Bourdon's patents expired, his company Schaeffer and Budenberg also manufactured Bourdon tube gauges.\n\nIn practice, a flattened thin-wall, closed-end tube is connected at the hollow end to a fixed pipe containing the fluid pressure to be measured. As the pressure increases, the closed end moves in an arc, and this motion is converted into the rotation of a (segment of a) gear by a connecting link that is usually adjustable. A small-diameter pinion gear is on the pointer shaft, so the motion is magnified further by the gear ratio. The positioning of the indicator card behind the pointer, the initial pointer shaft position, the linkage length and initial position, all provide means to calibrate the pointer to indicate the desired range of pressure for variations in the behavior of the Bourdon tube itself. Differential pressure can be measured by gauges containing two different Bourdon tubes, with connecting linkages.\n\nBourdon tubes measure gauge pressure, relative to ambient atmospheric pressure, as opposed to absolute pressure; vacuum is sensed as a reverse motion. Some aneroid barometers use Bourdon tubes closed at both ends (but most use diaphragms or capsules, see below). When the measured pressure is rapidly pulsing, such as when the gauge is near a reciprocating pump, an orifice restriction in the connecting pipe is frequently used to avoid unnecessary wear on the gears and provide an average reading; when the whole gauge is subject to mechanical vibration, the entire case including the pointer and indicator card can be filled with an oil or glycerin. Tapping on the face of the gauge is not recommended as it will tend to falsify actual readings initially presented by the gauge. The Bourdon tube is separate from the face of the gauge and thus has no effect on the actual reading of pressure. Typical high-quality modern gauges provide an accuracy of ±2% of span, and a special high-precision gauge can be as accurate as 0.1% of full scale.\n\nIn the following illustrations the transparent cover face of the pictured combination pressure and vacuum gauge has been removed and the mechanism removed from the case. This particular gauge is a combination vacuum and pressure gauge used for automotive diagnosis:\n\nStationary parts:\n\nMoving Parts:\n\nA second type of aneroid gauge uses deflection of a flexible membrane that separates regions of different pressure. The amount of deflection is repeatable for known pressures so the pressure can be determined by using calibration. The deformation of a thin diaphragm is dependent on the difference in pressure between its two faces. The reference face can be open to atmosphere to measure gauge pressure, open to a second port to measure differential pressure, or can be sealed against a vacuum or other fixed reference pressure to measure absolute pressure. The deformation can be measured using mechanical, optical or capacitive techniques. Ceramic and metallic diaphragms are used.\nFor absolute measurements, welded pressure capsules with diaphragms on either side are often used.\n\nshape:\n\nIn gauges intended to sense small pressures or pressure differences, or require that an absolute pressure be measured, the gear train and needle may be driven by an enclosed and sealed bellows chamber, called an aneroid, which means \"without liquid\". (Early barometers used a column of liquid such as water or the liquid metal mercury suspended by a vacuum.) This bellows configuration is used in aneroid barometers (barometers with an indicating needle and dial card), altimeters, altitude recording barographs, and the altitude telemetry instruments used in weather balloon radiosondes. These devices use the sealed chamber as a reference pressure and are driven by the external pressure. Other sensitive aircraft instruments such as air speed indicators and rate of climb indicators (variometers) have connections both to the internal part of the aneroid chamber and to an external enclosing chamber.\n\nThese gauges use the attraction of two magnets to translate differential pressure into motion of a dial pointer. As differential pressure increases, a magnet attached to either a piston or rubber diaphragm moves. A rotary magnet that is attached to a pointer then moves in unison. To create different pressure ranges, the spring rate can be increased or decreased.\n\nThe spinning-rotor gauge works by measuring the amount a rotating ball is slowed by the viscosity of the gas being measured. The ball is made of steel and is magnetically levitated inside a steel tube closed at one end and exposed to the gas to be measured at the other. The ball is brought up to speed (about 2500 rad/s), and the speed measured after switching off the drive, by electromagnetic transducers. The range of the instrument is 10 to 10 Pa (10 Pa with less accuracy). It is accurate and stable enough to be used as a secondary standard. The instrument requires some skill and knowledge to use correctly. Various corrections must be applied and the ball must be spun at a pressure well below the intended measurement pressure for five hours before using. It is most useful in calibration and research laboratories where high accuracy is required and qualified technicians are available.\n\n\nGenerally, as a real gas increases in density -which may indicate an increase in pressure- its ability to conduct heat increases. In this type of gauge, a wire filament is heated by running current through it. A thermocouple or resistance thermometer (RTD) can then be used to measure the temperature of the filament. This temperature is dependent on the rate at which the filament loses heat to the surrounding gas, and therefore on the thermal conductivity. A common variant is the Pirani gauge, which uses a single platinum filament as both the heated element and RTD. These gauges are accurate from 10 Torr to 10 Torr, but their calibration is sensitive to the chemical composition of the gases being measured.\n\nA Pirani gauge consist of a metal wire open to the pressure being measured. The wire is heated by a current flowing through it and cooled by the gas surrounding it. If the gas pressure is reduced, the cooling effect will decrease, hence the equilibrium temperature of the wire will increase. The resistance of the wire is a function of its temperature: by measuring the voltage across the wire and the current flowing through it, the resistance (and so the gas pressure) can be determined. This type of gauge was invented by Marcello Pirani.\n\nIn two-wire gauges, one wire coil is used as a heater, and the other is used to measure temperature due to convection. Thermocouple gauges and thermistor gauges work in this manner using thermocouple or thermistor, respectively, to measure the temperature of the heated wire.\n\nIonization gauges are the most sensitive gauges for very low pressures (also referred to as hard or high vacuum). They sense pressure indirectly by measuring the electrical ions produced when the gas is bombarded with electrons. Fewer ions will be produced by lower density gases. The calibration of an ion gauge is unstable and dependent on the nature of the gases being measured, which is not always known. They can be calibrated against a McLeod gauge which is much more stable and independent of gas chemistry.\n\nThermionic emission generate electrons, which collide with gas atoms and generate positive ions. The ions are attracted to a suitably biased electrode known as the collector. The current in the collector is proportional to the rate of ionization, which is a function of the pressure in the system. Hence, measuring the collector current gives the gas pressure. There are several sub-types of ionization gauge.\n\nMost ion gauges come in two types: hot cathode and cold cathode. In the hot cathode version, an electrically heated filament produces an electron beam. The electrons travel through the gauge and ionize gas molecules around them. The resulting ions are collected at a negative electrode. The current depends on the number of ions, which depends on the pressure in the gauge. Hot cathode gauges are accurate from 10 Torr to 10 Torr. The principle behind cold cathode version is the same, except that electrons are produced in the discharge of a high voltage. Cold Cathode gauges are accurate from 10 Torr to 10 Torr. Ionization gauge calibration is very sensitive to construction geometry, chemical composition of gases being measured, corrosion and surface deposits. Their calibration can be invalidated by activation at atmospheric pressure or low vacuum. The composition of gases at high vacuums will usually be unpredictable, so a mass spectrometer must be used in conjunction with the ionization gauge for accurate measurement.\n\nA hot-cathode ionization gauge is composed mainly of three electrodes acting together as a triode, wherein the cathode is the filament. The three electrodes are a collector or plate, a filament, and a grid. The collector current is measured in picoamperes by an electrometer. The filament voltage to ground is usually at a potential of 30 volts, while the grid voltage at 180–210 volts DC, unless there is an optional electron bombardment feature, by heating the grid, which may have a high potential of approximately 565 volts.\n\nThe most common ion gauge is the hot-cathode Bayard–Alpert gauge, with a small ion collector inside the grid. A glass envelope with an opening to the vacuum can surround the electrodes, but usually the nude gauge is inserted in the vacuum chamber directly, the pins being fed through a ceramic plate in the wall of the chamber. Hot-cathode gauges can be damaged or lose their calibration if they are exposed to atmospheric pressure or even low vacuum while hot. The measurements of a hot-cathode ionization gauge are always logarithmic.\n\nElectrons emitted from the filament move several times in back-and-forth movements around the grid before finally entering the grid. During these movements, some electrons collide with a gaseous molecule to form a pair of an ion and an electron (electron ionization). The number of these ions is proportional to the gaseous molecule density multiplied by the electron current emitted from the filament, and these ions pour into the collector to form an ion current. Since the gaseous molecule density is proportional to the pressure, the pressure is estimated by measuring the ion current.\n\nThe low-pressure sensitivity of hot-cathode gauges is limited by the photoelectric effect. Electrons hitting the grid produce x-rays that produce photoelectric noise in the ion collector. This limits the range of older hot-cathode gauges to 10 Torr and the Bayard–Alpert to about 10 Torr. Additional wires at cathode potential in the line of sight between the ion collector and the grid prevent this effect. In the extraction type the ions are not attracted by a wire, but by an open cone. As the ions cannot decide which part of the cone to hit, they pass through the hole and form an ion beam. This ion beam can be passed on to a:\n\nThere are two subtypes of cold-cathode ionization gauges: the Penning gauge (invented by Frans Michel Penning), and the Inverted magnetron, also called a Redhead gauge. The major difference between the two is the position of the anode with respect to the cathode. Neither has a filament, and each may require a DC potential of about 4 kV for operation. Inverted magnetrons can measure down to 1  Torr.\n\nLikewise, cold-cathode gauges may be reluctant to start at very low pressures, in that the near-absence of a gas makes it difficult to establish an electrode current - in particular in Penning gauges, which use an axially symmetric magnetic field to create path lengths for electrons that are of the order of metres. In ambient air, suitable ion-pairs are ubiquitously formed by cosmic radiation; in a Penning gauge, design features are used to ease the set-up of a discharge path. For example, the electrode of a Penning gauge is usually finely tapered to facilitate the field emission of electrons.\n\nMaintenance cycles of cold cathode gauges are, in general, measured in years, depending on the gas type and pressure that they are operated in. Using a cold cathode gauge in gases with substantial organic components, such as pump oil fractions, can result in the growth of delicate carbon films and shards within the gauge that eventually either short-circuit the electrodes of the gauge or impede the generation of a discharge path.\n\nWhen fluid flows are not in equilibrium, local pressures may be higher or lower than the average pressure in a medium. These disturbances propagate from their source as longitudinal pressure variations along the path of propagation. This is also called sound. Sound pressure is the instantaneous local pressure deviation from the average pressure caused by a sound wave. Sound pressure can be measured using a microphone in air and a hydrophone in water. The effective sound pressure is the root mean square of the instantaneous sound pressure over a given interval of time. Sound pressures are normally small and are often expressed in units of microbar.\n\nThe American Society of Mechanical Engineers (ASME) has developed two separate and distinct standards on pressure Measurement, B40.100 and PTC 19.2.\nB40.100 provides guidelines on Pressure Indicated Dial Type and Pressure Digital Indicating Gauges, Diaphragm Seals, Snubbers, and Pressure Limiter Valves.\nPTC 19.2 provides instructions and guidance for the accurate determination of pressure values in support of the ASME Performance Test Codes. The choice of method, instruments, required calculations, and corrections to be applied depends on the purpose of the measurement, the allowable uncertainty, and the characteristics of the equipment being tested.\n\nThe methods for pressure measurement and the protocols used for data transmission are also provided. Guidance is given for setting up the instrumentation and determining the uncertainty of the measurement. Information regarding the instrument type, design, applicable pressure range, accuracy, output, and relative cost is provided. Information is also provided on pressure-measuring devices that are used in field environments i.e., Piston Gauges, Manometers, and Low-Absolute-Pressure (Vacuum) Instruments.\n\nThese methods are designed to assist in the evaluation of measurement uncertainty based on current technology and engineering knowledge, taking into account published instrumentation specifications and measurement and application techniques. This Supplement provides guidance in the use of methods to establish the pressure-measurement uncertainty.\n\nPressure was first measured by torricelli. He had taken a glass in which he poured mercury and when it get filled he put his finger on its top and again put it in the container of mercury then he found that a vaccum space is created at the top which is called torricellium vaccum and then atmospheric pressure is measured easily.\n\n\n\n"}
{"id": "19953", "url": "https://en.wikipedia.org/wiki?curid=19953", "title": "Medieval dance", "text": "Medieval dance\n\nSources for an understanding of dance in Europe in the Middle Ages are limited and fragmentary, being composed of some interesting depictions in paintings and illuminations, a few musical examples of what may be dances, and scattered allusions in literary texts. The first detailed descriptions of dancing only date from 1451 in Italy, which is after the start of the Renaissance in Western Europe.\n\nThe most documented form of dance during the Middle Ages is the carol also called the \"carole\" or \"carola\" and known from the 12th and 13th centuries in Western Europe in rural and court settings. It consisted of a group of dancers holding hands usually in a circle, with the dancers singing in a leader and refrain style while dancing. No surviving lyrics or music for the carol have been identified. In northern France, other terms for this type of dance included \"ronde\" and its diminutives \"rondet\", \"rondel\", and \"rondelet\" from which the more modern music term \"rondeau\" derives. In the German-speaking areas, this same type of choral dance was known as \"reigen\".\n\nSome of the earliest mentions of the carol occur in the works of the French poet Chretien de Troyes in his series of Arthurian romances. In the wedding scene in Erec and Enide (about 1170)\n\nIn The Knight of the Cart, (probably late 1170s) at a meadow where there are knights and ladies, various games are played while:\n\nIn what is probably Chretien's last work, Perceval, the Story of the Grail, probably written 1181–1191, we find:\n\nand later at a court setting:\n\nDante (1265-1321) has a few minor references to dance in his works but a more substantive description of the round dance with song from Bologna comes from Giovanni del Virgilio (floruit 1319–1327).\n\nLater in the 14th century Giovanni Boccaccio (1313–1375) shows us the \"carola\" in Florence in the Decameron (about 1350–1353) which has several passages describing men and women dancing to their own singing or accompanied by musicians. Boccaccio also uses two other terms for contemporary dances, \"ridda\" and \"ballonchio\", both of which refer to round dances with singing.\n\nApproximately contemporary with the \"Decameron\" are a series of frescos in Siena by Ambrogio Lorenzetti painted about 1338–40, one of which shows a group of women doing a \"bridge\" figure while accompanied by another woman playing the tambourine.\n\nIn a life of Saint Dunstan composed about 1000, the author tells how Dunstan, going into a church, found maidens dancing in a ring and singing a hymn. According to the Oxford English Dictionary (1933) the term \"carol\" was first used in England for this type of circle dance accompanied by singing in manuscripts dating to as early as 1300. The word was used as both a noun and a verb and the usage of carol for a dance form persisted well into the 16th century. One of the earliest references is in Robert of Brunne's early 14th century \"Handlyng Synne\" (Handling Sin) where it occurs as a verb.\n\nMullally in his book on the carole makes the case that the dance, at least in France, was done in a closed circle with the dancers, usually men and women interspersed, holding hands. He adduces evidence that the general progression of the dance was to the left (clockwise) and that the steps probably were very simple consisting of a step to the left with the left foot followed by a step on the right foot closing to the left foot.\n\nCircle or line dances also existed in other parts of Europe outside England, France and Italy where the term carol was best known. These dances were of the same type with dancers hand-in-hand and a leader who sang the ballad.\n\nIn Denmark, old ballads mention a closed Ring dance which can open into a Chain dance. A fresco in Ørslev church in Zealand from about 1400 shows nine people, men and women, dancing in a line. The leader and some others in the chain carry bouquets of flowers. Dances could be for men and women, or for men alone, or women alone. In the case of women's dances, however, there may have been a man who acted as the leader. Two dances specifically named in the Danish ballads which appear to be line dances of this type are \"The Beggar Dance\", and \"The Lucky Dance\" which may have been a dance for women. A modern version of these medieval chains is seen in the Faroese chain dance, the earliest account of which goes back only to the 17th century.\n\nIn Sweden too, medieval songs often mentioned dancing. A long chain was formed, with the leader singing the verses and setting the time while the other dancers joined in the chorus. These \"Long Dances\" have lasted into modern times in Sweden. \nA similar type of song dance may have existed in Norway in the Middle Ages as well, but no historical accounts have been found.\n\nThe same dance in Germany was called \"Reigen\" and may have originated from devotional dances at early Christian festivals. Dancing around the church or a fire was frequently denounced by church authorities which only underscores how popular it was. There are records of church and civic officials in various German towns forbidding dancing and singing from the 8th to the 10th centuries. Once again, in singing processions, the leader provided the verse and the other dancers supplied the chorus. The minnesinger Neidhart von Reuental, who lived in the first half of the 13th century wrote several songs for dancing, some of which use the term \"reigen\".\nIn southern Tyrol, at Runkelstein Castle, a series of frescos was executed in the last years of the 14th century. One of the frescos depicts Elisabeth of Poland, Queen of Hungary leading a chain dance.\n\nCircle dances were also found in the area that is today the Czech Republic. Descriptions and illustrations of dancing can be found in church registers, chronicles and the 15th century writings of Bohuslav Hasištejnský z Lobkovic. Dancing was primarily done around trees on the village green but special houses for dancing appear from the 14th century. In Poland as well the earliest village dances were in circles or lines accompanied by the singing or clapping of the participants.\n\nThe present day folk dances in the Balkans consist of dancers linked together in a hand or shoulder hold in an open or closed circle or a line. The basic round dance goes by many names in the various countries of the region: \"kolo\", \"oro\", \"horo\" or \"hora\". The modern couple dance so common in western and northern Europe has only made a few inroads into the Balkan dance repertory.\nChain dances of a similar type to these modern dance forms have been documented from the medieval Balkans. Tens of thousands of medieval tombstones called \"Stećci\" are found in Bosnia and Hercegovina and neighboring areas in Montenegro, Serbia and Croatia. They date from the end of the 12th century to the 16th century. Many of the stones bear inscription and figures, several of which have been interpreted as dancers in a ring or line dance. These mostly date to the 14th and 15th centuries. Usually men and women are portrayed dancing together holding hands at shoulder level but occasionally the groups consist of only one sex.\n\nFurther south in Macedonia, near the town of Zletovo, Lesnovo monastery, originally built in the 11th century, was renovated in the middle of the 14th century and a series of murals were painted. One of these shows a group of young men linking arms in a round dance. They are accompanied by two musicians, one playing the kanun while the other beats on a long drum.\n\nThere is also some documentary evidence from the Dalmatian coast area of what is now Croatia. An anonymous chronicle from 1344 exhorts the people of the city of Zadar to sing and dance circle dances for a festival while in the 14th and 15th centuries, authorities in Dubrovnik forbid circle dances and secular songs on the cathedral grounds. Another early reference comes from the area of present-day Bulgaria in a manuscript of a 14th-century sermon which calls chain dances \"devilish and damned.\"\n\nAt a later period there are the accounts of two western European travelers to Constantinople, the capital of the Ottoman Empire. Salomon Schweigger (1551–1622) was a German preacher who traveled in the entourage of Jochim von Sinzendorf, Ambassador to Constantinople for Rudolf II in 1577. He describes the events at a Greek wedding:\n\nAnother traveler, the German pharmacist Reinhold Lubenau, was in Constantinople in November 1588 and reports on a Greek wedding in these terms:\n\nIf the story is true that troubadour Raimbaut de Vaqueiras (about 1150–1207) wrote the famous Provençal song \"Kalenda Maya\" to fit the tune of an estampie that he heard two jongleurs play, then the history of the estampie extends back to the 12th century. The only musical examples actually identified as \"estampie\" or \"istanpita\" occur in two 14th-century manuscripts. The same manuscripts also contain other pieces named \"danse real\" or other dance names. These are similar in musical structure to the estampies but consensus is divided as to whether these should be considered the same.\n\nIn addition to these instrumental music compositions, there are also mentions of the estampie in various literary sources from the 13th and 14th centuries. One of these as \"stampenie\" is found in Gottfried von Strassburg's \"Tristan\" from 1210 in a catalog of Tristan's accomplishments:\n\nLater, in a description of Isolde:\n\nA century and a half later in the poem \"La Prison amoreuse\" (1372–73) by French chronicler and poet Jean Froissart (c. 1337–1405), we find:\n\nOpinion is divided as to whether the Estampie was actually a dance or simply early instrumental music. Sachs believes the strong rhythm of the music, a derivation of the name from a term meaning \"to stamp\" and the quotation from the Froissart poem above definitely label the estampie as a dance. However, others stress the complex music in some examples as being uncharacteristic of dance melodies and interpret Froissart's poem to mean that the dancing begins with the carol. There is also debate on the derivation of the word \"estampie\". In any case, no description of dance steps or figures for the estampie are known.\n\nAccording to German dance historian Aenne Goldschmidt, the oldest notice of a couple dance comes from the southern German Latin romance Ruodlieb probably composed in the early to mid-11th century. The dance is done at a wedding feast and is described in the translation by Edwin Zeydel as follows:\n\nAnother literary mention comes from a later period in Germany with a description of couple dancing in Wolfram von Eschenbach's epic poem \"Parzival\", usually dated to the beginning of the 13th century. The scene occurs on manuscript page 639, the host is Gawain, the tables from the meal have been removed and musicians have been recruited:\n\nEschenbach also remarks that while many of the noblemen present were good fiddlers, they knew only the old style dances, not the many new dances from Thuringia.\n\nThe early 14th century Codex Manesse from Heidelberg has miniatures of many Minnesang poets of the period. The portrait of Heinrich von Stretelingen shows him engaged in a \"courtly pair dance\" while the miniature of Hiltbolt von Schwangau depicts him in a trio dance with two ladies, one in each hand, with a fiddler providing the music.\n\n\n\n"}
{"id": "19955", "url": "https://en.wikipedia.org/wiki?curid=19955", "title": "Megatokyo", "text": "Megatokyo\n\nSet in a fictional version of Tokyo, \"Megatokyo\" portrays the adventures of Piro, a young fan of anime and manga, and his friend Largo, an American video game enthusiast. The comic often parodies and comments on the archetypes and clichés of anime, manga, dating sims, arcade and video games, occasionally making direct references to real-world works. \"Megatokyo\" originally emphasized humor, with continuity of the story a subsidiary concern. Over time, it focused more on developing a complex plot and the personalities of its characters. This transition was due primarily to Gallagher's increasing control over the comic, which led to Caston choosing to leave the project. \"Megatokyo\" has received praise from such sources as \"The New York Times\", while Gallagher's changes to the comic have been criticized by sources including Websnark.\n\n\"Megatokyo\" began publication as a joint project between Fred Gallagher and Rodney Caston, along with a few internet acquaintances. Gallagher and Caston later became business partners, as well. According to Gallagher, the comic's first two strips were drawn in reaction to Caston being \"convinced that he and I could do [a webcomic] ... [and] bothering me incessantly about it\", without any planning or pre-determined storyline. The comic's title was derived from an Internet domain owned by Caston, which had hosted a short-lived gaming news site maintained by Caston before the comic's creation. With Caston co-writing the comic's scripts and Gallagher supplying its artwork, the comic's popularity quickly increased, eventually reaching levels comparable to those of such popular webcomics as \"Penny Arcade\" and \"PvP\". According to Gallagher, \"Megatokyo\"<nowiki>'</nowiki>s popularity was not intended, as the project was originally an experiment to help him improve his writing and illustrating skills for his future project, \"Warmth\".\n\nIn May 2002, Caston sold his ownership of the title to Gallagher, who has managed the comic on his own since then. In October of the same year, after Gallagher was laid off from his day job as an architect, he took up producing the comic as a full time profession. Caston's departure from \"Megatokyo\" was not fully explained at the time. Initially, Gallagher and Caston only briefly mentioned the split, with Gallagher publicly announcing Caston's departure on June 17, 2002. On January 15, 2005, Gallagher explained his view of the reasoning behind the split in response to a comment made by Scott Kurtz of \"PvP\", in which he suggested that Gallagher had stolen ownership of \"Megatokyo\" from Caston. Calling Kurtz's claim \"mean spirited\", Gallagher responded:\n\"While things were good at first, over time we found that we were not working well together creatively. There is no fault in this, it happens. I've never blamed Rodney for this creative 'falling out' nor do I blame myself. Not all creative relationships click, ours didn't in the long run.\"\nFour days later, Caston posted his view of the development on his website:\n\"After this he approached me and said either I would sell him my ownership of MegaTokyo or he would simply stop doing it entirely, and we'd divide up the company's assets and end it all.\nThis was right before the MT was to go into print form, and I really wanted to see it make it into print, rather [than] die on the vine.\"\nIn May 2011, it was announced that \"Endgames\" (a gameworld existing within \"Megatokyo\") was being revamped in a light novel format, with a story written by webfiction author Thomas Knapp, with four light novels planned. A short story \"Behind the Masque\" was also announced, and released on Amazon's Kindle Store on June 10, 2011.\n\n\"Megatokyo\" is usually hand-drawn in pencil by Fred Gallagher, without any digital or physical \"inking\". Inking was originally planned, but dropped as Gallagher decided it was unfeasible. \"Megatokyo\"<nowiki>'</nowiki>s first strips were created by roughly sketching on large sheets of paper, followed by tracing, scanning, digital clean-up of the traced comics with Adobe Photoshop, and final touches in Adobe Illustrator to achieve a finished product. Gallagher has stated that tracing was necessary because his sketches were not neat enough to use before tracing. Because of the tracing necessary, these comics regularly took six to eight hours to complete. As the comic progressed, Gallagher became able to draw \"cleaner\" comics without rough lines and tracing lines, and was able to abandon the tracing step. Gallagher believes \"that this eventually led to better looking and more expressive comics\".\n\n\"Megatokyo\"<nowiki>'</nowiki>s early strips were laid out in four square panels per strip, in a two-by-two square array – a formatting choice made as a compromise between the horizontal layout of American comic strips and the vertical layout of Japanese comic strips. The limitations of this format became apparent during the first year of \"Megatokyo\"<nowiki>'</nowiki>s publication, and in the spring of 2001, the comic switched to a manga-style, free-form panel layout. This format allowed for both large, detailed drawings and small, abstract progressions, as based on the needs of the script. Gallagher has commented that his drawing speed had increased since the comic's beginning, and with four panel comics taking much less time to produce, it \"made sense in some sort of twisted, masochistic way, that [he] could use that extra time to draw more for each comic\".\n\n\"Megatokyo\"<nowiki>'</nowiki>s earliest strips were drawn entirely on single sheets of paper. Following these, Gallagher began drawing the comic's panels separately and assembling them in Adobe Illustrator, allowing him to draw more detailed frames. This changed during \"Megatokyo\"<nowiki>'</nowiki>s eighth chapter, with Gallagher returning to drawing entire comics on single sheets of paper. Gallagher stated that this change allowed for more differentiated layouts, in addition to allowing him a better sense of momentum during comic creation.\n\nThe strip is currently drawn on inkjet paper in pencil, the text and speech being added later with Adobe Photoshop or Illustrator. In March 2009 he began Fredarting, a streaming live video feed of the comic being drawn.\n\nGallagher occasionally has guest artists participate in the production of the comic, including Mohammad F. Haque of \"Applegeeks\".\n\n\"Megatokyo\" has had several sources of funding during its production. In its early years, it was largely funded by Gallagher and Caston's full time jobs, with the additional support of banner advertisements. A store connected to ThinkGeek was launched during October 2000 in order to sell \"Megatokyo\" merchandise, and, in turn, help fund the comic. On August 1, 2004, this store was replaced by \"Megagear\", an independent online store created by Fred Gallagher and his wife, Sarah, to be used solely by \"Megatokyo\", although it now also offers \"Applegeeks\" and \"Angerdog\" merchandise.\n\nGallagher has emphasized that \"Megatokyo\" will continue to remain on the Internet free of charge, and that releasing it in book form is simply another way for the comic to reach readers, as opposed to replacing its webcomic counterpart entirely. Additionally, he has stated that he is against micropayments, as he believes that word of mouth and public attention are powerful property builders, and that a \"pay-per-click\" system would only dampen their effectiveness. He has claimed that such systems are a \"superior\" option to direct monetary compensation, and that human nature is opposed to micropayments.\n\nMuch of \"Megatokyo\"<nowiki>'</nowiki>s early humor consists of jokes related to the video game subculture, as well as culture-clash issues. In these early strips, the comic progressed at a pace which Gallagher has called \"haphazard\", often interrupted by purely punchline-driven installments. As Gallagher gradually gained more control over \"Megatokyo\"<nowiki>'</nowiki>s production, the comic began to gain more similarities to the Japanese shōjo manga that Gallagher enjoys. Following Gallagher's complete takeover of \"Megatokyo\", the comic's thematic relation to Japanese manga continued to grow.\n\nThe comic features characteristics borrowed from anime and manga archetypes, often parodying the medium's clichés. Examples include Junpei, a ninja who becomes Largo's apprentice; Rent-a-zillas, giant monsters based on Godzilla; the Tokyo Police Cataclysm Division, which fights the monsters with giant robots and supervises the systematic destruction and reconstruction of predesignated areas of the city; fan service; a Japanese school girl, Yuki, who has also started being a magical girl in recent comics; and Ping, a robot girl. In addition, Dom and Ed, hitmen employed by Sega and Sony, respectively, are associated with a Japanese stereotype that all Americans are heavily armed.\n\nCharacters in \"Megatokyo\" usually speak Japanese, although some speak English, or English-based l33t. Typically, when a character is speaking Japanese, it is signified by enclosing English text between angle brackets (<>). Not every character speaks every language, so occasionally characters are unable to understand one another. In several scenes (such as this one), a character's speech is written entirely in rōmaji Japanese to emphasize this.\n\n\"Megatokyo\" is divided into chapters. Chapter 0, which contains all of the comic's early phase, covers a time span in the comic of about six weeks. Each of the subsequent chapters chronicles the events of a single day. Chapter 0 was originally not given a title, although the book version retroactively dubbed it \"Relax, we understand j00.\" Between the chapters, and occasionally referenced in the main comic, are a number of omake.\n\nPiro, the protagonist, is an author surrogate of Fred Gallagher. Gallagher has stated that Piro is an idealized version of himself when he was in college. As a character, he is socially inept and frequently depressed. His design was originally conceived as a visual parody of the character Ruri Hoshino, from the \"Martian Successor Nadesico\" anime series. His name is derived from Gallagher's online nickname, which was in turn taken from Makoto Sawatari's cat in the Japanese visual novel \"Kanon\".\n\nIn the story, Piro has extreme difficulty understanding \"Megatokyo\"<nowiki>'</nowiki>s female characters, making him for the most part ignorant of the feelings that the character Nanasawa Kimiko has for him, though he has become much more aware of her attraction as the series progressed. Gallagher has commented that Piro is the focal point of emotional damage, while his friend, Largo, takes the physical damage in the comic.\n\nLargo is the comic's secondary protagonist, and the comic version of co-creator Rodney Caston. An impulsive alcoholic whose speech is rendered in L33t frequently, he serves as one of the primary sources of comic relief. A technologically gifted character, he is obsessed with altering devices, often with hazardous results. Gallagher designed Largo to be the major recipient of the comic's physical damage. Largo's name comes from Caston's online nickname, which refers to the villain from \"Bubblegum Crisis\". For various reasons (including fire and battle damage) he often ends up wearing very little clothing. Largo seems to have awkwardly blundered into a relatively successful relationship with Hayasaka Erika at the current time in the comic.\n\n is a strong-willed, cynical, and sometimes violent character. At the time of the story, she is a former popular Japanese idol (singer) and voice actress who has been out of the spotlight for three years, though she still possesses a considerable fanbase. Erika's past relationship troubles, combined with exposure to swarms of fanboys, have caused her to adopt a negative outlook on life. Gallagher has implied that her personality was loosely based on the \"tsundere\" (tough girl) stereotype often seen in anime and manga.\n\n is a Japanese girl who previously worked as a waitress at an Anna Miller's restaurant, and is Piro's romantic interest. At the current point in the story, she is a voice actress for the possibly-failing Lockart game \"Sight\", playing the main heroine, Kannazuki Kotone. Kimiko is a kind and soft-spoken character, though she is prone to mood-swings, and often causes herself embarrassment by saying things she does not mean. Gallagher has commented that Kimiko was the only female character not based entirely on anime stereotypes.\n\n is an enigmatic and manipulative young goth girl. She is drawn to resemble a \"Gothic Lolita\", and is often described as \"darkly cute,\" with Gallagher occasionally describing her as a \"perkigoth.\" Miho often acts strangely compared to the comic's other characters, and regularly accomplishes abnormal feats, such as leaping inhuman distances or perching herself atop telephone poles. Despite these displays of ability, it is hinted at that Miho has problems with her health. Little is revealed in the comic about Miho's past or motivations, although Gallagher states that these will eventually be explained. Largo believes that she (Miho) is the queen of the undead, and is the cause of the zombie invasion of Tokyo. It has been hinted that she is a magical girl who may have some past connection with the zombies. She is apparently killed in a robotic beam attack by Ed, but nine days later is found in the hospital reading and eating with no obvious signs of physical damage. More possibilities exist that she is some type of game prototype or archetype.\n\n\"Megatokyo\"<nowiki>'</nowiki>s story begins when Piro and Largo fly to Tokyo after an incident at the Electronic Entertainment Expo (E3). Piro has the proper paperwork; Largo must beat the ninja Junpei at a video game to enter. After a spending spree, the pair are stranded without enough money to buy plane tickets home, forcing them to live with Tsubasa, a Japanese friend of Piro's. When Tsubasa suddenly departs for America to seek his \"first true love\", the protagonists are forced out of the apartment. Tsubasa leaves Ping, a robot girl PlayStation 2 accessory, in their care. This leads to old friends of Piro and Largo showing up later. The two are shadow operatives for video game companies, Ed (Sony) and Dom (SEGA).\n\nAt one point, Piro, confronted with girl troubles, visits the local bookstore to \"research\"—look in the vast shelves of shoujo manga for a solution to his problem. A spunky schoolgirl, Sonoda Yuki, and her friends, Asako and Mami, see him sitting amidst piles of read manga, and ask him what he is doing. Piro, flustered, runs away, accidentally leaving behind his bookbag and sketchbook.\n\nAfter their eviction, Piro begins work at \"Megagamers\", a store specializing in anime, manga, and video games. His employer allows him and Largo to live in the apartment above the store. Largo is mistaken for the new English teacher at a local school, where he takes on the alias \"Great Teacher Largo\" and instructs his students in L33t, video games, and about computers. Yuki's father, Inspector Sonada Masamichi of the \"Tokyo Police Cataclysm Division\" (TPCD) hires Largo after Largo manipulates Ping into stopping a rampaging monster, the drunken turtle Gameru.\n\nAs Largo is working at the local high school, Piro encounters Yuki again while working at Megagamers, when she returns his bookbag and sketchbook, scribbled all over with comments about his drawings. She then, to his consternation, asks if he would give her drawing lessons. Piro, flustered, agrees, and promptly forgets about them.\n\nEarlier in the story, Piro had seen Nanasawa Kimiko at an Anna Miller's restaurant, where she is a waitress, after Tsubasa brought him and Largo there. Later on, Piro encounters Kimiko outside a train station, where she is worrying aloud that she will miss an audition because she has forgotten her money and railcard. Piro hands her his own railcard and walks off before she can refuse his offer. This event causes Kimiko to develop an idealized vision of her benefactor, an image which is shattered the next time they meet. Despite this, she gradually develops feelings for Piro, though she is too shy to admit them. Later on in the story, Kimiko's outburst on a radio talk show causes her to suddenly rise to idol status. Angered by the hosts' derisive comments about fanboys, she comes to the defense of her audience, immediately and unintentionally securing their obsessive adoration. Later, her new horde of fanboys find out where she works and flock to the restaurant, obsessively trying to get pictures up her skirt. Piro works undercover as a busboy to get rid of all cameras. The scene eventually builds to a climax, in which Kimiko shouts at the fanboys and lifts her skirt in defiance, and they take photographs. Piro, provoked by her outburst into actively defending her, threatens the fanboy crowd, and collects all of their memory cards with the photos. On the way back from the restaurant, Kimiko is suffering from the aftermath of the scene and lashes out at Piro on the subway, which causes him to walk off.\n\nMeanwhile, Largo develops a relationship with Hayasaka Erika, Piro's coworker at Megagamers. She and Kimiko share a house. As with Piro and Kimiko, Largo and Erika meet by coincidence early in the story. Later, it is revealed that Erika is a former pop idol, who caused a big scene then disappeared from the public eye after her fiancé left her. When she is rediscovered by her fans, Largo helps thwart a fanboy horde, but not well enough to escape being dismissed by the TPCD for it. He then offers to help Erika to deal with her \"vulnerabilities in the digital plane\". Erika insists on protecting herself, so Largo instructs her in computer-building. This leads into a little more relationship than Largo can handle, partly because he insists all computer building be done in the nude or as close to it as possible, to avoid static electrical discharge ruining components, and partly because his behavior, crude though it may appear, impresses Erika in many ways.\n\nThe enigmatic Tohya Miho frequently meddles in the lives of the protagonists. Miho knows Piro and Largo from the \"Endgames\" MMORPG previous to \"Megatokyo\"<nowiki>'</nowiki>s plot. She abused a hidden statistic in the game to gain control of nearly all of the game's player characters, but was ultimately defeated by Piro and Largo. In the comic, Miho becomes close friends with Ping, influencing Ping's relationship with Piro and pitting Ping against Largo in video game battles. Miho is also involved in Erika's backstory; Miho manipulated Erika's fans after Erika's disappearance. This effort ended badly, leaving Miho hospitalized, and the TPCD cleaning up the aftermath. Most of the exact details of what happened are left to the readers' imagination, as are her current motivations and ultimate goal. Miho and many of the events surrounding her involve a club in Harajuku, the Cave of Evil (CoE).\n\nAfter getting yelled at for retaining her waitress job, Kimiko quits her voice acting job and goes home to find Erika assembling a new computer in her undergarments. Not long after Erika tells Kimiko to strip, Piro comes by, who she tells to get undressed as well. While Erika and Piro talk about her, Kimiko, who hid when Piro showed up, runs out of the apartment. Kimiko runs into Ping, who wanted to talk to Piro about why, after an explosion at school, she had started to cry uncontrollably. They encounter Largo at the store, who explains what went wrong, although no one knows what he means until Piro comes in and translates. Ping is relieved to know that she won't shut down and Kimiko hugs Piro and apologizes for her actions. Largo leaves for Erika's apartment after she calls looking for help. That night, while Piro and Kimiko fall asleep watching TV, Erika, who finished the computer with Largo's help, tries to seduce Largo, but it freaks him out and he runs out for home. The next morning, after Kimiko departs, Piro finds out she quit her voice acting job and tries to find her.\n\nKimiko and Miho are in the same diner, to which Ed has sent an attack robot (Kill-Bot) against Miho, since she has disrupted his attempts to destroy Ping. Miho is in the diner trying to contact Piro, Kimiko is talking with Erika. Dom is also there to talk with Kimiko. After rescuing both herself and Kimiko from the Kill-Bot and chaos at the diner, the two talk about things. Miho talks to Piro on her phone, argues with him, and then Piro and Kimiko have a conversation about that as the two females are leaving the area. Dom follows and tries to coerce Kimiko into joining SEGA for protection from fans, but she refuses. Drained, she has Miho finish talking to Piro on the phone. Piro then encounters a group who found Kimiko's cell phone and other belongings after she and Miho escaped the diner. The group wants to help Piro get together with Kimiko, partially due to feeling bad for trying to snap a picture up Kimiko's skirt. Piro and the group set out for a press conference Kimiko is going to for the voice acting project, \"Sight\". Besides all of the other fans going to the event, a planned zombie outbreak occurs in the area. Miho, who helped Kimiko get ready for the event and accompanied her to it, later calls the zombies off for unexplained reasons through an unexplained mechanism.\n\nLargo and Yuki, who has since been revealed along the way to be a magical girl like her mother Meimi (likewise revealed), steal a Rent-a-Zilla to fight the zombie outbreak. Largo leaves Yuki to help Piro get to Kimiko. Unfortunately, the Rent-a-Zilla gets bitten by zombies and turns into one itself, resulting in the TPCD capturing it. Yuki protects it from the TPCD, teleports it out of the area, and adopts it as a pet in a miniaturized form, all much to her father's chagrin.\n\nAfter the event, Erika, Largo, Kimiko and Piro are reunited, and they talk a bit with Miho, who has shown up again after storming out following an argument with Kenji earlier. Miho declines an offer to eat with the group and wanders off thinking about games and Largo and Piro. She is shown walking amongst the zombies and then in Ed's gun-sights, and in the center of an attack by a number of Ed's Kill-Bots.\n\nDuring the next nine days, Piro and Kimiko have made up and Kimiko returned to both of her jobs, with them seeing little of each other. Largo and Erika are shown to likewise be involved but more often, including going to dinner with the Sonoda family, as the inspector's brother was Erika's fiancé. Kimiko is attempting to get Piro working as an artist on \"Sight\", which unbeknownst to them is now being funded by Dom. Ping is concerned about the whereabouts of Miho, who hasn't been seen during the time, but Piro is still upset about all that has happened and somewhat evasively refuses direct assistance. Ping and Junko, another one of Largo's students, who used to be a friend of Miho, work towards finding Miho. Yuki and Kobayashi Yutaka then also become involved with the attempt because of this. That night, Piro and Kimiko discuss Miho and \"Endgames\", which Yuki overhears, they unaware she is there. This leads Yuki to appropriate Piro's powerless laptop and leave, believing him to still be in love with Miho and that the device might hold clues to finding her. Kimiko and Piro work on his portfolio for \"Sight\" and then they say goodnight and leave. He returns to his apartment, but Kimiko goes to the CoE club using a pass Miho gave her long ago in the beginning. Once at the club, Dom mockingly advises her, Yuki unknowingly whisps past her, and she unexpectedly meets up with an old friend Komugiko. During all this, Piro has left his apartment after looking at his sketchbook and a drawing of Miho. His current location is unknown.\n\nAside from Kimiko, concurrent overlapping events have led to almost every main character converging upon the club for various reasons involving Miho, or in support of others involved. Ed, attempting to destroy Ping, fights with Largo, as the staff of the club have maneuvered Ed and Ping into the protective radius of ex-Idol Erika. Yuki and Yutaka get Piro's laptop powered on, she reads the old chat logs between Piro and Miho, and follows instructions from her to him. Going to a \"hidden-in-plain-sight\" hospital room, she finds Miho alive and well, although seemingly in a weakened state. During a heated argument and Miho's goading, Yuki then forcibly moves Miho to the club. Shortly after the arrival of the two in the center of everyone, the bulk of the denizens go into trance-like states while others are fighting or confused about what to do next. Miho appears to be collapsing. Upon instructions from Erika, Largo finds then uses his Largo-Phone and the club's sound system to knock out power in the immediate area of the club. During this event, Piro has gone to visit Miho at the \"hospital\" room, where he discovers that she is missing. Following the blackout, Largo, Erika, and Miho board a train, where Miho decides to return home. However, a large crowd has blocked her path home, apparently waiting for someone's return.\n\nThe next morning, Piro has been brought to jail, where he has been interrogated by police about Miho's disappearance. He is able to leave jail by paying a suspiciously set low bail of about $100 US, which is obtained through a 10,000 yen bill that has been shaped into an origami 'zilla and left in the cell. Piro walks back home, where he finds Miho sleeping on a beanbag in the apartment. Piro and Miho then work out some of the confusion between them, which reveals several background events. She explains the Analogue Support Facility as a sort of safehouse, where she was able to come and go when she wanted. Since Ping in her extreme attempt to find Miho had posted tons of pictures, videos, and information on the internet, people are now using that to \"build a 'real' me\", as Miho explains it. During the process, at one point Kimiko calls from the studio, updating Piro on his artwork and telling him some of how last night she and others found Miho and how crazy it was. Largo and Erika, who are riding on the roof of a train in the Miyagi prefecture also call during the conversation. After a short conversation with both Largo and Erika on the phone, and a bit more conversation with Miho, Piro instructs her to stay in the apartment until they can figure out what to do. Junko and Ping are shown leaving for school, with Junko seeming taking Ed's shotguns from last night with her.\n\nAfter receiving a phone call from Yutaka, whom Masamichi initially disapproves of, Yuki, who has not changed clothing from the events of the previous chapter, leaves her house, grabs him, and takes him to a rooftop, where they try to explain things after Yutaka was being questioned by Asako and Mami. She goes over everything, even why she referred to herself as a \"monster\", which Yuki's friends previously overheard and misunderstood. Realizing that Miho is the cause of this mess, Yutaka indirectly vows revenge, but Yuki stops him. Yutaka goes anyway and meets his brother in front of Megagamers, who has tracked Miho to the store since the previous night. Yutaka's brother is a member of a group of Nanasawa fans who plan to intervene and remind Piro who his true love is to get rid of Miho. However, Dom's van is blocking the store's entrance. Though Yuki protests against intervention to the group, Dom, who is unknown to them, performs his own method of intervention anyway and forces Piro to choose between Nanasawa and Miho. It is currently unknown if Dom knows who Miho is, but Miho, in a disguise, overhears the conversation and forces Piro to briefly wear a hat. At the same time, Yuki, deciding that she can wait no longer, steals Dom's van and guns, and rushes into the store with Yutaka in tow. Seeing this, Miho grabs Piro and rushes upstairs, discarding the hat in the process. Yuki subsequently collides with the hat and a presumed explosion occurs, stalling Yuki and Yutaka. Miho and Piro don cosplay outfits as a disguise, escape, and make their way to the local bath house. Just before Yuki grabs Yutaka again, Dom, now trapped under a pile of rubble, expresses his condolences to Yutaka, to which he does not understand. The pair quickly follow Miho and Piro and await for them to leave the bath house.\n\n\"Megatokyo\" was first published in print by Studio Ironcat, a partnership announced in September 2002. Following this, the first book, a compilation of \"Megatokyo\" strips under the title \"Megatokyo Volume One: Chapter Zero\", was released by Studio Ironcat in January 2003. According to Gallagher, Studio Ironcat was unable to meet demand for the book, due to problems the company was facing at the time. On July 7, 2003, Gallagher announced that Ironcat would not continue to publish \"Megatokyo\" in book form. This was followed by an announcement on August 27, 2003 that Dark Horse Comics would publish \"Megatokyo Volume 2\" and future collected volumes, including a revised edition of \"Megatokyo Volume 1\". The comic once more changed publishers in February 2006, moving from Dark Horse Comics to the CMX Manga imprint of DC Comics. The comic then transferred to CMX's parent Wildstorm, with its last volume published in July 2010.\nCMX, along with Wildstorm closed down in 2010. Former publisher Dark Horse regained the rights to the series and planned to release it in omnibus format in January 2013, but didn't.\n\n, six volumes are available for purchase: volumes 1 through 3 from Dark Horse, volumes 4 and 5 by CMX/DC, and volume 6 by Wildstorm. The books have also been translated into German, Italian, French, and Polish. In July 2004, \"Megatokyo\" was the tenth best-selling manga property in the United States, and during the week ending February 20, 2005, volume 3 ranked third in the Nielsen BookScan figures, which was not only its highest ranking to date (), but also made it the highest monthly rank for an original English-language manga title.\n\nIn July 2007, Kodansha announced that in 2008 it intends to publish \"Megatokyo\" in a Japanese-language edition, (in a silver slipcased box as part of Kodansha Box editions, a new manga line started in November 2006). Depending on reader response, Kodansha hoped to subsequently publish the entire \"Megatokyo\" book series. The first volume was released in Japan on May 7, 2009.\n\nThe artwork and characterizations of \"Megatokyo\" have received praise from such publications as \"The New York Times\" and Comics Bulletin. Many critics praise \"Megatokyo\"s character designs and pencil work, rendered entirely in grayscale; conversely, it has been criticized for perceived uniformity and simplicity in the designs of its peripheral characters, which have been regarded as confusing and difficult to tell apart due to their similar appearances.\n\nSome critics, such as Eric Burns of Websnark, have found the comic to suffer from \"incredibly slow pacing\" (, only about 2 months of in-universe time have elapsed), unclear direction or resolutions for plot threads, a lack of official character profiles and plot summaries for the uninitiated, and an erratic update schedule. Burns also harshly criticized the often uncanonical filler material Gallagher employs to prevent the comic's front page content from becoming stagnant, such as \"Shirt Guy Dom\", a punchline-driven stick figure comic strip written and illustrated by \"Megatokyo\" editor Dominic Nguyen. Following Gallagher taking on \"Megatokyo\" as a full-time occupation, some critics have complained that updates should be more frequent than when he worked on the comic part time. Update schedule issues prompted Gallagher to install an update progress bar for readers awaiting the next installment of the comic; however, it has since been removed as it itself often wasn't updated.\n\nIGN called \"Megatokyo\"'s fans \"some of the most patient and forgiving in the webcomic world.\" During an interview, Gallagher stated that \"Megatokyo\" fans \"always [tell him] they are patient and find that the final comics are always worth the wait,\" but he feels as though he \"[has] a commitment to [his] readers and to [himself] to deliver the best comics [he] can, and to do it on schedule,\" finally saying that nothing would make him happier than \"[getting] a better handle on the time it takes to create each page.\" Upon missing deadlines, Gallagher often makes self-disparaging comments. Poking fun at this, Jerry \"Tycho\" Holkins of \"Penny Arcade\" has claimed to have \"gotten on famously\" with Gallagher, ever since he \"figured out that [Gallagher] legitimately detests himself and is not hoisting some kind of \"glamour\".\"\n\nWhile \"Megatokyo\" was originally presented as a slapstick comedy, it began focusing more on the romantic relationships between its characters after Caston's departure from the project. As a result, some fans, preferring the comic's gag-a-day format, have claimed its quality was superior when Caston was writing it. Additionally, it has been said that, without Caston's input, Largo's antics appear contrived. Comics Bulletin regards \"Megatokyo\"'s characters as convincingly portrayed, commenting that \"the reader truly feels connected to the characters, their romantic hijinks, and their wacky misadventures with the personal touches supplied by the author.\" Likewise, Anime News Network has praised the personal tone in which the comic is written, stating that much of its appeal is a result of the \"friendly and casual feeling of a fan-made production.\"\n\nGallagher states early in \"Megatokyo Volume 1\" that he and Caston \"didn't want the humor ... to rely too heavily on what might be considered 'obscure knowledge.'\" An article in \"The New York Times\" insists that such scenarios were unavoidable, commenting that the comic \"sits at the intersection of several streams of obscure knowledge,\" including \"gaming and hacking; manga ... the boom in Web comics over the past few years; and comics themselves.\" The article also held that \"Gallagher doesn't mean to be exclusive ... he graciously offers translation of the strip's later occasional lapses into l33t ... [and] explains why the characters are occasionally dressed in knickers or as rabbits.\" The newspaper went on to argue that \"The pleasure of a story like \"Megatokyo\" comes not in its novelistic coherence, but in its loose ranginess.\"\n\n\"Megatokyo\" was nominated in at least one category of the Web Cartoonist's Choice Awards every year from 2001 through 2007. It won Best Comic in 2002, as well as Best Writing, Best Serial Comic, and Best Dramatic Comic. The largest number of nominations it has received in one year is 14 in 2003, when it won Outstanding Environment Design. The series tied with Svetlana Chmakova's \"Dramacon\" for the 2007 Best Continuing OEL Manga.\n\n"}
{"id": "19956", "url": "https://en.wikipedia.org/wiki?curid=19956", "title": "Medieval music", "text": "Medieval music\n\nMedieval music consists of songs, instrumental pieces, and liturgical music from about 500 A.D. to 1400. Medieval music was an era of Western music, including liturgical music (also known as sacred) used for the church, and secular music, non-religious music. Medieval music includes solely vocal music, such as Gregorian chant and choral music (music for a group of singers), solely instrumental music, and music that uses both voices and instruments (typically with the instruments accompanying the voices). Gregorian chant was sung by monks during Catholic Mass. The Mass is a reenactment of Christ's Last Supper, intended to provide a spiritual connection between man and God. Part of this connection was established through music.\nThis era begins with the fall of the Western Roman Empire in the fifth century and ends sometime in the early fifteenth century. Establishing the end of the medieval era and the beginning of the Renaissance music era is difficult, since the trends started at different times in different regions. The date range in this article is the one usually adopted by musicologists.\n\nDuring the Medieval period the foundation was laid for the music notation and music theory practices that would shape Western music into the norms that developed during the common-practice era, a period of shared music writing practices which encompassed the Baroque music composers from 1600–1750, such as J.S. Bach and Classical music period composers from the 1700s such as W.A. Mozart and Romantic music era composers from the 1800s such as Wagner. The most obvious of these is the development of a comprehensive music notational system which enabled composers to write out their song melodies and instrumental pieces on parchment or paper. Prior to the development of musical notation, songs and pieces had to be learned \"by ear\", from one person who knew a song to another person. This greatly limited how many people could be taught new music and how wide music could spread to other regions or countries. The development of music notation made it easier to disseminate (spread) songs and musical pieces to a larger number of people and to a wider geographic area. However the theoretical advances, particularly in regard to rhythm—the timing of notes—and polyphony—using multiple, interweaving melodies at the same time—are equally important to the development of Western music.\n\nMany instruments used to perform medieval music still exist in the 21st century, but in different and typically more technologically developed forms. The flute was made of wood in the medieval era rather than silver or other metal, and could be made as a side-blown or end-blown instrument. While modern orchestral flutes are usually made of metal and have complex key mechanisms and airtight pads, medieval flutes had holes that the performer had to cover with the fingers (as with the recorder). The recorder was made of wood during the Medieval era, and despite the fact that in the 2000s, it may be made of synthetic materials, it has more or less retained its past form. The gemshorn is similar to the recorder as it has finger holes on its front, though it is actually a member of the ocarina family. One of the flute's predecessors, the pan flute, was popular in medieval times, and is possibly of Hellenic origin. This instrument's pipes were made of wood, and were graduated in length to produce different pitches.\n\nMedieval music used many plucked string instruments like the lute, a fretted instrument with a pear-shaped hollow body which is the predecessor to the modern guitar. Other plucked stringed instruments included the mandore, gittern, citole and psaltery. The dulcimers, similar in structure to the psaltery and zither, were originally plucked, but musicians began to strike the dulcimer with hammers in the 14th century after the arrival of new metal technology that made metal strings possible.\n\nThe bowed lyra of the Byzantine Empire was the first recorded European bowed string instrument. Like the modern violin, a performer produced sound by moving a bow with tensioned hair over tensioned strings. The Persian geographer Ibn Khurradadhbih of the 9th century (d. 911) cited the Byzantine lyra, in his lexicographical discussion of instruments as a bowed instrument equivalent to the Arab rabāb and typical instrument of the Byzantines along with the \"urghun\" (organ), \"shilyani\" (probably a type of harp or lyre) and the \"salandj\" (probably a bagpipe). The hurdy-gurdy was (and still is) a mechanical violin using a rosined wooden wheel attached to a crank to \"bow\" its strings. Instruments without sound boxes like the jew's harp were also popular. Early versions of the pipe organ, fiddle (or vielle), and a precursor to the modern trombone (called the sackbut) were used.\n\nMedieval music was composed and, for some vocal and instrumental music, improvised for many different music genres (styles of music). Medieval music created for sacred (church use) and secular (non-religious use) was typically written by composers, except for some sacred vocal and secular instrumental music which was improvised (made up on-the-spot). During the earlier medieval period, the liturgical genre, predominantly Gregorian chant done by monks, was monophonic (\"monophonic\" means a single melodic line, without a harmony part or instrumental accompaniment). Polyphonic genres, in which multiple independent melodic lines are performed simultaneously, began to develop during the high medieval era, becoming prevalent by the later 13th and early 14th century. The development of polyphonic forms, with different voices interweaving, is often associated with the late Medieval Ars nova style which flourished in the 1300s. The Ars Nova, which means \"new art\" was an innovative style of writing music that served as a key transition from the medieval music style to the more expressive styles of the post-1400s Renaissance music era.\n\nThe earliest innovations upon monophonic plainchant were heterophonic. \"Heterophony\" is the performance of the same melody by two different performers at the same time, in which each performer slightly alters the ornaments she or he is using. Another simple form of heterophony is for singers to sing the same shape of melody, but with one person singing the melody and a second person singing the melody at a higher or lower pitch. Organum, for example, expanded upon plainchant melody using an accompanying line, sung at a fixed interval (often a perfect fifth or perfect fourth away from the main melody), with a resulting alternation between a simple form of polyphony and monophony. The principles of organum date back to an anonymous 9th century tract, the \"Musica enchiriadis\", which established the tradition of duplicating a preexisting plainchant in parallel motion at the interval of an octave, a fifth or a fourth.\n\nOf greater sophistication was the motet, which developed from the clausula genre of medieval plainchant. The motet would become the most popular form of medieval polyphony. While early motets were liturgical or sacred (designed for use in a church service), by the end of the thirteenth century the genre had expanded to include secular topics, such as courtly love. Courtly love was the respectful veneration of a lady from afar by an amorous, noble man. Many popular motets had lyrics about a man's love and adoration of beautiful, noble and much-admired woman.\n\nThe Medieval motet developed during the Renaissance music era (after 1400). During the Renaissance, the Italian secular genre of the Madrigal became popular. Similar to the polyphonic character of the motet, madrigals featured greater fluidity and motion in the leading melody line. The madrigal form also gave rise to polyphonic canons (songs in which multiple singers sing the same melody, but starting at different times), especially in Italy where they were called \"caccie.\" These were three-part secular pieces, which featured the two higher voices in canon, with an underlying instrumental long-note accompaniment.\n\nFinally, purely instrumental music also developed during this period, both in the context of a growing theatrical tradition and for court performances for the aristocracy. Dance music, often improvised around familiar tropes, was the largest purely instrumental genre. The secular Ballata, which became very popular in Trecento Italy, had its origins, for instance, in medieval instrumental dance music.\n\nDuring the Medieval period the foundation was laid for the notational and theoretical practices that would shape Western music into the norms that developed during the common practice era. The most obvious of these is the development of a comprehensive music notational system; however the theoretical advances, particularly in regard to rhythm and polyphony, are equally important to the development of Western music.\n\nThe earliest Medieval music did not have any kind of notational system. The tunes were primarily monophonic (a single melody without accompaniment) and transmitted by oral tradition. As Rome tried to centralize the various liturgies and establish the Roman rite as the primary church tradition the need to transmit these chant melodies across vast distances effectively was equally glaring. So long as music could only be taught to people \"by ear,\" it limited the ability of the church to get different regions to sing the same melodies, since each new person would have to spend time with a person who already knew a song and learn it \"by ear.\" The first step to fix this problem came with the introduction of various signs written above the chant texts to indicate direction of pitch movement, called \"neumes\".\n\nThe origin of \"neumes\" is unclear and subject to some debate; however, most scholars agree that their closest ancestors are the classic Greek and Roman grammatical signs that indicated important points of declamation by recording the rise and fall of the voice. The two basic signs of the classical grammarians were the \"acutus\", /, indicating a raising of the voice, and the \"gravis\", \\, indicating a lowering of the voice. A singer reading a chant text with neume markings would be able to get a general sense of whether the melody line went up in pitch, stayed the same, or went down in pitch. For a singer who already knew a song, seeing the written neume markings above the text could help to jog his or her memory about how the melody went. However, a singer reading a chant text with neume markings would not be able to sight read a song which he or she had never heard sung before.\n\nThese neumes eventually evolved into the basic symbols for \"neumatic\" notation, the \"virga\" (or \"rod\") which indicates a higher note and still looked like the \"acutus\" from which it came; and the \"punctum\" (or \"dot\") which indicates a lower note and, as the name suggests, reduced the \"gravis\" symbol to a point. Thus the \"acutus\" and the \"gravis\" could be combined to represent graphical vocal inflections on the syllable. This kind of notation seems to have developed no earlier than the eighth century, but by the ninth it was firmly established as the primary method of musical notation. The basic notation of the \"virga\" and the \"punctum\" remained the symbols for individual notes, but other \"neumes\" soon developed which showed several notes joined together. These new \"neumes\"—called ligatures—are essentially combinations of the two original signs.\n\nThe first music notation was the use of dots over the lyrics to a chant, with some dots being higher or lower, giving the reader a general sense of the direction of the melody. However, this form of notation only served as a memory aid for a singer who already knew the melody. This basic \"neumatic\" notation could only specify the number of notes and whether they moved up or down. There was no way to indicate exact pitch, any rhythm, or even the starting note. These limitations are further indication that the \"neumes\" were developed as tools to support the practice of oral tradition, rather than to supplant it. However, even though it started as a mere memory aid, the worth of having more specific notation soon became evident.\n\nThe next development in musical notation was \"heighted \"neumes\"\", in which \"neumes\" were carefully placed at different heights in relation to each other. This allowed the \"neumes\" to give a rough indication of the size of a given interval as well as the direction. This quickly led to one or two lines, each representing a particular note, being placed on the music with all of the \"neumes\" relating back to them. At first, these lines had no particular meaning and instead had a letter placed at the beginning indicating which note was represented. However, the lines indicating middle C and the F a fifth below slowly became most common. Having been at first merely scratched on the parchment, the lines now were drawn in two different colored inks: usually red for F, and yellow or green for C. This was the beginning of the musical staff as we know it today. The completion of the four-line staff is usually credited to Guido d’ Arezzo (c. 1000–1050), one of the most important musical theorists of the Middle Ages. While older sources attribute the development of the staff to Guido, some modern scholars suggest that he acted more as a codifier of a system that was already being developed. Either way, this new notation allowed a singer to learn pieces completely unknown to him in a much shorter amount of time. However, even though chant notation had progressed in many ways, one fundamental problem remained: rhythm. The \"neumatic\" notational system, even in its fully developed state, did not clearly define any kind of rhythm for the singing of notes.\n\nThe music theory of the Medieval period saw several advances over previous practice both in regard to tonal material, texture, and rhythm.\n\nConcerning rhythm, this period had several dramatic changes in both its conception and notation. During the early Medieval period there was no method to notate rhythm, and thus the rhythmical practice of this early music is subject to heated debate among scholars. The first kind of written rhythmic system developed during the 13th century and was based on a series of modes. This rhythmic plan was codified by the music theorist Johannes de Garlandia, author of the \"De Mensurabili Musica\" (c.1250), the treatise which defined and most completely elucidated these rhythmic modes. In his treatise Johannes de Garlandia describes six \"species\" of mode, or six different ways in which longs and breves can be arranged. Each mode establishes a rhythmic pattern in beats (or \"tempora\") within a common unit of three \"tempora\" (a \"perfectio\") that is repeated again and again. Furthermore, notation without text is based on chains of \"ligature\"s (the characteristic notations by which groups of notes are bound to one another).\n\nThe rhythmic mode can generally be determined by the patterns of ligatures used. Once a rhythmic mode had been assigned to a melodic line, there was generally little deviation from that mode, although rhythmic adjustments could be indicated by changes in the expected pattern of ligatures, even to the extent of changing to another rhythmic mode. The next step forward concerning rhythm came from the German theorist Franco of Cologne. In his treatise \"Ars cantus mensurabilis\" (\"The Art of Mensurable Music\"), written around 1280, he describes a system of notation in which differently shaped notes have entirely different rhythmic values. This is a striking change from the earlier system of de Garlandia. Whereas before the length of the individual note could only be gathered from the mode itself, this new inverted relationship made the mode dependent upon—and determined by—the individual notes or \"figurae\" that have incontrovertible durational values, an innovation which had a massive impact on the subsequent history of European music. Most of the surviving notated music of the 13th century uses the rhythmic modes as defined by Garlandia. The step in the evolution of rhythm came after the turn of the 13th century with the development of the \"Ars Nova\" style.\n\nThe theorist who is most well recognized in regard to this new style is Philippe de Vitry, famous for writing the \"Ars Nova\" (\"New Art\") treatise around 1320. This treatise on music gave its name to the style of this entire era. In some ways the modern system of rhythmic notation began with Vitry, who completely broke free from the older idea of the rhythmic modes. The notational predecessors of modern time meters also originate in the \"Ars Nova\". This new style was clearly built upon the work of Franco of Cologne. In Franco's system, the relationship between a breve and a semibreves (that is, half breves) was equivalent to that between a breve and a long: and, since for him \"modus\" was always perfect (grouped in threes), the \"tempus\" or beat was also inherently perfect and therefore contained three semibreves. Sometimes the context of the mode would require a group of only two semibreves, however, these two semibreves would always be one of normal length and one of double length, thereby taking the same space of time, and thus preserving the perfect subdivision of the \"tempus\". This ternary division held for all note values. In contrast, the \"Ars Nova\" period introduced two important changes: the first was an even smaller subdivision of notes (semibreves, could now be divided into \"minim\"), and the second was the development of \"mensuration.\"\n\nMensurations could be combined in various manners to produce metrical groupings. These groupings of mensurations are the precursors of simple and compound meter. By the time of \"Ars Nova\", the perfect division of the \"tempus\" was not the only option as duple divisions became more accepted. For Vitry the breve could be divided, for an entire composition, or section of one, into groups of two or three smaller semibreves. This way, the \"tempus\" (the term that came to denote the division of the breve) could be either \"perfect\" (\"tempus perfectum\"), with ternary subdivision, or \"imperfect\" (\"tempus imperfectum\"), with binary subdivision. In a similar fashion, the semibreve's division (termed \"prolation\") could be divided into three \"minima\" (\"prolatio perfectus\" or major prolation) or two \"minima\" (\"prolatio imperfectus\" or minor prolation) and, at the higher level, the longs division (called \"modus\") could be three or two breves (\"modus perfectus\" or perfect mode, or \"modus imperfectus\" or imperfect mode respectively). Vitry took this a step further by indicating the proper division of a given piece at the beginning through the use of a \"mensuration sign\", equivalent to our modern \"time signature\".\n\n\"Tempus perfectum\" was indicated by a circle, while \"tempus imperfectum\" was denoted by a half-circle (the current symbol , used as an alternative for the time signature, is actually a holdover of this symbol, not a letter \"C\" as an abbreviation for \"common time\", as popularly believed). While many of these innovations are ascribed to Vitry, and somewhat present in the \"Ars Nova\" treatise, it was a contemporary—and personal acquaintance—of de Vitry, named Johannes de Muris (Jehan des Mars) who offered the most comprehensive and systematic treatment of the new mensural innovations of the \"Ars Nova\" (for a brief explanation of the mensural notation in general, see the article Renaissance music). Many scholars, citing a lack of positive attributory evidence, now consider \"Vitry's\" treatise to be anonymous, but this does not diminish its importance for the history of rhythmic notation. However, this makes the first definitely identifiable scholar to accept and explain the mensural system to be de Muris, who can be said to have done for it what Garlandia did for the rhythmic modes.\n\nFor the duration of the medieval period, most music would be composed primarily in perfect tempus, with special effects created by sections of imperfect tempus; there is a great current controversy among musicologists as to whether such sections were performed with a breve of equal length or whether it changed, and if so, at what proportion. This \"Ars Nova\" style remained the primary rhythmical system until the highly syncopated works of the \"Ars subtilior\" at the end of the 14th century, characterized by extremes of notational and rhythmic complexity. This sub-genera pushed the rhythmic freedom provided by \"Ars Nova\" to its limits, with some compositions having different voices written in different mensurations simultaneously. The rhythmic complexity that was realized in this music is comparable to that in the 20th century.\n\nOf equal importance to the overall history of western music theory were the textural changes that came with the advent of polyphony. This practice shaped western music into the harmonically dominated music that we know today. The first accounts of this textural development were found in two anonymous yet widely circulated treatises on music, the \"Musica\" and the \"Scolica enchiriadis\". These texts are dated to sometime within the last half of the ninth century. The treatises describe a technique that seemed already to be well established in practice. This early polyphony is based on three simple and three compound intervals. The first group comprises fourths, fifths, and octaves; while the second group has octave-plus-fourths, octave-plus-fifths, and double octaves. This new practice is given the name \"organum\" by the author of the treatises. \"Organum\" can further be classified depending on the time period in which it was written. The early \"organum\" as described in the \"enchiriadis\" can be termed \"strict \"organum\"\" Strict \"organum\" can, in turn, be subdivided into two types: \"diapente\" (organum at the interval of a fifth) and \"diatesseron\" (organum at the interval of a fourth). However, both of these kinds of strict \"organum\" had problems with the musical rules of the time. If either of them paralleled an original chant for too long (depending on the mode) a tritone would result.\n\nThis problem was somewhat overcome with the use of a second type of \"organum\". This second style of \"organum\" was called \"free \"organum\"\". Its distinguishing factor is that the parts did not have to move only in parallel motion, but could also move in oblique, or contrary motion. This made it much easier to avoid the dreaded tritone. The final style of \"organum\" that developed was known as \"melismatic \"organum\"\", which was a rather dramatic departure from the rest of the polyphonic music up to this point. This new style was not note against note, but was rather one sustained line accompanied by a florid melismatic line. This final kind of \"organum\" was also incorporated by the most famous polyphonic composer of this time—Léonin. He united this style with measured discant passages, which used the rhythmic modes to create the pinnacle of \"organum\" composition. This final stage of \"organum\" is sometimes referred to as Notre Dame school of polyphony, since that was where Léonin (and his student Pérotin) were stationed. Furthermore, this kind of polyphony influenced all subsequent styles, with the later polyphonic genera of motets starting as a trope of existing Notre Dame \"organums\".\n\nAnother important element of Medieval music theory was the system by which pitches were arranged and understood. During the Middle Ages, this systematic arrangement of a series of whole steps and half steps, what we now call a scale, was known as a mode. The modal system worked like the scales of today, insomuch that it provided the rules and material for melodic writing. The eight church modes are: \"Dorian\", \"Hypodorian\", \"Phrygian\", \"Hypophrygian\", \"Lydian\", \"Hypolydian\", \"Mixolydian\", and \"Hypomixolydian\". Much of the information concerning these modes, as well as the practical application of them, was codified in the 11th century by the theorist Johannes Afflighemensis. In his work he describes three defining elements to each mode: the final (or \"finalis)\", the reciting tone (\"tenor\" or \"confinalis\"), and the range (or \"ambitus\"). The \"finalis\" is the tone that serves as the focal point for the mode and, as the name suggests, is almost always used as the final tone. The reciting tone is the tone that serves as the primary focal point in the melody (particularly internally). It is generally also the tone most often repeated in the piece, and finally the range delimits the upper and lower tones for a given mode. The eight modes can be further divided into four categories based on their final (\"finalis\").\n\nMedieval theorists called these pairs \"maneriae\" and labeled them according to the Greek ordinal numbers. Those modes that have d, e, f, and g as their final are put into the groups \"protus\", \"deuterus\", \"tritus\", and \"tetrardus\" respectively. These can then be divided further based on whether the mode is \"authentic\" or \"plagal.\" These distinctions deal with the range of the mode in relation to the final. The authentic modes have a range that is about an octave (one tone above or below is allowed) and start on the final, whereas the plagal modes, while still covering about an octave, start a perfect fourth below the authentic. Another interesting aspect of the modal system is the universal allowance for altering B to B no matter what the mode. The inclusion of this tone has several uses, but one that seems particularly common is in order to avoid melodic difficulties caused, once again, by the tritone.\n\nThese ecclesiastical modes, although they have Greek names, have little relationship to the modes as set out by Greek theorists. Rather, most of the terminology seems to be a misappropriation on the part of the medieval theorists Although the church modes have no relation to the ancient Greek modes, the overabundance of Greek terminology does point to an interesting possible origin in the liturgical melodies of the Byzantine tradition. This system is called \"octoechos\" and is also divided into eight categories, called \"echoi\". \nFor specific medieval music theorists, see also: Isidore of Seville, Aurelian of Réôme, Odo of Cluny, Guido of Arezzo, Hermannus Contractus, Johannes Cotto (Johannes Afflighemensis), Johannes de Muris, Franco of Cologne, Johannes de Garlandia (Johannes Gallicus), Anonymous IV, Marchetto da Padova (Marchettus of Padua), Jacques of Liège, Johannes de Grocheo, Petrus de Cruce (Pierre de la Croix), and Philippe de Vitry.\n\nChant (or plainsong) is a monophonic sacred (single, unaccompanied melody) form which represents the earliest known music of the Christian church. Chant developed separately in several European centres. Although the most important were Rome, Hispania, Gaul, Milan, and Ireland, there were others as well. These styles were all developed to support the regional liturgies used when celebrating the Mass there. Each area developed its own chant and rules for celebration. In Spain and Portugal, Mozarabic chant was used and shows the influence of North African music. The Mozarabic liturgy even survived through Muslim rule, though this was an isolated strand and this music was later suppressed in an attempt to enforce conformity on the entire liturgy. In Milan, Ambrosian chant, named after St. Ambrose, was the standard, while Beneventan chant developed around Benevento, another Italian liturgical center. Gallican chant was used in Gaul, and Celtic chant in Ireland and Great Britain.\n\nAround AD 1011, the Roman Catholic Church wanted to standardize the Mass and chant across its empire. At this time, Rome was the religious centre of western Europe, and Paris was the political centre. The standardization effort consisted mainly of combining these two (Roman and Gallican) regional liturgies. Pope Gregory I (540–604) and Charlemagne (742–814) sent trained singers throughout the Holy Roman Empire (800|962–1806) to teach this new form of chant. This body of chant became known as Gregorian Chant, named after Pope Gregory I. By the 12th and 13th centuries, Gregorian chant had superseded all the other Western chant traditions, with the exception of the Ambrosian chant in Milan and the Mozarabic chant in a few specially designated Spanish chapels. Hildegard von Bingen (1098–1179) was the earliest known female composer. She wrote many monophonic works for the Catholic Church, almost all of them for female voices.\n\nAround the end of the 9th century, singers in monasteries such as St. Gall in Switzerland began experimenting with adding another part to the chant, generally a voice in parallel motion, singing mostly in perfect fourths or fifths above the original tune (see interval). This development is called organum and represents the beginnings of counterpoint and, ultimately, harmony. Over the next several centuries, organum developed in several ways.\n\nThe most significant of these developments was the creation of \"florid organum\" around 1100, sometimes known as the school of St. Martial (named after a monastery in south-central France, which contains the best-preserved manuscript of this repertory). In \"florid organum\" the original tune would be sung in long notes while an accompanying voice would sing many notes to each one of the original, often in a highly elaborate fashion, all the while emphasizing the perfect consonances (fourths, fifths and octaves), as in the earlier organa. Later developments of organum occurred in England, where the interval of the third was particularly favoured, and where organa were likely improvised against an existing chant melody, and at Notre Dame in Paris, which was to be the centre of musical creative activity throughout the thirteenth century.\n\nMuch of the music from the early medieval period is anonymous. Some of the names may have been poets and lyric writers, and the tunes for which they wrote words may have been composed by others. Attribution of monophonic music of the medieval period is not always reliable. Surviving manuscripts from this period include the Musica Enchiriadis, Codex Calixtinus of Santiago de Compostela, the Magnus Liber, and the Winchester Troper. For information about specific composers or poets writing during the early medieval period, see Pope Gregory I, St. Godric, Hildegard of Bingen, Hucbald, Notker Balbulus, Odo of Arezzo, Odo of Cluny, and Tutilo.\n\nAnother musical tradition of Europe originating during the early Middle Ages was the liturgical drama. \n\nLiturgical drama developed possibly in the 10th century from the tropes—poetic embelishments of the liturgical texts. One of the tropes, the so-called Quem Quaeritis, belonging to the liturgy of Easter morning, developed into a short play around the year 950. The oldest surviving written source is the Winchester Troper. Around the year 1000 it was sung all around Europe.\n\nShortly, a similar Christmas play was developed, musically and textually following the Easter one, and other plays followed.\n\nThere is a controversy among musicologists as to the instrumental accompaniment of such plays, given that the stage directions, very elaborate and precise in other respects, do not request any participation of instruments. These dramas were performed by monks, nuns and priests. In contrast to secular plays, which were spoken, the liturgical drama was always sung. Many have been preserved sufficiently to allow modern reconstruction and performance (for example the \"Play of Daniel\", which has been recently recorded at least ten times).\n\nThe Goliards were itinerant poet-musicians of Europe from the tenth to the middle of the thirteenth century. Most were scholars or ecclesiastics, and they wrote and sang in Latin. Although many of the poems have survived, very little of the music has. They were possibly influential—even decisively so—on the troubadour-trouvère tradition which was to follow. Most of their poetry is secular and, while some of the songs celebrate religious ideals, others are frankly profane, dealing with drunkenness, debauchery and lechery. One of the most important extant sources of Goliards chansons is the Carmina Burana.\n\nThe flowering of the Notre Dame school of polyphony from around 1150 to 1250 corresponded to the equally impressive achievements in Gothic architecture: indeed the centre of activity was at the cathedral of Notre Dame itself. Sometimes the music of this period is called the Parisian school, or Parisian organum, and represents the beginning of what is conventionally known as \"Ars antiqua\". This was the period in which rhythmic notation first appeared in western music, mainly a context-based method of rhythmic notation known as the rhythmic modes.\n\nThis was also the period in which concepts of formal structure developed which were attentive to proportion, texture, and architectural effect. Composers of the period alternated florid and discant organum (more note-against-note, as opposed to the succession of many-note melismas against long-held notes found in the florid type), and created several new musical forms: clausulae, which were melismatic sections of organa extracted and fitted with new words and further musical elaboration; conductus, which was a song for one or more voices to be sung rhythmically, most likely in a procession of some sort; and tropes, which were additions of new words and sometimes new music to sections of older chant. All of these genres save one were based upon chant; that is, one of the voices, (usually three, though sometimes four) nearly always the lowest (the tenor at this point) sang a chant melody, though with freely composed note-lengths, over which the other voices sang organum. The exception to this method was the conductus, a two-voice composition that was freely composed in its entirety.\n\nThe motet, one of the most important musical forms of the high Middle Ages and Renaissance, developed initially during the Notre Dame period out of the clausula, especially the form using multiple voices as elaborated by Pérotin, who paved the way for this particularly by replacing many of his predecessor (as canon of the cathedral) Léonin's lengthy florid clausulae with substitutes in a discant style. Gradually, there came to be entire books of these substitutes, available to be fitted in and out of the various chants. Since, in fact, there were more than can possibly have been used in context, it is probable that the clausulae came to be performed independently, either in other parts of the mass, or in private devotions. The clausulae, thus practised, became the motet when troped with non-liturgical words, and were further developed into a form of great elaboration, sophistication and subtlety in the fourteenth century, the period of \"Ars nova\". Surviving manuscripts from this era include the Montpellier Codex, Bamberg Codex, and Las Huelgas Codex.\n\nComposers of this time include Léonin, Pérotin, W. de Wycombe, Adam de St. Victor, and Petrus de Cruce (Pierre de la Croix). Petrus is credited with the innovation of writing more than three semibreves to fit the length of a breve. Coming before the innovation of imperfect tempus, this practice inaugurated the era of what are now called \"Petronian\" motets. These late 13th-century works are in three to four parts and have multiple texts sung simultaneously. Originally, the tenor line (from the Latin \"tenere\", \"to hold\") held a preexisting liturgical chant line in the original Latin, while the text of the one, two, or even three voices above, called the \"voces organales\", provided commentary on the liturgical subject either in Latin or in the vernacular French. The rhythmic values of the \"voces organales\" decreased as the parts multiplied, with the \"duplum\" (the part above the tenor) having smaller rhythmic values than the tenor, the \"triplum\" (the line above the \"duplum\") having smaller rhythmic values than the \"duplum\", and so on. As time went by, the texts of the \"voces organales\" became increasingly secular in nature and had less and less overt connection to the liturgical text in the tenor line.\n\nThe Petronian motet is a highly complex genre, given its mixture of several semibreve breves with rhythmic modes and sometimes (with increasing frequency) substitution of secular songs for chant in the tenor. Indeed, ever-increasing rhythmic complexity would be a fundamental characteristic of the 14th century, though music in France, Italy, and England would take quite different paths during that time.\n\nThe Cantigas de Santa Maria (\"Canticles of Holy Mary\"; , ) are 420 poems with musical notation, written in Galician-Portuguese during the reign of Alfonso X \"El Sabio\" (1221–1284) and often attributed to him. It is one of the largest collections of monophonic (solo) songs from the Middle Ages and is characterized by the mention of the Virgin Mary in every song, while every tenth song is a hymn. The manuscripts have survived in four codices: two at El Escorial, one at Madrid's National Library, and one in Florence, Italy. Some have colored miniatures showing pairs of musicians playing a wide variety of instruments.\n\nThe music of the troubadours and trouvères was a vernacular tradition of monophonic secular song, probably accompanied by instruments, sung by professional, occasionally itinerant, musicians who were as skilled as poets as they were singers and instrumentalists. The language of the troubadours was Occitan (also known as the langue d'oc, or Provençal); the language of the trouvères was Old French (also known as langue d'oil). The period of the troubadours corresponded to the flowering of cultural life in Provence which lasted through the twelfth century and into the first decade of the thirteenth. Typical subjects of troubadour song were war, chivalry and courtly love—the love of an idealized woman from afar. The period of the troubadours wound down after the Albigensian Crusade, the fierce campaign by Pope Innocent III to eliminate the Cathar heresy (and northern barons' desire to appropriate the wealth of the south). Surviving troubadours went either to Portugal, Spain, northern Italy or northern France (where the trouvère tradition lived on), where their skills and techniques contributed to the later developments of secular musical culture in those places.\n\nThe trouvères and troubadours shared similar musical styes, but the trouvères were generally noblemen. The music of the trouvères was similar to that of the troubadours, but was able to survive into the thirteenth century unaffected by the Albigensian Crusade. Most of the more than two thousand surviving trouvère songs include music, and show a sophistication as great as that of the poetry it accompanies.\n\nThe Minnesinger tradition was the Germanic counterpart to the activity of the troubadours and trouvères to the west. Unfortunately, few sources survive from the time; the sources of Minnesang are mostly from two or three centuries after the peak of the movement, leading to some controversy over the accuracy of these sources. Among the Minnesingers with surviving music are Wolfram von Eschenbach, Walther von der Vogelweide, and Niedhart von Reuenthal.\n\nIn the Middle Ages, Galician-Portuguese was the language used in nearly all of Iberia for lyric poetry. From this language derive both modern Galician and Portuguese. The Galician-Portuguese school, which was influenced to some extent (mainly in certain formal aspects) by the Occitan troubadours, is first documented at the end of the twelfth century and lasted until the middle of the fourteenth.\n\nThe earliest extant composition in this school is usually agreed to be Ora faz ost' o senhor de Navarra by the Portuguese João Soares de Paiva, usually dated just before or after 1200. The troubadours of the movement, not to be confused with the Occitan troubadours (who frequented courts in nearby León and Castile), wrote almost entirely cantigas. Beginning probably around the middle of the thirteenth century, these songs, known also as cantares or trovas, began to be compiled in collections known as cancioneiros (songbooks). Three such anthologies are known: the Cancioneiro da Ajuda, the Cancioneiro Colocci-Brancuti (or Cancioneiro da Biblioteca Nacional de Lisboa), and the Cancioneiro da Vaticana. In addition to these there is the priceless collection of over 400 Galician-Portugues cantigas in the Cantigas de Santa Maria, which tradition attributes to Alfonso X.\n\nThe Galician-Portuguese cantigas can be divided into three basic genres: male-voiced love poetry, called cantigas de amor (or cantigas d'amor) female-voiced love poetry, called cantigas de amigo (cantigas d'amigo); and poetry of insult and mockery called cantigas d'escarnho e de mal dizer. All three are lyric genres in the technical sense that they were strophic songs with either musical accompaniment or introduction on a stringed instrument. But all three genres also have dramatic elements, leading early scholars to characterize them as lyric-dramatic.\n\nThe origins of the cantigas d'amor are usually traced to Provençal and Old French lyric poetry, but formally and rhetorically they are quite different. The cantigas d'amigo are probably rooted in a native song tradition (; ), though this view has been contested. The cantigas d'escarnho e maldizer may also (according to Lang) have deep local roots. The latter two genres (totalling around 900 texts) make the Galician-Portuguese lyric unique in the entire panorama of medieval Romance poetry.\n\n\nThe beginning of the \"Ars nova\" is one of the few clear chronological divisions in medieval music, since it corresponds to the publication of the \"Roman de Fauvel\", a huge compilation of poetry and music, in 1310 and 1314. The \"Roman de Fauvel\" is a satire on abuses in the medieval church, and is filled with medieval motets, lais, rondeaux and other new secular forms. While most of the music is anonymous, it contains several pieces by Philippe de Vitry, one of the first composers of the isorhythmic motet, a development which distinguishes the fourteenth century. The isorhythmic motet was perfected by Guillaume de Machaut, the finest composer of the time.\n\nDuring the \"Ars nova\" era, secular music acquired a polyphonic sophistication formerly found only in sacred music, a development not surprising considering the secular character of the early Renaissance (while this music is typically considered \"medieval\", the social forces that produced it were responsible for the beginning of the literary and artistic Renaissance in Italy—the distinction between Middle Ages and Renaissance is a blurry one, especially considering arts as different as music and painting). The term \"\"Ars nova\"\" (new art, or new technique) was coined by Philippe de Vitry in his treatise of that name (probably written in 1322), in order to distinguish the practice from the music of the immediately preceding age.\n\nThe dominant secular genre of the Ars Nova was the \"chanson\", as it would continue to be in France for another two centuries. These chansons were composed in musical forms corresponding to the poetry they set, which were in the so-called \"formes fixes\" of \"rondeau\", \"ballade\", and \"virelai\". These forms significantly affected the development of musical structure in ways that are felt even today; for example, the \"ouvert-clos\" rhyme-scheme shared by all three demanded a musical realization which contributed directly to the modern notion of antecedent and consequent phrases. It was in this period, too, in which began the long tradition of setting the mass ordinary. This tradition started around mid-century with isolated or paired settings of Kyries, Glorias, etc., but Machaut composed what is thought to be the first complete mass conceived as one composition. The sound world of Ars Nova music is very much one of linear primacy and rhythmic complexity. \"Resting\" intervals are the fifth and octave, with thirds and sixths considered dissonances. Leaps of more than a sixth in individual voices are not uncommon, leading to speculation of instrumental participation at least in secular performance. Surviving French manuscripts include the Ivrea Codex and the Apt Codex.\n\nFor information about specific French composers writing in late medieval era, see Jehan de Lescurel, Philippe de Vitry, Guillaume de Machaut, Borlet, Solage, and François Andrieu.\n\nMost of the music of \"Ars nova\" was French in origin; however, the term is often loosely applied to all of the music of the fourteenth century, especially to include the secular music in Italy. There this period was often referred to as \"Trecento\". Italian music has alway been known for its lyrical or melodic character, and this goes back to the 14th century in many respects. Italian secular music of this time (what little surviving liturgical music there is, is similar to the French except for somewhat different notation) featured what has been called the \"cantalina\" style, with a florid top voice supported by two (or even one; a fair amount of Italian Trecento music is for only two voices) that are more regular and slower moving. This type of texture remained a feature of Italian music in the popular 15th and 16th century secular genres as well, and was an important influence on the eventual development of the trio texture that revolutionized music in the 17th.\n\nThere were three main forms for secular works in the Trecento. One was the madrigal, not the same as that of 150–250 years later, but with a verse/refrain-like form. Three-line stanzas, each with different words, alternated with a two-line \"ritornello\", with the same text at each appearance. Perhaps we can see the seeds of the subsequent late-Renaissance and Baroque ritornello in this device; it too returns again and again, recognizable each time, in contrast with its surrounding disparate sections. Another form, the \"caccia\" (\"chase,\") was written for two voices in a canon at the unison. Sometimes, this form also featured a ritornello, which was occasionally also in a canonic style. Usually, the name of this genre provided a double meaning, since the texts of caccia were primarily about hunts and related outdoor activities, or at least action-filled scenes. The third main form was the \"ballata\", which was roughly equivalent to the French \"virelai\".\n\nSurviving Italian manuscripts include the Squarcialupi Codex and the Rossi Codex. For information about specific Italian composers writing in the late medieval era, see Francesco Landini, Gherardello da Firenze, Andrea da Firenze, Lorenzo da Firenze, Giovanni da Firenze (aka Giovanni da Cascia), Bartolino da Padova, Jacopo da Bologna, Donato da Cascia, Lorenzo Masini, Niccolò da Perugia, and Maestro Piero.\n\nThe Geisslerlieder were the songs of wandering bands of flagellants, who sought to appease the wrath of an angry God by penitential music accompanied by mortification of their bodies. There were two separate periods of activity of Geisslerlied: one around the middle of the thirteenth century, from which, unfortunately, no music survives (although numerous lyrics do); and another from 1349, for which both words and music survive intact due to the attention of a single priest who wrote about the movement and recorded its music. This second period corresponds to the spread of the Black Death in Europe, and documents one of the most terrible events in European history. Both periods of Geisslerlied activity were mainly in Germany.\n\nAs often seen at the end of any musical era, the end of the medieval era is marked by a highly manneristic style known as \"Ars subtilior\". In some ways, this was an attempt to meld the French and Italian styles. This music was highly stylized, with a rhythmic complexity that was not matched until the 20th century. In fact, not only was the rhythmic complexity of this repertoire largely unmatched for five and a half centuries, with extreme syncopations, mensural trickery, and even examples of \"augenmusik\" (such as a chanson by Baude Cordier written out in manuscript in the shape of a heart), but also its melodic material was quite complex as well, particularly in its interaction with the rhythmic structures. Already discussed under Ars Nova has been the practice of isorhythm, which continued to develop through late-century and in fact did not achieve its highest degree of sophistication until early in the 15th century. Instead of using isorhythmic techniques in one or two voices, or trading them among voices, some works came to feature a pervading isorhythmic texture which rivals the integral serialism of the 20th century in its systematic ordering of rhythmic and tonal elements. The term \"mannerism\" was applied by later scholars, as it often is, in response to an impression of sophistication being practised for its own sake, a malady which some authors have felt infected the \"Ars subtilior\".\n\nOne of the most important extant sources of Ars Subtilior chansons is the Chantilly Codex. For information about specific composers writing music in \"Ars subtilior\" style, see Anthonello de Caserta, Philippus de Caserta (aka Philipoctus de Caserta), Johannes Ciconia, Matteo da Perugia, Lorenzo da Firenze, Grimace, Jacob Senleches, and Baude Cordier.\n\nDemarcating the end of the medieval era and the beginning of the Renaissance era, with regard to the composition of music, is difficult. While the music of the fourteenth century is fairly obviously medieval in conception, the music of the early fifteenth century is often conceived as belonging to a transitional period, not only retaining some of the ideals of the end of the Middle Ages (such as a type of polyphonic writing in which the parts differ widely from each other in character, as each has its specific textural function), but also showing some of the characteristic traits of the Renaissance (such as the increasingly international style developing through the diffusion of Franco-Flemish musicians throughout Europe, and in terms of texture an increasing equality of parts). Music historians do not agree on when the Renaissance era began, but most historians agree that England was still a medieval society in the early fifteenth century (see periodization issues of the Middle Ages). While there is no consensus, 1400 is a useful marker, because it was around that time that the Renaissance came into full swing in Italy.\n\nThe increasing reliance on the interval of the third as a consonance is one of the most pronounced features of transition into the Renaissance. Polyphony, in use since the 12th century, became increasingly elaborate with highly independent voices throughout the 14th century. With John Dunstaple and other English composers, partly through the local technique of faburden (an improvisatory process in which a chant melody and a written part predominantly in parallel sixths above it are ornamented by one sung in perfect fourths below the latter, and which later took hold on the continent as \"fauxbordon\"), the interval of the third emerges as an important musical development; because of this \"Contenance Angloise\" (\"English countenance\"), English composers' music is often regarded as the first to sound less truly bizarre to 2000s-era audiences who are not trained in music history.\n\nEnglish stylistic tendencies in this regard had come to fruition and began to influence continental composers as early as the 1420s, as can be seen in works of the young Dufay, among others. While the Hundred Years' War continued, English nobles, armies, their chapels and retinues, and therefore some of their composers, travelled in France and performed their music there; it must also of course be remembered that the English controlled portions of northern France at this time. English manuscripts include the Worcester Fragments, the Old St. Andrews Music Book, the Old Hall Manuscript, and Egerton Manuscript. For information about specific composers who are considered transitional between the medieval and the Renaissance, see Zacara da Teramo, Paolo da Firenze, Giovanni Mazzuoli, Antonio da Cividale, Antonius Romanus, Bartolomeo da Bologna, Roy Henry, Arnold de Lantins, Leonel Power, and John Dunstaple.\n\nAn early composer from the Franco-Flemish School of the Renaissance was Johannes Ockeghem (1410/1425 –1497). He was the most famous member of the Franco-Flemish School in the last half of the 15th century, and is often considered the most influential composer between Dufay and Josquin des Prez. Ockeghem probably studied with Gilles Binchois, and at least was closely associated with him at the Burgundian court. Antoine Busnois wrote a motet in honor of Ockeghem. Ockeghem is a direct link from the Burgundian style to the next generation of Netherlanders, such as Obrecht and Josquin. A strong influence on Josquin des Prez and the subsequent generation of Netherlanders, Ockeghem was famous throughout Europe Charles VII for his expressive music, although he was equally renowned for his technical prowess.\n\nThe Schola Cantorum Basiliensis, university for old music in Basel, Switzerland, provides a full-time practical study course for the music of the Middle Ages.\n\nA two-year vocational training for musicians is offered at the academy Burg Fürsteneck in Germany.\n\nKees Boeke coordinates a new Master of Music- Musik des Mittelalters und des Renaissance for both singers and instrumentalists in the Staatliche Hochschule für Musik Trossingen, also in Germany.\n\nThe musical styles of Léonin and Pérotin influenced 20th century minimalist composers such as Steve Reich.\n\n\n\n\n"}
{"id": "19957", "url": "https://en.wikipedia.org/wiki?curid=19957", "title": "Maser", "text": "Maser\n\nA maser (, an acronym for \"microwave amplification by stimulated emission of radiation\") is a device that produces coherent electromagnetic waves through amplification by stimulated emission. The first maser was built by Charles H. Townes, James P. Gordon, and H. J. Zeiger at Columbia University in 1953. Townes, Nikolay Basov and Alexander Prokhorov were awarded the 1964 Nobel Prize in Physics for theoretical work leading to the maser. Masers are used as the timekeeping device in atomic clocks, and as extremely low-noise microwave amplifiers in radio telescopes and deep space spacecraft communication ground stations.\n\nModern masers can be designed to generate electromagnetic waves at not only microwave frequencies but also radio and infrared frequencies. For this reason Charles Townes suggested replacing \"microwave\" with the word \"molecular\" as the first word in the acronym \"maser\".\n\nThe laser works by the same principle as the maser, but produces higher frequency coherent radiation at visible wavelengths. The maser was the forerunner of the laser, inspiring theoretical work by Townes and Arthur Leonard Schawlow that led to the invention of the laser in 1960. When the coherent optical oscillator was first imagined in 1957, it was originally called the \"optical maser\". This was ultimately changed to laser for \"Light Amplification by Stimulated Emission of Radiation\". Gordon Gould is credited with creating this acronym in 1957.\n\nThe theoretical principles governing the operation of a maser were first described by Joseph Weber of the University of Maryland at the Electron Tube Research Conference in 1952 in Ottawa, with a summary published in the June 1953 Transactions of the Institute of Radio Engineers Professional Group on Electron Devices, and simultaneously by Nikolay Basov and Alexander Prokhorov from Lebedev Institute of Physics at an \"All-Union Conference on Radio-Spectroscopy\" held by the USSR Academy of Sciences in May 1952, subsequently published in October 1954.\n\nIndependently, Charles Hard Townes, James P. Gordon, and H. J. Zeiger built the first ammonia maser at Columbia University in 1953. This device used stimulated emission in a stream of energized ammonia molecules to produce amplification of microwaves at a frequency of about 24.0 gigahertz. Townes later worked with Arthur L. Schawlow to describe the principle of the \"optical maser\", or \"laser\", of which Theodore H. Maiman created the first working model in 1960.\n\nFor their research in the field of stimulated emission, Townes, Basov and Prokhorov were awarded the Nobel Prize in Physics in 1964.\n\nThe maser is based on the principle of stimulated emission proposed by Albert Einstein in 1917. When atoms have been induced into an excited energy state, they can amplify radiation at a frequency particular to the element or molecule used as the masing medium (similar to what occurs in the lasing medium in a laser).\n\nBy putting such an amplifying medium in a resonant cavity, feedback is created that can produce coherent radiation.\n\n\nIn 2012, a research team from the National Physical Laboratory and Imperial College London developed a solid-state maser that operated at room temperature by using optically pumped, pentacene-doped p-Terphenyl as the amplifier medium. It produced pulses of maser emission lasting for a few hundred microseconds. \n\nIn 2018, a research team from Imperial College London and University College London demonstrated continuous-wave maser oscillation using synthetic diamonds containing Nitrogen-Vacancy defects.\n\nMasers serve as high precision frequency references. These \"atomic frequency standards\" are one of the many forms of atomic clocks. They are often used as low-noise microwave amplifiers in radio telescopes.\n\n, the most important type of maser is the hydrogen maser which is currently used as an atomic frequency standard. Together with other kinds of atomic clocks, these help make up the International Atomic Time standard (\"Temps Atomique International\" or \"TAI\" in French). This is the international time scale coordinated by the International Bureau of Weights and Measures.\n\nNorman Ramsey and his colleagues first conceived of the maser as a timing standard. More recent masers are practically identical to their original design. Maser oscillations rely on the stimulated emission between two hyperfine energy levels of atomic hydrogen. Here is a brief description of how they work:\n\n\nMaser-like stimulated emission has also been observed in nature from interstellar space, and it is frequently called \"superradiant emission\" to distinguish it from laboratory masers. Such emission is observed from molecules such as water (HO), hydroxyl radicals (•OH), methanol (CHOH), formaldehyde (HCHO), and silicon monoxide (SiO). Water molecules in star-forming regions can undergo a population inversion and emit radiation at about 22.0 GHz, creating the brightest spectral line in the radio universe. Some water masers also emit radiation from a rotational transition at a frequency of 96 GHz.\n\nExtremely powerful masers, associated with active galactic nuclei, are known as megamasers and are up to a million times more powerful than stellar masers.\n\nThe meaning of the term \"maser\" has changed slightly since its introduction. Initially the acronym was universally given as \"microwave amplification by stimulated emission of radiation\", which described devices which emitted in the microwave region of the electromagnetic spectrum.\n\nThe principle and concept of stimulated emission has since been extended to more devices and frequencies. Thus, the original acronym is sometimes modified, as suggested by Charles H. Townes, to \"\"molecular\" amplification by stimulated emission of radiation.\" Some have asserted that Townes's efforts to extend the acronym in this way were primarily motivated by the desire to increase the importance of his invention, and his reputation in the scientific community. \n\nWhen the laser was developed, Townes and Schawlow and their colleagues at Bell Labs pushed the use of the term \"optical maser\", but this was largely abandoned in favor of \"laser\", coined by their rival Gordon Gould. In modern usage, devices that emit in the X-ray through infrared portions of the spectrum are typically called lasers, and devices that emit in the microwave region and below are commonly called \"masers\", regardless of whether they emit microwaves or other frequencies.\n\nGould originally proposed distinct names for devices that emit in each portion of the spectrum, including \"grasers\" (gamma ray lasers), \"xasers\" (x-ray lasers), \"uvasers\" (ultraviolet lasers), \" lasers\" (visible lasers), \"irasers\" (infrared lasers), \"masers\" (microwave masers), and \"rasers\" (RF masers). Most of these terms never caught on, however, and all have now become (apart from in science fiction) obsolete except for \"maser\" and \"laser\".\n\nDuring the early 1960s, the Jet Propulsion Laboratory developed a maser to provide ultra-low-noise amplification of S-band microwave signals received from deep space probes. This maser used deeply refrigerated hydrogen to chill the amplifier down to a temperature of four kelvin. Amplification was achieved by exciting a ruby comb with a 12.0 gigahertz klystron. In the early years, it took days to chill and remove the impurities from the hydrogen lines. Refrigeration was a two-stage process with a large Linde unit on the ground, and a crosshead compressor within the antenna. The final injection was at through a micrometer-adjustable entry to the chamber. The whole system noise temperature looking at cold sky (2.7 kelvins in the microwave band) was 17 kelvins. This gave such a low noise figure that the Mariner IV space probe could send still pictures from Mars back to the Earth even though the output power of its radio transmitter was only 15 watts, and hence the total signal power received was only -169 decibels with respect to a milliwatt (dBm).\n\n\n\n"}
{"id": "19958", "url": "https://en.wikipedia.org/wiki?curid=19958", "title": "Mario Botta", "text": "Mario Botta\n\nBotta designed his first buildings at age 16, a two-family house at Morbio Superiore in Ticino. While the arrangements of spaces in this structure is inconsistent, its relationship to its site, separation of living from service spaces, and deep window recesses echo of what would become his stark, strong, towering style. His designs tend to include a strong sense of geometry, often being based on very simple shapes, yet creating unique volumes of space. His buildings are often made of brick, yet his use of material is wide, varied, and often unique.\n\nHis trademark style can be seen widely in Switzerland particularly the Ticino region and also in the Mediatheque in Villeurbanne (1988), a cathedral in Évry (1995), and the San Francisco Museum of Modern Art or SFMOMA (1994). He also designed the Europa-Park Dome, which houses many major events at the Europa-Park theme park resort in Germany. Religious works by Botta, including the Cymbalista Synagogue and Jewish Heritage Center were shown in London at the Royal Institute of British Architects in an exhibition entitled, \"Architetture del Sacro: Prayers in Stone.\" “A church is the place, par excellence, of architecture,” he said in an interview with architectural historian Judith Dupré. “When you enter a church, you already are part of what has transpired and will transpire there. The church is a house that puts a believer in a dimension where he or she is the protagonist. The sacred directly lives in the collective. Man becomes a participant in a church, even if he never says anything.”\n\nIn 1998, he designed the new bus station for Vimercate (near Milan), a red brick building linked to many facilities, underlining the city's recent development.\nHe worked at La Scala's theatre renovation, which proved controversial as preservationists feared that historic details would be lost. \n\nIn 2004, he designed Museum One of the Leeum, Samsung Museum of Art in Seoul, South Korea. On January 1, 2006 he received the Grand Officer award from President of the Italian Republic Carlo Azeglio Ciampi. In 2006 he designed his first ever spa, the Bergoase Spa in Arosa, Switzerland. The spa opened in December 2006 and cost an estimated CHF 35 million. Mario Botta participated in the Stock Exchange of Visions project in 2007. He was a member of the Jury of the Global Holcim Awards in 2012. In 2014 he was awarded with the Prize Javier Carvajal by the Universidad de Navarra.\n\n\n"}
{"id": "19960", "url": "https://en.wikipedia.org/wiki?curid=19960", "title": "Mark Antony", "text": "Mark Antony\n\nMarcus Antonius (14 January 1 August 30 BC), commonly known in English as Mark Antony or Anthony, was a Roman politician and general who played a critical role in the transformation of the Roman Republic from an oligarchy into the autocratic Roman Empire.\n\nAntony was a supporter of Julius Caesar, and served as one of his generals during the conquest of Gaul and the Civil War. Antony was appointed administrator of Italy while Caesar eliminated political opponents in Greece, North Africa, and Spain. After Caesar's death in 44 BC, Antony joined forces with Marcus Aemilius Lepidus, another of Caesar's generals, and Octavian, Caesar's great-nephew and adopted son, forming a three-man dictatorship known to historians as the Second Triumvirate. The Triumvirs defeated Caesar's murderers, the Liberatores, at the Battle of Philippi in 42 BC, and divided the government of the Republic between themselves. Antony was assigned Rome's eastern provinces, including the client kingdom of Egypt, then ruled by Cleopatra VII Philopator, and was given the command in Rome's war against Parthia.\n\nRelations among the triumvirs were strained as the various members sought greater political power. Civil war between Antony and Octavian was averted in 40 BC, when Antony married Octavian's sister, Octavia. Despite this marriage, Antony carried on a love affair with Cleopatra, who bore him three children, further straining Antony's relations with Octavian. Lepidus was expelled from the association in 36 BC, and in 33 BC disagreements between Antony and Octavian caused a split between the remaining Triumvirs. Their ongoing hostility erupted into civil war in 31 BC, as the Roman Senate, at Octavian's direction, declared war on Cleopatra and proclaimed Antony a traitor. Later that year, Antony was defeated by Octavian's forces at the Battle of Actium. Antony and Cleopatra fled to Egypt, where they committed suicide.\n\nWith Antony dead, Octavian became the undisputed master of the Roman world. In 27 BC, Octavian was granted the title of \"Augustus,\" marking the final stage in the transformation of the Roman Republic into an empire, with himself as the first Roman emperor.\n\nA member of the plebeian Antonia gens, Antony was born in Rome on 14 January 83 BC. His father and namesake was Marcus Antonius Creticus, son of the noted orator by the same name who had been murdered during the Marian Terror of the winter of 87–86 BC. His mother was Julia Antonia, a distant cousin of Julius Caesar. Antony was an infant at the time of Lucius Cornelius Sulla's march on Rome in 82 BC.\n\nAccording to the Roman orator Marcus Tullius Cicero, Antony's father was incompetent and corrupt, and was only given power because he was incapable of using or abusing it effectively. In 74 BC he was given military command to defeat the pirates of the Mediterranean, but he died in Crete in 71 BC without making any significant progress. The elder Antony's death left Antony and his brothers, Lucius and Gaius, in the care of their mother, Julia, who later married Publius Cornelius Lentulus Sura, an eminent member of the old Patrician nobility. Lentulus, despite exploiting his political success for financial gain, was constantly in debt due to the extravagance of his lifestyle. He was a major figure in the Second Catilinarian Conspiracy and was summarily executed on the orders of the Consul Cicero in 63 BC for his involvement. His death resulted in a feud between the Antonia and the famous orator.\n\nAntony's early life was characterized by a lack of proper parental guidance. According to the historian Plutarch, he spent his teenage years wandering through Rome with his brothers and friends gambling, drinking, and becoming involved in scandalous love affairs. Antony's contemporary and enemy, Cicero, claimed he had a homosexual relationship with Gaius Scribonius Curio. There is little reliable information on his political activity as a young man, although it is known that he was an associate of Publius Clodius Pulcher and his street gang. He may also have been involved in the Lupercal cult as he was referred to as a priest of this order later in life. By age twenty, Antony had amassed an enormous debt. Hoping to escape his creditors, Antony fled to Greece in 58 BC, where he studied philosophy and rhetoric at Athens.\n\nIn 57 BC, Antony joined the military staff of Aulus Gabinius, the Proconsul of Syria, as chief of the cavalry. This appointment marks the beginning of his military career. As Consul the previous year, Gabinius had consented to the exile of Cicero by Antony's mentor, Publius Clodius Pulcher.\n\nHyrcanus II, the Roman-supported Hasmonean High Priest of Judea, fled Jerusalem to Gabinius to seek protection against his rival and son-in-law Alexander. Years earlier in 63 BC, the Roman general Pompey had captured him and his father, King Aristobulus II, during his war against the remnant of the Seleucid Empire. Pompey had deposed Aristobulus and installed Hyrcanus as Rome's client ruler over Judea. Antony achieved his first military distinctions after securing important victories at Alexandrium and Machaerus. With the rebellion defeated by 56 BC, Gabinius restored Hyrcanus to his position as High Priest in Judea.\n\nThe following year, in 55 BC, Gabinius intervened in the political affairs of Ptolemaic Egypt. Pharaoh Ptolemy XII Auletes had been deposed in a rebellion led by his daughter Berenice IV in 58 BC, forcing him to seek asylum in Rome. During Pompey's conquests years earlier, Ptolemy had received the support of Pompey, who named him an ally of Rome. Gabinius' invasion sought to restore Ptolemy to his throne. This was done against the orders of the Senate but with the approval of Pompey, then Rome's leading politician, and only after the deposed king provided a 10,000 talent bribe. The Greek historian Plutarch records it was Antony who convinced Gabinius to finally act. After defeating the frontier forces of the Egyptian kingdom, Gabinius's army proceeded to attack the palace guards but they surrendered before a battle commenced. With Ptolemy XII restored as Rome's client king, Gabinius garrisoned two thousand Roman soldiers, later known as the \"Gabiniani\", in Alexandria to ensure Ptolemy's authority. In return for its support, Rome exercised considerable power over the kingdom's affairs, particularly control of the kingdom's revenues and crop yields.\n\nDuring the campaign in Egypt, Antony first met Cleopatra, the 14-year-old daughter of Ptolemy XII.\n\nWhile Antony was serving Gabinius in the East, the domestic political situation had changed in Rome. In 60 BC, a secret agreement (known as the \"First Triumvirate\") was entered into between three men to control the Republic: Marcus Licinius Crassus, Gnaeus Pompey Magnus, and Gaius Julius Caesar. Crassus, Rome's wealthiest man, had defeated the slave rebellion of Spartacus in 70 BC; Pompey conquered much of the Eastern Mediterranean in the 60's BC; Caesar was Rome's Pontifex Maximus and a former general in Spain. In 59 BC, Caesar, with funding from Crassus, was elected Consul to pursue legislation favorable to Crassus and Pompey's interests. In return, Caesar was assigned the governorship of Illyricum, Cisalpine Gaul, and Transalpine Gaul for five years beginning in 58 BC. Caesar used his governorship as a launching point for his conquest of free Gaul. In 55 BC, Crassus and Pompey served as Consuls while Caesar's command was extended for another five years. Rome was effectively under the absolute power of these three men. The Triumvirate used the demagogue Publius Clodius Pulcher, Antony's patron, to exile their political rivals, notably Cicero and Cato the Younger.\n\nDuring his early military service, Antony married his cousin Antonia Hybrida Minor, the daughter of Gaius Antonius Hybrida. Sometime between 54 and 47 BC, the union produced a single daughter, Antonia Prima. It is unclear if this was Antony's first marriage.\n\nAntony's association with Publius Clodius Pulcher allowed him to achieve greater prominence. Clodius, through the influence of his benefactor Marcus Licinius Crassus, had developed a positive political relationship with Julius Caesar. Clodius secured Antony a position on Caesar's military staff in 54 BC, joining his conquest of Gaul. Serving under Caesar, Antony demonstrated excellent military leadership. Despite a temporary alienation later in life, Antony and Caesar developed friendly relations which would continue until Caesar's assassination in 44 BC. Caesar's influence secured greater political advancement for Antony. After a year of service in Gaul, Caesar dispatched Antony to Rome to formally begin his political career, receiving election as Quaestor for 52 BC as a member of the Populares faction. Assigned to assist Caesar, Antony returned to Gaul and commanded Caesar's cavalry during his victory at the Battle of Alesia against the Gallic High King Vercingetorix. Following his year in office, Antony was promoted by Caesar to the rank of Legate and assigned command of two legions (approximately 7,500 total soldiers).\n\nDuring this time, the alliance among Caesar, Pompey, and Crassus had effectively ended. Caesar's daughter Julia, who had married Pompey to secure the alliance, died in 54 BC while Crassus was killed at the Battle of Carrhae in 53 BC. Without the stability they provided, the divide between Caesar and Pompey grew ever larger. Caesar's glory in conquering Gaul had served to further strain his alliance with Pompey, who, having grown jealous of his former ally, had drifted away from Caesar's democratic Populares party towards the oligarchic Optimates faction led by Cato. The supporters of Caesar, led by Clodius, and the supporters of Pompey, led by Titus Annius Milo, routinely clashed. In 52 BC, Milo succeeded in assassinating Clodius, resulting in widespread riots and the burning of the Senate meeting house, the Curia Hostilia, by Clodius' street gang. Anarchy resulted, causing the Senate to look to Pompey. Fearing the persecutions of Lucius Cornelius Sulla only thirty-years earlier, they avoided granting Pompey the dictatorship by instead naming him sole Consul for the year, giving him extraordinary but limited powers. Pompey ordered armed soldiers into the city to restore order and to eliminate the remnants of Clodius' gang.\n\nAntony remained on Caesar's military staff until 50 BC, helping mopping-up actions across Gaul to secure Caesar's conquest. With the war over, Antony was sent back to Rome to act as Caesar's protector against Pompey and the other Optimates. With the support of Caesar, who as Pontifex Maximus was head of the Roman religion, Antony was appointed the College of Augurs, an important priestly office responsible for interpreting the will of the Roman gods by studying the flight of birds. All public actions required favorable auspices, granting the college considerable influence. Antony was then elected as one of the ten People's Tribunes for 49 BC. In this position, Antony could protect Caesar from his political enemies by vetoing any actions unfavorable to his patron.\n\nThe feud between Caesar and Pompey erupted into open confrontation by early 49 BC. The Consuls for the year, Gaius Claudius Marcellus Maior and Lucius Cornelius Lentulus Crus, were firm Optimates opposed to Caesar. Pompey, though remaining in Rome, was then serving as the governor of Spain and commanded several legions. Upon assuming office in January, Antony immediately summoned a meeting of the Senate to resolve the conflict: he proposed both Caesar and Pompey lay down their commands and return to the status of mere private citizens. His proposal was well received by most of the senators but the Consuls and Cato vehemently opposed it. Antony then made a new proposal: Caesar would retain only two of his eight legions and the governorship of Illyrium if he was allowed to stand for the Consulship \"in absentia\". This arrangement ensured his immunity from suit would continue: he had needed the Consulship to protect himself from prosecution by Pompey. Though Pompey found the concession satisfactory, Cato and Lentulus refused to back down, with Lentulus even expelling Antony from the Senate meeting by force. Antony fled Rome, fearing for his life, and returned to Caesar's camp on the banks of the Rubicon River, the southern limit of Caesar's lawful command.\n\nWithin days of Antony's expulsion, on 7 January 49 BC, the Senate reconvened. Under the leadership of Cato and with the tacit support of Pompey, the Senate passed the \"final decree\" (\"senatus consultum ultimum\") stripping Caesar of his command and ordering him to return to Rome and stand trial for war crimes. The Senate further declared Caesar a traitor and a public enemy if he did not immediately disband his army. With all hopes of finding a peaceful solution gone after Antony's expulsion, Caesar used Antony as a pretext for marching on Rome. As Tribune, Antony's person was sacrosanct and therefore it was unlawful to harm him or refuse to recognize his veto. Three days later, on 10 January, Caesar crossed the Rubicon River, starting a civil war. During the southern march, Caesar placed Antony as his second in command.\n\nCaesar's rapid advance surprised Pompey, who, along with the other chief members of the Optimates, fled Italy for Greece. After entering Rome, instead of pursuing Pompey, Caesar marched to Spain to defeat Pompeian-loyalists there. Meanwhile, Antony, with the rank of Propraetor despite never having served as Praetor, was installed as governor of Italy and commander of the army, stationed there while Marcus Lepidus, one of Caesar's staff officers, ran the provisional administration of Rome itself. Though Antony was well liked by his soldiers, most other citizens despised him for his lack of interest in the hardships they faced from the civil war.\n\nBy the end of the year 49 BC, Caesar, already the ruler of Gaul, had captured Italy, Spain, Sicily, and Sardinia out of Optimates control. In early 48 BC, he prepared to sail with seven legions to Greece to face Pompey. Caesar had entrusted the defense of Illyricum to Gaius Antonius, Antony's younger brother, and Publius Cornelius Dolabella. Pompey's forces, however, defeated them and assumed control of the Adriatic Sea along with it. Additionally, the two legions they commanded defected to Pompey. Without their fleet, Caesar lacked the necessary transport ships to cross into Greece with his seven legions. Instead, he sailed with only two and placed Antony in command of the remaining five at Brundisium with instructions to join him as soon as he was able. In early 48 BC, Lucius Scribonius Libo was given command of Pompey's fleet, comprising some fifty galleys. Moving off to Brundisium, he blockaded Antony. Antony, however, managed to trick Libo into pursuing some decoy ships, causing Libo's squadron to be trapped and attacked. Most of Libo's fleet managed to escape, but several of his troops were trapped and captured. With Libo gone, Antony joined Caesar in Greece by March 48 BC.\nDuring the Greek campaign, Plutarch records Antony was Caesar's top general and second to only him in reputation. Antony joined Caesar at the western Balkan Peninsula and besieged Pompey's larger army at Dyrrhachium. With food sources running low, Caesar, in July, ordered a nocturnal assault on Pompey's camp, but Pompey's larger forces pushed back the assault. Though an indecisive result, the victory was a tactical win for Pompey. Pompey, however, did not order a counter-assault on Caesar's camp, allowing Caesar to retreat unhindered. Caesar would later remark the civil war would have ended that day if Pompey had only attacked him. Caesar managed to retreat to Thessaly, with Pompey in pursuit.\n\nAssuming a defensive position at the plain of Pharsalus, Caesar's army prepared for pitched battle with Pompey's, which outnumbered his own two to one. At the Battle of Pharsalus on 9 August 48 BC, Caesar commanded the right wing opposite Pompey while Antony commanded the left, indicating Antony's status as Caesar's top general. The resulting battle was a decisive victory for Caesar. Though the civil war had not ended at Pharsulus, the battle marked the pinnacle of Caesar's power and effectively ended the Republic. The battle gave Caesar a much needed boost in legitimacy, as prior to the battle much of the Roman world outside Italy supported Pompey and the Optimates as the legitimate government of Rome. After Pompey's defeat, most of the Senate defected to Caesar, including many of the soldiers who had fought under Pompey. Pompey himself fled to Ptolemaic Egypt, but Pharaoh Ptolemy XIII Theos Philopator feared retribution from Caesar and had Pompey assassinated upon his arrival.\n\nInstead of immediately pursuing Pompey and the remaining Optimates, Caesar returned to Rome and was appointed Dictator with Antony as his Master of the Horse and second in command. Caesar presided over his own election to a second Consulship for 47 BC and then, after eleven days in office, resigned this dictatorship. Caesar then sailed to Egypt, where he deposed Ptolemy XIII in favor of his sister Cleopatra in 47 BC. The young Cleopatra became Caesar's mistress and bore him a son, Caesarion. Caesar's actions further strengthened Roman control over the already Roman-dominated kingdom.\n\nWhile Caesar was away in Egypt, Antony remained in Rome to govern Italy and restore order. Without Caesar to guide him, however, Antony quickly faced political difficulties and proved himself unpopular. The chief cause of his political challenges concerned debt forgiveness. One of the Tribunes for 47 BC, Publius Cornelius Dolabella, a former general under Pompey, proposed a law which would have canceled all outstanding debts. Antony opposed the law for political and personal reasons: he believed Caesar would not support such massive relief and suspected Dolabella had seduced his wife Antonia Hybrida Minor. When Dolabella sought to enact the law by force and seized the Roman Forum,\nAntony responded by unleashing his soldiers upon the assembled mass. The resulting instability, especially among Caesar's veterans who would have benefited from the law, forced Caesar to return to Italy by October 47 BC.\n\nAntony's handling of the affair with Dolabella caused a cooling of his relationship with Caesar. Antony's violent reaction had caused Rome to fall into a state of anarchy. Caesar sought to mend relations with the populist leader; he was elected to a third term as Consul for 46 BC, but proposed the Senate should transfer the consulship to Dolabella. When Antony protested, Caesar was forced to withdraw the motion out of shame. Later, Caesar sought to exercise his prerogatives as Dictator and directly proclaim Dolabella as Consul instead. Antony again protested and, in his capacity as an Augur, declared the omens were unfavorable and Caesar again backed down. Seeing the expediency of removing Dolabella from Rome, Caesar ultimately pardoned him for his role in the riots and took him as one of his generals in his campaigns against the remaining Optimates resistance. Antony, however, was stripped of all official positions and received no appointments for the year 46 BC or 45 BC. Instead of Antony, Caesar appointed Marcus Aemilius Lepidus to be his Consular colleague for 46 BC. While Caesar campaigned in North Africa, Antony remained in Rome as a mere private citizen. After returning victorious from North Africa, Caesar was appointed Dictator for ten years and brought Cleopatra and their son to Rome. Antony again remained in Rome while Caesar, in 45 BC, sailed to Spain to defeat the final opposition to his rule. When Caesar returned in late 45 BC, the civil war was over.\n\nDuring this time Antony married his third wife, Fulvia. Following the scandal with Dolabella, Antony had divorced his second wife and quickly married Fulvia. Fulvia had previously been married to both Publius Clodius Pulcher and Gaius Scribonius Curio, having been a widow since Curio's assassination in 52 BC. Though Antony and Fulvia were formally married in 47 BC, Cicero suggests the two had been in a relationship since at least 58 BC. The union produced two children: Marcus Antonius Antyllus (born 47) and Iullus Antonius (born 45)\n\nWhatever conflicts existed between himself and Caesar, Antony remained faithful to Caesar, ensuring their estrangement did not last long. Antony reunited with Caesar at Narbo in 45 BC with full reconciliation coming in 44 BC when Antony was elected Consul alongside Caesar. Caesar planned a new invasion of Parthia and desired to leave Antony in Italy to govern Rome in his name. The reconciliation came soon after Antony rejected an offer by Gaius Trebonius, one of Caesar's generals, to join a conspiracy to assassinate Caesar.\n\nSoon after they assumed office together, the Lupercalia festival was held on 15 February 44 BC. The festival was held in honor of Lupa, the she-wolf who suckled the infant orphans Romulus and Remus, the founders of Rome. The political atmosphere of Rome at the time of the festival was deeply divided. Caesar had enacted a number of constitutional reforms which centralized effectively all political powers within his own hands. He was granted further honors, including a form of semi-official cult, with Antony as his high priest. Additionally, the day before the festival, Caesar had been named Dictator for Life, effectively granting unlimited power. Caesar's political rivals feared these reforms were his attempts at transforming the Republic into an open monarchy. During the festival's activities, Antony publicly offered Caesar a diadem, which Caesar refused. The event presented a powerful message: a diadem was a symbol of a king. By refusing it, Caesar demonstrated he had no intention of making himself King of Rome. Antony's motive for such actions is not clear and it is unknown if he acted with Caesar's prior approval or on his own.\nA group of Senators resolved to kill Caesar to prevent him from seizing the throne. Chief among them were Marcus Junius Brutus and Gaius Cassius Longinus. Although Cassius was \"the moving spirit\" in the plot, winning over the chief assassins to the cause of tyrannicide, Brutus, with his family's history of deposing Rome's kings, became their leader. Cicero, though not personally involved in the conspiracy, later claimed Antony's actions sealed Caesar's fate as such an obvious display of Caesar's preeminence motivated them to act. Originally, the conspirators had planned to eliminate not only Caesar but also many of his supporters, including Antony, but Brutus rejected the proposal, limiting the conspiracy to Caesar alone. With Caesar preparing to depart for Parthia in late March, the conspirators prepared to act when Caesar appeared for the Senate meeting on the Ides of March (15 March).\n\nAntony was supposed to attend with Caesar, but was waylaid at the door by one of the plotters and prevented from intervening. According to the Greek historian Plutarch, as Caesar arrived at the Senate, Lucius Tillius Cimber presented him with a petition to recall his exiled brother. The other conspirators crowded round to offer their support. Within moments, the entire group, including Brutus, was striking out at the dictator. Caesar attempted to get away, but, blinded by blood, he tripped and fell; the men continued stabbing him as he lay defenseless on the lower steps of the portico. According to Roman historian Eutropius, around 60 or more men participated in the assassination. Caesar was stabbed 23 times and died from the blood loss attributable to multiple stab wounds.\n\nIn the turmoil surrounding the assassination, Antony escaped Rome dressed as a slave, fearing Caesar's death would be the start of a bloodbath among his supporters. When this did not occur, he soon returned to Rome. The conspirators, who styled themselves the \"Liberatores\" (\"The Liberators\"), had barricaded themselves on the Capitoline Hill for their own safety. Though they believed Caesar's death would restore the Republic, Caesar had been immensely popular with the Roman middle and lower classes, who became enraged upon learning a small group of aristocrats had killed their champion.\n\nAntony, as the sole Consul, soon took the initiative and seized the state treasury. Calpurnia, Caesar's widow, presented him with Caesar's personal papers and custody of his extensive property, clearly marking him as Caesar's heir and leader of the Caesarian faction. Caesar's Master of the Horse Marcus Aemilius Lepidus marched over 6,000 troops into Rome on 16 March to restore order and to act as the bodyguards of the Caesarian faction. Lepidus wanted to storm the Capitol, but Antony preferred a peaceful solution as a majority of both the Liberators and Caesar's own supporters preferred a settlement over civil war. On 17 March, at Antony's arrangement, the Senate met to discuss a compromise, which, due to the presence of Caesar's veterans in the city, was quickly reached. Caesar's assassins would be pardoned of their crimes and, in return, all of Caesar's actions would be ratified. In particular, the offices assigned to both Brutus and Cassius by Caesar were likewise ratified. Antony also agreed to accept the appointment of his rival Dolabella as his Consular colleague to replace Caesar. Having neither troops, money, nor popular support, the Liberatores were forced to accept Antony's proposal. This compromise was a great success for Antony, who managed to simultaneously appease Caesar's veterans, reconcile the Senate majority, and appear to the Liberatores as their partner and protector.\nOn 19 March, Caesar's will was opened and read. In it, Caesar posthumously adopted his great-nephew Gaius Octavius and named him his principal heir. Then only 19 years old and stationed with Caesar's army in Macedonia, the youth became a member of Caesar's Julian clan, changing his name to \"Gaius Julius Caesar Octavianus\" (Octavian) in accordance with the conventions of Roman adoption. Though not the chief beneficiary, Antony did receive some bequests.\n\nShortly after the compromise was reached, as a sign of good faith, Brutus, against the advice of Cassius and Cicero, agreed Caesar would be given a public funeral and his will would be validated. Caesar's funeral was held on 20 March. Antony, as Caesar's faithful lieutenant and reigning Consul, was chosen to preside over the ceremony and to recite the elegy. During the demagogic speech, he enumerated the deeds of Caesar and, publicly reading his will, detailed the donations Caesar had left to the Roman people. Antony then seized the blood-stained toga from Caesar's body and presented it to the crowd. Worked into a fury by the bloody spectacle, the assembly rioted. Several buildings in the Forum and some houses of the conspirators were burned to the ground. Panicked, many of the conspirators fled Italy. Under the pretext of not being able to guarantee their safety, Antony relieved Brutus and Cassius of their judicial duties in Rome and instead assigned them responsibility for procuring wheat for Rome from Sicily and Asia. Such an assignment, in addition to being unworthy of their rank, would have kept them far from Rome and shifted the balance towards Antony. Refusing such secondary duties, the two traveled to Greece instead. Additionally, Cleopatra left Rome to return to Egypt.\n\nDespite the provisions of Caesar's will, Antony proceeded to act as leader of the Caesarian faction, including appropriating for himself a portion of Caesar's fortune rightfully belonging to Octavian. Antony enacted the Lex Antonia, which formally abolished the Dictatorship, in an attempt to consolidate his power by gaining the support of the Senatorial class. He also enacted a number of laws he claimed to have found in Caesar's papers to ensure his popularity with Caesar's veterans, particularly by providing land grants to them. Lepidus, with Antony's support, was named Pontifex Maximus to succeed Caesar. To solidify the alliance between Antony and Lepidus, Antony's daughter Antonia Prima was engaged to Lepidus's son, also named Lepidus. Surrounding himself with a bodyguard of over six thousand of Caesar's veterans, Antony presented himself as Caesar's true successor, largely ignoring Octavian.\n\nOctavian arrived in Rome in May to claim his inheritance. Although Antony had amassed political support, Octavian still had opportunity to rival him as the leading member of the Caesarian faction. The Senatorial Republicans increasingly viewed Antony as a new tyrant. Antony had lost the support of many Romans and supporters of Caesar when he opposed the motion to elevate Caesar to divine status. When Antony refused to relinquish Caesar's vast fortune to him, Octavian borrowed heavily to fulfill the bequests in Caesar's will to the Roman people and to his veterans, as well as to establish his own bodyguard of veterans. This earned him the support of Caesarian sympathizers who hoped to use him as a means of eliminating Antony. The Senate, and Cicero in particular, viewed Antony as the greater danger of the two. By summer 44 BC, Antony was in a difficult position due to his actions regarding his compromise with the Liberatores following Caesar's assassination. He could either denounce the Liberatores as murderers and alienate the Senate or he could maintain his support for the compromise and risk betraying the legacy of Caesar, strengthening Octavian's position. In either case, his situation as ruler of Rome would be weakened. Roman historian Cassius Dio later recorded that while Antony, as reigning Consul, maintained the advantage in the relationship, the general affection of the Roman people was shifting to Octavian due to his status as Caesar's son.\nSupporting the Senatorial faction against Antony, Octavian, in September 44 BC, encouraged the leading Senator Marcus Tullius Cicero to attack Antony in a series of speeches portraying him as a threat to the Republican order. Risk of civil war between Antony and Octavian grew. Octavian continued to recruit Caesar's veterans to his side, away from Antony, with two of Antony's legions defecting in November 44 BC. At that time, Octavian, only a private citizen, lacked legal authority to command the Republic's armies, making his command illegal. With popular opinion in Rome turning against him and his Consular term nearing its end, Antony attempted to secure a favorable military assignment to secure an army to protect himself. The Senate, as was custom, assigned Antony and Dolabella the provinces of Macedonia and Syria, respectively, to govern in 43 BC after their Consular terms expired. Antony, however, objected to the assignment, preferring to govern Cisalpine Gaul which had been assigned to Decimus Junius Brutus Albinus, one of Caesar's assassins. When Decimus refused to surrender his province, Antony marched north in December 44 BC with his remaining soldiers to take the province by force, besieging Decimus at Mutina. The Senate, led by a fiery Cicero, denounced Antony's actions and declared him an outlaw.\n\nRatifying Octavian's extraordinary command on 1 January 43 BC, the Senate dispatched him along with Consuls Hirtius and Pansa to defeat Antony and his five legions. Antony's forces were defeated at the Battle of Mutina in April 43 BC, forcing Antony to retreat to Transalpine Gaul. Both consuls were killed, however, leaving Octavian in sole command of their armies, some eight legions.\n\nWith Antony defeated, the Senate, hoping to eliminate Octavian and the remainder of the Caesarian party, assigned command of the Republic's legions to Decimus. Sextus Pompey, son of Caesar's old rival Pompey Magnus, was given command of the Republic's fleet from his base in Sicily while Brutus and Cassius were granted the governorships of Macedonia and Syria respectively. These appointments attempted to renew the \"Republican\" cause. However, the eight legions serving under Octavian, composed largely of Caesar's veterans, refused to follow one of Caesar's murderers, allowing Octavian to retain his command. Meanwhile, Antony recovered his position by joining forces with Marcus Aemilius Lepidus, who had been assigned the governorship of Transalpine Gaul and Nearer Spain. Antony sent Lepidus to Rome to broker a conciliation. Though he was an ardent Caesarian, Lepidus had maintained friendly relations with the Senate and with Sextus Pompey. His legions, however, quickly joined Antony, giving him control over seventeen legions, the largest army in the West.\n\nBy mid-May, Octavian began secret negotiations to form an alliance with Antony to provide a united Caesarian party against the Liberators. Remaining in Cisalpine Gaul, Octavian dispatched emissaries to Rome in July 43 BC demanding he be appointed Consul to replace Hirtius and Pansa and that the decree declaring Antony a public enemy be rescinded. When the Senate refused, Octavian marched on Rome with his eight legions and assumed control of the city in August 43 BC. Octavian proclaimed himself Consul, rewarded his soldiers, and then set about prosecuting Caesar's murderers. By the lex Pedia, all of the conspirators and Sextus Pompey were convicted ″in absentia″ and declared public enemies. Then, at the instigation of Lepidus, Octavian went to Cisalpine Gaul to meet Antony.\n\nIn November 43 BC, Octavian, Lepidus, and Antony met near Bononia. After two days of discussions, the group agreed to establish a three man dictatorship to govern the Republic for five years, known as the \"Three Men for the Restoration of the Republic\" (Latin: \"Triumviri Rei publicae Constituendae\"), known to modern historians as the Second Triumvirate. They shared military command of the Republic's armies and provinces among themselves: Antony received Gaul, Lepidus Spain, and Octavian (as the junior partner) Africa. They jointly governed Italy. The Triumvirate would have to conquer the rest of Rome's holdings; Brutus and Cassius held the Eastern Mediterranean, and Sextus Pompey held the Mediterranean islands. On 27 November 43 BC, the Triumvirate was formally established by a new law, the lex Titia. Octavian and Antony reinforced their alliance through Octavian's marriage to Antony's stepdaughter, Clodia Pulchra.\n\nThe primary objective of the Triumvirate was to avenge Caesar's death and to make war upon his murderers. Before marching against Brutus and Cassius in the East, the Triumvirs issued proscriptions against their enemies in Rome. The Dictator Lucius Cornelius Sulla had taken similar action to purge Rome of his opponents in 82 BC. The proscribed were named on public lists, stripped of citizenship, and outlawed. Their wealth and property were confiscated by the state, and rewards were offered to anyone who secured their arrest or death. With such encouragements, the proscription produced deadly results; two thousand Roman knights were executed, and one third of the Senate, among them Cicero, who was executed on 7 December. The confiscations helped replenish the State Treasury, which had been depleted by Caesar's civil war the decade before; when this seemed insufficient to fund the imminent war against Brutus and Cassius, the Triumvirs imposed new taxes, especially on the wealthy. By January 42 BC the proscription had ended; it had lasted two months, and though less bloody than Sulla's, it traumatized Roman society. A number of those named and outlawed had fled to either Sextus Pompey in Sicily or to the Liberators in the East. Senators who swore loyalty to the Triumvirate were allowed to keep their positions; on 1 January 42 BC, the senate officially deified Caesar as \"The Divine Julius\", and confirmed Antony's position as his high priest.\n\nDue to the infighting within the Triumvirate during 43 BC, Brutus and Cassius had assumed control of much of Rome's eastern territories, and amassed a large army. Before the Triumvirate could cross the Adriatic Sea into Greece where the Liberators had stationed their army, the Triumvirate had to address the threat posed by Sextus Pompey and his fleet. From his base in Sicily, Sextus raided the Italian coast and blockaded the Triumvirs. Octavian's friend and admiral Quintus Rufus Salvidienus thwarted an attack by Sextus against the southern Italian mainland at Rhegium, but Salvidienus was then defeated in the resulting naval battle because of the inexperience of his crews. Only when Antony arrived with his fleet was the blockade broken. Though the blockade was defeated, control of Sicily remained in Sextus's hand, but the defeat of the Liberators was the Triumvirate's first priority.\n\nIn the summer of 42 BC, Octavian and Antony sailed for Macedonia to face the Liberators with nineteen legions, the vast majority of their army (approximately 100,000 regular infantry plus supporting cavalry and irregular auxiliary units), leaving Rome under the administration of Lepidus. Likewise, the army of the Liberators also commanded an army of nineteen legions; their legions, however, were not at full strength while the legions of Antony and Octavian were. While the Triumvirs commanded a larger number of infantry, the Liberators commanded a larger cavalry contingent. The Liberators, who controlled Macedonia, did not wish to engage in a decisive battle, but rather to attain a good defensive position and then use their naval superiority to block the Triumvirs' communications with their supply base in Italy. They had spent the previous months plundering Greek cities to swell their war-chest and had gathered in Thrace with the Roman legions from the Eastern provinces and levies from Rome's client kingdoms.\n\nBrutus and Cassius held a position on the high ground along both sides of the via Egnatia west of the city of Philippi. The south position was anchored to a supposedly impassable marsh, while the north was bordered by impervious hills. They had plenty of time to fortify their position with a rampart and a ditch. Brutus put his camp on the north while Cassius occupied the south of the via Egnatia. Antony arrived shortly and positioned his army on the south of the via Egnatia, while Octavian put his legions north of the road. Antony offered battle several times, but the Liberators were not lured to leave their defensive stand. Thus, Antony tried to secretly outflank the Liberators' position through the marshes in the south. This provoked a pitched battle on 3 October 42 BC. Antony commanded the Triumvirate's army due to Octavian's sickness on the day, with Antony directly controlling the right flank opposite Cassius. Because of his health, Octavian remained in camp while his lieutenants assumed a position on the left flank opposite Brutus. In the resulting first battle of Philippi, Antony defeated Cassius and captured his camp while Brutus overran Octavian's troops and penetrated into the Triumvirs' camp but was unable to capture the sick Octavian. The battle was a tactical draw but due to poor communications Cassius believed the battle was a complete defeat and committed suicide to prevent being captured.\n\nBrutus assumed sole command of the Liberator army and preferred a war of attrition over open conflict. His officers, however, were dissatisfied with these defensive tactics and his Caesarian veterans threatened to defect, forcing Brutus to give battle at the second battle of Philippi on 23 October. While the battle was initially evenly matched, Antony's leadership routed Brutus's forces. Brutus committed suicide the day after the defeat and the remainder of his army swore allegiance to the Triumvirate. Over fifty thousand Romans died in the two battles. While Antony treated the losers mildly, Octavian dealt cruelly with his prisoners and even beheaded Brutus's corpse.\n\nThe battles of Philippi ended the civil war in favor of the Caesarian faction. With the defeat of the Liberators, only Sextus Pompey and his fleet remained to challenge the Triumvirate's control over the Republic.\n\nThe victory at Philippi left the members of the Triumvirate as masters of the Republic, save Sextus Pompey in Sicily. Upon returning to Rome, the Triumvirate repartitioned rule of Rome's provinces among themselves, with Antony as the clear senior partner. He received the largest distribution, governing all of the Eastern provinces while retaining Gaul in the West. Octavian's position improved, as he received Spain, which was taken from Lepidus. Lepidus was then reduced to holding only Africa, and he assumed a clearly tertiary role in the Triumvirate. Rule over Italy remained undivided, but Octavian was assigned the difficult and unpopular task of demobilizing their veterans and providing them with land distributions in Italy. Antony assumed direct control of the East while he installed one of his lieutenants as the ruler of Gaul. During his absence, several of his supporters held key positions in Rome to protect his interests there.\n\nThe East was in need of reorganization after the rule of the Liberators in the previous years. In addition, Rome contended with the Parthian Empire for dominance of the Near East. The Parthian threat to the Triumvirate's rule was urgent due to the fact that the Parthians supported the Liberators in the recent civil war, aid which included the supply troops at Philippi. As ruler of the East, Antony also assumed responsibility for overseeing Caesar's planned invasion of Parthia to avenge the defeat of Marcus Licinius Crassus at the Battle of Carrhae in 53 BC.\n\nIn 42 BC, the Roman East was composed of several directly controlled provinces and client kingdoms. The provinces included Macedonia, Asia, Bithynia, Cilicia, Cyprus, Syria, and Cyrenaica. Approximately half of the eastern territory was controlled by Rome's client kingdoms, nominally independent kingdoms subject to Roman direction. These kingdoms included:\n\nAntony spent the winter of 42 BC in Athens, where he ruled generously towards the Greek cities. A proclaimed \"philhellene\" (\"Friend of all things Greek\"), Antony supported Greek culture to win the loyalty of the inhabitants of the Greek East. He attended religious festivals and ceremonies, including initiation into the Eleusinian Mysteries, a secret cult dedicated to the worship of the goddesses Demeter and Persephone. Beginning in 41 BC, he traveled across the Aegean Sea to Anatolia, leaving his friend Lucius Marcius Censorius as governor of Macedonia and Achaea. Upon his arrival in Ephesus in Asia, Antony was worshiped as the god Dionysus born anew. He demanded heavy taxes from the Hellenic cities in return for his pro-Greek culture policies, but exempted those cities which had remained loyal to Caesar during the civil war and compensated those cities which had suffered under Caesar's assassins, including Rhodes, Lycia, and Tarsus. He granted pardons to all Roman nobles living in the East who had supported the Republican cause, except for Caesar's assassins.\n\nRuling from Ephesus, Antony consolidated Rome's hegemony in the East, receiving envoys from Rome's client kingdoms and intervening in their dynastic affairs, extracting enormous financial \"gifts\" from them in the process. Though King Deiotarus of Galatia supported Brutus and Cassius following Caesar's assassination, Antony allowed him to retain his position. He also confirmed Ariarathes X as king of Cappadocia after the execution of his brother Ariobarzanes III of Cappadocia by Cassius before the Battle of Philippi. In Hasmonean Judea, several Jewish delegations complained to Antony of the harsh rule of Phasael and Herod, the sons of Rome's assassinated chief Jewish minister Antipater the Idumaean. After Herod offered him a large financial gift, Antony confirmed the brothers in their positions. Subsequently, influenced by the beauty and charms of Glaphyra, the widow of Archelaüs (formerly the high priest of Comana), Antony deposed Ariarathes, and appointed Glaphyra's son, Archelaüs, to rule Cappadocia.\nIn October 41, Antony requested Rome's chief eastern vassal, the queen of Ptolemaic Egypt Cleopatra, meet him at Tarsus in Cilicia. Antony had first met a young Cleopatra while campaigning in Egypt in 55 BC and again in 48 BC when Caesar had backed her as queen of Egypt over the claims of her half-sister Arsinoe. Cleopatra would bear Caesar a son, Caesarion, in 47 BC and the two living in Rome as Caesar's guests until his assassination in 44 BC. After Caesar's assassination, Cleopatra and Caesarion returned to Egypt, where she named the child as her co-ruler. In 42 BC, the Triumvirate, in recognition for Cleopatra's help towards Publius Cornelius Dolabella in opposition to the Liberators, granted official recognition to Caesarion's position as king of Egypt. Arriving in Tarsus aboard her magnificent ship, Cleopatra invited Antony to a grand banquet to solidify their alliance. As the most powerful of Rome's eastern vassals, Egypt was indispensable in Rome's planned military invasion of the Parthian Empire. At Cleopatra's request, Antony ordered the execution of Arsinoe, who, though marched in Caesar's triumphal parade in 46 BC, had been granted sanctuary at the temple of Artemis in Ephesus. Antony and Cleopatra then spent the winter of 41 BC together in Alexandria. Cleopatra bore Antony twin children, Alexander Helios and Cleopatra Selene II, in 40 BC, and a third, Ptolemy Philadelphus, in 36 BC. Antony also granted formal control over Cyprus, which had been under Egyptian control since 47 BC during the turmoil of Caesar's civil war, to Cleopatra in 40 BC as a gift for her loyalty to Rome.\n\nAntony, in his first months in the East, raised money, reorganized his troops, and secured the alliance of Rome's client kingdoms. He also promoted himself as Hellenistic ruler, which won him the affection of the Greek peoples of the East but also made him the target of Octavian's propaganda in Rome. According to some ancient authors, Antony led a carefree life of luxury in Alexandria. Upon learning the Parthian Empire had invaded Rome's territory in early 40 BC, Antony left Egypt for Syria to confront the invasion. However, after a short stay in Tyre, he was forced to sail with his army to Italy to confront Octavian due to Octavian's war against Antony's wife and brother.\n\nFollowing the defeat of Brutus and Cassius, while Antony was stationed in the East, Octavian had authority over the West. Octavian's chief responsibility was distributing land to tens of thousands of Caesar's veterans who had fought for the Triumvirate. Additionally, tens of thousands of veterans who had fought for the Republican cause in the war also required land grants. This was necessary to ensure they would not support a political opponent of the Triumvirate. However, the Triumvirs did not possess sufficient state-controlled land to allot to the veterans. This left Octavian with two choices: alienating many Roman citizens by confiscating their land, or alienating many Roman soldiers who might back a military rebellion against the Triumvirate's rule. Octavian chose the former. As many as eighteen Roman towns through Italy were affected by the confiscations of 41 BC, with entire populations driven out.\n\nLed by Fulvia, the wife of Antony, the Senators grew hostile towards Octavian over the issue of the land confiscations. According to the ancient historian Cassius Dio, Fulvia was the most powerful woman in Rome at the time. According to Dio, while Publius Servilius Vatia and Lucius Antonius were the Consuls for the year 41 BC, real power was vested in Fulvia. As the mother-in-law of Octavian and the wife of Antony, no action was taken by the Senate without her support. Fearing Octavian's land grants would cause the loyalty of the Caesarian veterans to shift away from Antony, Fulvia traveled constantly with her children to the new veteran settlements in order to remind the veterans of their debt to Antony. Fulvia also attempted to delay the land settlements until Antony returned to Rome, so that he could share credit for the settlements. With the help of Antony's brother, the Consul of 41 BC Lucius Antonius, Fulvia encouraged the Senate to oppose Octavian's land policies.\n\nThe conflict between Octavian and Fulvia caused great political and social unrest throughout Italy. Tensions escalated into open war, however, when Octavian divorced Clodia Pulchra, Fulvia's daughter from her first husband Publius Clodius Pulcher. Outraged, Fulvia, supported by Lucius, raised an army to fight for Antony's rights against Octavian. According to the ancient historian Appian, Fulvia's chief reason for the war was her jealousy of Antony's affairs with Cleopatra in Egypt and desire to draw Antony back to Rome. Lucius and Fulvia took a political and martial gamble in opposing Octavian and Lepidus, however, as the Roman army still depended on the Triumvirs for their salaries. Lucius and Fulvia, supported by their army, marched on Rome and promised the people an end to the Triumvirate in favor of Antony's sole rule. However, when Octavian returned to the city with his army, the pair was forced to retreat to Perusia in Etruria. Octavian placed the city under siege while Lucius waited for Antony's legions in Gaul to come to his aid. Away in the East and embarrassed by Fulvia's actions, Antony gave no instructions to his legions. Without reinforcements, Lucius and Fulvia were forced to surrender in February 40 BC. While Octavian pardoned Lucius for his role in the war and even granted him command in Spain as his chief lieutenant there, Fulvia was forced to flee to Greece with her children. With the war over, Octavian was left in sole control over Italy. When Antony's governor of Gaul died, Octavian took over his legions there, further strengthening his control over the West.\n\nDespite the Parthian Empire's invasion of Rome's eastern territories, Fulvia's civil war forced Antony to leave the East and return to Rome in order to secure his position. Meeting her in Athens, Antony rebuked Fulvia for her actions before sailing on to Italy with his army to face Octavian, laying siege to Brundisium. This new conflict proved untenable for both Octavian and Antony, however. Their centurions, who had become important figures politically, refused to fight due to their shared service under Caesar. The legions under their command followed suit. Meanwhile, in Sicyon, Fulvia died of a sudden and unknown illness. Fulvia's death and the mutiny of their soldiers allowed the triumvirs to effect a reconciliation through a new power sharing agreement in September 40 BC. The Roman world was redivided, with Antony receiving the Eastern provinces, Octavian the Western provinces, and Lepidus relegated to a clearly junior position as governor of Africa. This agreement, known as the \"Treaty of Brundisium\", reinforced the Triumvirate and allowed Antony to begin preparing for Caesar's long-awaited campaign against the Parthian Empire. As a symbol of their renewed alliance, Antony married Octavia, Octavian's sister, in October 40 BC.\n\nThe rise of the Parthian Empire in the 3rd century BC and Rome's expansion into the Eastern Mediterranean during the 2nd century BC brought the two powers into direct contact, causing centuries of tumultuous and strained relations. Though periods of peace developed cultural and commercial exchanges, war was a constant threat. Influence over the buffer state of the Kingdom of Armenia, located to the north-east of Roman Syria, was often a central issue in the Roman-Parthian conflict. In 95 BC, Parthian Shah Mithridates II, installed Tigranes the Great as Parthian's client-king over Armenia. Tigranes would wage a series of three wars against Rome before being ultimately defeated by Pompey in 66 BC. Thereafter, with his son Artavasdes II in Rome as a hostage, Tigranes would rule Armenia as an ally of Rome until his death in 55 BC. Rome then installed Artavasdes II as king and continued its influence over Armenia.\n\nIn 53 BC, Rome's governor of Syria, Marcus Licinius Crassus, led an expedition across the Euphrates River into Parthian territory to confront the Parthian Shah Orodes II. Artavasdes II offered Crassus the aid of nearly forty thousand troops to assist his Parthian expedition on the condition that Crassus invade through Armenia as the safer route. Crassus refused, choosing instead the more direct route by crossing the Euphrates directly into desert Parthian territory. Crassus' actions proved disastrous as his army was defeated at the Battle of Carrhae by a numerically inferior Parthian force. Crassus' defeat forced Armenia to shift its loyalty to Parthia, with Artavasdes II's sister marrying Orodes' son and heir Pacorus.\n\nIn early 44 BC, Julius Caesar announced his intentions to invade Parthia and restore Roman power in the East. His reasons were to punish the Parthians for assisting Pompey in the recent civil war, to avenge Crassus' defeat at Carrhae, and especially to match the glory of Alexander the Great for himself. Before Caesar could launch his campaign, however, he was assassinated. As part of the compromise between Antony and the Republicans to restore order following Caesar's murder, Publius Cornelius Dolabella was assigned the governorship of Syria and command over Caesar's planned Parthian campaign. The compromise did not hold, however, and the Republicans were forced to flee to the East. The Republicans directed Quintus Labienus to attract the Parthians to their side in the resulting war against Antony and Octavian. After the Republicans were defeated at the Battle of Philippi, Labienus joined the Parthians. Despite Rome's internal turmoil during the time, the Parthians did not immediately benefit from the power vacuum in the East due to Orodes II's reluctance despite Labienus' urgings to the contrary.\n\nIn the summer of 41 BC, Antony, to resassert Roman power in the East, conquered Palmyra on the Roman-Parthian border. Antony then spent the winter of 41 BC in Alexandria with Cleopatra, leaving only two legions to defend the Syrian border against Parthian incursions. The legions, however, were composed of former Republican troops and Labienus convinced Orodes II to invade.\n\nA Parthian army, led by Orodes II's eldest son Pacorus, invaded Syria in early 40 BC. Labienus, the Republican ally of Brutus and Cassius, accompanied him to advise him and to rally the former Republican soldiers stationed in Syria to the Parthian cause. Labienus recruited many of the former Republican soldiers to the Parthian campaign in opposition to Antony. The joint Parthian–Roman force, after initial success in Syria, separated to lead their offensive in two directions: Pacorus marched south toward Hasmonean Judea while Labienus crossed the Taurus Mountains to the north into Cilicia. Labienus conquered southern Anatolia with little resistance. The Roman governor of Asia, Lucius Munatius Plancus, a partisan of Antony, was forced to flee his province, allowing Labienus to recruit the Roman soldiers stationed there. For his part, Pacorus advanced south to Phoenicia and Palestine. In Hasmonean Judea, the exiled prince Antigonus allied himself with the Parthians. When his brother, Rome's client king Hyrcanus II, refused to accept Parthian domination, he was deposed in favor of Antigonus as Parthia's client king in Judea. Pacorus' conquest had captured much of the Syrian and Palestinian interior, with much of the Phoenician coast occupied as well. The city of Tyre remained the last major Roman outpost in the region.\n\nAntony, then in Egypt with Cleopatra, did not respond immediately to the Parthian invasion. Though he left Alexandria for Tyre in early 40 BC, when he learned of the civil war between his wife and Octavian, he was forced to return to Italy with his army to secure his position in Rome rather than defeat the Parthians. Instead, Antony dispatched Publius Ventidius Bassus to check the Parthian advance. Arriving in the East in spring 39 BC, Ventidius surprised Labienus near the Taurus Mountains, claiming victory at the Cilician Gates. Ventidius ordered Labienus executed as a traitor and the formerly rebellious Roman soldiers under his command were reincorporated under Antony's control. He then met a Parthian army at the border between Cilicia and Syria, defeating it and killing a large portion of the Parthian soldiers at the Amanus Pass. Ventidius's actions temporarily halted the Parthian advance and restored Roman authority in the East, forcing Pacorus to abandon his conquests and return to Parthia.\n\nIn the spring of 38 BC, the Parthians resumed their offensive with Pacorus leading an army across the Euphrates. Ventidius, in order to gain time, leaked disinformation to Pacorus implying that he should cross the Euphrates River at their usual ford. Pacorus did not trust this information and decided to cross the river much farther downstream; this was what Ventidius hoped would occur and gave him time to get his forces ready. The Parthians faced no opposition and proceeded to the town of Gindarus in Cyrrhestica where Ventidius's army was waiting. At the Battle of Cyrrhestica, Ventidius inflicted an overwhelming defeat against the Parthians which resulted in the death of Pacorus. Overall, the Roman army had achieved a complete victory with Ventidius' three successive victories forcing the Parthians back across the Euphrates. Pacorus' death threw the Parthian Empire into chaos. Shah Orodes II, overwhelmed by the grief of his son's death, appointed his younger son Phraates IV as his successor. However, Phraates IV assassinated Orodes II in late 38 BC, succeeding him on the throne.\n\nVentidius feared Antony's wrath if he invaded Parthian territory, thereby stealing his glory; so instead he attacked and subdued the eastern kingdoms, which had revolted against Roman control following the disastrous defeat of Crassus at Carrhae. One such rebel was King Antiochus of Commagene, whom he besieged in Samosata. Antiochus tried to make peace with Ventidius, but Ventidius told him to approach Antony directly. After peace was concluded, Antony sent Ventidius back to Rome where he celebrated a triumph, the first Roman to triumph over the Parthians.\n\nWhile Antony and the other Triumvirs ratified the Treaty of Brundisium to redivide the Roman world among themselves, the rebel general Sextus Pompey, the son of Caesar's rival Pompey the Great, was largely ignored. From his stronghold on Sicily, he continued his piratical activities across Italy and blocked the shipment of grain to Rome. The lack of food in Rome caused the public to blame the Triumvirate and shift its sympathies towards Pompey. This pressure forced the Triumvirs to meet with Sextus in early 39 BC.\n\nWhile Octavian wanted an end to the ongoing blockade of Italy, Antony sought peace in the West in order to make the Triumvirate's legions available for his service in his planned campaign against the Parthians. Though the Triumvirs rejected Sextus' initial request to replace Lepidus as the third man within the Triumvirate, they did grant other concessions. Under the terms of the Treaty of Misenum, Sextus was allowed to retain control over Sicily and Sardinia, with the provinces of Corsica and Greece being added to his territory. He was also promised a future position with the Priestly College of Augurs and the Consulship for 35 BC. In exchange, Sextus agreed to end his naval blockade of Italy, supply Rome with grain, and halt his piracy of Roman merchant ships. However, the most important provision of the Treaty was the end of the proscription the Trimumvirate had begun in late 43 BC. Many of the proscribed Senators, rather than face death, fled to Sicily seeking Sextus' protection. With the exception of those responsible for Caesar's assassination, all those proscribed were allowed to return to Rome and promised compensation. This caused Sextus to lose many valuable allies as the formerly exiled Senators gradually aligned themselves with either Octavian or Antony. To secure the peace, Octavian betrothed his three-year-old nephew and Antony's stepson Marcus Claudius Marcellus to Sextus' daughter Pompeia. With peace in the West secured, Antony planned to retaliate against Parthia by invading their territory. Under an agreement with Octavian, Antony would be supplied with extra troops for his campaign. With this military purpose on his mind, Antony sailed to Greece with Octavia, where he behaved in a most extravagant manner, assuming the attributes of the Greek god Dionysus in 39 BC.\nThe peace with Sextus was short lived, however. When Sextus demanded control over Greece as the agreement provided, Antony demanded the province's tax revenues be to fund the Parthian campaign. Sextus refused. Meanwhile, Sextus' admiral Menas betrayed him, shifting his loyalty to Octavian and thereby granting him control of Corsica, Sardinia, three of Sextus' legions, and a larger naval force. These actions worked to renew Sextus' blockade of Italy, preventing Octavian from sending the promised troops to Antony for the Parthian campaign. This new delay caused Antony to quarrel with Octavian, forcing Octavia to mediate a truce between them. Under the Treaty of Tarentum, Antony provided a large naval force for Octavian's use against Sextus while Octavian promised to raise new legions for Antony to support his invasion of Parthia. As the term of the Triumvirate was set to expire at the end of 38 BC, the two unilaterally extended their term of office another five years until 33 BC without seeking approval of the Senate or the popular assemblies. To seal the Treaty, Antony's elder son Marcus Antonius Antyllus, then only 6 years old, was betrothed to Octavian's only daughter Julia, then only an infant. With the Treaty signed, Antony returned to the East, leaving Octavia in Italy.\n\nWith Publius Ventidius Bassus returned to Rome in triumph for his defensive campaign against the Parthians, Antony appointed Gaius Sosius as the new governor of Syria and Cilicia in early 38 BC. Antony, still in the West negotiating with Octavian, ordered Sosius to depose Antigonus, who had been installed in the recent Parthian invasion as the ruler of Hasmonean Judea, and to make Herod the new Roman client king in the region. Years before in 40 BC, the Roman Senate had proclaimed Herod \"King of the Jews\" because Herod had been a loyal supporter of Hyrcanus II, Rome's previous client king before the Parthian invasion, and was from a family with long standing connections to Rome. The Romans hoped to use Herod as a bulwark against the Parthians in the coming campaign.\n\nAdvancing south, Sosius captured the island-city of Aradus on the coast of Phoenicia by the end of 38 BC. The following year, the Romans besieged Jerusalem. After a forty-day siege, the Roman soldiers stormed the city and, despite Herod's pleas for restraint, acted without mercy, pillaging and killing all in their path, prompting Herod to complain to Antony. Herod finally resorted to bribing Sosius and his troops in order that they would not leave him \"king of a desert\". Antigonus was forced to surrender to Sosius, and was sent to Antony for the triumphal procession in Rome. Herod, however, fearing that Antigonus would win backing in Rome, bribed Antony to execute Antigonus. Antony, who recognized that Antigonus would remain a permanent threat to Herod, ordered him beheaded in Antioch. Now secure on his throne, Herod would rule the Herodian Kingdom until his death in 4 BC, and would be an ever-faithful client king of Rome.\n\nWith the Triumvirate renewed in 38 BC, Antony returned to Athens in the winter with his new wife Octavia, the sister of Octavian. With the assassination of the Parthian king Orodes II by his son Phraates IV, who then seized the Parthian throne, in late 38 BC, Antony prepared to invade Parthia himself.\nAntony, however, realized Octavian had no intention of sending him the additional legions he had promised under the Treaty of Tarentum. To supplement his own armies, Antony instead looked to Rome's principal vassal in the East: his lover Cleopatra. In addition to significant financial resources, Cleopatra's backing of his Parthian campaign allowed Antony to amass the largest army Rome had ever assembled in the East. Wintering in Antioch during 37, Antony's combined Roman–Egyptian army numbered some 200,000, including sixteen legions (approximately 160,000 soldiers) plus an additional 40,000 auxiliaries. Such a force was twice the size of Marcus Licinius Crassus's army from his failed Parthian invasion of 53 BC and three times those of Lucius Licinius Lucullus and Lucius Cornelius Sulla during the Mithridatic Wars. The size of his army indicated Antony's intention to conquer Parthia, or at least receive its submission by capturing the Parthian capital of Ecbatana. Antony's rear was protected by Rome's client kingdoms in Anatolia, Syria, and Judea, while the client kingdoms of Cappadocia, Pontus, and Commagene would provide supplies along the march.\n\nAntony's first target for his invasion was the Kingdom of Armenia. Ruled by King Artavasdes II of Armenia, Armenia had been an ally of Rome since the defeat of Tigranes the Great by Pompey the Great in 66 BC during the Third Mithridatic War. However, following Marcus Licinius Crassus's defeat at the Battle of Carrhae in 53 BC, Armenia was forced into an alliance with Parthia due to Rome's weakened position in the East. Antony dispatched Publius Canidius Crassus to Armenia, receiving Artavasdes II's surrender without opposition. Canidius then led an invasion into the Transcaucasia, subduing Iberia. There, Canidius forced the Iberian King Pharnavaz II into an alliance against Zober, king of neighboring Albania, subduing the kingdom and reducing it to a Roman protectorate.\n\nWith Armenia and the Caucasus secured, Antony marched south, crossing into the Parthian province of Media Atropatene. Though Antony desired a pitched battle, the Parthians would not engage, allowing Antony to march deep into Parthian territory by mid-August of 36 BC. This forced Antony to leave his logistics train in the care of two legions (approximately 10,000 soldiers), which was then attacked and completely destroyed by the Parthian army before Antony could rescue them. Though the Armenian King Artavasdes II and his cavalry were present during the massacre, they did not intervene. Despite the ambush, Antony continued the campaign. However, Antony was soon forced to retreat in mid-October after a failed two-month siege of the provincial capital.\n\nThe retreat soon proved a disaster as Antony's demoralized army faced increasing supply difficulties in the mountainous terrain during winter while constantly being harassed by the Parthian army. According to the Greek historian Plutarch, eighteen battles were fought between the retreating Romans and the Parthians during the month-long march back to Armenia, with approximately 20,000 infantry and 4,000 cavalry dying during the retreat alone. Once in Armenia, Antony quickly marched back to Syria to protect his interests there by late 36 BC, losing an additional 8,000 soldiers along the way. In all, two-fifths of his original army (some 80,000 men) had died during his failed campaign.\n\nMeanwhile, in Rome, the triumvirate was no more. Octavian forced Lepidus to resign after the older triumvir attempted to take control of Sicily after the defeat of Sextus. Now in sole power, Octavian was occupied in wooing the traditional Republican aristocracy to his side. He married Livia and started to attack Antony in order to raise himself to power. He argued that Antony was a man of low morals to have left his faithful wife abandoned in Rome with the children to be with the promiscuous queen of Egypt. Antony was accused of everything, but most of all, of \"going native\", an unforgivable crime to the proud Romans. Several times Antony was summoned to Rome, but remained in Alexandria with Cleopatra.\n\nAgain with Egyptian money, Antony invaded Armenia, this time successfully. In the return, a mock Roman triumph was celebrated in the streets of Alexandria. The parade through the city was a pastiche of Rome's most important military celebration. For the finale, the whole city was summoned to hear a very important political statement. Surrounded by Cleopatra and her children, Antony ended his alliance with Octavian.\n\nHe distributed kingdoms among his children: Alexander Helios was named king of Armenia, Media and Parthia (territories which were not for the most part under the control of Rome), his twin Cleopatra Selene got Cyrenaica and Libya, and the young Ptolemy Philadelphus was awarded Syria and Cilicia. As for Cleopatra, she was proclaimed Queen of Kings and Queen of Egypt, to rule with Caesarion (Ptolemy XV Caesar, son of Cleopatra by Julius Caesar), King of Kings and King of Egypt. Most important of all, Caesarion was declared legitimate son and heir of Caesar. These proclamations were known as the \"Donations of Alexandria\" and caused a fatal breach in Antony's relations with Rome.\n\nWhile the distribution of nations among Cleopatra's children was hardly a conciliatory gesture, it did not pose an immediate threat to Octavian's political position. Far more dangerous was the acknowledgment of Caesarion as legitimate and heir to Caesar's name. Octavian's base of power was his link with Caesar through adoption, which granted him much-needed popularity and loyalty of the legions. To see this convenient situation attacked by a child borne by the richest woman in the world was something Octavian could not accept. The triumvirate expired on the last day of 33 BC and was not renewed. Another civil war was beginning.\n\nDuring 33 and 32 BC, a propaganda war was fought in the political arena of Rome, with accusations flying between sides. Antony (in Egypt) divorced Octavia and accused Octavian of being a social upstart, of usurping power, and of forging the adoption papers by Caesar. Octavian responded with treason charges: of illegally keeping provinces that should be given to other men by lots, as was Rome's tradition, and of starting wars against foreign nations (Armenia and Parthia) without the consent of the Senate.\n\nAntony was also held responsible for Sextus Pompey's execution without a trial. In 32 BC, the Senate deprived him of his powers and declared war against Cleopatra – not Antony, because Octavian had no wish to advertise his role in perpetuating Rome's internecine bloodshed. Both consuls, Gnaeus Domitius Ahenobarbus and Gaius Sosius, and a third of the Senate abandoned Rome to meet Antony and Cleopatra in Greece.\nIn 31 BC, the war started. Octavian's general Marcus Vipsanius Agrippa captured the Greek city and naval port of Methone, loyal to Antony. The enormous popularity of Octavian with the legions secured the defection of the provinces of Cyrenaica and Greece to his side. On September 2, the naval Battle of Actium took place. Antony and Cleopatra's navy was overwhelmed, and they were forced to escape to Egypt with 60 ships.\n\nOctavian, now close to absolute power, did not intend to give Antony and Cleopatra any rest. In August 30 BC, assisted by Agrippa, he invaded Egypt. With no other refuge to escape to, Antony committed suicide by stabbing himself with his sword in the mistaken belief that Cleopatra had already done so. When he found out that Cleopatra was still alive, his friends brought him to Cleopatra's monument in which she was hiding, and he died in her arms.\n\nCleopatra was allowed to conduct Antony's burial rites after she had been captured by Octavian. Realising that she was destined for Octavian's triumph in Rome, she made several attempts to take her life and finally succeeded in mid-August. Octavian had Caesarion murdered, but he spared Antony's children by Cleopatra, who were paraded through the streets of Rome. Antony's daughters by Octavia were spared, as was his son, Iullus Antonius. But his elder son, Marcus Antonius Antyllus, was killed by Octavian's men while pleading for his life in the Caesareum.\n\nCicero's son, Cicero Minor, announced Antony's death to the senate. Antony's honours were revoked and his statues removed, but he was not subject to a complete damnatio memoriae. Cicero Minor also made a decree that no member of the Antonii would ever bear the name Marcus again. \"In this way Heaven entrusted the family of Cicero the final acts in the punishment of Antony.\"\n\nWhen Antony died, Octavian became uncontested ruler of Rome. In the following years, Octavian, who was known as Augustus after 27 BC, managed to accumulate in his person all administrative, political, and military offices. When Augustus died in AD 14, his political powers passed to his adopted son Tiberius; the Roman Principate had begun.\n\nThe rise of Caesar and the subsequent civil war between his two most powerful adherents effectively ended the credibility of the Roman oligarchy as a governing power and ensured that all future power struggles would centre upon which one individual would achieve supreme control of the government, eliminating the Senate and the former magisterial structure as important foci of power in these conflicts. Thus, in history, Antony appears as one of Caesar's main adherents, he and Octavian Augustus being the two men around whom power coalesced following the assassination of Caesar, and finally as one of the three men chiefly responsible for the demise of the Roman Republic.\n\nAntony had been married in succession to Fadia, Antonia, Fulvia, Octavia and Cleopatra, and left behind him a number of children. Through his daughters by Octavia, he would be ancestor to the Roman Emperors Caligula, Claudius and Nero.\n\nThrough his daughters by Octavia, he would become the paternal great grandfather of Roman Emperor Caligula, the maternal grandfather of Emperor Claudius, and both maternal great-great-grandfather and paternal great-great uncle of the Emperor Nero of the Julio-Claudian dynasty, the very family, as represented by Octavian Augustus, that he had fought to defeat. Through his eldest daughter, he would become ancestor to the long line of kings and co-rulers of the Bosporan Kingdom, the longest-living Roman client kingdom, as well as the rulers and royalty of several other Roman client states. Through his daughter by Cleopatra, Antony would become ancestor to the royal family of Mauretania, another Roman client kingdom, while through his sole surviving son Iullus, he would be ancestor to several famous Roman statesmen.\n\nWorks in which the character of Mark Antony plays a central role:\n\n\n\n\n\n"}
{"id": "19961", "url": "https://en.wikipedia.org/wiki?curid=19961", "title": "Manchester United F.C.", "text": "Manchester United F.C.\n\nManchester United Football Club, commonly known as Man United, or simply United, is a professional football club based in Old Trafford, Greater Manchester, England, that competes in the Premier League, the top flight of English football. Nicknamed \"the Red Devils\", the club was founded as Newton Heath LYR Football Club in 1878, changed its name to Manchester United in 1902 and moved to its current stadium, Old Trafford, in 1910.\n\nManchester United have won more trophies than any other club in English football, with a record 20 League titles, 12 FA Cups, 5 League Cups and a record 21 FA Community Shields. United have also won three UEFA Champions Leagues, one UEFA Europa League, one UEFA Cup Winners' Cup, one UEFA Super Cup, one Intercontinental Cup and one FIFA Club World Cup. In 1998–99, the club became the first in the history of English football to achieve the continental European treble. By winning the UEFA Europa League in 2016–17, they became one of five clubs to have won all three main UEFA club competitions, and the only English club to have won every competition available to them.\n\nThe 1958 Munich air disaster claimed the lives of eight players. In 1968, under the management of Matt Busby, Manchester United became the first English football club to win the European Cup. Alex Ferguson won 38 trophies as manager, including 13 Premier League titles, 5 FA Cups and 2 UEFA Champions Leagues, between 1986 and 2013, when he announced his retirement.\n\nManchester United was the highest-earning football club in the world for 2016–17, with an annual revenue of €676.3 million, and the world's most valuable football club in 2018, valued at £3.1 billion. As of June 2015, it is the world's most valuable football brand, estimated to be worth $1.2 billion. After being floated on the London Stock Exchange in 1991, the club was purchased by Malcolm Glazer in May 2005 in a deal valuing the club at almost £800 million, after which the company was taken private again, before going public once more in August 2012, when they made an initial public offering on the New York Stock Exchange. Manchester United is one of the most widely supported football clubs in the world, and has rivalries with Liverpool, Manchester City, Arsenal, and Leeds United.\n\nManchester United was formed in 1878 as Newton Heath LYR Football Club by the Carriage and Wagon department of the Lancashire and Yorkshire Railway (LYR) depot at Newton Heath. The team initially played games against other departments and railway companies, but on 20 November 1880, they competed in their first recorded match; wearing the colours of the railway company – green and gold – they were defeated 6–0 by Bolton Wanderers' reserve team. By 1888, the club had become a founding member of The Combination, a regional football league. Following the league's dissolution after only one season, Newton Heath joined the newly formed Football Alliance, which ran for three seasons before being merged with the Football League. This resulted in the club starting the 1892–93 season in the First Division, by which time it had become independent of the railway company and dropped the \"LYR\" from its name. After two seasons, the club was relegated to the Second Division.\nIn January 1902, with debts of £2,670 – equivalent to £ in 2019 – the club was served with a winding-up order. Captain Harry Stafford found four local businessmen, including John Henry Davies (who became club president), each willing to invest £500 in return for a direct interest in running the club and who subsequently changed the name; on 24 April 1902, Manchester United was officially born. Under Ernest Mangnall, who assumed managerial duties in 1903, the team finished as Second Division runners-up in 1906 and secured promotion to the First Division, which they won in 1908 – the club's first league title. The following season began with victory in the first ever Charity Shield and ended with the club's first FA Cup title. Manchester United won the First Division for the second time in 1911, but at the end of the following season, Mangnall left the club to join Manchester City.\n\nIn 1922, three years after the resumption of football following the First World War, the club was relegated to the Second Division, where it remained until regaining promotion in 1925. Relegated again in 1931, Manchester United became a yo-yo club, achieving its all-time lowest position of 20th place in the Second Division in 1934. Following the death of principal benefactor John Henry Davies in October 1927, the club's finances deteriorated to the extent that Manchester United would likely have gone bankrupt had it not been for James W. Gibson, who, in December 1931, invested £2,000 and assumed control of the club. In the 1938–39 season, the last year of football before the Second World War, the club finished 14th in the First Division.\n\nIn October 1945, the impending resumption of football led to the managerial appointment of Matt Busby, who demanded an unprecedented level of control over team selection, player transfers and training sessions. Busby led the team to second-place league finishes in 1947, 1948 and 1949, and to FA Cup victory in 1948. In 1952, the club won the First Division, its first league title for 41 years. With an average age of 22, the back-to-back title winning side of 1956 were labelled \"the Busby Babes\" by the media, a testament to Busby's faith in his youth players. In 1957, Manchester United became the first English team to compete in the European Cup, despite objections from The Football League, who had denied Chelsea the same opportunity the previous season. En route to the semi-final, which they lost to Real Madrid, the team recorded a 10–0 victory over Belgian champions Anderlecht, which remains the club's biggest victory on record.\nThe following season, on the way home from a European Cup quarter-final victory against Red Star Belgrade, the aircraft carrying the Manchester United players, officials and journalists crashed while attempting to take off after refuelling in Munich, Germany. The Munich air disaster of 6 February 1958 claimed 23 lives, including those of eight players – Geoff Bent, Roger Byrne, Eddie Colman, Duncan Edwards, Mark Jones, David Pegg, Tommy Taylor and Billy Whelan – and injured several more.\nAssistant manager Jimmy Murphy took over as manager while Busby recovered from his injuries and the club's makeshift side reached the FA Cup final, which they lost to Bolton Wanderers. In recognition of the team's tragedy, UEFA invited the club to compete in the 1958–59 European Cup alongside eventual League champions Wolverhampton Wanderers. Despite approval from the FA, the Football League determined that the club should not enter the competition, since it had not qualified. Busby rebuilt the team through the 1960s by signing players such as Denis Law and Pat Crerand, who combined with the next generation of youth players – including George Best – to win the FA Cup in 1963. The following season, they finished second in the league, then won the title in 1965 and 1967. In 1968, Manchester United became the first English (and second British) club to win the European Cup, beating Benfica 4–1 in the final with a team that contained three European Footballers of the Year: Bobby Charlton, Denis Law and George Best. Matt Busby resigned as manager in 1969 and was replaced by the reserve team coach, former Manchester United player Wilf McGuinness.\n\nFollowing an eighth-place finish in the 1969–70 season and a poor start to the 1970–71 season, Busby was persuaded to temporarily resume managerial duties, and McGuinness returned to his position as reserve team coach. In June 1971, Frank O'Farrell was appointed as manager, but lasted less than 18 months before being replaced by Tommy Docherty in December 1972. Docherty saved Manchester United from relegation that season, only to see them relegated in 1974; by that time the trio of Best, Law, and Charlton had left the club. The team won promotion at the first attempt and reached the FA Cup final in 1976, but were beaten by Southampton. They reached the final again in 1977, beating Liverpool 2–1. Docherty was dismissed shortly afterwards, following the revelation of his affair with the club physiotherapist's wife.\n\nDave Sexton replaced Docherty as manager in the summer of 1977. Despite major signings, including Joe Jordan, Gordon McQueen, Gary Bailey, and Ray Wilkins, the team failed to achieve any significant results; they finished in the top two in 1979–80 and lost to Arsenal in the 1979 FA Cup Final. Sexton was dismissed in 1981, even though the team won the last seven games under his direction. He was replaced by Ron Atkinson, who immediately broke the British record transfer fee to sign Bryan Robson from West Bromwich Albion. Under Atkinson, Manchester United won the FA Cup twice in three years – in 1983 and 1985. In 1985–86, after 13 wins and two draws in its first 15 matches, the club was favourite to win the league, but finished in fourth place. The following season, with the club in danger of relegation by November, Atkinson was dismissed.\n\nAlex Ferguson and his assistant Archie Knox arrived from Aberdeen on the day of Atkinson's dismissal, and guided the club to an 11th-place finish in the league. Despite a second-place finish in 1987–88, the club was back in 11th place the following season. Reportedly on the verge of being dismissed, victory over Crystal Palace in the 1990 FA Cup Final replay (after a 3–3 draw) saved Ferguson's career. The following season, Manchester United claimed its first Cup Winners' Cup title and competed in the 1991 UEFA Super Cup, beating European Cup holders Red Star Belgrade 1–0 in the final at Old Trafford. A second consecutive League Cup final appearance followed in 1992, in which the team beat Nottingham Forest 1–0 at Wembley. In 1993, the club won its first league title since 1967, and a year later, for the first time since 1957, it won a second consecutive title – alongside the FA Cup – to complete the first \"Double\" in the club's history.\nIn the 1998–99 season, Manchester United became the first team to win the Premier League, FA Cup and UEFA Champions League – \"The Treble\" – in the same season. Losing 1–0 going into injury time in the 1999 UEFA Champions League Final, Teddy Sheringham and Ole Gunnar Solskjær scored late goals to claim a dramatic victory over Bayern Munich, in what is considered one of the greatest comebacks of all time. The club also won the Intercontinental Cup after beating Palmeiras 1–0 in Tokyo. Ferguson was subsequently knighted for his services to football.\n\nManchester United won the league again in the 1999–2000 and 2000–01 seasons. The team finished third in 2001–02, before regaining the title in 2002–03. They won the 2003–04 FA Cup, beating Millwall 3–0 in the final at the Millennium Stadium in Cardiff to lift the trophy for a record 11th time. In the 2005–06 season, Manchester United failed to qualify for the knockout phase of the UEFA Champions League for the first time in over a decade, but recovered to secure a second-place league finish and victory over Wigan Athletic in the 2006 Football League Cup Final. The club regained the Premier League in the 2006–07 and 2007–08 seasons, and completed the European double by beating Chelsea 6–5 on penalties in the 2008 UEFA Champions League Final in Moscow's Luzhniki Stadium. Ryan Giggs made a record 759th appearance for the club in this game, overtaking previous record holder Bobby Charlton. In December 2008, the club won the 2008 FIFA Club World Cup and followed this with the 2008–09 Football League Cup, and its third successive Premier League title. That summer, Cristiano Ronaldo was sold to Real Madrid for a world record £80 million. In 2010, Manchester United defeated Aston Villa 2–1 at Wembley to retain the League Cup, its first successful defence of a knockout cup competition.\n\nAfter finishing as runner-up to Chelsea in the 2009–10 season, United achieved a record 19th league title in 2010–11, securing the championship with a 1–1 away draw against Blackburn Rovers on 14 May 2011. This was extended to 20 league titles in 2012–13, securing the championship with a 3–0 home win against Aston Villa on 22 April 2013.\n\nOn 8 May 2013, Ferguson announced that he was to retire as manager at the end of the football season, but would remain at the club as a director and club ambassador. The club announced the next day that Everton manager David Moyes would replace him from 1 July, having signed a six-year contract. Ryan Giggs took over as interim player-manager 10 months later, on 22 April 2014, when Moyes was sacked after a poor season in which the club failed to defend their Premier League title and failed to qualify for the UEFA Champions League for the first time since 1995–96. They also failed to qualify for the Europa League, meaning that it was the first time Manchester United hadn't qualified for a European competition since 1990. On 19 May 2014, it was confirmed that Louis van Gaal would replace Moyes as Manchester United manager on a three-year deal, with Giggs as his assistant. Malcolm Glazer, the patriarch of the Glazer family that owns the club, died on 28 May 2014.\n\nAlthough Van Gaal's first season saw United once again qualify for the Champions League through a fourth-place finish in the Premier League, his second season saw United go out of the same tournament in the group stage. United also fell behind in the title race for the third consecutive season, finishing in 5th place, in spite of several expensive signings during Van Gaal's tenure. However, that same season, Manchester United won the FA Cup for a 12th time, this being their first major trophy won since the departure of Sir Alex Ferguson. Despite this victory, Van Gaal was sacked as manager just two days later, with José Mourinho appointed in his place on 27 May, signing a three-year contract. That season, United finished in sixth place while winning the EFL Cup for the fifth time and the Europa League for the first time, as well as the FA Community Shield for a record 21st time. Despite not finishing in the top four, United qualified for the Champions League through their Europa League win. Wayne Rooney scored his 250th goal with United, surpassing Sir Bobby Charlton as United's all-time top scorer, before leaving the club at the end of the season to return to Everton. Mourinho was sacked on 18 December 2018 with United in sixth place, 19 points behind league leaders Liverpool and 11 points outside the Champions League places. Former United player and manager of the Norwegian side Molde Ole Gunnar Solskjær was appointed caretaker manager the next day.\n\nThe club crest is derived from the Manchester City Council coat of arms, although all that remains of it on the current crest is the ship in full sail. The devil stems from the club's nickname \"The Red Devils\"; it was included on club programmes and scarves in the 1960s, and incorporated into the club crest in 1970, although the crest was not included on the chest of the shirt until 1971.\n\nNewton Heath's uniform in 1879, four years before the club played its first competitive match, has been documented as 'white with blue cord'. A photograph of the Newton Heath team, taken in 1892, is believed to show the players wearing red-and-white quartered jerseys and navy blue knickerbockers. Between 1894–96, the players wore distinctive green and gold jerseys which were replaced in 1896 by white shirts, which were worn with navy blue shorts.\n\nAfter the name change in 1902, the club colours were changed to red shirts, white shorts, and black socks, which has become the standard Manchester United home kit. Very few changes were made to the kit until 1922 when the club adopted white shirts bearing a deep red \"V\" around the neck, similar to the shirt worn in the 1909 FA Cup Final. They remained part of their home kits until 1927. For a period in 1934, the cherry and white hooped change shirt became the home colours, but the following season the red shirt was recalled after the club's lowest ever league placing of 20th in the Second Division and the hooped shirt dropped back to being the change. The black socks were changed to white from 1959 to 1965, where they were replaced with red socks up until 1971 with white used on occasion, when the club reverted to black. Black shorts and/or white socks were sometimes worn with the home strip, most often in away games, if there is a clash with the opponent's kit. For 2018–19, black shorts and red socks became the primary choice for the home kit. Since 1997–98, white socks have been the preferred choice for European games, which are typically played on weeknights, to aid with player visibility. The current home kit is a red shirt with black stripes gradually getting thicker towards the base and the trademark Adidas three stripes in black on the shoulders, black shorts, and red socks with black tops and gradual stripes in between.\n\nThe Manchester United away strip has often been a white shirt, black shorts and white socks, but there have been several exceptions. These include an all-black strip with blue and gold trimmings between 1993 and 1995, the navy blue shirt with silver horizontal pinstripes worn during the 1999–2000 season, and the 2011–12 away kit, which had a royal blue body and sleeves with hoops made of small midnight navy blue and black stripes, with black shorts and blue socks. An all-grey away kit worn during the 1995–96 season was dropped after just five games, most notoriously against Southampton where Alex Ferguson forced the team to change into the third kit during half-time of its final outing. The reason for dropping it being that the players claimed to have trouble finding their teammates against the crowd, United failed to win a competitive game in the kit. In 2001, to celebrate 100 years as \"Manchester United\", a reversible white/gold away kit was released, although the actual match day shirts were not reversible.\n\nThe club's third kit is often all-blue, this was most recently the case during the 2014–15 season. Exceptions include a green-and-gold halved shirt worn between 1992 and 1994, a blue-and-white striped shirt worn during the 1994–95 and 1995–96 seasons and once in 1996–97, an all-black kit worn during the Treble-winning 1998–99 season, and a white shirt with black-and-red horizontal pinstripes worn between 2003–04 and 2005–06. From 2006–07 to 2013–14, the third kit was the previous season's away kit, albeit updated with the new club sponsor in 2006–07 and 2010–11, apart from 2008–09 when an all-blue kit was launch to mark the 40th anniversary of the 1967–68 European Cup success.\n\n\nNewton Heath initially played on a field on North Road, close to the railway yard; the original capacity was about 12,000, but club officials deemed the facilities inadequate for a club hoping to join The Football League. Some expansion took place in 1887, and in 1891, Newton Heath used its minimal financial reserves to purchase two grandstands, each able to hold 1,000 spectators. Although attendances were not recorded for many of the earliest matches at North Road, the highest documented attendance was approximately 15,000 for a First Division match against Sunderland on 4 March 1893. A similar attendance was also recorded for a friendly match against Gorton Villa on 5 September 1889.\n\nIn June 1893, after the club was evicted from North Road by its owners, Manchester Deans and Canons, who felt it was inappropriate for the club to charge an entry fee to the ground, secretary A. H. Albut procured the use of the Bank Street ground in Clayton. It initially had no stands, by the start of the 1893–94 season, two had been built; one spanning the full length of the pitch on one side and the other behind the goal at the \"Bradford end\". At the opposite end, the \"Clayton end\", the ground had been \"built up, thousands thus being provided for\". Newton Heath's first league match at Bank Street was played against Burnley on 1 September 1893, when 10,000 people saw Alf Farman score a hat-trick, Newton Heath's only goals in a 3–2 win. The remaining stands were completed for the following league game against Nottingham Forest three weeks later. In October 1895, before the visit of Manchester City, the club purchased a 2,000-capacity stand from the Broughton Rangers rugby league club, and put up another stand on the \"reserved side\" (as distinct from the \"popular side\"). However, weather restricted the attendance for the Manchester City match to just 12,000.\n\nWhen the Bank Street ground was temporarily closed by bailiffs in 1902, club captain Harry Stafford raised enough money to pay for the club's next away game at Bristol City and found a temporary ground at Harpurhey for the next reserves game against Padiham. Following financial investment, new club president John Henry Davies paid £500 for the erection of a new 1,000-seat stand at Bank Street. Within four years, the stadium had cover on all four sides, as well as the ability to hold approximately 50,000 spectators, some of whom could watch from the viewing gallery atop the Main Stand.\n\nFollowing Manchester United's first league title in 1908 and the FA Cup a year later, it was decided that Bank Street was too restrictive for Davies' ambition; in February 1909, six weeks before the club's first FA Cup title, Old Trafford was named as the home of Manchester United, following the purchase of land for around £60,000. Architect Archibald Leitch was given a budget of £30,000 for construction; original plans called for seating capacity of 100,000, though budget constraints forced a revision to 77,000. The building was constructed by Messrs Brameld and Smith of Manchester. The stadium's record attendance was registered on 25 March 1939, when an FA Cup semi-final between Wolverhampton Wanderers and Grimsby Town drew 76,962 spectators.\n\nBombing in the Second World War destroyed much of the stadium; the central tunnel in the South Stand was all that remained of that quarter. After the war, the club received compensation from the War Damage Commission in the amount of £22,278. While reconstruction took place, the team played its \"home\" games at Manchester City's Maine Road ground; Manchester United was charged £5,000 per year, plus a nominal percentage of gate receipts. Later improvements included the addition of roofs, first to the Stretford End and then to the North and East Stands. The roofs were supported by pillars that obstructed many fans' views, and they were eventually replaced with a cantilevered structure. The Stretford End was the last stand to receive a cantilevered roof, completed in time for the 1993–94 season. First used on 25 March 1957 and costing £40,000, four pylons were erected, each housing 54 individual floodlights. These were dismantled in 1987 and replaced by a lighting system embedded in the roof of each stand, which remains in use today.\n\nThe Taylor Report's requirement for an all-seater stadium lowered capacity at Old Trafford to around 44,000 by 1993. In 1995, the North Stand was redeveloped into three tiers, restoring capacity to approximately 55,000. At the end of the 1998–99 season, second tiers were added to the East and West Stands, raising capacity to around 67,000, and between July 2005 and May 2006, 8,000 more seats were added via second tiers in the north-west and north-east quadrants. Part of the new seating was used for the first time on 26 March 2006, when an attendance of 69,070 became a new Premier League record. The record was pushed steadily upwards before reaching its peak on 31 March 2007, when 76,098 spectators saw Manchester United beat Blackburn Rovers 4–1, with just 114 seats (0.15 per cent of the total capacity of 76,212) unoccupied. In 2009, reorganisation of the seating resulted in a reduction of capacity by 255 to 75,957. Manchester United has the second highest average attendance of European football clubs only behind Borussia Dortmund.\n\nManchester United is one of the most popular football clubs in the world, with one of the highest average home attendance in Europe. The club states that its worldwide fan base includes more than 200 officially recognised branches of the Manchester United Supporters Club (MUSC), in at least 24 countries. The club takes advantage of this support through its worldwide summer tours. Accountancy firm and sports industry consultants Deloitte estimate that Manchester United has 75 million fans worldwide, while other estimates put this figure closer to 333 million. The club has the third highest social media following in the world among sports teams (after Barcelona and Real Madrid), with over 71 million Facebook fans as of September 2016. A 2014 study showed that Manchester United had the loudest fans in the Premier League.\n\nSupporters are represented by two independent bodies; the Independent Manchester United Supporters' Association (IMUSA), which maintains close links to the club through the MUFC Fans Forum, and the Manchester United Supporters' Trust (MUST). After the Glazer family's takeover in 2005, a group of fans formed a splinter club, F.C. United of Manchester. The West Stand of Old Trafford – the \"Stretford End\" – is the home end and the traditional source of the club's most vocal support.\n\nManchester United has rivalries with Arsenal, Leeds United, Liverpool, and Manchester City, against whom they contest the Manchester derby.\n\nThe rivalry with Liverpool is rooted in competition between the cities during the Industrial Revolution when Manchester was famous for its textile industry while Liverpool was a major port. The two clubs are the most successful English teams in both domestic and European competitions; and between them they have won 38 league titles, 8 European Cups, 4 UEFA Cups, 4 UEFA Super Cups, 19 FA Cups, 13 League Cups, 1 FIFA Club World Cup, 1 Intercontinental Cup and 36 FA Community Shields. It is considered to be one of the biggest rivalries in the association football world and is considered the most famous fixture in English football.\n\nThe \"Roses Rivalry\" with Leeds stems from the Wars of the Roses, fought between the House of Lancaster and the House of York, with Manchester United representing Lancashire and Leeds representing Yorkshire.\n\nThe rivalry with Arsenal arises from the numerous times the two teams, as well as managers Alex Ferguson and Arsène Wenger, have battled for the Premier League title. With 33 titles between them (20 for Manchester United, 13 for Arsenal) this fixture has become known as one of the finest Premier League match-ups in history.\n\nManchester United has been described as a global brand; a 2011 report by Brand Finance, valued the club's trademarks and associated intellectual property at £412 million – an increase of £39 million on the previous year, valuing it at £11 million more than the second best brand, Real Madrid – and gave the brand a strength rating of AAA (Extremely Strong). In July 2012, Manchester United was ranked first by \"Forbes\" magazine in its list of the ten most valuable sports team brands, valuing the Manchester United brand at $2.23 billion. The club is ranked third in the Deloitte Football Money League (behind Real Madrid and Barcelona). In January 2013, the club became the first sports team in the world to be valued at $3 billion. Forbes Magazine valued the club at $3.3 billion – $1.2 billion higher than the next most valuable sports team. They were overtaken by Real Madrid for the next four years, but Manchester United returned to the top of the Forbes list in June 2017, with a valuation of $3.689 billion.\n\nThe core strength of Manchester United's global brand is often attributed to Matt Busby's rebuilding of the team and subsequent success following the Munich air disaster, which drew worldwide acclaim. The \"iconic\" team included Bobby Charlton and Nobby Stiles (members of England's World Cup winning team), Denis Law and George Best. The attacking style of play adopted by this team (in contrast to the defensive-minded \"catenaccio\" approach favoured by the leading Italian teams of the era) \"captured the imagination of the English footballing public\". Busby's team also became associated with the liberalisation of Western society during the 1960s; George Best, known as the \"Fifth Beatle\" for his iconic haircut, was the first footballer to significantly develop an off-the-field media profile.\n\nAs the second English football club to float on the London Stock Exchange in 1991, the club raised significant capital, with which it further developed its commercial strategy. The club's focus on commercial and sporting success brought significant profits in an industry often characterised by chronic losses. The strength of the Manchester United brand was bolstered by intense off-the-field media attention to individual players, most notably David Beckham (who quickly developed his own global brand). This attention often generates greater interest in on-the-field activities, and hence generates sponsorship opportunities – the value of which is driven by television exposure. During his time with the club, Beckham's popularity across Asia was integral to the club's commercial success in that part of the world.\n\nBecause higher league placement results in a greater share of television rights, success on the field generates greater income for the club. Since the inception of the Premier League, Manchester United has received the largest share of the revenue generated from the BSkyB broadcasting deal. Manchester United has also consistently enjoyed the highest commercial income of any English club; in 2005–06, the club's commercial arm generated £51 million, compared to £42.5 million at Chelsea, £39.3 million at Liverpool, £34 million at Arsenal and £27.9 million at Newcastle United. A key sponsorship relationship was with sportswear company Nike, who managed the club's merchandising operation as part of a £303 million 13-year partnership between 2002 and 2015. Through Manchester United Finance and the club's membership scheme, One United, those with an affinity for the club can purchase a range of branded goods and services. Additionally, Manchester United-branded media services – such as the club's dedicated television channel, MUTV – have allowed the club to expand its fan base to those beyond the reach of its Old Trafford stadium.\n\nIn an initial five-year deal worth £500,000, Sharp Electronics became the club's first shirt sponsor at the beginning of the 1982–83 season, a relationship that lasted until the end of the 1999–2000 season, when Vodafone agreed a four-year, £30 million deal. Vodafone agreed to pay £36 million to extend the deal by four years, but after two seasons triggered a break clause in order to concentrate on its sponsorship of the Champions League.\n\nTo commence at the start of the 2006–07 season, American insurance corporation AIG agreed a four-year £56.5 million deal which in September 2006 became the most valuable in the world. At the beginning of the 2010–11 season, American reinsurance company Aon became the club's principal sponsor in a four-year deal reputed to be worth approximately £80 million, making it the most lucrative shirt sponsorship deal in football history. Manchester United announced their first training kit sponsor in August 2011, agreeing a four-year deal with DHL reported to be worth £40 million; it is believed to be the first instance of training kit sponsorship in English football. The DHL contract lasted for over a year before the club bought back the contract in October 2012, although they remained the club's official logistics partner. The contract for the training kit sponsorship was then sold to Aon in April 2013 for a deal worth £180 million over eight years, which also included purchasing the naming rights for the Trafford Training Centre.\n\nThe club's first kit manufacturer was Umbro, until a five-year deal was agreed with Admiral Sportswear in 1975. Adidas received the contract in 1980, before Umbro started a second spell in 1992. Umbro's sponsorship lasted for ten years, followed by Nike's record-breaking £302.9 million deal that lasted until 2015; 3.8 million replica shirts were sold in the first 22 months with the company. In addition to Nike and Chevrolet, the club also has several lower-level \"platinum\" sponsors, including Aon and Budweiser.\n\nOn 30 July 2012, United signed a seven-year deal with American automotive corporation General Motors, which replaced Aon as the shirt sponsor from the 2014–15 season. The new $80m-a-year shirt deal is worth $559m over seven years and features the logo of General Motors brand Chevrolet. Nike announced that they would not renew their kit supply deal with Manchester United after the 2014–15 season, citing rising costs. Since the start of the 2015–16 season, Adidas has manufactured Manchester United's kit as part of a world-record 10-year deal worth a minimum of £750 million. Plumbing products manufacturer Kohler became the club's first sleeve sponsor ahead of the 2018–19 season.\n\nOriginally funded by the Lancashire and Yorkshire Railway Company, the club became a limited company in 1892 and sold shares to local supporters for £1 via an application form. In 1902, majority ownership passed to the four local businessmen who invested £500 to save the club from bankruptcy, including future club president John Henry Davies. After his death in 1927, the club faced bankruptcy yet again, but was saved in December 1931 by James W. Gibson, who assumed control of the club after an investment of £2,000. Gibson promoted his son, Alan, to the board in 1948, but died three years later; the Gibson family retained ownership of the club through James' wife, Lillian, but the position of chairman passed to former player Harold Hardman.\n\nPromoted to the board a few days after the Munich air disaster, Louis Edwards, a friend of Matt Busby, began acquiring shares in the club; for an investment of approximately £40,000, he accumulated a 54 per cent shareholding and took control in January 1964. When Lillian Gibson died in January 1971, her shares passed to Alan Gibson who sold a percentage of his shares to Louis Edwards' son, Martin, in 1978; Martin Edwards went on to become chairman upon his father's death in 1980. Media tycoon Robert Maxwell attempted to buy the club in 1984, but did not meet Edwards' asking price. In 1989, chairman Martin Edwards attempted to sell the club to Michael Knighton for £20 million, but the sale fell through and Knighton joined the board of directors instead.\n\nManchester United was floated on the stock market in June 1991 (raising £6.7 million), and received yet another takeover bid in 1998, this time from Rupert Murdoch's British Sky Broadcasting Corporation. This resulted in the formation of \"Shareholders United Against Murdoch\" – now the \"Manchester United Supporters' Trust\" – who encouraged supporters to buy shares in the club in an attempt to block any hostile takeover. The Manchester United board accepted a £623 million offer, but the takeover was blocked by the Monopolies and Mergers Commission at the final hurdle in April 1999. A few years later, a power struggle emerged between the club's manager, Alex Ferguson, and his horse-racing partners, John Magnier and J. P. McManus, who had gradually become the majority shareholders. In a dispute that stemmed from contested ownership of the horse Rock of Gibraltar, Magnier and McManus attempted to have Ferguson removed from his position as manager, and the board responded by approaching investors to attempt to reduce the Irishmen's majority.\n\nIn May 2005, Malcolm Glazer purchased the 28.7 per cent stake held by McManus and Magnier, thus acquiring a controlling interest through his investment vehicle Red Football Ltd in a highly leveraged takeover valuing the club at approximately £800 million (then approx. $1.5 billion). Once the purchase was complete, the club was taken off the stock exchange. In July 2006, the club announced a £660 million debt refinancing package, resulting in a 30 per cent reduction in annual interest payments to £62 million a year. In January 2010, with debts of £716.5 million ($1.17 billion), Manchester United further refinanced through a bond issue worth £504 million, enabling them to pay off most of the £509 million owed to international banks. The annual interest payable on the bonds – which were to mature on 1 February 2017 – is approximately £45 million per annum. Despite restructuring, the club's debt prompted protests from fans on 23 January 2010, at Old Trafford and the club's Trafford Training Centre. Supporter groups encouraged match-going fans to wear green and gold, the colours of Newton Heath. On 30 January, reports emerged that the Manchester United Supporters' Trust had held meetings with a group of wealthy fans, dubbed the \"Red Knights\", with plans to buying out the Glazers' controlling interest.\n\nIn August 2011, the Glazers were believed to have approached Credit Suisse in preparation for a $1 billion (approx. £600 million) initial public offering (IPO) on the Singapore stock exchange that would value the club at more than £2 billion. However, in July 2012, the club announced plans to list its IPO on the New York Stock Exchange instead. Shares were originally set to go on sale for between $16 and $20 each, but the price was cut to $14 by the launch of the IPO on 10 August, following negative comments from Wall Street analysts and Facebook's disappointing stock market debut in May. Even after the cut, Manchester United was valued at $2.3 billion, making it the most valuable football club in the world.\n\n\nManchester United are one of the most successful clubs in Europe in terms of trophies won. The club's first trophy was the Manchester Cup, which it won as Newton Heath LYR in 1886. In 1908, the club won its first league title, and won the FA Cup for the first time the following year. Manchester United won the most trophies in the 1990s; five league titles, four FA Cups, one League Cup, five Charity Shields (one shared), one UEFA Champions League, one UEFA Cup Winners' Cup, one UEFA Super Cup and one Intercontinental Cup.\n\nThe club holds the record for most top-division titles (20) – including a record 13 Premier League titles – and FA Community Shields (21). It was also the first English club to win the European Cup in 1968, and, , is the only British club to have won the Club World Cup, in 2008. United also became the sole British club to win the Intercontinental Cup, in 1999. The club's most recent trophy came in May 2017, with the 2016–17 UEFA Europa League.\n\nIn winning the 2016–17 UEFA Europa League, United became the fifth club in history to have won the \"European Treble\" of European Cup/UEFA Champions League, European Cup Winners' Cup/UEFA Cup Winners' Cup, and UEFA Cup/UEFA Europa League after Juventus, Ajax, Bayern Munich and Chelsea.\n\n\n\n\n\n\nEspecially short competitions such as the Charity/Community Shield, Intercontinental Cup (now defunct), FIFA Club World Cup or UEFA Super Cup are not generally considered to contribute towards a Double or Treble.\n\nManchester United formed a women's football team in 2018.\n\n\n\n"}
{"id": "19962", "url": "https://en.wikipedia.org/wiki?curid=19962", "title": "Mesa (programming language)", "text": "Mesa (programming language)\n\nMesa is a programming language developed in the late 1970s at the Xerox Palo Alto Research Center in Palo Alto, California, United States. The language name was a pun based upon the programming language catchphrases of the time, because Mesa is a \"high level\" programming language.\n\nMesa is an ALGOL-like language with strong support for modular programming. Every library module has at least two source files: a \"definitions\" file specifying the library's interface plus one or more \"program\" files specifying the implementation of the procedures in the interface. To use a library, a program or higher-level library must \"import\" the definitions. The Mesa compiler type-checks all uses of imported entities; this combination of separate compilation with type-checking was unusual at the time.\n\nMesa introduced several other innovations in language design and implementation, notably in the handling of software exceptions, thread synchronization, and incremental compilation.\n\nMesa was developed on the Xerox Alto, one of the first personal computers with a graphical user interface, however most of the Alto's system software was written in BCPL. Mesa was the system programming language of the later Xerox Star workstations, and for the GlobalView desktop environment. Xerox PARC later developed Cedar, which was a superset of Mesa.\n\nMesa and Cedar had a major influence on the design of other important languages, such as Modula-2 and Java, and was an important vehicle for the development and dissemination of the fundamentals of GUIs, networked environments, and the other advances Xerox contributed to the field of computer science.\n\nMesa was originally designed in the Computer Systems Laboratory (CSL), a branch of the Xerox Palo Alto Research Center, for the Alto, an experimental micro-coded workstation. Initially its spread was confined to PARC and a few universities to which Xerox had donated some Altos.\n\nMesa was later adopted as the systems programming language for Xerox's commercial workstations such as the Xerox 8010 (Xerox Star, Dandelion) and Xerox 6085 (Daybreak), in particular for the Pilot operating system.\n\nA secondary development environment, called the Xerox Development Environment (XDE) allowed developers to debug both the operating system Pilot as well as ViewPoint GUI applications using a world swap mechanism. This allowed the entire \"state\" of the world to be swapped out, and allowed low level system crashes which paralyzed the whole system to be debugged. This technique did not scale very well to large application images (several megabytes), and so the Pilot/Mesa world in later releases moved away from the world swap view when the micro-coded machines were phased out in favor of SPARC workstations and Intel PCs running a Mesa PrincOps emulator for the basic hardware instruction set.\n\nMesa was compiled into a stack-machine language, purportedly with the highest code density ever achieved (roughly 4 bytes per high-level language statement). This was touted in a 1981 paper where implementors from the Xerox Systems Development Department (then, the development arm of PARC), tuned up the instruction set and published a paper on the resultant code density.\n\nMesa was taught via the Mesa Programming Course that took people through the wide range of technology Xerox had available at the time and ended with the programmer writing a \"hack\", a workable program designed to be useful. An actual example of such a hack is the BWSMagnifier, which was written in 1988 and allowed people to magnify sections of the workstation screen as defined by a resizable window and a changeable magnification factor. Trained Mesa programmers from Xerox were well versed in the fundamental of GUIs, networking, exceptions, and multi-threaded programming, almost a decade before they became standard tools of the trade.\n\nWithin Xerox, Mesa was eventually superseded by the Cedar programming language. Many Mesa programmers and developers left Xerox in 1985; some of them went to DEC Systems Research Center where they used their experience with Mesa in the design of Modula-2+, and later of Modula-3.\n\nMesa was a strongly typed programming language with type-checking across module boundaries, but with enough flexibility in its type system that heap allocators could be written in Mesa.\n\nBecause of its strict separation between interface and implementation, Mesa allows true incremental compilation and encourages architecture- and platform-independent programming. They also simplified source-level debugging, including remote debugging via the Ethernet.\n\nMesa had rich exception handling facilities, with four types of exceptions. It had support for thread synchronization via monitors. Mesa was the first language to implement monitor BROADCAST, a concept introduced by the Pilot operating system.\n\nMesa has an \"imperative\" and \"algebraic\" syntax, based on ALGOL and Pascal rather than on BCPL or C; for instance, compound commands are indicated by the and keywords rather than braces. In Mesa, all keywords are written in uppercase.\n\nDue to a peculiarity of the ASCII variant used at PARC, the Alto's character set included a left-pointing arrow (←) rather than an underscore. The result of this is that Alto programmers (including those using Mesa, Smalltalk etc.) conventionally used CamelCase for compound identifiers, a practice which was incorporated in PARC's standard programming style. On the other hand, the availability of the left-pointing arrow allowed them to use it for the assignment operator, as it originally had been in ALGOL.\n\nWhen the Mesa designers wanted to implement an exception facility, they hired a recent M.S. graduate from Colorado who had written his thesis on exception handling facilities in algorithmic languages. This led to the richest exception facility for its time, with primitives , , , , , and . Because the language did not have type-safe checks to verify full coverage for signal handling, uncaught exceptions were a common cause of bugs in released software.\n\nXerox PARC later developed Cedar, which was a superset of Mesa, with a number of additions including garbage collection; better string support, called Ropes; and later a native compiler for Sun SPARC workstations. Most importantly, Cedar contained a type-safe subset and the compiler had a subset-checking mode to ensure deterministic execution and no memory leaks from conformant Cedar code.\n\nMesa was the precursor to the programming language Cedar. Cedar's main additions were garbage collection, dynamic types, a limited form of type parameterization, and special syntax to identify the \"type-safe\" parts of a multi-module software package.\n\n\n\n"}
{"id": "19963", "url": "https://en.wikipedia.org/wiki?curid=19963", "title": "Marsilio Ficino", "text": "Marsilio Ficino\n\nMarsilio Ficino (; Latin name: \"Marsilius Ficinus\"; 19 October 1433 – 1 October 1499) was an Italian scholar and Catholic priest who was one of the most influential humanist philosophers of the early Italian Renaissance. He was an astrologer, a reviver of Neoplatonism in touch with the major academics of his day and the first translator of Plato's complete extant works into Latin. His Florentine Academy, an attempt to revive Plato's Academy, influenced the direction and tenor of the Italian Renaissance and the development of European philosophy.\n\nFicino was born at Figline Valdarno. His father Diotifeci d'Agnolo was a physician under the patronage of Cosimo de' Medici, who took the young man into his household and became the lifelong patron of Marsilio, who was made tutor to his grandson, Lorenzo de' Medici. Giovanni Pico della Mirandola, the Italian humanist philosopher and scholar was another of his students.\n\nDuring the sessions at Florence of the Council of Ferrara-Florence in 1438–1445, during the failed attempts to heal the schism of the Orthodox and Catholic churches, Cosimo de' Medici and his intellectual circle had made acquaintance with the Neoplatonic philosopher George Gemistos Plethon, whose discourses upon Plato and the Alexandrian mystics so fascinated the learned society of Florence that they named him the second Plato. In 1459 John Argyropoulos was lecturing on Greek language and literature at Florence, and Ficino became his pupil.\n\nWhen Cosimo decided to refound Plato's Academy at Florence he chose Ficino as its head. In 1462, Cosimo supplied Ficino with Greek manuscripts of Plato's work, whereupon Ficino started translating the entire corpus to Latin (draft translation of the dialogues finished 1468–9; published 1484). Ficino also produced a translation of a collection of Hellenistic Greek documents found by Leonardo da Pistoia later called Hermetica, and the writings of many of the Neoplatonists, including Porphyry, Iamblichus and Plotinus.\n\nAmong his many students was Francesco Cattani da Diacceto, who was considered by Ficino to be his successor as the head of the Florentine Platonic Academy. Diacceto's student, Giovanni di Bardo Corsi, produced a short biography of Ficino in 1506.\n\nA physician and a vegetarian, Ficino became a priest in 1473.\n\nIn 1474 Ficino completed his treatise on the immortality of the soul, \"Theologia Platonica de immortalitate animae\" (Platonic Theology). In the rush of enthusiasm for every rediscovery from Antiquity, he exhibited a great interest in the arts of astrology, which landed him in trouble with the Roman Catholic Church. In 1489 he was accused of magic before Pope Innocent VIII and needed strong defense to preserve him from the condemnation of heresy.\n\nWriting in 1492 Ficino proclaimed: \"This century, like a golden age, has restored to light the liberal arts, which were almost extinct: grammar, poetry, rhetoric, painting, sculpture, architecture, music ... this century appears to have perfected astrology.\"\n\nFicino's letters, extending over the years 1474–1494, survive and have been published. He wrote \"De amore\" (1484). \"De vita libri tres\" (Three books on life), or \"De triplici vita\", published in 1489, provides a great deal of medical and astrological advice for maintaining health and vigor, as well as espousing the Neoplatonist view of the world's ensoulment and its integration with the human soul:\nOne metaphor for this integrated \"aliveness\" is Ficino's astrology. In the \"Book of Life\", he details the interlinks between behavior and consequence. It talks about a list of things that hold sway over a man's destiny.\n\nProbably due to early influences from his father Diotifeci, who was a doctor to Cosimo de' Medici, Ficino published Latin and Italian treatises on medical subjects such as \"Consiglio contro la pestilenza\" (Recommendations for the treatment of the plague) and \"De vita libri tres\" (Three books on life). His medical works exerted considerable influence on Renaissance physicians such as Paracelsus, with whom he shared the perception on the unity of the micro- and macrocosmos, and their interactions, through somatic and psychological manifestations, with the aim to investigate their signatures to cure diseases. Those works, which were very popular at the time, dealt with astrological and alchemical concepts. Thus Ficino came under the suspicion of heresy; especially after the publication of the third book in 1489, which contained specific instructions on healthful living.\n\nFicino introduced the term and concept of \"platonic love\" in the West. It first appeared in a letter to Alamanno Donati in 1476, but was later fully developed all along his work, mainly his famous \"De amore\". He also practiced this love metaphysic with Giovanni Cavalcanti, whom he made the principal character in his commentary on the \"Convivio\", and to whom he wrote ardent love letters in Latin that were published in his \"Epistulae\" in 1492; there are also numerous other indications to suggest that Ficino's erotic impulses were directed exclusively towards men. After his death his biographers had a difficult task trying to refute those who spoke of his homosexual tendencies. But his sincere and deep faith, and membership of the clergy, put him beyond the reach of gossip, and while praising love for the same sex, he also condemned sodomy in the \"Convivium\". His Latin translations of Plato's texts put into practice the theories of anti-homosexuality in his \"Convivium\".\n\nFicino died on 1 October 1499 at Careggi. In 1521 his memory was honored with a bust sculpted by Andrea Ferrucci, which is located in the south side of the nave in the Cathedral of Santa Maria del Fiore.\n\n\n\n\n\n"}
{"id": "19965", "url": "https://en.wikipedia.org/wiki?curid=19965", "title": "Morphogenesis", "text": "Morphogenesis\n\nMorphogenesis (from the Greek \"morphê\" shape and \"genesis\" creation, literally, \"beginning of the shape\") is the biological process that causes an organism to develop its shape. It is one of three fundamental aspects of developmental biology along with the control of cell growth and cellular differentiation, unified in evolutionary developmental biology (evo-devo).\n\nThe process controls the organized spatial distribution of cells during the embryonic development of an organism. Morphogenesis can take place also in a mature organism, in cell culture or inside tumor cell masses. Morphogenesis also describes the development of unicellular life forms that do not have an embryonic stage in their life cycle, or describes the evolution of a body structure within a taxonomic group.\n\nMorphogenetic responses may be induced in organisms by hormones, by environmental chemicals ranging from substances produced by other organisms to toxic chemicals or radionuclides released as pollutants, and other plants, or by mechanical stresses induced by spatial patterning of the cells.\n\nSome of the earliest ideas and mathematical descriptions on how physical processes and constraints affect biological growth, and hence natural patterns such as the spirals of phyllotaxis, were written by D'Arcy Wentworth Thompson in his 1917 book \"On Growth and Form\" and Alan Turing in his \"The Chemical Basis of Morphogenesis\" (1952). Where Thompson explained animal body shapes as being created by varying rates of growth in different directions, for instance to create the spiral shell of a snail, Turing correctly predicted a mechanism of morphogenesis, the diffusion of two different chemical signals, one activating and one deactivating growth, to set up patterns of development, decades before the formation of such patterns was observed. The fuller understanding of the mechanisms involved in actual organisms required the discovery of the structure of DNA in 1953, and the development of molecular biology and biochemistry.\n\nSeveral types of molecules are important in morphogenesis. Morphogens are soluble molecules that can diffuse and carry signals that control cell differentiation via concentration gradients. Morphogens typically act through binding to specific protein receptors. An important class of molecules involved in morphogenesis are transcription factor proteins that determine the fate of cells by interacting with DNA. These can be coded for by master regulatory genes, and either activate or deactivate the transcription of other genes; in turn, these secondary gene products can regulate the expression of still other genes in a regulatory cascade of gene regulatory networks. At the end of this cascade are classes of molecules that control cellular behaviors such as cell migration, or, more generally, their properties, such as cell adhesion or cell contractility. For example, during gastrulation, clumps of stem cells switch off their cell-to-cell adhesion, become migratory, and take up new positions within an embryo where they again activate specific cell adhesion proteins and form new tissues and organs. Developmental signaling pathways implicated in morphogenesis include Wnt, Hedgehog, and ephrins.\n\nAt a tissue level, ignoring the means of control, morphogenesis arises because of cellular proliferation and motility. Morphogenesis also involves changes in the cellular structure or how cells interact in tissues. These changes can result in tissue elongation, thinning, folding, invasion or separation of one tissue into distinct layers. The latter case is often referred as cell sorting. Cell \"sorting out\" consists of cells moving so as to sort into clusters that maximize contact between cells of the same type. The ability of cells to do this has been proposed to arise from differential cell adhesion by Malcolm Steinberg through his Differential Adhesion Hypothesis. Tissue separation can also occur via more dramatic cellular differentiation events during which epithelial cells become mesenchymal (see Epithelial-mesenchymal transition). Mesenchymal cells typically leave the epithelial tissue as a consequence of changes in cell adhesive and contractile properties. Following epithelial-mesenchymal transition, cells can migrate away from an epithelium and then associate with other similar cells in a new location.\n\nDuring embryonic development, cells are restricted to different layers due to differential affinities. One of the ways this can occur is when cells share the same cell-to-cell adhesion molecules. For instance, homotypic cell adhesion can maintain boundaries between groups of cells that have different adhesion molecules. Furthermore, cells can sort based upon differences in adhesion between the cells, so even two populations of cells with different levels of the same adhesion molecule can sort out. In cell culture cells that have the strongest adhesion move to the center of a mixed aggregates of cells. Moreover, cell-cell adhesion is often modulated by cell contractility, which can exert forces on the cell-cell contacts so that two cell populations with equal levels of the same adhesion molecule can sort out. The molecules responsible for adhesion are called cell adhesion molecules (CAMs). Several types of cell adhesion molecules are known and one major class of these molecules are cadherins. There are dozens of different cadherins that are expressed on different cell types. Cadherins bind to other cadherins in a like-to-like manner: E-cadherin (found on many epithelial cells) binds preferentially to other E-cadherin molecules. Mesenchymal cells usually express other cadherin types such as N-cadherin.\n\nThe extracellular matrix (ECM) is involved in keeping tissues separated, providing structural support or providing a structure for cells to migrate on. Collagen, laminin, and fibronectin are major ECM molecules that are secreted and assembled into sheets, fibers, and gels. Multisubunit transmembrane receptors called integrins are used to bind to the ECM. Integrins bind extracellularly to fibronectin, laminin, or other ECM components, and intracellularly to microfilament-binding proteins α-actinin and talin to link the cytoskeleton with the outside. Integrins also serve as receptors to trigger signal transduction cascades when binding to the ECM. A well-studied example of morphogenesis that involves ECM is mammary gland ductal branching.\n\nTissues can change their shape and separate into distinct layers via cell contractility. Just as in muscle cells, myosin can contract different parts of the cytoplasm to change its shape or structure. Myosin-driven contractility in embryonic tissue morphogenesis is seen during the separation of germ layers in the model organisms \"Caenorhabditis elegans\", \"Drosophila\" and zebrafish. There are often periodic pulses of contraction in embryonic morphogenesis. A model called the cell state splitter involves alternating cell contraction and expansion, initiated by a bistable organelle at the apical end of each cell. The organelle consists of microtubules and microfilaments in mechanical opposition. It responds to local mechanical perturbations caused by morphogenetic movements. These then trigger traveling embryonic differentiation waves of contraction or expansion over presumptive tissues that determine cell type and is followed by cell differentiation. The cell state splitter was first proposed to explain neural plate morphogenesis during gastrulation of the axolotl and the model was later generalized to all of morphogenesis.\n\nCancer can result from disruption of normal morphogenesis, including both tumor formation and tumor metastasis. Mitochondrial dysfunction can result in increased cancer risk due to disturbed morphogen signaling.\n\na. Thompson's book is often cited. An abridged version, comprising 349 pages, remains in print and readily obtainable.\nAn unabridged version, comprising 1116 pages, has also been published.\n\n\n"}
{"id": "19967", "url": "https://en.wikipedia.org/wiki?curid=19967", "title": "Medium", "text": "Medium\n\nMedium may refer to:\n\n\n\n\n\n\n\n\n\n"}
{"id": "19971", "url": "https://en.wikipedia.org/wiki?curid=19971", "title": "MCA", "text": "MCA\n\nMCA may refer to:\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "19972", "url": "https://en.wikipedia.org/wiki?curid=19972", "title": "Magical organization", "text": "Magical organization\n\nA magical organization or magical order is an organization created for the practice of magic or to further the knowledge of magic among its members. \"Magic\" in this case refers to occult, metaphysical and paranormal activities, not to the performance of stage magic. Magical organizations can include hermetic orders, Wiccan covens or Wiccan circles, esoteric societies, arcane colleges, witches' covens, and other groups which may use different terminology and similar though diverse practices.\n\nThe Hermetic Order of the Golden Dawn has been credited with a vast revival of occult literature and practices and was founded in 1887 or 1888 by William Wynn Westcott, Samuel Liddell MacGregor Mathers and William Robert Woodman. The teachings of the Order include Enochian magic, Christian mysticism, Qabalah, Hermeticism, the paganism of ancient Egypt, theurgy, and alchemy.\n\nOrdo Templi Orientis was founded by Carl Kellner in 1895. The Order was reworked by Aleister Crowley after he took control of the Order in the early 1920s.\n\nOrdo Aurum Solis, founded in 1897, is a Western Mystery Tradition group teaching Hermetic Qabalah. Its rituals and system are different from the more popular Golden Dawn, this is because the group follows the ogdoadic tradition instead of rosicrucianism.\n\nThe A∴A∴ was created in 1907 by Aleister Crowley and also teaches magick and Thelema. Their main text is \"The Book of the Law\".\n\nBuilders of the Adytum (or B.O.T.A.) was created in 1922 by Paul Foster Case and was extended by Dr. Ann Davies. It teaches Hermetic Qabalah, astrology and occult tarot.\n\nSome (in many cases, equally notable) organizations.\n\n\n\n\n"}
{"id": "19975", "url": "https://en.wikipedia.org/wiki?curid=19975", "title": "Muhammad ibn Abd al-Wahhab", "text": "Muhammad ibn Abd al-Wahhab\n\nMuhammad ibn Abd al-Wahhab (; ; 1703 – 22 June 1792) was a religious leader and theologian from Najd in central Arabia who founded the movement now called Wahhabism. Born to a family of jurists, Ibn 'Abd al-Wahhab's early education consisted of learning a fairly standard curriculum of orthodox jurisprudence according to the Hanbali school of law, which was the school of law most prevalent in his area of birth. Despite his initial rudimentary training in classical Sunni Muslim tradition, Ibn 'Abd al-Wahhab gradually became opposed to many of the most popular Sunni practices such as the visitation to and the veneration of the tombs of saints, which he felt amounted to heretical religious innovation or even idolatry. Despite his teachings being rejected and opposed by many of the most notable Sunni Muslim scholars of the period, including his own father and brother, Ibn 'Abd al-Wahhab charted a religio-political pact with Muhammad bin Saud to help him to establish the Emirate of Diriyah, the first Saudi state, and began a dynastic alliance and power-sharing arrangement between their families which continues to the present day in the Kingdom of Saudi Arabia. The Al ash-Sheikh, Saudi Arabia's leading religious family, are the descendants of Ibn ʿAbd al-Wahhab, and have historically led the ulama in the Saudi state, dominating the state's clerical institutions.\n\nIbn 'Abd al-Wahhab is generally acknowledged to have been born in 1703 into the sedentary and impoverished Arab clan of Banu Tamim in 'Uyayna, a village in the Najd region of central Arabia. Before the emergence of Wahhabism there was a very limited history of Islamic education in the area. For this reason, Ibn 'Abd al-Wahhab had modest access to Islamic education during his youth. Despite this, the area had nevertheless produced several notable jurists of the Hanbali school of orthodox Sunni jurisprudence, which was the school of law most prominently practiced in the area. In fact, Ibn 'Abd-al-Wahhab's own family \"had produced several doctors of the school,\" with his father, Sulaymān b. Muḥammad, having been the Hanbali jurisconsult of the Najd and his grandfather, ʿAbd al-Wahhāb, having been a judge of Hanbali law.\n\nIbn 'Abd-al-Wahhab's early education consisted of learning the Quran by heart and studying a rudimentary level of Hanbali jurisprudence and theology as outlined in the works of Ibn Qudamah (d. 1223), one of the most influential medieval representatives of the Hanbali school whose works were regarded \"as having great authority\" in the Najd. As the veneration of saints and the belief in their ability to perform miracles by the grace of God had become one of the most omnipresent and established aspects of Sunni Muslim practice throughout the Islamic world, being an agreed upon tenet of the faith by the vast majority of the classical scholars, it was not long before Ibn 'Abd-al-Wahhab began to encounter the omnipresence of saint-veneration in his area as well; and it is probable that he chose to leave Najd and look elsewhere for studies in order to see if the honoring of saints was as popular in the neighboring places of the Muslim world.\n\nAfter leaving 'Uyayna, Ibn 'Abd al-Wahhab performed the Greater Pilgrimage in Mecca, where the scholars appear to have held opinions and espoused teachings that were unpalatable to him. After this, he went to Medina, the stay at which seems to have been \"decisive in shaping the later direction of his thought.\" In Medina, he met a Hanbali theologian from Najd named ʿAbd Allāh b. Ibrāhīm al-Najdī, who had been a supporter of the neo-Hanbali works of Ibn Taymiyyah (d. 1328), the controversial medieval scholar whose teachings had been considered heterodox and misguided on a number of important points by the vast majority of Sunni Muslim scholars up to that point in history.\n\nIbn 'Abd al-Wahhab's teacher Abdallah ibn Ibrahim ibn Sayf introduced the relatively young man to Mohammad Hayya Al-Sindhi in Medina who belonged to the Naqshbandi order (tariqa) of Sufism and recommended him as a student. Mohammad Ibn Abd-al-Wahhab and al-Sindhi became very close and Mohammad Ibn Abd-al-Wahhab stayed with him for some time. Muhammad Hayya also taught Mohammad Ibn ʿAbd-al-Wahhab to reject popular religious practices associated with walis and their tombs that resembles later Wahhabi teachings.\n\nFollowing his early education in Medina, Ibn Abdul Wahhab traveled outside of the peninsula, venturing first to Basra.\n\nAfter his return home, Ibn ʿAbd al-Wahhab began to attract followers, including the ruler of 'Uyayna, Uthman ibn Mu'ammar. With Ibn Mu'ammar, Ibn ʿAbd al-Wahhab came to an agreement to support Ibn Mu'ammar's political ambitions to expand his rule \"over Najd and possibly beyond\", in exchange for the ruler's support for Ibn ʿAbd al-Wahhab's religious teachings. Ibn ʿAbd al-Wahhab began to implement some of his ideas for reform. First, he persuaded Ibn Mu'ammar to help him level the grave of Zayd ibn al-Khattab, a companion of Muhammad, whose grave was revered by locals. Secondly, he ordered the cutting down of trees considered sacred by locals, cutting down \"the most glorified of all of the trees\" himself. Third, he organised the stoning of a woman who confessed to having committed adultery.\n\nThese actions gained the attention of Sulaiman ibn Muhammad ibn Ghurayr of the tribe of Bani Khalid, the chief of Al-Hasa and Qatif, who held substantial influence in Najd. Ibn Ghurayr threatened Ibn Mu'ammar with denying him the ability to collect a land tax for some properties that Ibn Mu'ammar owned in Al-Hasa if he did not kill or drive away Ibn ʿAbd al-Wahhab. Consequently, Ibn Mu'ammar forced Ibn ʿAbd al-Wahhab to leave.\n\nUpon his expulsion from 'Uyayna, Ibn ʿAbd al-Wahhab was invited to settle in neighboring Diriyah by its ruler Muhammad bin Saud. After some time in Diriyah, Muhammad ibn ʿAbd al-Wahhab concluded his second and more successful agreement with a ruler. Ibn ʿAbd al-Wahhab and Muhammad bin Saud agreed that, together, they would bring the Arabs of the peninsula back to the \"true\" principles of Islam as they saw it. According to one source, when they first met, bin Saud declared:\n\nMuhammad ibn ʿAbd al-Wahhab replied: \n\nThe agreement was confirmed with a mutual oath of loyalty (\"bay'ah\") in 1744.\nIbn Abd al-Wahhab would be responsible for religious matters and Ibn Saud in charge of political and military issues. This agreement became a \"mutual support pact\"\n\nThe 1744 pact between Muhammad bin Saud and Muhammad ibn ʿAbd al-Wahhab marked the emergence of the first Saudi state, the Emirate of Diriyah. By offering the Al Saud a clearly defined religious mission, the alliance provided the ideological impetus to Saudi expansion. First conquering Najd, Saud's forces expanded the Salafi influence to most of the present-day territory of Saudi Arabia, eradicating various popular practices they viewed as akin to polytheism and propagating the doctrines of ʿAbd al-Wahhab.\n\nAccording to academic publications such as the Encyclopædia Britannica while in Baghdad, Ibn ʿAbd al-Wahhab married an affluent woman. When she died, he inherited her property and wealth. Muhammad ibn 'Abd Al-Wahhab had six sons; Hussain, Abdullah, Hassan, Ali and Ibrahim and Abdul-Aziz who died in his youth. All his surviving sons established religious schools close to their homes and taught the young students from Diriyah and other places.\nThe descendants of Ibn ʿAbd al-Wahhab, the Al ash-Sheikh, have historically led the ulama in the Saudi state, dominating the state's religious institutions. Within Saudi Arabia, the family is held in prestige similar to the Saudi royal family, with whom they share power, and has included several religious scholars and officials. The arrangement between the two families is based on the Al Saud maintaining the Al ash-Sheikh's authority in religious matters and upholding and propagating Salafi doctrine. In return, the Al ash-Sheikh support the Al Saud's political authority thereby using its religious-moral authority to legitimise the royal family's rule.\n\nIbn ʿAbd al-Wahhab considered his movement an effort to purify Islam by returning Muslims to what, he believed, were the original principles of that religion. He taught that the primary doctrine of Islam was the uniqueness and unity of God (\"Tawhid\"). He also denounced popular beliefs as polytheism (shirk), rejected much of the medieval law of the scholars (ulema) and called for a new interpretation of Islam.\n\nThe \"core\" of Ibn ʿAbd al-Wahhab's teaching is found in \"Kitab al-Tawhid\", a short essay which draws from material in the Quran and the recorded doings and sayings (\"hadith\") of the Islamic prophet Muhammad. It preaches that worship in Islam includes conventional acts of worship such as the five daily prayers (\"salat\"); fasting (\"sawm\"); supplication (\"Dua\"); seeking protection or refuge (\"Istia'dha\"); seeking help (\"Ist'ana\" and \"Istighatha\") of Allah.\n\nIbn ʿAbd al-Wahhab was keen on emphasizing that other acts, such as making \"dua\" or calling upon/supplication to or seeking help, protection or intercession from anyone or anything other than Allah, are acts of \"shirk\" and contradict the tenets of tawhid and that those who tried would never be forgiven.\n\nTraditionally, most Muslims throughout history have held the view that declaring the testimony of faith is sufficient in becoming a Muslim. Ibn 'Abd al-Wahhab did not agree with this. He held the view that an individual who believed that there could be intercessors with God was actually performing shirk. This was the major difference between him and his opponents and led him to declare Muslims outside of his group to be apostates (takfir) and idolators (mushrikin).\n\nIbn 'Abd al-Wahhab's movement is today often known as Wahhabism, although many adherents see this as a derogatory term coined by his opponents, and prefer it to be known as the Salafi movement. Scholars point out that Salafism is a term applicable to several forms of puritanical Islam in various parts of the world, while Wahhabism refers to the specific Saudi school, which is seen as a more strict form of Salafism. According to Ahmad Moussalli, professor of political science at the American University of Beirut, \"As a rule, all Wahhabis are Salafists, but not all Salafists are Wahhabis\". Yet others say that while Wahhabism and Salafism originally were two different things, they became practically indistinguishable in the 1970s.\n\nAt the end of his treatise, \"Al-Hadiyyah al-Suniyyah\", Ibn ʿAbd al-Wahhab's son 'Abd Allah speaks positively on the practice of tazkiah (purification of the inner self).\n\nAccording to author Dore Gold, in \"Kitab al-Tawhid\", Ibn Abd al-Wahhab described followers of both the Christian and Jewish faiths as sorcerers who believed in devil worship, and cited a hadith of Muhammad stating that punishment for the sorcerer is `that he be struck with the sword.` Wahhab asserted that both religions had improperly made the graves of their prophet into places of worship and warned Muslims not to imitate this practice. Wahhab concluded that `The ways of the people of the book are condemned as those of polytheists.`\n\nHowever author Natana J. DeLong-Bas defends Abdul Wahhab, stating that\n\ndespite his at times vehement denunciations of other religious groups for their supposedly heretical beliefs, Ibn Abd al Wahhab never called for their destruction or death … he assumed that these people would be punished in the Afterlife …\"\nHistorical accounts of Wahhab also state that, \"Muhammad ibn ʿAbd al-Wahhab saw it as his mission to restore a more purer and original form of the faith of Islam. … Anyone who didn't adhere to this interpretation were considered polytheists worthy of death, including fellow Muslims (especially Shi'ite who venerate the family of Muhammad), Christians and others. He also advocated for a literalist interpretation of the Quran and its laws\"\n\nDespite his great aversion to venerating the saints after their earthly passing and seeking their intercession, it should nevertheless be noted that Ibn 'Abd-al-Wahhab did not deny the existence of saints as such; on the contrary, he acknowledged that \"the miracles of saints (\"karāmāt al-awliyāʾ\") are not to be denied, and their right guidance by God is acknowledged\" when they acted properly during their life.\n\nIbn ʿAbd al-Wahhab's teachings were criticized by a number of Islamic scholars during his life for disregarding Islamic history, monuments, traditions and the sanctity of Muslim life. One scholar named Ibn Muhammad compared Ibn 'Abd al-Wahhab with Musaylimah the liar alayhi la'na. He also accused Ibn 'Abd al-Wahhab of wrongly declaring the Muslims to be infidels based on a misguided reading of Qur'anic passages and Prophetic traditions and of wrongly declaring all scholars as infidels who did not agree with his \"deviant innovation\".\n\nThe traditional Hanbali scholar Ibn Fayruz al-Tamimi (d. 1801/1802) publicly repudiated Ibn 'Abd al-Wahhab's teachings when he sent an envoy to him and referred to the Wahhabis as the \"seditious Kharijites\" of Najd. In response, the Wahhabis considered Ibn Fayruz an idolater (mushrik) and one of their worst enemies.\n\nAccording to the historian Ibn Humayd, Ibn 'Abd al-Wahhab's father criticized his son for his unwillingness to specialize in jurisprudence and disagreed with his doctrine and declared that he would be the cause of wickedness. Similarly his own brother, Suleyman ibn 'Abd al-Wahhab wrote one of the first treatises' refuting Wahhabi doctrine claiming he was ill-educated and intolerant, and classing Ibn ʿAbd al-Wahhab's views as fringe and fanatical.\n\nThe Shafi'i mufti of Mecca, Ahmed ibn Zayni Dehlan, wrote an anti-Wahhabi treatise, the bulk of which consists of arguments and proof from the sunna to uphold the validity of practices the Wahhabis considered idolatrous: Visiting the tombs of Muhammad, seeking the intercession of saints, venerating Muhammad and obtaining the blessings of saints. He also accused Ibn 'Abd al-Wahhab of not adhering to the Hanbali school and that he was deficient in learning.\n\nPakistani Muslim scholars such as Israr Ahmed have spoken positively on him. Ibn ʿAbd al-Wahhab is accepted by Salafi scholars as an authority and source of reference. 20th century Albanian scholar Nasiruddin Albani refers to Ibn Abdul Wahhab's activism as \"Najdi da'wah.\"\n\nA list of scholars with opposing views, along with names of their books and related information, was compiled by the Islamic scholar Muhammad Hisham.\n\nThe state mosque of Qatar is named after him. The \"Imam Muhammad ibn Abd al-Wahhab Mosque\" was opened in 2011, with the Emir of Qatar presiding over the occasion. The mosque can hold a congregation of 30,000 people. There has been request from descendants of Ibn Abd al-Wahhab that the name of the mosque be changed.\n\nDespite Wahhabi destruction of many Islamic, non-Islamic, and historical sites associated with the first Muslims (the Prophet's family and companions), the Saudi government undertook a large-scale development of Muhammad ibn Abd al-Wahhab's domain, Diriyah, turning it into a major tourist attraction. Other features in the area include the \"Sheikh Muhammad bin Abdul Wahab Foundation\" which is planned to include a light and sound presentation located near the \"Mosque of Sheikh Mohammad bin Abdulwahab\".\n\n\n\nThere are two contemporary histories of Muhammed ibn ʿAbd al-Wahhab and his religious movement from the point of view of his supporters: Ibn Ghannam's \"Rawdhat al-Afkar wal-Afham\" or \"Tarikh Najd\" (History of Najd) and Ibn Bishr's \"Unwan al-Majd fi Tarikh Najd\". Husain ibn Ghannam (d. 1811), an alim from al-Hasa was the only historian to have observed the beginnings of Ibn ʿAbd al-Wahhab's movement first-hand. His chronicle ends at the year 1797. Ibn Bishr's chronicle, which stops at the year 1854, was written a generation later than Ibn Ghannam's, but is considered valuable partly because Ibn Bishr was a native of Najd and because he adds many details to Ibn Ghannam's account.\n\nA third account, dating from around 1817 is \"Lam' al-Shihab\", written by an anonymous Sunni author who respectfully disapproved of Ibn ʿAbd al-Wahhab's movement, regarding it as a \"bid'ah\". It is also commonly cited because it is considered to be a relatively objective contemporary treatment of the subject. However, unlike Ibn Ghannam and Ibn Bishr, its author did not live in Najd and his work is believed to contain some apocryphal and legendary material with respect to the details of Ibn ʿAbd al-Wahhab's life.\n\n"}
{"id": "19977", "url": "https://en.wikipedia.org/wiki?curid=19977", "title": "Maine", "text": "Maine\n\nMaine () is a state in the New England region of the northeastern United States. Maine is the 12th smallest by area, the 9th least populous, and the 38th most densely populated of the 50 U.S. states. It is bordered by New Hampshire to the west, the Atlantic Ocean to the southeast, and the Canadian provinces of New Brunswick and Quebec to the northeast and northwest respectively. Maine is the easternmost state in the contiguous United States, and the northernmost state east of the Great Lakes. It is known for its jagged, rocky coastline; low, rolling mountains; heavily forested interior; and picturesque waterways, as well as its seafood cuisine, especially lobster and clams. There is a humid continental climate throughout most of the state, including in coastal areas such as its most populous city of Portland. The capital is Augusta.\n\nFor thousands of years, indigenous peoples were the only inhabitants of the territory that is now Maine. At the time of European arrival in what is now Maine, several Algonquian-speaking peoples inhabited the area. The first European settlement in the area was by the French in 1604 on Saint Croix Island, by Pierre Dugua, Sieur de Mons. The first English settlement was the short-lived Popham Colony, established by the Plymouth Company in 1607. A number of English settlements were established along the coast of Maine in the 1620s, although the rugged climate, deprivations, and conflict with the local peoples caused many to fail over the years.\n\nAs Maine entered the 18th century, only a half dozen European settlements had survived. Loyalist and Patriot forces contended for Maine's territory during the American Revolution and the War of 1812. During the War of 1812, the largely-undefended eastern region of Maine was occupied by British forces, but returned to the United States after the war following major defeats in New York, Maryland and Louisiana, as part of a peace treaty that was to include dedicated land on the Michigan peninsula for Native American peoples. Maine was part of the Commonwealth of Massachusetts until 1820, when it voted to secede from Massachusetts to become a separate state. On March 15, 1820, under the Missouri Compromise, it was admitted to the Union as the 23rd state.\n\nThere is no definitive explanation for the origin of the name \"Maine\", but the most likely origin is that the name was given by early explorers after the former province of Maine in France. Whatever the origin, the name was fixed for English settlers in 1665 when the English King's Commissioners ordered that the \"Province of Maine\" be entered from then on in official records. The state legislature in 2001 adopted a resolution establishing Franco-American Day, which stated that the state was named after the former French province of Maine.\n\nOther theories mention earlier places with similar names, or claim it is a nautical reference to the mainland. Attempts to uncover the history of the name of Maine began with James Sullivan's 1795 \"History of the District of Maine\". He made the unsubstantiated claim that the Province of Maine was a compliment to the queen of Charles I, Henrietta Maria, who once \"owned\" the Province of Maine in France. This was quoted by Maine historians until the 1845 biography of that queen by Agnes Strickland established that she had no connection to the province; further, King Charles I married Henrietta Maria in 1625, three years after the name Maine first appeared on the charter. A new theory, put forward by Carol B. Smith Fisher in 2002, is that Sir Ferdinando Gorges chose the name in 1622 to honor the village where his ancestors first lived in England, rather than the province in France. \"MAINE\" appears in the Domesday Book of 1086 in reference to the county of Dorset, which is today Broadmayne, just southeast of Dorchester.\n\nThe view generally held among British place name scholars is that Mayne in Dorset is Brythonic, corresponding to modern Welsh \"maen\", plural \"main\" or \"meini\". Some early spellings are: MAINE 1086, MEINE 1200, MEINES 1204, MAYNE 1236. Today the village is known as Broadmayne, which is primitive Welsh or Brythonic, \"main\" meaning rock or stone, considered a reference to the many large sarsen stones still present around Little Mayne farm, half a mile northeast of Broadmayne village.\n\nThe first known record of the name appears in an August 10, 1622 land charter to Sir Ferdinando Gorges and Captain John Mason, English Royal Navy veterans, who were granted a large tract in present-day Maine that Mason and Gorges \"intend to name the Province of Maine\". Mason had served with the Royal Navy in the Orkney Islands, where the chief island is called Mainland, a possible name derivation for these English sailors. In 1623, the English naval captain Christopher Levett, exploring the New England coast, wrote: \"The first place I set my foote upon in New England was the Isle of Shoals, being Ilands in the sea, above two Leagues from the Mayne.\" Initially, several tracts along the coast of New England were referred to as \"Main\" or \"Maine\" (cf. the Spanish Main). A reconfirmed and enhanced April 3, 1639, charter, from England's King Charles I, gave Sir Ferdinando Gorges increased powers over his new province and stated that it \"shall forever hereafter, be called and named the PROVINCE OR COUNTIE OF MAINE, and not by any other name or names whatsoever...\" Maine is the only U.S. state whose name has exactly one syllable.\n\nThe original inhabitants of the territory that is now Maine were Algonquian-speaking Wabanaki peoples, including the Passamaquoddy, Maliseet, Penobscot, Androscoggin and Kennebec. During the later King Philip's War, many of these peoples would merge in one form or another to become the Wabanaki Confederacy, aiding the Wampanoag of Massachusetts & the Mahican of New York. Afterwards, many of these people were driven from their natural territories, but most of the tribes of Maine continued, unchanged, until the American Revolution. Before this point, however, most of these people were considered separate nations. Many had adapted to living in permanent, Iroquois-inspired settlements, while those along the coast tended to be semi-nomadic—traveling from settlement to settlement on a yearly cycle. They would usually winter inland & head to the coasts by summer.\n\nEuropean contact with what is now called Maine started around 1200 CE when Norwegians interacted with the native Penobscot in present-day Hancock County, most likely through trade. About 200 years earlier, from the settlements in Iceland and Greenland, Norwegians had first identified America and attempted to settle areas such as Newfoundland, but failed to establish a permanent settlement there. Archeological evidence suggests that Norwegians in Greenland returned to North America for several centuries after the initial discovery to collect timber and to trade, with the most relevant evidence being the Maine Penny, an 11th-century Norwegian coin found at a Native American dig site in 1954.\n\nThe first European settlement in Maine was in 1604 on Saint Croix Island, led by French explorer Pierre Dugua, Sieur de Mons; his party included Samuel de Champlain, noted as an explorer. The French named the entire area Acadia, including the portion that later became the state of Maine. The first English settlement in Maine was established by the Plymouth Company at the Popham Colony in 1607, the same year as the settlement at Jamestown, Virginia. The Popham colonists returned to Britain after 14 months.\n\nThe French established two Jesuit missions: one on Penobscot Bay in 1609, and the other on Mount Desert Island in 1613. The same year, Castine was established by Claude de La Tour. In 1625, Charles de Saint-Étienne de la Tour erected Fort Pentagouet to protect Castine. The coastal areas of western Maine first became the Province of Maine in a 1622 land patent. The part of Eastern Maine north of the Kennebec River was more sparsely settled, and was known in the 17th century as the Territory of Sagadahock. A second settlement was attempted in 1623 by English explorer and naval Captain Christopher Levett at a place called York, where he had been granted by King Charles I of England. It also failed.\n\nCentral Maine was formerly inhabited by people of the Androscoggin tribe of the Abenaki nation, also known as Arosaguntacook. They were driven out of the area in 1690 during King William's War. They were relocated at St. Francis, Canada, which was destroyed by Rogers' Rangers in 1759, and is now Odanak. The other Abenaki tribes suffered several severe defeats, particularly during Dummer's War, with the capture of Norridgewock in 1724 and the defeat of the Pequawket in 1725, which greatly reduced their numbers. They finally withdrew to Canada, where they were settled at Bécancour and Sillery, and later at St. Francis, along with other refugee tribes from the south.\n\nThe province within its current boundaries became part of Massachusetts Bay Colony in 1652. Maine was much fought over by the French, English, and allied natives during the 17th and early 18th centuries, who conducted raids against each other, taking captives for ransom or, in some cases, adoption by Native American tribes. A notable example was the early 1692 Abenaki raid on York, where about 100 English settlers were killed and another estimated 80 taken hostage. The Abenaki took captives taken during raids of Massachusetts in Queen Anne's War of the early 1700s to Kahnewake, a Catholic Mohawk village near Montreal, where some were adopted and others ransomed.\n\nAfter the British defeated the French in Acadia in the 1740s, the territory from the Penobscot River east fell under the nominal authority of the Province of Nova Scotia, and together with present-day New Brunswick formed the Nova Scotia county of Sunbury, with its court of general sessions at Campobello. American and British forces contended for Maine's territory during the American Revolution and the War of 1812, with the British occupying eastern Maine in both conflicts. The territory of Maine was confirmed as part of Massachusetts when the United States was formed following the Treaty of Paris ending the revolution, although the final border with British North America was not established until the Webster–Ashburton Treaty of 1842.\n\nMaine was physically separate from the rest of Massachusetts. Long-standing disagreements over land speculation and settlements led to Maine residents and their allies in Massachusetts proper forcing an 1807 vote in the Massachusetts Assembly on permitting Maine to secede; the vote failed. Secessionist sentiment in Maine was stoked during the War of 1812 when Massachusetts pro-British merchants opposed the war and refused to defend Maine from British invaders. In 1819, Massachusetts agreed to permit secession, sanctioned by voters of the rapidly growing region the following year. Formal secession and formation of the state of Maine as the 23rd state occurred on March 15, 1820, as part of the Missouri Compromise, which geographically limited the spread of slavery and enabled the admission to statehood of Missouri the following year, keeping a balance between slave and free states.\n\nMaine's original state capital was Portland, Maine's largest city, until it was moved to the more central Augusta in 1832. The principal office of the Maine Supreme Judicial Court remains in Portland.\n\nThe 20th Maine Volunteer Infantry Regiment, under the command of Colonel Joshua Lawrence Chamberlain, prevented the Union Army from being flanked at Little Round Top by the Confederate Army during the Battle of Gettysburg.\n\nFour U.S. Navy ships have been named USS \"Maine\", most famously the armored cruiser , whose sinking by an explosion on February 15, 1898 precipitated the Spanish–American War.\n\nTo the south and east is the Atlantic Ocean and to the north and northeast is New Brunswick, a province of Canada. The Canadian province of Quebec is to the northwest. Maine is both the northernmost state in New England and the largest, accounting for almost half of the region's entire land area. Maine is the only state in the continental US to border only one other American state (New Hampshire to the South and West).\n\nMaine is the easternmost state in the United States in both its extreme points and its geographic center. The town of Lubec is the easternmost organized settlement in the United States. Its Quoddy Head Lighthouse is also the closest place in the United States to Africa and Europe. Estcourt Station is Maine's northernmost point, as well as the northernmost point in New England. (For more information see extreme points of the United States.)\n\nMaine's Moosehead Lake is the largest lake wholly in New England, as Lake Champlain is located between Vermont, New York and Quebec. A number of other Maine lakes, such as South Twin Lake, are described by Thoreau in \"The Maine Woods\" (1864). Mount Katahdin is both the northern terminus of the Appalachian Trail, which extends southerly to Springer Mountain, Georgia, and the southern terminus of the new International Appalachian Trail which, when complete, will run to Belle Isle, Newfoundland and Labrador.\n\nMaine has several unique geographical features. Machias Seal Island and North Rock, off the state's Downeast coast, are claimed by both Canada and the American town of Cutler, and are within one of four areas between the two countries whose sovereignty is still in dispute, but it is the only one of the disputed areas containing land. Also in this easternmost area in the Bay of Fundy is the Old Sow, the largest tidal whirlpool in the Western Hemisphere.\n\nMaine is the least densely populated U.S. state east of the Mississippi River. It is called the Pine Tree State; over 80% of its total land is forested and/or unclaimed. the most forest cover of any U.S. state. In the forested areas of the interior lies much uninhabited land, some of which does not have formal political organization into local units (a rarity in New England). The Northwest Aroostook, Maine unorganized territory in the northern part of the state, for example, has an area of and a population of 10, or one person for every .\n\nMaine is in the temperate broadleaf and mixed forests biome. The land near the southern and central Atlantic coast is covered by the mixed oaks of the Northeastern coastal forests. The remainder of the state, including the North Woods, is covered by the New England-Acadian forests.\n\nMaine has almost of coastline (and of tidal coastline). West Quoddy Head, in Lubec, Maine, is the easternmost point of land in the 48 contiguous states. Along the famous rock-bound coast of Maine are lighthouses, beaches, fishing villages, and thousands of offshore islands, including the Isles of Shoals which straddle the New Hampshire border. There are jagged rocks and cliffs and many bays and inlets. Inland are lakes, rivers, forests, and mountains. This visual contrast of forested slopes sweeping down to the sea has been summed up by American poet Edna St. Vincent Millay of Rockland and Camden, Maine, in \"Renascence\":\n\nGeologists describe this type of landscape as a \"drowned coast\", where a rising sea level has invaded former land features, creating bays out of valleys and islands out of mountain tops. A rise in the elevation of the land due to the melting of heavy glacier ice caused a slight rebounding effect of underlying rock; this land rise, however, was not enough to eliminate all the effect of the rising sea level and its invasion of former land features.\n\nMuch of Maine's geomorphology was created by extended glacial activity at the end of the last ice age. Prominent glacial features include Somes Sound and Bubble Rock, both part of Acadia National Park on Mount Desert Island. Carved by glaciers, Somes Sound is considered to be the only fjord on the eastern seaboard and reaches depths of . The extreme depth and steep drop-off allow large ships to navigate almost the entire length of the sound. These features also have made it attractive for boat builders, such as the prestigious Hinckley Yachts.\n\nBubble Rock, a glacial erratic, is a large boulder perched on the edge of Bubble Mountain in Acadia National Park. By analyzing the type of granite, geologists were able to discover that glaciers carried Bubble Rock to its present location from near Lucerne – away. The Iapetus Suture runs through the north and west of the state, being underlain by the ancient Laurentian terrane, and the south and east underlain by the Avalonian terrane.\n\nAcadia National Park is the only national park in New England. Areas under the protection and management of the National Park Service include:\n\nMaine has a humid continental climate (Köppen climate classification \"Dfb\"), with warm (although generally not hot), humid summers. Winters are cold and very snowy throughout the state, and are especially severe in the Northern and Western parts of Maine. Coastal areas are moderated slightly by the Atlantic Ocean, resulting in slightly milder winters and cooler summers in immediate coastal areas. Daytime highs are generally in the range throughout the state in July, with overnight lows in the high 50s °F (around 15 °C). January temperatures range from highs near on the southern coast to overnight lows averaging below in the far north.\n\nThe state's record high temperature is , set in July 1911, at North Bridgton.\nPrecipitation in Maine is evenly distributed year-round, but with a slight summer maximum in northern/northwestern Maine and a slight late-fall or early-winter maximum along the coast due to \"nor'easters\" or intense cold-season rain/snow storms. In coastal Maine, the late spring and summer months are usually driest – a rarity across the Eastern United States. Maine has fewer days of thunderstorms than any other state east of the Rockies, with most of the state averaging less than 20 days of thunderstorms a year. Tornadoes are rare in Maine, with the state averaging fewer than four per year, although this number is increasing. Most severe thunderstorms and tornadoes occur in the Sebago Lakes & Foothills region of the state. Maine only occasionally sees tropical cyclones.\n\nIn January 2009, a new record low temperature for the state was set at Big Black River of , tying the New England record.\n\nAnnual precipitation varies from in Presque Isle, to in Acadia National Park.\n\nThe United States Census Bureau estimates that the population of Maine was 1,329,328 on July 1, 2015, a 0.07% increase since the 2010 United States Census. The population density of the state is 41.3 people per square mile, making it the least densely populated state in New England, the American northeast, the eastern seaboard, of all of the states with an Atlantic coastline and of all of the states east of the Mississippi River.\n\nThe mean population center of Maine is located in Kennebec County, just east of Augusta. The Greater Portland metropolitan area is the most densely populated with nearly 40% of Maine's population. Portland's estimated population in 2016 was 66,937. As explained in detail under \"Geography\", there are large tracts of uninhabited land in some remote parts of the interior.\n\nMaine has experienced a very slow rate of population growth since the 1990 census; its rate of growth (0.57%) since the 2010 census ranks 45th of the 50 states. The modest population growth in the state has been concentrated in the southern coastal counties; with more diverse populations slowly moving into these areas of the state. However, the northern, more rural areas of the state have experienced a slight decline in population in recent years.\n\nAccording to the 2010 Census, Maine has the highest percentage of non-Hispanic whites of any state, at 94.4% of the total population. In 2011, 89.0% of all births in the state were to non-Hispanic white parents.\n\nThe table below shows the racial composition of Maine's population as of 2016.\n\nAccording to the 2016 American Community Survey, 1.5% of Maine's population were of Hispanic or Latino origin (of any race): Mexican (0.4%), Puerto Rican (0.4%), Cuban (0.1%), and other Hispanic or Latino origin (0.6%). The five largest ancestry groups were: English (20.7%), Irish (17.3%), French (15.7%), German (8.1%), and American (7.8%).\n\nPeople citing that they are American are of overwhelmingly English descent, but have ancestry that has been in the region for so long (often since the 1600s) that they choose to identify simply as Americans.\n\nMaine has the highest percentage of French Americans of any state. Most of them are of Canadian origin, but in some cases have been living there since prior to the American Revolutionary War. There are particularly high concentrations in the northern part of Maine in Aroostook County, which is part of a cultural region known as Acadia that goes over the border into New Brunswick. Along with the Acadian population in the north, many French came from Quebec as immigrants between 1840 and 1930.\n\nThe upper Saint John River valley area was once part of the so-called Republic of Madawaska, before the frontier was decided in the Webster-Ashburton Treaty of 1842. Over one quarter of the population of Lewiston, Waterville, and Biddeford are Franco-American. Most of the residents of the Mid Coast and Down East sections are chiefly of British heritage. Smaller numbers of various other groups, including Irish, Italian and Polish, have settled throughout the state since the late 19th and early 20th century immigration waves.\n\n\"Note: Births in table don't add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number.\"\n\n\nMaine does not have an official language, but the most widely spoken language in the state is English. The 2000 Census reported 92.25% of Maine residents aged five and older spoke only English at home. French-speakers are the state's chief linguistic minority; census figures show that Maine has the highest percentage of people speaking French at home of any state: 5.28% of Maine households are French-speaking, compared with 4.68% in Louisiana, which is the second highest state. Although rarely spoken, Spanish is the third-most-common language in Maine, after English and French.\n\nAccording to the Association of Religion Data Archives (ARDA), the religious affiliations of Maine in 2010 were:\n\nThe Catholic Church was the largest religious institution with 202,106 members, the United Methodist Church had 28,329 members, the United Church of Christ had 22,747 members\n\nIn 2010, a study named Maine as the least religious state in the United States.\n\nThe Bureau of Economic Analysis estimates that Maine's total gross state product for 2010 was $52 billion. Its per capita personal income for 2007 was US$33,991, 34th in the nation. , Maine's unemployment rate is 3.0%\nMaine's agricultural outputs include poultry, eggs, dairy products, cattle, wild blueberries, apples, maple syrup, and maple sugar. Aroostook County is known for its potato crops. Commercial fishing, once a mainstay of the state's economy, maintains a presence, particularly lobstering and groundfishing. Western Maine aquifers and springs are a major source of bottled water.\n\nMaine's industrial outputs consist chiefly of paper, lumber and wood products, electronic equipment, leather products, food products, textiles, and bio-technology. Naval shipbuilding and construction remain key as well, with Bath Iron Works in Bath and Portsmouth Naval Shipyard in Kittery.\n\nBrunswick Landing, formerly Naval Air Station Brunswick, is also in Maine. Formerly a large support base for the U.S. Navy, the BRAC campaign initiated the Naval Air Station's closing, despite a government-funded effort to upgrade its facilities. The former base has since been changed into a civilian business park, as well as a new satellite campus for Southern Maine Community College.\n\nMaine is the number one US producer of low-bush blueberries (Vaccinium angustifolium). Preliminary data from the USDA for 2012 also indicate Maine was the largest blueberry producer of the major blueberry producing states in the US, with 91,100,000 lbs. This data includes both low (wild), and high-bush (cultivated) blueberries: Vaccinium corymbosum. The largest toothpick manufacturing plant in the United States used to be located in Strong, Maine. The Strong Wood Products plant produced 20 million toothpicks a day. It closed in May 2003.\n\nTourism and outdoor recreation play a major and increasingly important role in Maine's economy. The state is a popular destination for sport hunting (particularly deer, moose and bear), sport fishing, snowmobiling, skiing, boating, camping and hiking, among other activities.\n\nHistorically, Maine ports played a key role in national transportation. Beginning around 1880, Portland's rail link and ice-free port made it Canada's principal winter port, until the aggressive development of Halifax, Nova Scotia, in the mid-1900s. In 2013, 12,039,600 short tons passed into and out of Portland by sea, which places it 45th of US water ports. Portland Maine's Portland International Jetport was recently expanded, providing the state with increased air traffic from carriers such as JetBlue and Southwest Airlines.\n\nMaine has very few large companies that maintain headquarters in the state, and that number has fallen due to consolidations and mergers, particularly in the pulp and paper industry. Some of the larger companies that do maintain headquarters in Maine include Fairchild Semiconductor in South Portland; IDEXX Laboratories, in Westbrook; Hannaford Bros. Co. in Scarborough; Unum in Portland; TD Bank in Portland; L.L.Bean in Freeport; and Cole Haan in Yarmouth. Maine is also the home of The Jackson Laboratory, the world's largest non-profit mammalian genetic research facility and the world's largest supplier of genetically purebred mice.\n\nMaine has an income tax structure containing two brackets, 6.5 and 7.95 percent of personal income. Before July 2013 Maine had four brackets: 2, 4.5, 7, and 8.5 percent. Maine's general sales tax rate is 5.5 percent. The state also levies charges of 9 percent on lodging and prepared food and 10 percent on short-term auto rentals. Commercial sellers of blueberries, a Maine staple, must keep records of their transactions and pay the state 1.5 cents per pound ($1.50 per 100 pounds) of the fruit sold each season. All real and tangible personal property located in the state of Maine is taxable unless specifically exempted by statute. The administration of property taxes is handled by the local assessor in incorporated cities and towns, while property taxes in the unorganized territories are handled by the State Tax Assessor.\n\nMaine has a long-standing tradition of being home to many shipbuilding companies. In the 18th and 19th centuries, Maine was home to many shipyards that produced wooden sailing ships. The main function of these ships was to transport either cargos or passengers overseas. One of these yards was located in Pennellville Historic District in what is now Brunswick, Maine. This yard, owned by the Pennell family, was typical of the many family-owned shipbuilding companies of the time period. Other such examples of shipbuilding families were the Skolfields and the Morses. During the 18th and 19th centuries, wooden shipbuilding of this sort made up a sizable portion of the economy.\n\nMaine receives passenger jet service at its two largest airports, the Portland International Jetport in Portland, and the Bangor International Airport in Bangor. Both are served daily by many major airlines to destinations such as New York, Atlanta, and Orlando. Essential Air Service also subsidizes service to a number of smaller airports in Maine, bringing small turboprop aircraft to regional airports such as the Augusta State Airport, Hancock County-Bar Harbor Airport, Knox County Regional Airport, and the Northern Maine Regional Airport at Presque Isle. These airports are served by regional providers such as Cape Air with Cessna 402s, PenAir with Saab 340s, and CommutAir with Embraer ERJ 145 aircraft.\n\nMany smaller airports are scattered throughout Maine, only serving general aviation traffic. The Eastport Municipal Airport, for example, is a city-owned public-use airport with 1,200 general aviation aircraft operations each year from single-engine and ultralight aircraft.\n\nInterstate 95 (I-95) travels through Maine, as well as its easterly branch I-295 and spurs 195, 395 and the unsigned I-495. In addition, U.S. Route 1 (US 1) starts in Fort Kent and travels to Florida. The eastern terminus of the eastern section of US 2 starts in Houlton, near the New Brunswick, Canada border to Rouses Point, New York, at US 11. US 2A connects Old Town and Orono, primarily serving the University of Maine campus. US 201 and US 202 flow through the state. US 2, Maine State Route 6 (Route 6), and Route 9 are often used by truckers and other motorists of the Maritime Provinces \"en route\" to other destinations in the United States or as a short cut to Central Canada.\n\nThe \"Downeaster\" passenger train, operated by Amtrak, provides passenger service between Brunswick and Boston's North Station, with stops in Freeport, Portland, Old Orchard Beach, Saco, and Wells. The \"Downeaster\" makes five daily trips, three of which continue past Portland to Brunswick.\n\nFreight service throughout the state is provided by a handful of regional and shortline carriers: Pan Am Railways (formerly known as Guilford Rail System), which operates the former Boston & Maine and Maine Central railroads; St. Lawrence and Atlantic Railroad; Maine Eastern Railroad; Central Maine and Quebec Railway; and New Brunswick Southern Railway.\n\nThe Maine Constitution structures Maine's state government, composed of three co-equal branches—the executive, legislative, and judicial branches. The state of Maine also has three Constitutional Officers (the Secretary of State, the State Treasurer, and the State Attorney General) and one Statutory Officer (the State Auditor).\n\nThe legislative branch is the Maine Legislature, a bicameral body composed of the Maine House of Representatives, with 151 members, and the Maine Senate, with 35 members. The Legislature is charged with introducing and passing laws.\n\nThe executive branch is responsible for the execution of the laws created by the Legislature and is headed by the Governor of Maine (currently Janet Mills). The Governor is elected every four years; no individual may serve more than two consecutive terms in this office. The current attorney general of Maine is Aaron Frey. As with other state legislatures, the Maine Legislature can by a two-thirds majority vote from both the House and Senate override a gubernatorial veto. Maine is one of seven states that do not have a lieutenant governor.\n\nThe judicial branch is responsible for interpreting state laws. The highest court of the state is the Maine Supreme Judicial Court. The lower courts are the District Court, Superior Court and Probate Court. All judges except for probate judges serve full-time, are nominated by the Governor and confirmed by the Legislature for terms of seven years. Probate judges serve part-time and are elected by the voters of each county for four-year terms.\n\nMaine is divided into political jurisdictions designated as counties. Since 1860 there have been 16 counties in the state, ranging in size from .\n\nIn state general elections, Maine voters tend to accept independent and third-party candidates more frequently than most states. Maine has had two independent governors recently (James B. Longley, 1975–1979 and current U.S. Senator Angus King, 1995–2003). Maine state politicians, Democrats and Republicans alike, are noted for having more moderate views than many in the national wings of their respective parties.\n\nMaine is an alcoholic beverage control state.\n\nOn May 6, 2009, Maine became the fifth state to legalize same-sex marriage; however, the law was repealed by voters on November 3, 2009. On November 6, 2012, Maine, along with Maryland and Washington, became the first state to legalize same-sex marriage at the ballot box.\n\nIn the 1930s, Maine was one of very few states which retained Republican sentiments. In the 1936 presidential election, Franklin D. Roosevelt received the electoral votes of every state other than Maine and Vermont; these were the only two states in the nation that never voted for Roosevelt in any of his presidential campaigns, though Maine was closely fought in 1940 and 1944. In the 1960s, Maine began to lean toward the Democrats, especially in presidential elections. In 1968, Hubert Humphrey became just the second Democrat in half a century to carry Maine, perhaps because of the presence of his running mate, Maine Senator Edmund Muskie, although the state voted Republican in every presidential election in the 1970s and 1980s.\nSince 1969, two of Maine's four electoral votes have been awarded based on the winner of the statewide election; the other two go to the highest vote-getter in each of the state's two congressional districts. Every other state except Nebraska gives all its electoral votes to the candidate who wins the popular vote in the state at large, without regard to performance within districts. Maine split its electoral vote for the first time in 2016, with Donald Trump's strong showing in the more rural central and northern Maine allowing him to capture one of the state's four votes in the Electoral College.\n\nRoss Perot achieved a great deal of success in Maine in the presidential elections of 1992 and 1996. In 1992, as an independent candidate, Perot came in second to Democrat Bill Clinton, despite the long-time presence of the Bush family summer home in Kennebunkport. In 1996, as the nominee of the Reform Party, Perot did better in Maine than in any other state.\n\nMaine has voted for Democratic Bill Clinton twice, Al Gore in 2000, John Kerry in 2004, and Barack Obama in 2008 and 2012. In 2016, Republican Donald Trump won one of Maine's electoral votes with Democratic opponent Hillary Clinton winning the other three. Although Democrats have mostly carried the state in presidential elections in recent years, Republicans have largely maintained their control of the state's U.S. Senate seats, with Edmund Muskie, William Hathaway and George J. Mitchell being the only Maine Democrats serving in the U.S. Senate in the past fifty years.\n\nIn the 2010 midterm elections, Republicans made major gains in Maine. They captured the governor's office as well as majorities in both chambers of the state legislature for the first time since the early 1970s. However, in the 2012 elections Democrats managed to recapture both houses of Maine Legislature.\n\nMaine's U.S. senators are Republican Susan Collins and Independent Angus King. The governor is Democrat Janet Mills. The state's two members of the United States House of Representatives are Democrats Chellie Pingree and \nJared Golden.\n\nAn organized municipality has a form of elected local government which administers and provides local services, keeps records, collects licensing fees, and can pass locally binding ordinances, among other responsibilities of self-government. The governmental format of most organized towns and plantations is the town meeting, while the format of most cities is the council-manager form. the organized municipalities of Maine consist of 23 cities, 431 towns, and 34 plantations. Collectively these 488 organized municipalities cover less than half of the state's territory. Maine also has 3 Reservations: Indian Island, Indian Township Reservation, and Pleasant Point Indian Reservation.\n\nUnorganized territory has no local government. Administration, services, licensing, and ordinances are handled by the state government as well as by respective county governments who have townships within each county’s bounds. The unorganized territory of Maine consists of over 400 townships (towns are incorporated, townships are unincorporated), plus many coastal islands that do not lie within any municipal bounds. The UT land area is slightly over one half the entire area of the State of Maine. Year-round residents in the UT number approximately 9,000, about 1.3% of the state's total population, with many more people residing only seasonally within the UT. Only four of Maine's sixteen counties (Androscoggin, Cumberland, Waldo and York) are entirely incorporated, although a few others are nearly so, and most of the unincorporated area is in the vast and sparsely populated Great North Woods of Maine.\n\nQuickFacts US Census Maine Portland:\n\nThroughout Maine, many municipalities, although each separate governmental entities, nevertheless form portions of a much larger population base. There are many such population clusters throughout Maine, but some examples from the municipalities appearing in the above listing are:\n\nThere are thirty institutions of higher learning in Maine. These institutions include the University of Maine, which is the oldest, largest and only research university in the state. UMaine was founded in 1865 and is the state's only land grant and sea grant college. The University of Maine is located in the town of Orono and is the flagship of Maine. There are also branch campuses in Augusta, Farmington, Fort Kent, Machias, and Presque Isle.\n\nBowdoin College is a liberal arts college founded in 1794 in Brunswick, making it the oldest institution of higher learning in the state. Colby College in Waterville was founded in 1813 making it the second oldest college in Maine. Bates College in Lewiston was founded in 1855 making it the third oldest institution in the state and the oldest coeducational college in New England. The three colleges collectively form the Colby-Bates-Bowdoin Consortium and are ranked among the best colleges in the United States; often placing in the top 10% of all liberal arts colleges.Maine's per-student public expenditure for elementary and secondary schools was 21st in the nation in 2012, at $12,344.\n\nThe collegiate system of Maine also includes numerous baccalaureate colleges such as: the Maine Maritime Academy (MMA), Unity College, and Thomas College. There is only one medical school in the state, (University of New England's College of Osteopathic Medicine) and only one law school (The University of Maine School of Law).\n\nPrivate schools in Maine are funded independently of the state and its furthered domains. Private schools are less common than public schools. A large number of private elementary schools with under 20 students exist, but most private high schools in Maine can be described as \"semi-private\".\n\n\n\nAdapted from the Maine facts site.\n\n\nA citizen of Maine is known as a \"Mainer\", though the term is often reserved for those whose roots in Maine go back at least three generations. The term \"Downeaster\" may be applied to residents of the northeast coast of the state. The term \"Mainiac\" is considered by some to be derogatory, but embraced with pride by others, and is used for a variety of organizations and for events such as the YMCA Mainiac Sprint Triathlon & Duathlon.\n\n\nState government\n\nU.S. government\n\nInformation\n"}
{"id": "19978", "url": "https://en.wikipedia.org/wiki?curid=19978", "title": "Montana", "text": "Montana\n\nMontana () is a state in the Northwestern United States. Montana has several nicknames, although none are official, including \"Big Sky Country\" and \"The Treasure State\", and slogans that include \"Land of the Shining Mountains\" and more recently \"The Last Best Place\".\n\nMontana is the 4th largest in area, the 8th least populous, and the 3rd least densely populated of the 50 U.S. states. The western half of Montana contains numerous mountain ranges. Smaller island ranges are found throughout the state. In total, 77 named ranges are part of the Rocky Mountains. The eastern half of Montana is characterized by western prairie terrain and badlands. Montana is bordered by Idaho to the west, Wyoming to the south, North Dakota and South Dakota to the east, and the Canadian provinces of British Columbia, Alberta, and Saskatchewan to the north.\n\nThe economy is primarily based on agriculture, including ranching and cereal grain farming. Other significant economic resources include oil, gas, coal, hard rock mining, and lumber. The health care, service, and government sectors also are significant to the state's economy.\n\nThe state's fastest-growing sector is tourism. Nearly 13 million tourists annually visit Glacier National Park, Yellowstone National Park, the Beartooth Highway, Flathead Lake, Big Sky Resort, and other attractions.\n\nThe name Montana comes from the Spanish word \"Montaña,\" which in turn comes from the Latin word \"Montanea\", meaning \"mountain\", or more broadly, \"mountainous country\". \"Montaña del Norte\" was the name given by early Spanish explorers to the entire mountainous region of the west. The name Montana was added to a bill by the United States House Committee on Territories, which was chaired at the time by Rep. James Ashley of Ohio, for the territory that would become Idaho Territory. The name was changed by Representatives Henry Wilson (Massachusetts) and Benjamin F. Harding (Oregon), who complained Montana had \"no meaning\". When Ashley presented a bill to establish a temporary government in 1864 for a new territory to be carved out of Idaho, he again chose Montana Territory. This time Rep. Samuel Cox, also of Ohio, objected to the name. Cox complained that the name was a misnomer given most of the territory was not mountainous and that a Native American name would be more appropriate than a Spanish one. Other names such as Shoshone were suggested, but it was decided that the Committee on Territories could name it whatever they wanted, so the original name of Montana was adopted.\n\nMontana is one of the nine Mountain States, located in the north of the region known as the Western United States. It borders North Dakota and South Dakota to the east. Wyoming is to the south, Idaho is to the west and southwest, and three Canadian provinces, British Columbia, Alberta, and Saskatchewan, are to the north.\n\nWith an area of , Montana is slightly larger than Japan. It is the fourth largest state in the United States after Alaska, Texas, and California; it is the largest landlocked U.S. state.\n\nThe state's topography is roughly defined by the Continental Divide, which splits much of the state into distinct eastern and western regions. Most of Montana's 100 or more named mountain ranges are in the state's western half, most of which is geologically and geographically part of the Northern Rocky Mountains. The Absaroka and Beartooth ranges in the state's south-central part are technically part of the Central Rocky Mountains. The Rocky Mountain Front is a significant feature in the state's north-central portion, and isolated island ranges that interrupt the prairie landscape common in the central and eastern parts of the state. About 60 percent of the state is prairie, part of the northern Great Plains.\n\nThe Bitterroot Mountains—one of the longest continuous ranges in the Rocky Mountain chain from Alaska to Mexico—along with smaller ranges, including the Coeur d'Alene Mountains and the Cabinet Mountains, divide the state from Idaho. The southern third of the Bitterroot range blends into the Continental Divide. Other major mountain ranges west of the Divide include the Cabinet Mountains, the Anaconda Range, the Missions, the Garnet Range, Sapphire Mountains, and Flint Creek Range.\nThe Divide's northern section, where the mountains rapidly give way to prairie, is part of the Rocky Mountain Front. The front is most pronounced in the Lewis Range, located primarily in Glacier National Park. Due to the configuration of mountain ranges in Glacier National Park, the Northern Divide (which begins in Alaska's Seward Peninsula) crosses this region and turns east in Montana at Triple Divide Peak. It causes the Waterton River, Belly, and Saint Mary rivers to flow north into Alberta, Canada. There they join the Saskatchewan River, which ultimately empties into Hudson Bay.\n\nEast of the divide, several roughly parallel ranges cover the state's southern part, including the Gravelly Range, the Madison Range, Gallatin Range, Absaroka Mountains and the Beartooth Mountains. The Beartooth Plateau is the largest continuous land mass over high in the continental United States. It contains the state's highest point, Granite Peak, high. North of these ranges are the Big Belt Mountains, Bridger Mountains, Tobacco Roots, and several island ranges, including the Crazy Mountains and Little Belt Mountains.\n\nBetween many mountain ranges are rich river valleys. The Big Hole Valley, Bitterroot Valley, Gallatin Valley, Flathead Valley, and Paradise Valley have extensive agricultural resources and multiple opportunities for tourism and recreation.\n\nEast and north of this transition zone are the expansive and sparsely populated Northern Plains, with tableland prairies, smaller island mountain ranges, and badlands. The isolated island ranges east of the Divide include the Bear Paw Mountains, Bull Mountains, Castle Mountains, Crazy Mountains, Highwood Mountains, Judith Mountains, Little Belt Mountains, Little Rocky Mountains, the Pryor Mountains, Snowy Mountains, Sweet Grass Hills, and—in the state's southeastern corner near Ekalaka—the Long Pines. Many of these isolated eastern ranges were created about 120 to 66 million years ago when magma welling up from the interior cracked and bowed the earth's surface here.\n\nThe area east of the divide in the state' north-central portion is known for the Missouri Breaks and other significant rock formations. Three buttes south of Great Falls are major landmarks: Cascade, Crown, Square, Shaw and Buttes. Known as laccoliths, they formed when igneous rock protruded through cracks in the sedimentary rock. The underlying surface consists of sandstone and shale. Surface soils in the area are highly diverse, and greatly affected by the local geology, whether glaciated plain, intermountain basin, mountain foothills, or tableland. Foothill regions are often covered in weathered stone or broken slate, or consist of uncovered bare rock (usually igneous, quartzite, sandstone, or shale). The soil of intermountain basins usually consists of clay, gravel, sand, silt, and volcanic ash, much of it laid down by lakes which covered the region during the Oligocene 33 to 23 million years ago. Tablelands are often topped with argillite gravel and weathered quartzite, occasionally underlain by shale. The glaciated plains are generally covered in clay, gravel, sand, and silt left by the proglacial Lake Great Falls or by moraines or gravel-covered former lake basins left by the Wisconsin glaciation 85,000 to 11,000 years ago. Farther east, areas such as Makoshika State Park near Glendive and Medicine Rocks State Park near Ekalaka contain some of the most scenic badlands regions in the state.\n\nThe Hell Creek Formation in Northeast Montana is a major source of dinosaur fossils. Paleontologist Jack Horner of the Museum of the Rockies in Bozeman brought this formation to the world's attention with several major finds.\n\nMontana has thousands of named rivers and creeks, of which are known for \"blue-ribbon\" trout fishing. Montana's water resources provide for recreation, hydropower, crop and forage irrigation, mining, and water for human consumption. Montana is one of few geographic areas in the world whose rivers form parts of three major watersheds (i.e. where two continental divides intersect). Its rivers feed the Pacific Ocean, the Gulf of Mexico, and Hudson Bay. The watersheds divide at Triple Divide Peak in Glacier National Park.\n\nWest of the divide, the Clark Fork of the Columbia (not to be confused with the Clarks Fork of the Yellowstone River) rises near Butte and flows northwest to Missoula, where it is joined by the Blackfoot River and Bitterroot River. Farther downstream, it is joined by the Flathead River before entering Idaho near Lake Pend Oreille. The Pend Oreille River forms the outflow of Lake Pend Oreille. The Pend Oreille River joined the Columbia River, which flows to the Pacific Ocean—making the long Clark Fork/Pend Oreille (considered a single river system) the longest river in the Rocky Mountains. The Clark Fork discharges the greatest volume of water of any river exiting the state. The Kootenai River in northwest Montana is another major tributary of the Columbia.\n\nEast of the divide the Missouri River, which is formed by the confluence of the Jefferson, Madison and Gallatin rivers near Three Forks, flows due north through the west-central part of the state to Great Falls. From this point, it then flows generally east through fairly flat agricultural land and the Missouri Breaks to Fort Peck reservoir. The stretch of river between Fort Benton and the Fred Robinson Bridge at the western boundary of Fort Peck Reservoir was designated a National Wild and Scenic River in 1976. The Missouri enters North Dakota near Fort Union, having drained more than half the land area of Montana (). Nearly one-third of the Missouri River in Montana lies behind 10 dams: Toston, Canyon Ferry, Hauser, Holter, Black Eagle, Rainbow, Cochrane, Ryan, Morony, and Fort Peck.\n\nThe Yellowstone River rises on the continental divide near Younts Peak in Wyoming's Teton Wilderness. It flows north through Yellowstone National Park, enters Montana near Gardiner, and passes through the Paradise Valley to Livingston. It then flows northeasterly across the state through Billings, Miles City, Glendive, and Sidney. The Yellowstone joins the Missouri in North Dakota just east of Fort Union. It is the longest undammed, free-flowing river in the contiguous United States, and drains about a quarter of Montana ().\n\nOther major Montana tributaries of the Missouri include the Smith, Milk, Marias, Judith, and Musselshell Rivers. Montana also claims the disputed title of possessing the world's shortest river, the Roe River, just outside Great Falls. Through the Missouri, these rivers ultimately join the Mississippi River and flow into the Gulf of Mexico.\n\nMajor tributaries of the Yellowstone include the Boulder, Stillwater, Clarks Fork, Bighorn, Tongue, and Powder Rivers.\n\nThe Northern Divide turns east in Montana at Triple Divide Peak, causing the Waterton River, Belly, and Saint Mary rivers to flow north into Alberta. There they join the Saskatchewan River, which ultimately empties into Hudson Bay.\n\nThere are some 3,000 named lakes and reservoirs in Montana, including Flathead Lake, the largest natural freshwater lake in the western United States. Other major lakes include Whitefish Lake in the Flathead Valley and Lake McDonald and St. Mary Lake in Glacier National Park. The largest reservoir in the state is Fort Peck Reservoir on the Missouri river, which is contained by the second largest earthen dam and largest hydraulically filled dam in the world. Other major reservoirs include Hungry Horse on the Flathead River; Lake Koocanusa on the Kootenai River; Lake Elwell on the Marias River; Clark Canyon on the Beaverhead River; Yellowtail on the Bighorn River, Canyon Ferry, Hauser, Holter, Rainbow; and Black Eagle on the Missouri River.\n\nVegetation of the state includes lodgepole pine, ponderosa pine; Douglas fir, larch, spruce; aspen, birch, red cedar, hemlock, ash, alder; rocky mountain maple and cottonwood trees. Forests cover approximately 25 percent of the state. Flowers native to Montana include asters, bitterroots, daisies, lupins, poppies, primroses, columbine, lilies, orchids, and dryads. Several species of sagebrush and cactus and many species of grasses are common. Many species of mushrooms and lichens are also found in the state.\n\nMontana is home to a diverse array of fauna that includes 14 amphibian, 90 fish, 117 mammal, 20 reptile and 427 bird species. Additionally, there are over 10,000 invertebrate species, including 180 mollusks and 30 crustaceans. Montana has the largest grizzly bear population in the lower 48 states. Montana hosts five federally endangered species–black-footed ferret, whooping crane, least tern, pallid sturgeon and white sturgeon and seven threatened species including the grizzly bear, Canadian lynx and bull trout. The Montana Department of Fish, Wildlife and Parks manages fishing and hunting seasons for at least 17 species of game fish including seven species of trout, walleye and smallmouth bass and at least 29 species of game birds and animals including ring-neck pheasant, grey partridge, elk, pronghorn antelope, mule deer, whitetail deer, gray wolf and bighorn sheep.\n\nMontana contains Glacier National Park, \"The Crown of the Continent\"; and portions of Yellowstone National Park, including three of the park's five entrances. Other federally recognized sites include the Little Bighorn National Monument, Bighorn Canyon National Recreation Area, Big Hole National Battlefield, and the National Bison Range. Approximately , or 35 percent of Montana's land is administered by federal or state agencies. The U.S. Department of Agriculture Forest Service administers of forest land in ten National Forests. There are approximately of wilderness in 12 separate wilderness areas that are part of the National Wilderness Preservation System established by the Wilderness Act of 1964. The U.S. Department of the Interior Bureau of Land Management controls of federal land. The U.S. Department of the Interior Fish and Wildlife Service administers of 1.1 million acres of National Wildlife Refuges and waterfowl production areas in Montana. The U.S. Department of the Interior Bureau of Reclamation administers approximately of land and water surface in the state. The Montana Department of Fish, Wildlife and Parks operates approximately of state parks and access points on the state's rivers and lakes. The Montana Department of Natural Resources and Conservation manages of School Trust Land ceded by the federal government under the Land Ordinance of 1785 to the state in 1889 when Montana was granted statehood. These lands are managed by the state for the benefit of public schools and institutions in the state.\n\nAreas managed by the National Park Service include:\n\nMontana is a large state with considerable variation in geography, topography and altitude, and the climate is, therefore, equally varied. The state spans from below the 45th parallel (the line equidistant between the equator and North Pole) to the 49th parallel, and elevations range from under to nearly above sea level. The western half is mountainous, interrupted by numerous large valleys. Eastern Montana comprises plains and badlands, broken by hills and isolated mountain ranges, and has a semi-arid, continental climate (Köppen climate classification \"BSk\"). The Continental Divide has a considerable effect on the climate, as it restricts the flow of warmer air from the Pacific from moving east, and drier continental air from moving west. The area west of the divide has a modified northern Pacific coast climate, with milder winters, cooler summers, less wind and a longer growing season. Low clouds and fog often form in the valleys west of the divide in winter, but this is rarely seen in the east.\n\nAverage daytime temperatures vary from in January to in July. The variation in geography leads to great variation in temperature. The highest observed summer temperature was at Glendive on July 20, 1893, and Medicine Lake on July 5, 1937. Throughout the state, summer nights are generally cool and pleasant. Extreme hot weather is less common above . Snowfall has been recorded in all months of the year in the more mountainous areas of central and western Montana, though it is rare in July and August.\n\nThe coldest temperature on record for Montana is also the coldest temperature for the entire contiguous U.S. On January 20, 1954, was recorded at a gold mining camp near Rogers Pass. Temperatures vary greatly on cold nights, and Helena, to the southeast had a low of only on the same date, and an all-time record low of . Winter cold spells are usually the result of cold continental air coming south from Canada. The front is often well defined, causing a large temperature drop in a 24-hour period. Conversely, air flow from the southwest results in \"chinooks.\" These steady (or more) winds can suddenly warm parts of Montana, especially areas just to the east of the mountains, where temperatures sometimes rise up to for periods of ten days or longer.\n\nLoma is the site of the most extreme recorded temperature change in a 24-hour period in the United States. On January 15, 1972, a chinook wind blew in and the temperature rose from .\n\nAverage annual precipitation is , but great variations are seen. The mountain ranges block the moist Pacific air, holding moisture in the western valleys, and creating rain shadows to the east. Heron, in the west, receives the most precipitation, . On the eastern (leeward) side of a mountain range, the valleys are much drier; Lonepine averages , and Deer Lodge of precipitation. The mountains can receive over , for example the Grinnell Glacier in Glacier National Park gets . An area southwest of Belfry averaged only over a sixteen-year period. Most of the larger cities get of snow each year. Mountain ranges can accumulate of snow during a winter. Heavy snowstorms may occur from September through May, though most snow falls from November to March.\n\nThe climate has become warmer in Montana and continues to do so. The glaciers in Glacier National Park have receded and are predicted to melt away completely in a few decades. Many Montana cities set heat records during July 2007, the hottest month ever recorded in Montana. Winters are warmer, too, and have fewer cold spells. Previously these cold spells had killed off bark beetles, but these are now attacking the forests of western Montana. The warmer winters in the region have allowed various species to expand their ranges and proliferate. The combination of warmer weather, attack by beetles, and mismanagement during past years has led to a substantial increase in the severity of forest fires in Montana. According to a study done for the U.S. Environmental Protection Agency by the Harvard School of Engineering and Applied Science, portions of Montana will experience a 200-percent increase in area burned by wildfires, and an 80-percent increase in related air pollution.\n\nThe table below lists average temperatures for the warmest and coldest month for Montana's seven largest cities. The coldest month varies between December and January depending on location, although figures are similar throughout.\n\nMontana is one of only two continental US states (along with Colorado) which is antipodal to land. The Kerguelen Islands are antipodal to the Montana–Saskatchewan–Alberta border. No towns are precisely antipodal to Kerguelen, though Chester and Rudyard are close.\n\nVarious indigenous peoples lived in the territory of the present-day state of Montana for thousands of years. Historic tribes encountered by Europeans and settlers from the United States included the Crow in the south-central area; the Cheyenne in the very southeast; the Blackfeet, Assiniboine and Gros Ventres in the central and north-central area; and the Kootenai and Salish in the west. The smaller Pend d'Oreille and Kalispel tribes lived near Flathead Lake and the western mountains, respectively. A part of southeastern Montana was used as a corridor between the Crows and the related Hidatsas in North Dakota.\n\nThe land in Montana east of the continental divide was part of the Louisiana Purchase in 1803. Subsequent to and particularly in the decades following the Lewis and Clark Expedition, American, British and French traders operated a fur trade, typically working with indigenous peoples, in both eastern and western portions of what would become Montana. These dealings were not always peaceful, and though the fur trade brought some material gain for indigenous tribal groups it also brought exposure to European diseases and altered their economic and cultural traditions. The trading post Fort Raymond (1807-1811) was constructed in Crow Indian country in 1807. Until the Oregon Treaty (1846), land west of the continental divide was disputed between the British and U.S. and was known as the Oregon Country. The first permanent settlement by Euro-Americans in what today is Montana was St. Mary's (1841) near present-day Stevensville. In 1847, Fort Benton was established as the uppermost fur-trading post on the Missouri River. In the 1850s, settlers began moving into the Beaverhead and Big Hole valleys from the Oregon Trail and into the Clark's Fork valley.\n\nThe first gold discovered in Montana was at Gold Creek near present-day Garrison in 1852. A series of major mining discoveries in the western third of the state starting in 1862 found gold, silver, copper, lead, coal (and later oil) that attracted tens of thousands of miners to the area. The richest of all gold placer diggings was discovered at Alder Gulch, where the town of Virginia City was established. Other rich placer deposits were found at Last Chance Gulch, where the city of Helena now stands, Confederate Gulch, Silver Bow, Emigrant Gulch, and Cooke City. Gold output from 1862 through 1876 reached $144 million; silver then became even more important. The largest mining operations were in the city of Butte, which had important silver deposits and gigantic copper deposits.\n\nBefore the creation of Montana Territory (1864–1889), various parts of what is now Montana were parts of Oregon Territory (1848–1859), Washington Territory (1853–1863), Idaho Territory (1863–1864), and Dakota Territory (1861–1864). Montana became a United States territory (Montana Territory) on May 26, 1864. The first territorial capital was at Bannack. The first territorial governor was Sidney Edgerton. The capital moved to Virginia City in 1865 and to Helena in 1875. In 1870, the non-Indian population of Montana Territory was 20,595. The Montana Historical Society, founded on February 2, 1865, in Virginia City is the oldest such institution west of the Mississippi (excluding Louisiana). In 1869 and 1870 respectively, the Cook–Folsom–Peterson and the Washburn–Langford–Doane Expeditions were launched from Helena into the Upper Yellowstone region and directly led to the creation of Yellowstone National Park in 1872.\n\nAs white settlers began populating Montana from the 1850s through the 1870s, disputes with Native Americans ensued, primarily over land ownership and control. In 1855, Washington Territorial Governor Isaac Stevens negotiated the Hellgate treaty between the United States Government and the Salish, Pend d'Oreille, and the Kootenai people of western Montana, which established boundaries for the tribal nations. The treaty was ratified in 1859. While the treaty established what later became the Flathead Indian Reservation, trouble with interpreters and confusion over the terms of the treaty led whites to believe that the Bitterroot Valley was opened to settlement, but the tribal nations disputed those provisions. The Salish remained in the Bitterroot Valley until 1891.\n\nThe first U.S. Army post established in Montana was Camp Cooke in 1866, on the Missouri River, to protect steamboat traffic going to Fort Benton, Montana. More than a dozen additional military outposts were established in the state. Pressure over land ownership and control increased due to discoveries of gold in various parts of Montana and surrounding states. Major battles occurred in Montana during Red Cloud's War, the Great Sioux War of 1876, the Nez Perce War and in conflicts with Piegan Blackfeet. The most notable of these were the Marias Massacre (1870), Battle of the Little Bighorn (1876), Battle of the Big Hole (1877) and Battle of Bear Paw (1877). The last recorded conflict in Montana between the U.S. Army and Native Americans occurred in 1887 during the Battle of Crow Agency in the Big Horn country. Indian survivors who had signed treaties were generally required to move onto reservations.\nSimultaneously with these conflicts, bison, a keystone species and the primary protein source that Native people had survived on for centuries were being destroyed. Some estimates say there were over 13 million bison in Montana in 1870. In 1875, General Philip Sheridan pleaded to a joint session of Congress to authorize the slaughtering of herds in order to deprive the Indians of their source of food. By 1884, commercial hunting had brought bison to the verge of extinction; only about 325 bison remained in the entire United States.\n\nCattle ranching has been central to Montana's history and economy since Johnny Grant began wintering cattle in the Deer Lodge Valley in the 1850s and traded cattle fattened in fertile Montana valleys with emigrants on the Oregon Trail. Nelson Story brought the first Texas Longhorn cattle into the territory in 1866. Granville Stuart, Samuel Hauser and Andrew J. Davis started a major open range cattle operation in Fergus County in 1879. The Grant-Kohrs Ranch National Historic Site in Deer Lodge is maintained today as a link to the ranching style of the late 19th century. Operated by the National Park Service, it is a working ranch.\n\nTracks of the Northern Pacific Railroad (NPR) reached Montana from the west in 1881 and from the east in 1882. However, the railroad played a major role in sparking tensions with Native American tribes in the 1870s. Jay Cooke, the NPR president launched major surveys into the Yellowstone valley in 1871, 1872 and 1873 which were challenged forcefully by the Sioux under chief Sitting Bull. These clashes, in part, contributed to the Panic of 1873, a financial crisis that delayed construction of the railroad into Montana. Surveys in 1874, 1875 and 1876 helped spark the Great Sioux War of 1876. The transcontinental NPR was completed on September 8, 1883, at Gold Creek.\n\nTracks of the Great Northern Railroad (GNR) reached eastern Montana in 1887 and when they reached the northern Rocky Mountains in 1890, the GNR became a significant promoter of tourism to Glacier National Park region. The transcontinental GNR was completed on January 6, 1893, at Scenic, Washington.\n\nIn 1881, the Utah and Northern Railway a branch line of the Union Pacific completed a narrow gauge line from northern Utah to Butte. A number of smaller spur lines operated in Montana from 1881 into the 20th century including the Oregon Short Line, Montana Railroad and Milwaukee Road.\n\nUnder Territorial Governor Thomas Meagher, Montanans held a constitutional convention in 1866 in a failed bid for statehood. A second constitutional convention was held in Helena in 1884 that produced a constitution ratified 3:1 by Montana citizens in November 1884. For political reasons, Congress did not approve Montana statehood until 1889. Congress approved Montana statehood in February 1889 and President Grover Cleveland signed an omnibus bill granting statehood to Montana, North Dakota, South Dakota and Washington once the appropriate state constitutions were crafted. In July 1889, Montanans convened their third constitutional convention and produced a constitution accepted by the people and the federal government. On November 8, 1889 President Benjamin Harrison proclaimed Montana the forty-first state in the union. The first state governor was Joseph K. Toole. In the 1880s, Helena (the current state capital) had more millionaires per capita than any other United States city.\n\nThe Homestead Act of 1862 provided free land to settlers who could claim and \"prove-up\" of federal land in the Midwest and western United States. Montana did not see a large influx of immigrants from this act because 160 acres was usually insufficient to support a family in the arid territory. The first homestead claim under the act in Montana was made by David Carpenter near Helena in 1868. The first claim by a woman was made near Warm Springs Creek by Gwenllian Evans, the daughter of Deer Lodge Montana pioneer, Morgan Evans. By 1880, there were farms in the more verdant valleys of central and western Montana, but few on the eastern plains.\n\nThe Desert Land Act of 1877 was passed to allow settlement of arid lands in the west and allotted to settlers for a fee of $.25 per acre and a promise to irrigate the land. After three years, a fee of one dollar per acre would be paid and the land would be owned by the settler. This act brought mostly cattle and sheep ranchers into Montana, many of whom grazed their herds on the Montana prairie for three years, did little to irrigate the land and then abandoned it without paying the final fees. Some farmers came with the arrival of the Great Northern and Northern Pacific Railroads throughout the 1880s and 1890s, though in relatively small numbers.\n\nIn the early 1900s, James J. Hill of the Great Northern began promoting settlement in the Montana prairie to fill his trains with settlers and goods. Other railroads followed suit. In 1902, the Reclamation Act was passed, allowing irrigation projects to be built in Montana's eastern river valleys. In 1909, Congress passed the Enlarged Homestead Act that expanded the amount of free land from per family and in 1912 reduced the time to \"prove up\" on a claim to three years. In 1916, the Stock-Raising Homestead Act allowed homesteads of 640 acres in areas unsuitable for irrigation. This combination of advertising and changes in the Homestead Act drew tens of thousands of homesteaders, lured by free land, with World War I bringing particularly high wheat prices. In addition, Montana was going through a temporary period of higher-than-average precipitation. Homesteaders arriving in this period were known as \"Honyockers\", or \"scissorbills.\" Though the word \"honyocker\", possibly derived from the ethnic slur \"hunyak,\" was applied in a derisive manner at homesteaders as being \"greenhorns\", \"new at his business\" or \"unprepared\", the reality was that a majority of these new settlers had previous farming experience, though there were also many who did not.\n\nHowever, farmers faced a number of problems. Massive debt was one. Also, most settlers were from wetter regions, unprepared for the dry climate, lack of trees, and scarce water resources. In addition, small homesteads of fewer than were unsuited to the environment. Weather and agricultural conditions are much harsher and drier west of the 100th meridian. Then, the droughts of 1917–1921 proved devastating. Many people left, and half the banks in the state went bankrupt as a result of providing mortgages that could not be repaid. As a result, farm sizes increased while the number of farms decreased\n\nBy 1910, homesteaders filed claims on over five million acres, and by 1923, over 93 million acres were farmed. In 1910, the Great Falls land office alone saw over 1,000 homestead filings per month, and the peak of 1917– 1918 saw 14,000 new homesteads each year. But significant drop occurred following drought in 1919.\n\nAs World War I broke out, Jeannette Rankin, the first woman in the United States to be a member of Congress, was a pacifist and voted against the United States' declaration of war. Her actions were widely criticized in Montana, where public support for the war was strong, and wartime sentiment reached high levels of patriotism among many Montanans. In 1917–18, due to a miscalculation of Montana's population, approximately 40,000 Montanans, ten percent of the state's population, either volunteered or were drafted into the armed forces. This represented a manpower contribution to the war that was 25 percent higher than any other state on a per capita basis. Approximately 1500 Montanans died as a result of the war and 2437 were wounded, also higher than any other state on a per capita basis. Montana's Remount station in Miles City provided 10,000 cavalry horses for the war, more than any other Army post in the US. The war created a boom for Montana mining, lumber and farming interests as demand for war materials and food increased.\n\nIn June 1917, the U.S. Congress passed the Espionage Act of 1917 which was later extended by the Sedition Act of 1918, enacted in May 1918. In February 1918, the Montana legislature had passed the Montana Sedition Act, which was a model for the federal version. In combination, these laws criminalized criticism of the U.S. government, military, or symbols through speech or other means. The Montana Act led to the arrest of over 200 individuals and the conviction of 78, mostly of German or Austrian descent. Over 40 spent time in prison. In May 2006, then-Governor Brian Schweitzer posthumously issued full pardons for all those convicted of violating the Montana Sedition Act.\n\nThe Montanans who opposed U.S. entry into the war included certain immigrant groups of German and Irish heritage as well as pacifist Anabaptist people such as the Hutterites and Mennonites, many of whom were also of Germanic heritage. In turn, pro-War groups formed, such as the Montana Council of Defense, created by Governor Samuel V. Stewart as well as local \"loyalty committees.\"\n\nWar sentiment was complicated by labor issues. The Anaconda Copper Company, which was at its historic peak of copper production, was an extremely powerful force in Montana, but also faced criticism and opposition from socialist newspapers and unions struggling to make gains for their members. In Butte, a multi-ethnic community with significant European immigrant population, labor unions, particularly the newly formed Metal Mine Workers' Union, opposed the war on grounds that it mostly profited large lumber and mining interests. In the wake of ramped-up mine production and the Speculator Mine disaster in June 1917, Industrial Workers of the World organizer Frank Little arrived in Butte to organize miners. He gave some speeches with inflammatory anti-war rhetoric. On August 1, 1917, he was dragged from his boarding house by masked vigilantes, and hanged from a railroad trestle, considered a lynching. Little's murder and the strikes that followed resulted in the National Guard being sent to Butte to restore order. Overall, anti-German and anti-labor sentiment increased and created a movement that led to the passage of the Montana Sedition Act the following February. In addition, the Council of Defense was made a state agency with the power to prosecute and punish individuals deemed in violation of the Act. The Council also passed rules limiting public gatherings and prohibiting the speaking of German in public.\n\nIn the wake of the legislative action in 1918, emotions rose. U.S. Attorney Burton K. Wheeler and several District Court Judges who hesitated to prosecute or convict people brought up on charges were strongly criticized. Wheeler was brought before the Council of Defense, though he avoided formal proceedings, and a District Court judge from Forsyth was impeached. There were burnings of German-language books and several near-hangings. The prohibition on speaking German remained in effect into the early 1920s. Complicating the wartime struggles, the 1918 Influenza epidemic claimed the lives of over 5,000 Montanans. The period has been dubbed \"Montana's Agony\" by some historians due to the suppression of civil liberties that occurred.\n\nAn economic depression began in Montana after World War I and lasted through the Great Depression until the beginning of World War II. This caused great hardship for farmers, ranchers, and miners. The wheat farms in eastern Montana make the state a major producer; the wheat has a relatively high protein content and thus commands premium prices.\n\nWhen the U.S. entered World War II on December 7, 1941, many Montanans already had enlisted in the military to escape the poor national economy of the previous decade. Another 40,000-plus Montanans entered the armed forces in the first year following the declaration of war, and over 57,000 joined up before the war ended. These numbers constituted about 10 percent of the state's total population, and Montana again contributed one of the highest numbers of soldiers per capita of any state. Many Native Americans were among those who served, including soldiers from the Crow Nation who became Code Talkers. At least 1500 Montanans died in the war. Montana also was the training ground for the First Special Service Force or \"Devil's Brigade,\" a joint U.S-Canadian commando-style force that trained at Fort William Henry Harrison for experience in mountainous and winter conditions before deployment. Air bases were built in Great Falls, Lewistown, Cut Bank and Glasgow, some of which were used as staging areas to prepare planes to be sent to allied forces in the Soviet Union. During the war, about 30 Japanese Fu-Go balloon bombs were documented to have landed in Montana, though no casualties nor major forest fires were attributed to them.\n\nIn 1940, Jeannette Rankin was again elected to Congress. In 1941, as she had in 1917, she voted against the United States' declaration of war after the Japanese attack on Pearl Harbor. Hers was the only vote against the war, and in the wake of public outcry over her vote, Rankin required police protection for a time. Other pacifists tended to be those from \"peace churches\" who generally opposed war. Many individuals claiming conscientious objector status from throughout the U.S. were sent to Montana during the war as smokejumpers and for other forest fire-fighting duties.\n\nDuring World War II, the planned battleship USS \"Montana\" was named in honor of the state. However, the battleship was never completed. Montana is the only one of the first 48 states lacking a completed battleship being named for it. Alaska and Hawaii have both had nuclear submarines named after them. Montana is the only state in the union without a modern naval ship named in its honor. However, in August 2007 Senator Jon Tester made a request to the Navy that a submarine be christened USS \"Montana\". Secretary of the Navy Ray Mabus announced on September 3, 2015 that Virginia Class attack Submarine SSN-794 will bear the state's namesake. This will be the second commissioned warship to bear the name \"Montana.\"\n\nIn the post-World War II Cold War era, Montana became host to U.S. Air Force Military Air Transport Service (1947) for airlift training in C-54 Skymasters and eventually, in 1953 Strategic Air Command air and missile forces were based at Malmstrom Air Force Base in Great Falls. The base also hosted the 29th Fighter Interceptor Squadron, Air Defense Command from 1953 to 1968. In December 1959, Malmstrom AFB was selected as the home of the new Minuteman I ballistic missile. The first operational missiles were in-place and ready in early 1962. In late 1962 missiles assigned to the 341st Strategic Missile Wing would play a major role in the Cuban Missile Crisis. When the Soviets removed their missiles from Cuba, President John F. Kennedy said the Soviets backed down because they knew he had an \"ace in the hole,\" referring directly to the Minuteman missiles in Montana. Montana eventually became home to the largest ICBM field in the U.S. covering .\n\nThe United States Census Bureau estimates that the population of Montana was 1,032,949 on July 1, 2015, a 4.40% increase since the 2010 United States Census. The 2010 Census put Montana's population at 989,415 which is an increase of 43,534 people, or 4.40 percent, since 2010. During the first decade of the new century, growth was mainly concentrated in Montana's seven largest counties, with the highest percentage growth in Gallatin County, which saw a 32 percent increase in its population from 2000–2010. The city seeing the largest percentage growth was Kalispell with 40.1 percent, and the city with the largest increase in actual residents was Billings with an increase in population of 14,323 from 2000–2010.\n\nOn January 3, 2012, the Census and Economic Information Center (CEIC) at the Montana Department of Commerce estimated Montana had hit the one million population mark sometime between November and December 2011. The United States Census Bureau estimates that the population of Montana was 1,005,141 on July 1, 2012, a 1.6 percent increase since the 2010 United States Census.\n\nAccording to the 2010 Census, 89.4 percent of the population was White (87.8 percent Non-Hispanic White), 6.3 percent American Indian and Alaska Native, 2.9 percent Hispanics and Latinos of any race, 0.6 percent Asian, 0.4 percent Black or African American, 0.1 percent Native Hawaiian and Other Pacific Islander, 0.6 percent from Some Other Race, and 2.5 percent from two or more races. The largest European ancestry groups in Montana as of 2010 are: German (27.0 percent), Irish (14.8 percent), English (12.6 percent), Norwegian (10.9 percent), French (4.7 percent) and Italian (3.4 percent).\n\nMontana has a larger Native American population numerically and percentage-wise than most U.S. states. Although the state ranked 45th in population (according to the 2010 U.S. Census), it ranked 19th in total native people population. Native people constituted 6.5 percent of the state's total population, the sixth highest percentage of all 50 states. Montana has three counties in which Native Americans are a majority: Big Horn, Glacier, and Roosevelt. Other counties with large Native American populations include Blaine, Cascade, Hill, Missoula, and Yellowstone counties. The state's Native American population grew by 27.9 percent between 1980 and 1990 (at a time when Montana's entire population rose just 1.6 percent), and by 18.5 percent between 2000 and 2010. As of 2009, almost two-thirds of Native Americans in the state live in urban areas. Of Montana's 20 largest cities, Polson (15.7 percent), Havre (13.0 percent), Great Falls (5.0 percent), Billings (4.4 percent), and Anaconda (3.1 percent) had the greatest percentage of Native American residents in 2010. Billings (4,619), Great Falls (2,942), Missoula (1,838), Havre (1,210), and Polson (706) have the most Native Americans living there. The state's seven reservations include more than twelve distinct Native American ethnolinguistic groups.\n\nWhile the largest European-American population in Montana overall is German, pockets of significant Scandinavian ancestry are prevalent in some of the farming-dominated northern and eastern prairie regions, parallel to nearby regions of North Dakota and Minnesota. Farmers of Irish, Scots, and English roots also settled in Montana. The historically mining-oriented communities of western Montana such as Butte have a wider range of European-American ethnicity; Finns, Eastern Europeans and especially Irish settlers left an indelible mark on the area, as well as people originally from British mining regions such as Cornwall, Devon and Wales. The nearby city of Helena, also founded as a mining camp, had a similar mix in addition to a small Chinatown. Many of Montana's historic logging communities originally attracted people of Scottish, Scandinavian, Slavic, English and Scots-Irish descent.\n\nThe Hutterites, an Anabaptist sect originally from Switzerland, settled here, and today Montana is second only to South Dakota in U.S. Hutterite population with several colonies spread across the state. Beginning in the mid-1990s, the state also saw an influx of Amish, who relocated to Montana from the increasingly urbanized areas of Ohio and Pennsylvania.\n\nMontana's Hispanic population is concentrated around the Billings area in south-central Montana, where many of Montana's Mexican-Americans have been in the state for generations. Great Falls has the highest percentage of African-Americans in its population, although Billings has more African American residents than Great Falls.\n\nThe Chinese in Montana, while a low percentage today, have historically been an important presence. About 2000–3000 Chinese miners were in the mining areas of Montana by 1870, and 2500 in 1890. However, public opinion grew increasingly negative toward them in the 1890s and nearly half of the state's Asian population left the state by 1900. Today, there is a significant Hmong population centered in the vicinity of Missoula. Montanans who claim Filipino ancestry amount to almost 3,000, making them currently the largest Asian American group in the state.\n\nEnglish is the official language in the state of Montana, as it is in many U.S. states. According to the 2000 U.S. Census, 94.8 percent of the population aged 5 and older speak English at home. Spanish is the language most commonly spoken at home other than English. There were about 13,040 Spanish-language speakers in the state (1.4 percent of the population) in 2011. There were also 15,438 (1.7 percent of the state population) speakers of Indo-European languages other than English or Spanish, 10,154 (1.1 percent) speakers of a Native American language, and 4,052 (0.4 percent) speakers of an Asian or Pacific Islander language. Other languages spoken in Montana (as of 2013) include Assiniboine (about 150 speakers in the Montana and Canada), Blackfoot (about 100 speakers), Cheyenne (about 1,700 speakers), Plains Cree (about 100 speakers), Crow (about 3,000 speakers), Dakota (about 18,800 speakers in Minnesota, Montana, Nebraska, North Dakota, and South Dakota), German Hutterite (about 5,600 speakers), Gros Ventre (about 10 speakers), Kalispel-Pend d'Oreille (about 64 speakers), Kutenai (about 6 speakers), and Lakota (about 6,000 speakers in Minnesota, Montana, Nebraska, North Dakota, South Dakota). The United States Department of Education estimated in 2009 that 5,274 students in Montana spoke a language at home other than English. These included a Native American language (64 percent), German (4 percent), Spanish (3 percent), Russian (1 percent), and Chinese (less than 0.5 percent).\n\nAccording to the Pew Forum, the religious affiliations of the people of Montana are as follows: Protestant 47%, Catholic 23%, LDS (Mormon) 5%, Jehovah's Witness 2%, Buddhist 1%, Jewish 0.5%, Muslim 0.5%, Hindu 0.5% and Non-Religious at 20%.\n\nThe largest denominations in Montana as of 2010 were the Catholic Church with 127,612 adherents, The Church of Jesus Christ of Latter-day Saints with 46,484 adherents, Evangelical Lutheran Church in America with 38,665 adherents, and non-denominational Evangelical Protestant with 27,370 adherents.\n\nApproximately 66,000 people of Native American heritage live in Montana. Stemming from multiple treaties and federal legislation, including the Indian Appropriations Act (1851), the Dawes Act (1887), and the Indian Reorganization Act (1934), seven Indian reservations, encompassing eleven federally recognized tribal nations, were created in Montana. A twelfth nation, the Little Shell Chippewa is a \"landless\" people headquartered in Great Falls; it is recognized by the state of Montana but not by the U.S. government. The Blackfeet nation is headquartered on the Blackfeet Indian Reservation (1851) in Browning, Crow on the Crow Indian Reservation (1868) in Crow Agency, Confederated Salish and Kootenai and Pend d'Oreille on the Flathead Indian Reservation (1855) in Pablo, Northern Cheyenne on the Northern Cheyenne Indian Reservation (1884) at Lame Deer, Assiniboine and Gros Ventre on the Fort Belknap Indian Reservation (1888) in Fort Belknap Agency, Assiniboine and Sioux on the Fort Peck Indian Reservation (1888) at Poplar, and Chippewa-Cree on the Rocky Boy's Indian Reservation (1916) near Box Elder. Approximately 63% of all Native people live off the reservations, concentrated in the larger Montana cities, with the largest concentration of urban Indians in Great Falls. The state also has a small Métis population, and 1990 census data indicated that people from as many as 275 different tribes lived in Montana.\n\nMontana's Constitution specifically reads that \"the state recognizes the distinct and unique cultural heritage of the American Indians and is committed in its educational goals to the preservation of their cultural integrity.\" It is the only state in the U.S. with such a constitutional mandate. The Indian Education for All Act (IEFA) was passed in 1999 to provide funding for this mandate and ensure implementation. It mandates that all schools teach American Indian history, culture, and heritage from preschool through college. For kindergarten through 12th-grade students, an \"Indian Education for All\" curriculum from the Montana Office of Public Instruction is available free to all schools. The state was sued in 2004 because of lack of funding, and the state has increased its support of the program. South Dakota passed similar legislation in 2007, and Wisconsin was working to strengthen its own program based on this model – and the current practices of Montana's schools. Each Indian reservation in the state has a fully accredited tribal college. The University of Montana \"was the first to establish dual admission agreements with all of the tribal colleges and as such it was the first institution in the nation to actively facilitate student transfer from the tribal colleges\"\n\n\"Note: Births in table don't add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number.\"\n\n\n The Bureau of Economic Analysis estimates that Montana's total state product in 2014 was $44.3 billion. per capita personal income in 2014 was $40,601, 35th in the nation.\nMontana is a relative hub of beer microbrewing, ranking third in the nation in number of craft breweries per capita in 2011. There are significant industries for lumber and mineral extraction; the state's resources include gold, coal, silver, talc, and vermiculite. Ecotaxes on resource extraction are numerous. A 1974 state severance tax on coal (which varied from 20 to 30 percent) was upheld by the Supreme Court of the United States in \"Commonwealth Edison Co. v. Montana\", 453 U.S. 609 (1981).\n\nTourism is also important to the economy with over ten million visitors a year to Glacier National Park, Flathead Lake, the Missouri River headwaters, the site of the Battle of Little Bighorn and three of the five entrances to Yellowstone National Park.\n\nMontana's personal income tax contains 7 brackets, with rates ranging from 1 percent to 6.9 percent. Montana has no sales tax. In Montana, household goods are exempt from property taxes. However, property taxes are assessed on livestock, farm machinery, heavy equipment, automobiles, trucks, and business equipment. The amount of property tax owed is not determined solely by the property's value. The property's value is multiplied by a tax rate, set by the Montana Legislature, to determine its taxable value. The taxable value is then multiplied by the mill levy established by various taxing jurisdictions—city and county government, school districts and others.\n\nAs of June 2015, the state's unemployment rate is 3.9 percent.\n\nThe Montana Territory was formed on April 26, 1864, when the U.S. passed the Organic Act. Schools started forming in the area before it was officially a territory as families started settling into the area. The first schools were subscription schools that typically held in the teacher's home. The first formal school on record was at Fort Owen in Bitterroot valley in 1862. The students were Indian children and the children of Fort Owen employees. The first school term started in early winter and only lasted until February 28. Classes were taught by Mr. Robinson. Another early subscription school was started by Thomas Dimsdale in Virginia City in 1863. In this school students were charged $1.75 per week. The Montana Territorial Legislative Assembly had its inaugural meeting in 1864. The first legislature authorized counties to levy taxes for schools, which set the foundations for public schooling. Madison County was the first to take advantage of the newly authorized taxes and it formed fhe first public school in Virginia City in 1886. The first school year was scheduled to begin in January 1866, but severe weather postponed its opening until March. The first school year ran through the summer and didn't end until August 17. One of the first teachers at the school was Sarah Raymond. She was a 25-year-old woman who had traveled to Virginia City via wagon train in 1865. To become a certified teacher, Raymond took a test in her home and paid a $6 fee in gold dust to obtain a teaching certificate. With the help of an assistant teacher, Mrs. Farley, Raymond was responsible for teaching 50 to 60 students each day out of the 81 students enrolled at the school. Sarah Raymond was paid at a rate of $125 per month, and Mrs. Farley was paid $75 per month. There were no textbooks used in the school. In their place was an assortment of books brought in by various emigrants. Sarah quit teaching the following year, but would later become the Madison County superintendent of schools.\n\nMany well-known artists, photographers and authors have documented the land, culture and people of Montana in the last 130 years. Painter and sculptor Charles Marion Russell, known as \"the cowboy artist\" created more than 2,000 paintings of cowboys, Native Americans, and landscapes set in the Western United States and in Alberta, Canada. The C. M. Russell Museum Complex located in Great Falls, Montana, houses more than 2,000 Russell artworks, personal objects, and artifacts.\n\nPioneering feminist author, film-maker, and media personality Mary MacLane attained international fame in 1902 with her memoir of three months in her life in Butte, \"The Story of Mary MacLane\". She referred to Butte throughout the rest of her career and remains a controversial figure there for her mixture of criticism and love for Butte and its people.\n\nEvelyn Cameron, a naturalist and photographer from Terry documented early 20th century life on the Montana prairie, taking startlingly clear pictures of everything around her: cowboys, sheepherders, weddings, river crossings, freight wagons, people working, badlands, eagles, coyotes and wolves.\n\nMany notable Montana authors have documented or been inspired by life in Montana in both fiction and non-fiction works. Pulitzer Prize winner Wallace Earle Stegner from Great Falls was often called \"The Dean of Western Writers\". James Willard Schultz (\"Apikuni\") from Browning is most noted for his prolific stories about Blackfeet life and his contributions to the naming of prominent features in Glacier National Park.\n\nMontana hosts numerous arts and cultural festivals and events every year. Major events include:\n\nThere are no major league sports franchises in Montana due to the state's relatively small and dispersed population, but a number of minor league teams play in the state. Baseball is the minor-league sport with the longest heritage in the state, and Montana is currently home to three Minor League Baseball teams, all members of the Pioneer League: the Billings Mustangs, Great Falls Voyagers, and Missoula Osprey.\n\nAll of Montana's four-year colleges and universities field intercollegiate sports teams. The two largest schools, the University of Montana and Montana State University, are members of the Big Sky Conference and have enjoyed a strong athletic rivalry since the early twentieth century. Six of Montana's smaller four-year schools are members of the Frontier Conference. One is a member of the Great Northwest Athletic Conference.\n\nA variety of sports are offered at Montana high schools. Montana allows the smallest—\"Class C\"—high schools to utilize six-man football teams, dramatized in the independent 2002 film, \"The Slaughter Rule\".\n\nThere are junior ice hockey teams in Montana, four of which are affiliated with the North American 3 Hockey League: the Bozeman Icedogs, Great Falls Americans, Helena Bighorns, and Missoula Jr. Bruins.\n\n\nMontanans have been a part of several major sporting achievements:\n\nMontana provides year-round outdoor recreation opportunities for residents and visitors. Hiking, fishing, hunting, watercraft recreation, camping, golf, cycling, horseback riding, and skiing are popular activities.\n\nMontana has been a destination for its world-class trout fisheries since the 1930s. Fly fishing for several species of native and introduced trout in rivers and lakes is popular for both residents and tourists throughout the state. Montana is the home of the Federation of Fly Fishers and hosts many of the organizations annual conclaves. The state has robust recreational lake trout and kokanee salmon fisheries in the west, walleye can be found in many parts of the state, while northern pike, smallmouth and largemouth bass fisheries as well as catfish and paddlefish can be found in the waters of eastern Montana. Robert Redford's 1992 film of Norman Mclean's novel, \"A River Runs Through It\", was filmed in Montana and brought national attention to fly fishing and the state.\n\nMontana is home to the Rocky Mountain Elk Foundation and has a historic big game hunting tradition. There are fall bow and general hunting seasons for elk, pronghorn antelope, whitetail deer and mule deer. A random draw grants a limited number of permits for moose, mountain goats and bighorn sheep. There is a spring hunting season for black bear and in most years, limited hunting of bison that leave Yellowstone National Park is allowed. Current law allows both hunting and trapping of a specific number of wolves and mountain lions. Trapping of assorted fur bearing animals is allowed in certain seasons and many opportunities exist for migratory waterfowl and upland bird hunting.\n\nBoth downhill skiing and cross-country skiing are popular in Montana, which has 15 developed downhill ski areas open to the public, including:\n\nBig Sky Resort and Whitefish Mountain Resort are destination resorts, while the remaining areas do not have overnight lodging at the ski area, though several host restaurants and other amenities. \n\nMontana also has millions of acres open to cross-country skiing on nine of its national forests plus in Glacier National Park. In addition to cross-country trails at most of the downhill ski areas, there are also 13 private cross-country skiing resorts. Yellowstone National Park also allows cross-country skiing.\n\nSnowmobiling is popular in Montana which boasts over 4000 miles of trails and frozen lakes available in winter. There are 24 areas where snowmobile trails are maintained, most also offering ungroomed trails. West Yellowstone offers a large selection of trails and is the primary starting point for snowmobile trips into Yellowstone National Park, where \"oversnow\" vehicle use is strictly limited, usually to guided tours, and regulations are in considerable flux.\n\nSnow coach tours are offered at Big Sky, Whitefish, West Yellowstone and into Yellowstone National Park. Equestrian skijoring has a niche in Montana, which hosts the World Skijoring Championships in Whitefish as part of the annual Whitefish Winter Carnival.\n\nMontana does not have a Trauma I hospital, but does have Trauma II hospitals in Missoula, Billings, and Great Falls. In 2013 \"AARP The Magazine\" named the Billings Clinic one of the safest hospitals in the United States.\n\nMontana is ranked as the least obese state in the U.S., at 19.6%, according to the 2014 Gallup Poll.\nAs of 2010, Missoula is the 166th largest media market in the United States as ranked by Nielsen Media Research, while Billings is 170th, Great Falls is 190th, the Butte-Bozeman area 191st, and Helena is 206th. There are 25 television stations in Montana, representing each major U.S. network. As of August 2013, there are 527 FCC-licensed FM radio stations broadcast in Montana, with 114 such AM stations.\n\nDuring the age of the Copper Kings, each Montana copper company had its own newspaper. This changed in 1959 when Lee Enterprises bought several Montana newspapers. Montana's largest circulating daily city newspapers are the \"Billings Gazette\" (circulation 39,405), \"Great Falls Tribune\" (26,733), and \"Missoulian\" (25,439).\n\nRailroads have been an important method of transportation in Montana since the 1880s. Historically, the state was traversed by the main lines of three east-west transcontinental routes: the Milwaukee Road, the Great Northern, and the Northern Pacific. Today, the BNSF Railway is the state's largest railroad, its main transcontinental route incorporating the former Great Northern main line across the state. Montana RailLink, a privately held Class II railroad, operates former Northern Pacific trackage in western Montana.\n\nIn addition, Amtrak's \"Empire Builder\" train runs through the north of the state, stopping in Libby, Whitefish, West Glacier, Essex, East Glacier Park, Browning, Cut Bank, Shelby, Havre, Malta, Glasgow, and Wolf Point.\n\nBozeman Yellowstone International Airport is the busiest airport in the state of Montana, surpassing Billings Logan International Airport in the spring of 2013. Montana's other major Airports include Billings Logan International Airport, Missoula International Airport, Great Falls International Airport, Glacier Park International Airport, Helena Regional Airport, Bert Mooney Airport and Yellowstone Airport. Eight smaller communities have airports designated for commercial service under the Essential Air Service program.\n\nHistorically, U.S. Route 10 was the primary east-west highway route across Montana, connecting the major cities in the southern half of the state. Still the state's most important east-west travel corridor, the route is today served by Interstate 90 and Interstate 94 which roughly follow the same route as the Northern Pacific. U.S. Routes 2 and 12 and Montana Highway 200 also traverse the entire state from east to west.\n\nMontana's only north-south Interstate Highway is Interstate 15. Other major north-south highways include U.S. Routes 87, 89, 93 and 191.\n\nMontana and South Dakota are the only states to share a land border which is not traversed by a paved road. Highway 212, the primary paved route between the two, passes through the northeast corner of Wyoming between Montana and South Dakota.\n\nMontana is governed by a constitution. The first constitution was drafted by a constitutional convention in 1889, in preparation for statehood. Ninety percent of its language came from an 1884 constitution which was never acted upon by Congress for national political reasons. The 1889 constitution mimicked the structure of the United States Constitution, as well as outlining almost the same civil and political rights for citizens. However, the 1889 Montana constitution significantly restricted the power of state government, the legislature was much more powerful than the executive branch, and the jurisdiction of the District Courts very specifically described. Montana voters amended the 1889 constitution 37 times between 1889 and 1972. In 1914, Montana granted women the vote. In 1916, Montana became the first state to elect a woman, Progressive Republican Jeannette Rankin, to Congress.\n\nIn 1971, Montana voters approved the call for a state constitutional convention. A new constitution was drafted, which made the legislative and executive branches much more equal in power and which was much less prescriptive in outlining powers, duties, and jurisdictions. The draft included an expanded, more progressive list of civil and political rights, extended these rights to children for the first time, transferred administration of property taxes to the counties from the state, implemented new water rights, eliminated sovereign immunity, and gave the legislature greater power to spend tax revenues. The constitution was narrowly approved, 116,415 to 113,883, and declared ratified on June 20, 1972. Three issues which the constitutional convention were unable to resolve were submitted to voters simultaneously with the proposed constitution. Voters approved the legalization of gambling, a bicameral legislature, and retention of the death penalty.\n\nThe 1972 constitution has been amended 31 times as of 2015. Major amendments include establishment of a reclamation trust (funded by taxes on natural resource extraction) to restore mined land (1974); restoration of sovereign immunity, when such immunity has been approved by a two-thirds vote in each house (1974); establishment of a 90-day biennial (rather than annual) legislative session (1974); establishment of a coal tax trust fund, funded by a tax on coal extraction (1976); conversion of the mandatory decennial review of county government into a voluntary one, to be approaved or disallowed by residents in each county (1978); conversion of the provision of public assistance from a mandatory civil right to a non-fundamental legislative prerogative (1988); a new constitutional right to hunt and fish (2004); a prohibition on gay marriage (2004); and a prohibition on new taxes on the sale or transfer of real property (2010). In 1992, voters approved a constitutional amendment implementing term limits for certain statewide elected executive branch offices (governor, lieutenant governor, secretary of state, state auditor, attorney general, superintendent of public instruction) and for members of the Montana Legislature. Extensive new constitutional rights for victims of crime were approved in 2016.\n\nThe 1972 constitution requires that voters determine every 20 years whether to hold a new constitutional convention. Voters turned down a new convention in 1990 (84 percent no) and again in 2010 (58.6 percent no).\n\nMontana has three branches of state government: Legislative, executive, and judicial. The executive branch is headed by an elected governor. The current Governor is Steve Bullock, a Democrat elected in 2012. There are nine other statewide elected offices in the executive branch as well: Lieutenant Governor, Attorney General, Secretary of State, State Auditor (who also serves as Commissioner of Securities and Insurance), and Superintendent of Public Instruction. There are five Public Service Commissioners, who are elected on a regional basis. (The Public Service Commission's jurisdiction is statewide.)\n\nThere are 18 departments and offices which make up the executive branch: Administration; Agriculture; Auditor (securities and insurance); Commerce; Corrections; Environmental Quality; Fish, Wildlife & Parks; Justice; Labor and Industry; Livestock; Military Affairs; Natural Resources and Conservation; Public Health and Human Services; Revenue; State; and Transportation. Elementary and secondary education are overseen by the Office of Public Instruction (led by the elected Superintendent of Public Instruction), in cooperation with the governor-appointed Board of Public Education. Higher education is overseen by a governor-appointed Board of Regents, which in turn appoints a Commissioner of Higher Education. The Office of the Commissioner of Higher Education acts in an executive capacity on behalf of the regents, and oversees the state-run Montana University System.\n\nIndependent state agencies, not located within a department or office, include the Montana Arts Council, Montana Board of Crime Control, Montana Historical Society, Montana Public Employees Retirement Administration, Commissioner of Political Practices, the Montana Lottery, Office of the State Public Defender, Public Service Commission, the Montana School for the Deaf and Blind, the Montana State Fund (which operates the state's unemployment insurance, worker compensation, and self-insurance operations), the Montana State Library, and the Montana Teachers Retirement System.\n\nMontana is an Alcoholic beverage control state. It is an equitable distribution and no-fault divorce state. It is one of five states to have no sales tax.\n\nThe Montana Legislature is bicameral, and consists of the 50-member Montana Senate and the 100-member Montana House of Representatives. The legislature meets in the Montana State Capitol in Helena in odd-numbered years for 90 days, beginning the first weekday of the year. The deadline for a legislator to introduce a general bill is the 40th legislative day. The deadline for a legislator to introduce an appropriations, revenue, or referenda bill is the 62nd legislative day. Senators serve four-year terms, while Representatives serve two-year terms. All members are limited to serving no more than eight years in a single 16-year period.\n\nThe Courts of Montana are established by the Constitution of Montana. The constitution requires the establishment of a Montana Supreme Court and Montana District Courts, and permits the legislature to establish Justice Courts, City Courts, Municipal Courts, and other inferior courts such as the legislature sees fit to establish.\n\nThe Montana Supreme Court is the court of last resort in the Montana court system. The constitution of 1889 provided for the election of no fewer than three Supreme Court justices, and one Chief Justice. Each court member served a six-year term. The legislature increased the number of justices to five in 1919. The 1972 constitution lengthened the term of office to eight years, and established the minimum number of justices at five. It allowed the legislature to increase the number of justices by two, which the legislature did in 1979. The Montana Supreme Court has the authority to declare acts of the legislature and executive unconstitutional under either the Montana or U.S. constitutions. Its decisions may be appealed directly to the U.S. Supreme Court. The Clerk of the Supreme Court is also an elected position, and serves a six-year term. Neither justices nor the clerk are term limited.\n\nMontana District Courts are the courts of general jurisdiction in Montana. There are no intermediate appellate courts. District Courts have jurisdiction primarily over most civil cases, cases involving a monetary claim against the state, felony criminal cases, probate, and cases at law and in equity. When so authorized by the legislature, actions of executive branch agencies may be appealed directly to a District Court. The District Courts also have \"de novo\" appellate jurisdiction from inferior courts (city courts, justice courts, and municipal courts), and oversee naturalization proceedings. District Court judges are elected, and serve six-year terms. They are not term limited. There are 22 judicial districts in Montana, served by 56 District Courts and 46 District Court judges. The District Courts suffer from excessive workload, and the legislature has struggled to find a solution to the problem.\n\nMontana Youth Courts were established by the Montana Youth Court Act of 1974. They are overseen by District Court judges. They consist of a chief probation officer, one or more juvenile probation officers, and support staff. Youth Courts have jurisdiction over misdemeanor and felony acts committed by those charged as a juvenile under the law. There is a Youth Court in every judicial district, and decisions of the Youth Court are appealable directly to the Montana Supreme Court.\n\nThe Montana Worker's Compensation Court was established by the Montana Workers' Compensation Act in 1975. There is a single Workers' Compensation Court. It has a single judge, appointed by the governor. The Worker's Compensation Court has statewide jurisdiction and holds trials in Billings, Great Falls, Helena, Kalispell, and Missoula. The court hears cases arising under the Montana Workers' Compensation Act, and is the court of original jurisdiction for reviews of orders and regulations issued by the Montana Department of Labor and Industry. Decisions of the court are appealable directly to the Montana Supreme Court.\n\nThe Montana Water Court was established by the Montana Water Court Act of 1979. The Water Court consists of a Chief Water Judge and four District Water Judges (Lower Missouri River Basin, Upper Missouri River Basin, Yellowstone River Basin, and Clark Fork River Basin). The court employs 12 permanent special masters. The Montana Judicial Nomination Commission develops short lists of nominees for all five Water Judges, who are then appointed by the Chief Justice of the Montana Supreme Court (subject to confirmation by the Montana Senate). The Water Court adjudicates water rights claims under the Montana Water Use Act of 1973, and has statewide jurisdiction. District Courts have the authority to enforce decisions of the Water Court, but only the Montana Supreme Court has the authority to review decisions of the Water Court.\n\nFrom 1889 to 1909, elections for judicial office in Montana were partisan. Beginning in 1909, these elections became nonpartisan. The Montana Supreme Court struck down the nonpartisan law in 1911 on technical grounds, but a new law was enacted in 1935 which barred political parties from endorsing, making contributions to, or making expenditures on behalf of or against judicial candidates. In 2012, the U.S. Supreme Court struck down Montana's judicial nonpartisan election law in Although candidates must remain nonpartisan, spending by partisan entities is now permitted. Spending on state supreme court races exponentially increased to $1.6 million in 2014, and to more than $1.6 million in 2016 (both new records).\n\nThe U.S. Constitution provides each state with two Senators. Montana's two U.S. senators are Jon Tester (Democrat), who was reelected in 2018, and Steve Daines (Republican), first elected in 2014. The U.S. Constitution provides each state with a single Representative, with additional representatives apportioned based on population. From statehood in 1889 until 1913, Montana was represented in the United States House of Representatives by a single representative, elected at-large. Montana received a second representative in 1913, following the 1910 census and reapportionment. Both members, however, were still elected at-large. Beginning in 1919, Montana moved to district, rather than at-large, elections for its two House members. This created Montana's 1st congressional district in the west and Montana's 2nd congressional district in the east. In the reapportionment following the 1990 census, Montana lost one of its House seats. The remaining seat was again elected at-large. Greg Gianforte is the current officeholder.\n\nMontana's Senate district is the fourth largest by area, behind Alaska, Texas, and California. The most notorious of Montana's early Senators was William A. Clark, a \"Copper King\" and one of the 50 richest Americans ever. He is well known for having bribed his way into the U.S. Senate. Among Montana's most historically prominent Senators are Thomas J. Walsh (serving from 1913 to 1933), who was President-elect Franklin D. Roosevelt's choice for Attorney General when he died; Burton K. Wheeler (serving from 1923 to 1947), an oft-mentioned presidential candidate and strong supporter of isolationism; Mike Mansfield, the longest-serving Senate Majority Leader in U.S. history; Max Baucus (served 1978 to 2014), longest-serving U.S. Senator in Montana history, and the senator who shepherded the Patient Protection and Affordable Care Act through the Senate in 2010; and Lee Metcalf (served 1961 to 1978), a pioneer of the environmental movement.\n\nMontana's House district is currently the largest congressional district in the United States by population, with just over 1,023,000 constituents. It is currently the second largest House district by area, after Alaska's at-large congressional district. Of Montana's House delegates, Jeannette Rankin was the first woman to hold national office in the United States when she was elected to the U.S. House of Representatives in 1916. Also notable is Representative (later Senator) Thomas H. Carter, the first Catholic to serve as chairman of the Republican National Committee (from 1892 to 1896).\n\nFederal courts located in Montana include the United States District Court for the District of Montana and the United States Bankruptcy Court for the District of Montana. Three former Montana politicians have been named judges on the U.S. District Court: Charles Nelson Pray (who served in the U.S. House of Representatives from 1907 to 1913), James Franklin Battin (who served in the U.S. House of Representatives from 1961 to 1969), and Paul G. Hatfield (who served as an appointed U.S. Senator in 1978). Brian Morris, who served as an Associate Justice of the Montana Supreme Court from 2005 to 2013, currently served as a judge on the court.\n\nElections in the state have been competitive, with the Democrats usually holding an edge, thanks to the support among unionized miners and railroad workers. Large-scale battles revolved around the giant Anaconda Copper company, based in Butte and controlled by Rockefeller interests, until it closed in the 1970s. Until 1959, the company owned five of the state's six largest newspapers.\n\nHistorically, Montana is a swing state of cross-ticket voters who tend to fill elected offices with individuals from both parties. Through the mid-20th century, the state had a tradition of \"sending the liberals to Washington and the conservatives to Helena.\" Between 1988 and 2006, the pattern flipped, with voters more likely to elect conservatives to federal offices. There have also been long-term shifts of party control. From 1968 through 1988, the state was dominated by the Democratic Party, with Democratic governors for a 20-year period, and a Democratic majority of both the national congressional delegation and during many sessions of the state legislature. This pattern shifted, beginning with the 1988 election, when Montana elected a Republican governor for the first time since 1964 and sent a Republican to the U.S. Senate for the first time since 1948. This shift continued with the reapportionment of the state's legislative districts that took effect in 1994, when the Republican Party took control of both chambers of the state legislature, consolidating a Republican party dominance that lasted until the 2004 reapportionment produced more swing districts and a brief period of Democratic legislative majorities in the mid-2000s.\n\nIn more recent presidential elections, Montana has voted for the Republican candidate in all but two elections from 1952 to the present. The state last supported a Democrat for president in 1992, when Bill Clinton won a plurality victory. Overall, since 1889 the state has voted for Democratic governors 60 percent of the time and Republican presidents 40 percent of the time. In the 2008 presidential election, Montana was considered a swing state and was ultimately won by Republican John McCain, albeit by a narrow margin of two percent.\n\nAt the state level, the pattern of split-ticket voting and divided government holds. Democrats currently hold one of the state's U.S. Senate seats, as well as one of the five statewide offices (Governor). The lone congressional district has been Republican since 1996 and in 2014 Steve Daines won one of the state's Senate seats for the GOP. The Legislative branch had split party control between the house and senate most years between 2004 and 2010, when the mid-term elections returned both branches to Republican control. The state Senate is, as of 2017, controlled by the Republicans 32 to 18, and the State House of Representatives at 59 to 41. Historically, Republicans are strongest in the east, while Democrats are strongest in the west.\n\nMontana currently has only one representative in the U.S. House, having lost its second district in the 1990 census reapportionment. Montana's single congressional district holds the largest population of any district in the country, which means its one member in the House of Representatives represents more people than any other member of the U.S. House (see List of U.S. states by population). Montana's population grew at about the national average during the 2000s, but it failed to regain its second seat in 2010. Like all other states, Montana has two senators.\n\nMontana has 56 counties with the United States Census Bureau stating Montana's contains 364 \"places\", broken down into 129 incorporated places and 235 census-designated places. Incorporated places consist of 52 cities, 75 towns, and two consolidated city-counties. Montana has one city, Billings, with a population over 100,000; and two cities with populations over 50,000, Missoula and Great Falls. These three communities are considered the centers of Montana's three Metropolitan Statistical Areas.\n\nThe state also has five Micropolitan Statistical Areas centered on Bozeman, Butte, Helena, Kalispell and Havre. These communities, excluding Havre, are colloquially known as the \"big 7\" Montana cities, as they are consistently the seven largest communities in Montana, with a significant population difference when these communities are compared to those that are 8th or lower on the list. According to the 2010 U.S. Census, the population of Montana's seven most populous cities, in rank order, are Billings, Missoula, Great Falls, Bozeman, Butte, Helena and Kalispell. Based on 2013 census numbers, they collectively contain 35 percent of Montana's population. and the counties containing these communities hold 62 percent of the state's population. The geographic center of population of Montana is located in sparsely populated Meagher County, in the town of White Sulphur Springs.\n\nMontana's motto, \"Oro y Plata\", Spanish for \"Gold and Silver\", recognizing the significant role of mining, was first adopted in 1865, when Montana was still a territory. A state seal with a miner's pick and shovel above the motto, surrounded by the mountains and the Great Falls of the Missouri River, was adopted during the first meeting of the territorial legislature in 1864–65. The design was only slightly modified after Montana became a state and adopted it as the Great Seal of the State of Montana, enacted by the legislature in 1893. The state flower, the Bitterroot, was adopted in 1895 with the support of a group called the Floral Emblem Association, which formed after Montana's Women's Christian Temperance Union adopted the bitterroot as the organization's state flower. All other symbols were adopted throughout the 20th century, save for Montana's newest symbol, the state butterfly, the mourning cloak, adopted in 2001, and the state lullaby, \"Montana Lullaby\", adopted in 2007.\n\nThe state song was not composed until 21 years after statehood, when a musical troupe led by Joseph E. Howard stopped in Butte in September 1910. A former member of the troupe who lived in Butte buttonholed Howard at an after-show party, asking him to compose a song about Montana and got another partygoer, the city editor for the \"Butte Miner\" newspaper, Charles C. Cohan, to help. The two men worked up a basic melody and lyrics in about a half-hour for the entertainment of party guests, then finished the song later that evening, with an arrangement worked up the following day. Upon arriving in Helena, Howard's troupe performed 12 encores of the new song to an enthusiastic audience and the governor proclaimed it the state song on the spot, though formal legislative recognition did not occur until 1945. Montana is one of only three states to have a \"state ballad\", \"Montana Melody\", chosen by the legislature in 1983. Montana was the first state to also adopt a State Lullaby.\n\nMontana schoolchildren played a significant role in selecting several state symbols. The state tree, the ponderosa pine, was selected by Montana schoolchildren as the preferred state tree by an overwhelming majority in a referendum held in 1908. However, the legislature did not designate a state tree until 1949, when the Montana Federation of Garden Clubs, with the support of the state forester, lobbied for formal recognition. Schoolchildren also chose the western meadowlark as the state bird, in a 1930 vote, and the legislature acted to endorse this decision in 1931. Similarly, the secretary of state sponsored a children's vote in 1981 to choose a state animal, and after 74 animals were nominated, the grizzly bear won over the elk by a 2–1 margin. The students of Livingston started a statewide school petition drive plus lobbied the governor and the state legislature to name the \"Maiasaura\" as the state fossil in 1985.\n\nVarious community civic groups also played a role in selecting the state grass and the state gemstones. When broadcaster Norma Ashby discovered there was no state fish, she initiated a drive via her television show, \"Today in Montana\", and an informal citizen's election to select a state fish resulted in a win for the blackspotted cutthroat trout after hot competition from the Arctic grayling. The legislature in turn adopted this recommendation by a wide margin.\n\n\n"}
{"id": "19980", "url": "https://en.wikipedia.org/wiki?curid=19980", "title": "Machine translation", "text": "Machine translation\n\nMachine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation (MAHT) or interactive translation) is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.\n\nOn a basic level, MT performs simple substitution of words in one language for words in another, but that alone usually cannot produce a good translation of a text because recognition of whole phrases and their closest counterparts in the target language is needed. Solving this problem with corpus statistical, and neural techniques is a rapidly growing field that is leading to better translations, handling differences in linguistic typology, translation of idioms, and the isolation of anomalies.\n\nCurrent machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text.\n\nImproved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports).\n\nThe progress and potential of machine translation have been debated much through its history. Since the 1950s, a number of scholars have questioned the possibility of achieving fully automatic machine translation of high quality, first and most notably by Yehoshua Bar-Hillel. Some critics claim that there are in-principle obstacles to automating the translation process.\n\nThe idea of machine translation may be traced back to the 17th century. In 1629, René Descartes proposed a universal language, with equivalent ideas in different tongues sharing one symbol. The field of \"machine translation\" appeared in Warren Weaver's Memorandum on Translation (1949). The first researcher in the field, Yehosha Bar-Hillel, began his research at MIT (1951). A Georgetown University MT research team followed (1951) with a public demonstration of its Georgetown-IBM experiment system in 1954. MT research programs popped up in Japan and Russia (1955), and the first MT conference was held in London (1956). Researchers continued to join the field as the Association for Machine Translation and Computational Linguistics was formed in the U.S. (1962) and the National Academy of Sciences formed the Automatic Language Processing Advisory Committee (ALPAC) to study MT (1964). Real progress was much slower, however, and after the ALPAC report (1966), which found that the ten-year-long research had failed to fulfill expectations, funding was greatly reduced. According to a 1972 report by the Director of Defense Research and Engineering (DDR&E), the feasibility of large-scale MT was reestablished by the success of the Logos MT system in translating military manuals into Vietnamese during that conflict.\n\nThe French Textile Institute also used MT to translate abstracts from and into French, English, German and Spanish (1970); Brigham Young University started a project to translate Mormon texts by automated translation (1971); and Xerox used SYSTRAN to translate technical manuals (1978). Beginning in the late 1980s, as computational power increased and became less expensive, more interest was shown in statistical models for machine translation. Various MT companies were launched, including Trados (1984), which was the first to develop and market translation memory technology (1989). The first commercial MT system for Russian / English / German-Ukrainian was developed at Kharkov State University (1991).\n\nMT on the web started with SYSTRAN Offering free translation of small texts (1996), followed by AltaVista Babelfish, which racked up 500,000 requests a day (1997). Franz-Josef Och (the future head of Translation Development AT Google) won DARPA's speed MT competition (2003). More innovations during this time included MOSES, the open-source statistical MT engine (2007), a text/SMS translation service for mobiles in Japan (2008), and a mobile phone with built-in speech-to-speech translation functionality for English, Japanese and Chinese (2009). Recently, Google announced that Google Translate translates roughly enough text to fill 1 million books in one day (2012).\n\nThe idea of using digital computers for translation of natural languages was proposed as early as 1946 by A. D. Booth and possibly others. Warren Weaver wrote an important memorandum \"Translation\" in 1949. The Georgetown experiment was by no means the first such application, and a demonstration was made in 1954 on the APEXC machine at Birkbeck College (University of London) of a rudimentary translation of English into French. Several papers on the topic were published at the time, and even articles in popular journals (see for example \"Wireless World\", Sept. 1955, Cleave and Zacharov). A similar application, also pioneered at Birkbeck College at the time, was reading and composing Braille texts by computer.\n\nThe human translation process may be described as:\n\nBehind this ostensibly simple procedure lies a complex cognitive operation. To decode the meaning of the source text in its entirety, the translator must interpret and analyse all the features of the text, a process that requires in-depth knowledge of the grammar, semantics, syntax, idioms, etc., of the source language, as well as the culture of its speakers. The translator needs the same in-depth knowledge to re-encode the meaning in the target language.\n\nTherein lies the challenge in machine translation: how to program a computer that will \"understand\" a text as a person does, and that will \"create\" a new text in the target language that sounds as if it has been written by a person.\n\nIn its most general application, this is beyond current technology. Though it works much faster, no automated translation program or procedure, with no human participation, can produce output even close to the quality a human translator can produce. What it can do, however, is provide a general, though imperfect, approximation of the original text, getting the \"gist\" of it (a process called \"gisting\"). This is sufficient for many purposes, including making best use of the finite and expensive time of a human translator, reserved for those cases in which total accuracy is indispensable.\n\nThis problem may be approached in a number of ways, through the evolution of which accuracy has improved.\n\nMachine translation can use a method based on linguistic rules, which means that words will be translated in a linguistic way – the most suitable (orally speaking) words of the target language will replace the ones in the source language.\n\nIt is often argued that the success of machine translation requires the problem of natural language understanding to be solved first.\n\nGenerally, rule-based methods parse a text, usually creating an intermediary, symbolic representation, from which the text in the target language is generated. According to the nature of the intermediary representation, an approach is described as interlingual machine translation or transfer-based machine translation. These methods require extensive lexicons with morphological, syntactic, and semantic information, and large sets of rules.\n\nGiven enough data, machine translation programs often work well enough for a native speaker of one language to get the approximate meaning of what is written by the other native speaker. The difficulty is getting enough data of the right kind to support the particular method. For example, the large multilingual corpus of data needed for statistical methods to work is not necessary for the grammar-based methods. But then, the grammar methods need a skilled linguist to carefully design the grammar that they use.\n\nTo translate between closely related languages, the technique referred to as rule-based machine translation may be used.\n\nThe rule-based machine translation paradigm includes transfer-based machine translation, interlingual machine translation and dictionary-based machine translation paradigms. This type of translation is used mostly in the creation of dictionaries and grammar programs. Unlike other methods, RBMT involves more information about the linguistics of the source and target languages, using the morphological and syntactic rules and semantic analysis of both languages. The basic approach involves linking the structure of the input sentence with the structure of the output sentence using a parser and an analyzer for the source language, a generator for the target language, and a transfer lexicon for the actual translation. RBMT's biggest downfall is that everything must be made explicit: orthographical variation and erroneous input must be made part of the source language analyser in order to cope with it, and lexical selection rules must be written for all instances of ambiguity. Adapting to new domains in itself is not that hard, as the core grammar is the same across domains, and the domain-specific adjustment is limited to lexical selection adjustment.\n\nTransfer-based machine translation is similar to interlingual machine translation in that it creates a translation from an intermediate representation that simulates the meaning of the original sentence. Unlike interlingual MT, it depends partially on the language pair involved in the translation.\n\nInterlingual machine translation is one instance of rule-based machine-translation approaches. In this approach, the source language, i.e. the text to be translated, is transformed into an interlingual language, i.e. a \"language neutral\" representation that is independent of any language. The target language is then generated out of the interlingua. One of the major advantages of this system is that the interlingua becomes more valuable as the number of target languages it can be turned into increases. However, the only interlingual machine translation system that has been made operational at the commercial level is the KANT system (Nyberg and Mitamura, 1992), which is designed to translate Caterpillar Technical English (CTE) into other languages.\n\nMachine translation can use a method based on dictionary entries, which means that the words will be translated as they are by a dictionary.\n\nStatistical machine translation tries to generate translations using statistical methods based on bilingual text corpora, such as the Canadian Hansard corpus, the English-French record of the Canadian parliament and EUROPARL, the record of the European Parliament. Where such corpora are available, good results can be achieved translating similar texts, but such corpora are still rare for many language pairs. The first statistical machine translation software was CANDIDE from IBM. Google used SYSTRAN for several years, but switched to a statistical translation method in October 2007. In 2005, Google improved its internal translation capabilities by using approximately 200 billion words from United Nations materials to train their system; translation accuracy improved. Google Translate and similar statistical translation programs work by detecting patterns in hundreds of millions of documents that have previously been translated by humans and making intelligent guesses based on the findings. Generally, the more human-translated documents available in a given language, the more likely it is that the translation will be of good quality. Newer approaches into Statistical Machine translation such as METIS II and PRESEMT use minimal corpus size and instead focus on derivation of syntactic structure through pattern recognition. With further development, this may allow statistical machine translation to operate off of a monolingual text corpus. SMT's biggest downfall includes it being dependent upon huge amounts of parallel texts, its problems with morphology-rich languages (especially with translating \"into\" such languages), and its inability to correct singleton errors.\n\nExample-based machine translation (EBMT) approach was proposed by Makoto Nagao in 1984. Example-based machine translation is based on the idea of analogy. In this approach, the corpus that is used is one that contains texts that have already been translated. Given a sentence that is to be translated, sentences from this corpus are selected that contain similar sub-sentential components. The similar sentences are then used to translate the sub-sentential components of the original sentence into the target language, and these phrases are put together to form a complete translation.\n\nHybrid machine translation (HMT) leverages the strengths of statistical and rule-based translation methodologies. Several MT organizations (such as Omniscien Technologies (formerly Asia Online), LinguaSys, Systran, and Polytechnic University of Valencia) claim a hybrid approach that uses both rules and statistics. The approaches differ in a number of ways:\nMore recently, with the advent of Neural MT, a new version of hybrid machine translation is emerging that combines the benefits of rules, statistical and neural machine translation. The approach allows benefitting from pre- and post-processing in a rule guided workflow as well as benefitting from NMT and SMT. The downside is the inherent complexity which makes the approach suitable only for specific use cases. One of the proponents of this approach for complex use cases is Omniscien Technologies.\n\nA deep learning based approach to MT, neural machine translation has made rapid progress in recent years, and Google has announced its translation services are now using this technology in preference to its previous statistical methods. Other providers including Pangeanic, KantanMT, Omniscien Technologies and SDL have announced the deployment of neural machine translation technology in 2017 as well.\n\nWord-sense disambiguation concerns finding a suitable translation when a word can have more than one meaning. The problem was first raised in the 1950s by Yehoshua Bar-Hillel. He pointed out that without a \"universal encyclopedia\", a machine would never be able to distinguish between the two meanings of a word. Today there are numerous approaches designed to overcome this problem. They can be approximately divided into \"shallow\" approaches and \"deep\" approaches.\n\nShallow approaches assume no knowledge of the text. They simply apply statistical methods to the words surrounding the ambiguous word. Deep approaches presume a comprehensive knowledge of the word. So far, shallow approaches have been more successful.\n\nClaude Piron, a long-time translator for the United Nations and the World Health Organization, wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved:\n\nThe ideal deep approach would require the translation software to do all the research necessary for this kind of disambiguation on its own; but this would require a higher degree of AI than has yet been attained. A shallow approach which simply guessed at the sense of the ambiguous English phrase that Piron mentions (based, perhaps, on which kind of prisoner-of-war camp is more often mentioned in a given corpus) would have a reasonable chance of guessing wrong fairly often. A shallow approach that involves \"ask the user about each ambiguity\" would, by Piron's estimate, only automate about 25% of a professional translator's job, leaving the harder 75% still to be done by a human.\n\nOne of the major pitfalls of MT is its inability to translate non-standard language with the same accuracy as standard language. Heuristic or statistical based MT takes input from various sources in standard form of a language. Rule-based translation, by nature, does not include common non-standard usages. This causes errors in translation from a vernacular source or into colloquial language. Limitations on translation from casual speech present issues in the use of machine translation in mobile devices.\n\nName entities, in narrow sense, refer to concrete or abstract entities in the real world including people, organizations, companies, places etc. It also refers to expressing of time, space, quantity such as 1 July 2011, $79.99 and so on.\n\nNamed entities occur in the text being analyzed in statistical machine translation. The initial difficulty that arises in dealing with named entities is simply identifying them in the text. Consider the list of names common in a particular language to illustrate this – the most common names are different for each language and also are constantly changing. If named entities cannot be recognized by the machine translator, they may be erroneously translated as common nouns, which would most likely not affect the BLEU rating of the translation but would change the text's human readability. It is also possible that, when not identified, named entities will be omitted from the output translation, which would also have implications for the text's readability and message.\n\nAnother way to deal with named entities is to use transliteration instead of translation, meaning that you find the letters in the target language that most closely correspond to the name in the source language. There have been attempts to incorporate this into machine translation by adding a transliteration step into the translation procedure. However, these attempts still have their problems and have even been cited as worsening the quality of translation. Named entities were still identified incorrectly, with words not being transliterated when they should or being transliterated when they shouldn't. For example, for \"Southern California\" the first word should be translated directly, while the second word should be transliterated. However, machines would often transliterate both because they treated them as one entity. Words like these are hard for machine translators, even those with a transliteration component, to process.\n\nThe lack of attention to the issue of named entity translation has been recognized as potentially stemming from a lack of resources to devote to the task in addition to the complexity of creating a good system for named entity translation. One approach to named entity translation has been to transliterate, and not translate, those words. A second is to create a \"do-not-translate\" list, which has the same end goal – transliteration as opposed to translation. Both of these approaches still rely on the correct identification of named entities, however.\n\nA third approach to successful named entity translation is a class-based model. In this method, named entities are replaced with a token to represent the class they belong to. For example, \"Ted\" and \"Erica\" would both be replaced with \"person\" class token. In this way the statistical distribution and use of person names in general can be analyzed instead of looking at the distributions of \"Ted\" and \"Erica\" individually. A problem that the class based model solves is that the probability of a given name in a specific language will not affect the assigned probability of a translation. A study by Stanford on improving this area of translation gives the examples that different probabilities will be assigned to \"David is going for a walk\" and \"Ankit is going for a walk\" for English as a target language due to the different number of occurrences for each name in the training data. A frustrating outcome of the same study by Stanford (and other attempts to improve named recognition translation) is that many times, a decrease in the BLEU scores for translation will result from the inclusion of methods for named entity translation.\n\nSome work has been done in the utilization of multiparallel corpora, that is a body of text that has been translated into 3 or more languages. Using these methods, a text that has been translated into 2 or more languages may be utilized in combination to provide a more accurate translation into a third language compared with if just one of those source languages were used alone.\n\nAn ontology is a formal representation of knowledge which includes the concepts (such as objects, processes etc.) in a domain and some relations between them. If the stored information is of linguistic nature, one can speak of a lexicon.\nIn NLP, ontologies can be used as a source of knowledge for machine translation systems. With access to a large knowledge base, systems can be enabled to resolve many (especially lexical) ambiguities on their own.\nIn the following classic examples, as humans, we are able to interpret the prepositional phrase according to the context because we use our world knowledge, stored in our lexicons:\nA machine translation system initially would not be able to differentiate between the meanings because syntax does not change. With a large enough ontology as a source of knowledge however, the possible interpretations of ambiguous words in a specific context can be reduced.\nOther areas of usage for ontologies within NLP include information retrieval, information extraction and text summarization.\n\nThe ontology generated for the PANGLOSS knowledge-based machine translation system in 1993 may serve as an example of how an ontology for NLP purposes can be compiled:\n\nWhile no system provides the holy grail of fully automatic high-quality machine translation of unrestricted text, many fully automated systems produce reasonable output. The quality of machine translation is substantially improved if the domain is restricted and controlled.\n\nDespite their inherent limitations, MT programs are used around the world. Probably the largest institutional user is the European Commission. The MOLTO project, for example, coordinated by the University of Gothenburg, received more than 2.375 million euros project support from the EU to create a reliable translation tool that covers a majority of the EU languages. The further development of MT systems comes at a time when budget cuts in human translation may increase the EU's dependency on reliable MT programs. The European Commission contributed 3.072 million euros (via its ISA programme) for the creation of MT@EC, a statistical machine translation program tailored to the administrative needs of the EU, to replace a previous rule-based machine translation system.\n\nIn 2005, Google claimed that promising results were obtained using a proprietary statistical machine translation engine. The statistical translation engine used in the Google language tools for Arabic <-> English and Chinese <-> English had an overall score of 0.4281 over the runner-up IBM's BLEU-4 score of 0.3954 (Summer 2006) in tests conducted by the National Institute for Standards and Technology.\n\nWith the recent focus on terrorism, the military sources in the United States have been investing significant amounts of money in natural language engineering. \"In-Q-Tel\" (a venture capital fund, largely funded by the US Intelligence Community, to stimulate new technologies through private sector entrepreneurs) brought up companies like Language Weaver. Currently the military community is interested in translation and processing of languages like Arabic, Pashto, and Dari. Within these languages, the focus is on key phrases and quick communication between military members and civilians through the use of mobile phone apps. The Information Processing Technology Office in DARPA hosts programs like TIDES and Babylon translator. US Air Force has awarded a $1 million contract to develop a language translation technology.\n\nThe notable rise of social networking on the web in recent years has created yet another niche for the application of machine translation software – in utilities such as Facebook, or instant messaging clients such as Skype, GoogleTalk, MSN Messenger, etc. – allowing users speaking different languages to communicate with each other. Machine translation applications have also been released for most mobile devices, including mobile telephones, pocket PCs, PDAs, etc. Due to their portability, such instruments have come to be designated as mobile translation tools enabling mobile business networking between partners speaking different languages, or facilitating both foreign language learning and unaccompanied traveling to foreign countries without the need of the intermediation of a human translator.\n\nDespite being labelled as an unworthy competitor to human translation in 1966 by the Automated Language Processing Advisory Committee put together by the United States government, the quality of machine translation has now been improved to such levels that its application in online collaboration and in the medical field are being investigated. The application of this technology in medical settings where human translators are absent is another topic of research, but difficulties arise due to the importance of accurate translations in medical diagnoses.\n\nThere are many factors that affect how machine translation systems are evaluated. These factors include the intended use of the translation, the nature of the machine translation software, and the nature of the translation process.\n\nDifferent programs may work well for different purposes. For example, statistical machine translation (SMT) typically outperforms example-based machine translation (EBMT), but researchers found that when evaluating English to French translation, EBMT performs better. The same concept applies for technical documents, which can be more easily translated by SMT because of their formal language.\n\nIn certain applications, however, e.g., product descriptions written in a controlled language, a dictionary-based machine-translation system has produced satisfactory translations that require no human intervention save for quality inspection.\n\nThere are various means for evaluating the output quality of machine translation systems. The oldest is the use of human judges to assess a translation's quality. Even though human evaluation is time-consuming, it is still the most reliable method to compare different systems such as rule-based and statistical systems. Automated means of evaluation include BLEU, NIST, METEOR, and LEPOR.\n\nRelying exclusively on unedited machine translation ignores the fact that communication in human language is context-embedded and that it takes a person to comprehend the context of the original text with a reasonable degree of probability. It is certainly true that even purely human-generated translations are prone to error. Therefore, to ensure that a machine-generated translation will be useful to a human being and that publishable-quality translation is achieved, such translations must be reviewed and edited by a human. The late Claude Piron wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved. Such research is a necessary prelude to the pre-editing necessary in order to provide input for machine-translation software such that the output will not be meaningless.\n\nIn addition to disambiguation problems, decreased accuracy can occur due to varying levels of training data for machine translating programs. Both example-based and statistical machine translation rely on a vast array of real example sentences as a base for translation, and when too many or too few sentences are analyzed accuracy is jeopardized. Researchers found that when a program is trained on 203,529 sentence pairings, accuracy actually decreases. The optimal level of training data seems to be just over 100,000 sentences, possibly because as training data increases, the number of possible sentences increases, making it harder to find an exact translation match.\n\nAlthough there have been concerns about machine translation's accuracy, Dr. Ana Nino of the University of Manchester has researched some of the advantages in utilizing machine translation in the classroom. One such pedagogical method is called using \"MT as a Bad Model.\" MT as a Bad Model forces the language learner to identify inconsistencies or incorrect aspects of a translation; in turn, the individual will (hopefully) possess a better grasp of the language. Dr. Nino cites that this teaching tool was implemented in the late 1980s. At the end of various semesters, Dr. Nino was able to obtain survey results from students who had used MT as a Bad Model (as well as other models.) Overwhelmingly, students felt that they had observed improved comprehension, lexical retrieval, and increased confidence in their target language.\n\nIn the early 2000s, options for machine translation between spoken and signed languages were severely limited. It was a common belief that deaf individuals could use traditional translators. However, stress, intonation, pitch, and timing are conveyed much differently in spoken languages compared to signed languages. Therefore, a deaf individual may misinterpret or become confused about the meaning of written text that is based on a spoken language.\n\nResearchers Zhao, et al. (2000), developed a prototype called TEAM (translation from English to ASL by machine) that completed English to American Sign Language (ASL) translations. The program would first analyze the syntactic, grammatical, and morphological aspects of the English text. Following this step, the program accessed a sign synthesizer, which acted as a dictionary for ASL. This synthesizer housed the process one must follow to complete ASL signs, as well as the meanings of these signs. Once the entire text is analyzed and the signs necessary to complete the translation are located in the synthesizer, a computer generated human appeared and would use ASL to sign the English text to the user.\n\nOnly works that are original are subject to copyright protection, so some scholars claim that machine translation results are not entitled to copyright protection because MT does not involve creativity. The copyright at issue is for a derivative work; the author of the original work in the original language does not lose his rights when a work is translated: a translator must have permission to publish a translation.\n\n\n"}
{"id": "19983", "url": "https://en.wikipedia.org/wiki?curid=19983", "title": "Central moment", "text": "Central moment\n\nIn probability theory and statistics, a central moment is a moment of a probability distribution of a random variable about the random variable's mean; that is, it is the expected value of a specified integer power of the deviation of the random variable from the mean. The various moments form one set of values by which the properties of a probability distribution can be usefully characterised. Central moments are used in preference to ordinary moments, computed in terms of deviations from the mean instead of from zero, because the higher-order central moments relate only to the spread and shape of the distribution, rather than also to its location.\n\nSets of central moments can be defined for both univariate and multivariate distributions.\n\nThe \"n\"th moment about the mean (or \"n\"th central moment) of a real-valued random variable \"X\" is the quantity \"μ\" := E[(\"X\" − E[\"X\"])], where E is the expectation operator. For a continuous univariate probability distribution with probability density function \"f\"(\"x\"), the \"n\"th moment about the mean \"μ\" is\n\nFor random variables that have no mean, such as the Cauchy distribution, central moments are not defined.\n\nThe first few central moments have intuitive interpretations:\n\nThe \"n\"th central moment is translation-invariant, i.e. for any random variable \"X\" and any constant \"c\", we have\n\nFor all \"n\", the \"n\"th central moment is homogeneous of degree \"n\":\n\n\"Only\" for \"n\" such that n equals 1, 2, or 3 do we have an additivity property for random variables \"X\" and \"Y\" that are independent:\n\nA related functional that shares the translation-invariance and homogeneity properties with the \"n\"th central moment, but continues to have this additivity property even when \"n\" ≥ 4 is the \"n\"th cumulant κ(\"X\"). For \"n\" = 1, the \"n\"th cumulant is just the expected value; for \"n\" = either 2 or 3, the \"n\"th cumulant is just the \"n\"th central moment; for \"n\" ≥ 4, the \"n\"th cumulant is an \"n\"th-degree monic polynomial in the first \"n\" moments (about zero), and is also a (simpler) \"n\"th-degree polynomial in the first \"n\" central moments.\n\nSometimes it is convenient to convert moments about the origin to moments about the mean. The general equation for converting the \"n\"th-order moment about the origin to the moment about the mean is\n\nwhere \"μ\" is the mean of the distribution, and the moment about the origin is given by\n\nFor the cases \"n\" = 2, 3, 4 — which are of most interest because of the relations to variance, skewness, and kurtosis, respectively — this formula becomes (noting that formula_7 and formula_8):,\n\n... and so on, following Pascal's triangle, i.e.\n\nbecause formula_14\n\nThe following sum is a stochastic variable having a compound distribution\n\nwhere the formula_16 are mutually independent random variables sharing the same common distribution and formula_17 a random integer variable independent of the formula_18 with its own distribution. The moments of formula_19 are obtained as \n\nwhere formula_21 is defined as zero for formula_22.\n\nIn a symmetric distribution (one that is unaffected by being reflected about its mean), all odd central moments equal zero, because in the formula for the \"n\"th moment, each term involving a value of \"X\" less than the mean by a certain amount exactly cancels out the term involving a value of \"X\" greater than the mean by the same amount.\n\nFor a continuous bivariate probability distribution with probability density function \"f\"(\"x\",\"y\") the (\"j\",\"k\") moment about the mean \"μ\" = (\"μ\", \"μ\") is\n\n"}
{"id": "19986", "url": "https://en.wikipedia.org/wiki?curid=19986", "title": "Murad I", "text": "Murad I\n\nMurad I (; (nicknamed Hüdavendigâr, from Persian: خداوندگار, \"Khodāvandgār\", \"the devotee of God\" – but meaning \"sovereign\" in this context); 29 June 1326 – 15 June 1389) was the Ottoman Sultan from 1362 to 1389. He was a son of Orhan and the Valide Nilüfer Hatun.\n\nMurad I conquered Adrianople, renamed it to Edirne, and in 1363 made it the new capital of the Ottoman Sultanate. Then he further expanded the Ottoman realm in Southeast Europe by bringing most of the Balkans under Ottoman rule, and forced the princes of northern Serbia and Bulgaria as well as the Byzantine emperor John V Palaiologos to pay him tribute. Murad I administratively divided his sultanate into the two provinces of Anatolia (Asia Minor) and Rumelia (the Balkans). Murad's death against the Serbs would cause the Ottomans to halt their expansion into the territory temporarily and focus their attention once more on the ailing Byzantine Empire.\n\nMurad fought against the powerful beylik of Karaman in Anatolia and against the Serbs, Albanians, Bulgarians and Hungarians in Europe. In particular, a Serb expedition to expel the Turks from Adrianople led by the Serbian brothers King Vukašin and Despot Uglješa, was defeated on September 26, 1371, by Murad's capable second lieutenant Lala Şâhin Paşa, the first governor (\"beylerbey\") of Rumeli. In 1385, Sofia fell to the Ottomans. In 1386 Prince Lazar Hrebeljanović defeated an Ottoman force at the Battle of Pločnik. The Ottoman army suffered heavy casualties, and was unable to capture Niš on the way back.\n\nIn 1389, Murad's army defeated the Serbian Army and its allies under the leadership of Lazar at the Battle of Kosovo.\nThere are different accounts from different sources about when and how Murad I was assassinated. The contemporary sources mainly noted that the battle took place and that both Prince Lazar and the Sultan lost their lives in the battle. The existing evidence of the additional stories and speculations as to how Murad I died were disseminated and recorded in the 15th century and later, decades after the actual event. One Western source states that during first hours of the battle, Murad I was assassinated by Serbian nobleman and knight Miloš Obilić by knife. Most Ottoman chroniclers (including Dimitrie Cantemir) state that he was assassinated after the finish of the battle while going around the battlefield. Others state that he was assassinated in the evening after the battle at his tent by the assassin who was admitted to ask a special favour. His older son Bayezid, who was in charge of the left wing of the Ottoman forces, took charge after that. His other son, Yakub Bey, who was in charge of the other wing, was called to the Sultan's command center tent by Bayezid, but when Yakub Bey arrived he was strangled, leaving Bayezid as the sole claimant to the throne.\n\nIn a letter from the Florentine senate (written by Coluccio Salutati) to the King Tvrtko I of Bosnia, dated 20 October 1389, Murad I's (and Jakub Bey's) killing was described. A party of twelve Serbian lords slashed their way through the Ottoman lines defending Murad I. One of them, allegedly Miloš Obilić, had managed to get through to the Sultan's tent and kill him with sword stabs to the throat and belly.\n\nSultan Murad's internal organs were buried in Kosovo field and remains to this day on a corner of the battlefield in a location called \"Meshed-i Hudavendigar\" which has gained a religious significance by the local Muslims. It has been vandalized between 1999-2006 and renovated recently. His other remains were carried to Bursa, his Anatolian capital city, and were buried in a tomb at the complex built in his name.\n\nHe established the sultanate by building up a society and government in the newly conquered city of Adrianople (Edirne in Turkish) and by expanding the realm in Europe, bringing most of the Balkans under Ottoman rule and forcing the Byzantine emperor to pay him tribute. It was Murad who established the former Osmanli tribe into an sultanate. He established the title of sultan in 1383 and the corps of the \"janissaries\" and the \"devşirme\" recruiting system. He also organised the government of the \"Divan\", the system of timars and timar-holders (timariots) and the military judge, the \"kazasker\". He also established the two provinces of Anadolu (Anatolia) and Rumeli (Europe).\n\nHe was the son of Orhan and the Valide Hatun Nilüfer Hatun, daughter of the Prince of Yarhisar, who was of ethnic Greek descent\n\n\n\n\n\n\nNotes:\n\nReferences:\n"}
{"id": "19987", "url": "https://en.wikipedia.org/wiki?curid=19987", "title": "Mehmed I", "text": "Mehmed I\n\nMehmed I (1379 – 26 May 1421), also known as Mehmed Çelebi (, \"the noble-born\") or Kirişci (from Greek \"Kyritzes\", \"lord's son\"), was the Ottoman Sultan from 1413 to 1421. The fourth son of Sultan Bayezid I and Devlet Hatun, he fought with his brothers over control of the Ottoman realm in the Ottoman Interregnum (1402–1413). Starting from the province of Rûm he managed to bring first Anatolia and then the European territories (Rumelia) under his control, reuniting the Ottoman state by 1413, and ruling it until his death in 1421.\n\nMehmed was born in 1386 or 1387 as the fourth son of Sultan Bayezid I () and one of his consorts, the slave girl Devlet Hatun. Following Ottoman custom, when he reached adolescence in 1399, he was sent to gain experience as provincial governor over the Rûm Eyalet (central northern Anatolia), recently conquered from its Eretnid rulers.\n\nOn 20 July 1402, his father Bayezid was defeated in the Battle of Ankara by the Turko-Mongol conqueror and ruler Timur. The brothers (with the exception of Mustafa, who was captured and taken along with Bayezid to Samarkand) were rescued from the battlefield, Mehmed being saved by Bayezid Pasha, who took him to his hometown of Amasya. Mehmed later made Bayezid Pasha his grand vizier (1413–1421).\n\nThe early Ottoman Empire had no regulated succession, and according to Turkish tradition, every son could succeed his father. Of Mehmed's brothers, the eldest, Ertuğrul, had died in 1400, while the next in line, Mustafa, was a prisoner of Timur. Leaving aside the underage siblings, this left four princes—Mehmed, Süleyman, İsa, and Musa, to contend over control of the remaining Ottoman territories in the civil war known as the \"Ottoman Interregnum\". In modern historiography, these princes are usually called by the title \"Çelebi\", but in contemporary sources, the title is reserved for Mehmed and Musa. The Byzantine sources translated the title as \"Kyritzes\" (Κυριτζής), which was in turn adopted into Turkish as \"kirişçi\", sometimes misinterpreted as \"güreşçi\", \"the wrestler\".\n\nAfter winning the Interregnum, Mehmed crowned himself sultan in the Thracian city of Edirne that lay in the European part of the empire (the area dividing the Anatolian and European sides of the empire, Constantinople and the surrounding region, was still held by the Byzantine Empire), becoming Mehmed I. He consolidated his power, made Edirne the most important of the dual capitals, and conquered parts of Albania, the Jandarid emirate, and the Armenian Kingdom of Cilicia from the Mamelukes. Taking his many achievements into consideration, Mehmed is widely known as the \"second founder\" of the Ottoman Sultanate.\n\nSoon after Mehmed began his reign, his brother Mustafa Çelebi, who had originally been captured along with their father Bayezid I during the Battle of Ankara and held captive in Samarkand, hiding in Anatolia during the Interregnum, reemerged and asked Mehmed to partition the empire with him. Mehmed refused and met Mustafa's forces in battle, easily defeating them. Mustafa escaped to the Byzantine city of Thessaloniki, but after an agreement with Mehmed, the Byzantine emperor Manuel II Palaiologos exiled Mustafa to the island of Lemnos.\n\nHowever, Mehmed still faced some problems, first being the problem of his nephew Orhan, who Mehmed perceived as a threat to his rule, much like his late brothers had been. There was allegedly a plot involving him by Manuel II Palaiologos, who tried to use Orhan against Sultan Mehmed; however, the sultan found out about the plot and had Orhan blinded for betrayal, according to a common Byzantine practice.\n\nFurthermore, as a result of the Battle of Ankara and other civil wars, the population of the empire had become unstable and traumatized. A very powerful social and religious movement arose in the empire and became disruptive. The movement was led by Sheikh Bedreddin (1359–1420), a famous Muslim Sufi and charismatic theologian. He was an eminent Ulema, born of a Greek mother and a Muslim father in Simavna (Kyprinos) southwest of Edirne (formerly Adrianople). Mehmed's brother Musa had made Bedreddin his \"qadi of the army,\" or the supreme judge. Bedreddin created a populist religious movement in the Ottoman Sultanate, \"subversive conclusions promoting the suppression of social differences between rich and poor as well as the barriers between different forms of monotheism.\" Successfully developing a popular social revolution and syncretism of the various religions and sects of the empire, Bedreddin's movement began in the European side of the empire and underwent further expansion in western Anatolia.\n\nIn 1416, Sheikh Bedreddin started his rebellion against the throne. After a four-year struggle, he was finally captured by Mehmed's grand vizier Bayezid Pasha and hanged in the city off Serres, a city in modern-day Greece, in 1420.\n\nThe reign of Mehmed I as sultan of the re-united empire lasted only eight years before his death, but he had also been the most powerful brother contending for the throne and \"de facto\" ruler of most of the empire for nearly the whole preceding period of 11 years of the Ottoman Interregnum that passed between his father's captivity at Ankara and his own final victory over his brother Musa Çelebi at the Battle of Çamurlu.\nHe was buried in Bursa, in a mausoleum erected by himself near the celebrated mosque which he built there, and which, because of its decorations of green glazed tiles, is called the Green Mosque. Mehmed I also completed another mosque in Bursa, which his grandfather Murad I had commenced but which had been neglected during the reign of Bayezid. Mehmed founded in the vicinity of his own Green Mosque and mausoleum two other characteristic institutions, one a school and one a refectory for the poor, both of which he endowed with royal munificence.\n\n\n\n\n"}
{"id": "19988", "url": "https://en.wikipedia.org/wiki?curid=19988", "title": "Murad II", "text": "Murad II\n\nMurad II's reign was marked by the long war he fought against the Christian feudal lords of the Balkans and the Turkish beyliks in Anatolia, a conflict that lasted 25 years. He was brought up in Amasya, and ascended the throne on the death of his father Mehmed I. His mother was Valide Sultan Emine Hatun (daughter of Suleyman Bey, ruler of Dulkadirids), his father's third consort. Their marriage served as an alliance between the Ottomans and this buffer state, and produced a son, Mehmed II, who would go on to successfully conquer the Byzantine Empire's capital, Constantinople, in 1453.\n\nMurad was born in June 1404 to Sultan Mehmed I and his wife Emine Hatun, and he spent his early childhood in Amasya. In 1410, Murad came along with his father to the Ottoman capital, Edirne. After his father ascended to the Ottoman throne, he made Murad governor of the Amasya Sanjak. Murad remained at Amasya until the death of Mehmed I in 1421. He was solemnly recognized as sultan of the Ottoman Sultanate at sixteen years of age, girded with the sabre of Osman at Bursa, and the troops and officers of the state willingly paid homage to him as their sovereign.\n\nMurad's reign was troubled by insurrection early on. The Byzantine Emperor, Manuel II, released the 'pretender' Mustafa Çelebi (known as Düzmece Mustafa) from confinement and acknowledged him as the legitimate heir to the throne of Bayezid I (1389–1402). The Byzantine Emperor had first secured a stipulation that Mustafa should, if successful, repay him for his liberation by giving up a large number of important cities. The pretender was landed by the Byzantine galleys in the European dominion of the sultan and for a time made rapid progress. Many Turkish soldiers joined him, and he defeated and killed the veteran general Beyazid Pasha, whom Murad had sent to fight him. Mustafa defeated Murad's army and declared himself Sultan of Adrianople (modern Edirne). He then crossed the Dardanelles to Asia with a large army but Murad out-manoeuvered Mustafa. Mustafa's force passed over in large numbers to Murad II. Mustafa took refuge in the city of Gallipoli, but the sultan, who was greatly aided by a Genoese commander named Adorno, besieged him there and stormed the place. Mustafa was taken and put to death by the sultan, who then turned his arms against the Roman emperor and declared his resolution to punish the Palaiologos for their unprovoked enmity by the capture of Constantinople.\n\nMurad II then formed a new army called Azap in 1421 and marched through the Byzantine Empire and laid siege to Constantinople. While Murad was besieging the city, the Byzantines, in league with some independent Turkish Anatolian states, sent the sultan's younger brother Küçük Mustafa (who was only 13 years old) to rebel against the sultan and besiege Bursa. Murad had to abandon the siege of Constantinople in order to deal with his rebellious brother. He caught Prince Mustafa and executed him. The Anatolian states that had been constantly plotting against him — Aydinids, Germiyanids, Menteshe and Teke — were annexed and henceforth became part of the Ottoman Sultanate.\n\nMurad II then declared war against Venice, the Karamanid Emirate, Serbia and Hungary. The Karamanids were defeated in 1428 and Venice withdrew in 1432 following the defeat at the second Siege of Thessalonica in 1430. In the 1430s Murad captured vast territories in the Balkans and succeeded in annexing Serbia in 1439. In 1441 the Holy Roman Empire and Poland joined the Serbian-Hungarian coalition. Murad II won the Battle of Varna in 1444 against János Hunyadi.\n\nMurad II relinquished his throne in 1444 to his son Mehmed II, but a Janissary revolt in the Empire forced him to return.\n\nIn 1448 he defeated the Christian coalition at the Second Battle of Kosovo (the first one took place in 1389). When the Balkan front was secured, Murad II turned east to defeat Timur's son, Shah Rokh, and the emirates of Karamanid and Çorum-Amasya. In 1450 Murad II led his army into Albania and unsuccessfully besieged the Castle of Kruje in an effort to defeat the resistance led by Skanderbeg. In the winter of 1450–1451, Murad II fell ill, and died in Edirne. He was succeeded by his son Mehmed II (1451–81).\n\nWhen Murad II ascended to the throne, he sought to regain the lost Ottoman territories that had reverted to autonomy following his grandfather Bayezid I’s defeat at the Battle of Ankara in 1402 at the hands of Timur Lang. He needed the support of both the public and the nobles “who would enable him to exercise his rule”, and utilized the old and potent Islamic trope of Ghazi King.\n\nIn order to gain popular, international support for his conquests, Murad II modeled himself after the legendary Ghazi kings of old. The Ottomans already presented themselves as ghazis, painting their origins as rising from the ghazas of Osman, the founder of the dynasty. For them, ghaza was the noble championing of Islam and justice against non-Muslims and Muslims alike, if they were cruel; for example, Bayezid I labeled Timur Lang, also a Muslim, an apostate prior to the Battle of Ankara because of the violence his troops had committed upon innocent civilians and because “all you do is to break promises and vows, shed blood, and violate the honor of women.” Murad II only had to capitalize on this dynastic inheritance of doing ghaza, which he did by actively crafting the public image of Ghazi Sultan.\n\nAfter his accession, there was a flurry of translating and compiling activity where old Persian, Arab, and Anatolian epics were translated into Turkish so Murad II could uncover the ghazi king legends. He drew from the noble behavior of the nameless Caliphs in the \"Battalname\", an epic about a fictional Arab warrior who fought against the Byzantines, and modelled his actions on theirs. He was careful to embody the simplicity, piety, and noble sense of justice that was part of the Ghazi King persona.\n\nFor example, the Caliph in \"Battalname\" saw the battle turning in his enemy’s favor, and got down from his horse and prayed, after which the battle ended in a victory for him. In the Battle of Varna in 1444, Murad II saw the Hungarians gaining the upper hand, and he got down from his horse and prayed just like the Caliph, and soon after, the tide turned in the Ottoman’s favor and the Hungarian king Wladyslaw was killed. Similarly, the Caliph in the epic roused his warriors by saying “Those of you who die will be martyrs. Those of you who kill will be ghazis”; before the Battle of Varna, Murad II repeated these words to his army, saying “Those of us who kill will be ghazis; those of us who die will be martyrs.” In another instance, since the Ghazi King is meant to be a just and fair, when Murad took Thessalonica in the Balkans, he took care to keep the troops in check and prevented widespread looting. Finally, just as the fictional Caliph’s ghazas were immortalized in \"Battalname\", Murad II’s battles and victories were also compiled and given the title \"The Ghazas of Sultan Murad\" (\"Gazavat- i Sultan Murad)\".\n\nMurad II successfully painted himself as a simple soldier who did not partake in royal excesses, and as a noble ghazi sultan who sought to consolidate Muslim power against non-Muslims such as the Venetians and Hungarians. Through this self-presentation, he got the support of the Muslim population of not only the Ottoman territories, for both himself and his extensive, expensive campaigns, but also the greater Muslim populations in the Dar-al-Islam – such as the Mamluks and the Muslim Delhi Sultanates of India. Murad II was basically presenting himself not only as “a ghazi king who fights caffres [nonmuslims], but also serves as protector and master of lesser ghazis.”\n\nMurad II had four known wives:\nMurad had five sons:\n\nMurad had four daughters:\n\nMurad II is portrayed by İlker Kurt in 2012 film \"Fetih 1453\". He was also portrayed by Vahram Papazian in the Albanian movie \"The Great Warrior Skanderbeg\" in 1953.\n\n\n\n"}
{"id": "19989", "url": "https://en.wikipedia.org/wiki?curid=19989", "title": "Murad III", "text": "Murad III\n\nMurad III (Ottoman Turkish: مراد ثالث \"Murād-i sālis\", Turkish: \"III.Murat\") (4 July 1546 – 15/16 January 1595) was the Sultan of the Ottoman Empire from 1574 until his death in 1595.\n\nBorn in Manisa on 4 July 1546, Şehzade Murad was the oldest son of Sultan Selim II and Afife Nurbanu Sultan. After his ceremonial circumcision in 1557, Murad was appointed \"sancakbeyi\" of Akşehir by Suleiman I (his grandfather) in 1558. At the age of 18 he was appointed \"sancakbeyi\" of Saruhan. Suleiman died when Murad was 22, and his father became the new sultan. Selim II broke with tradition by sending only his oldest son out of the palace to govern a province, and Murad was sent to Manisa.\n\nSelim died in 1574 and was succeeded by Murad, who began his reign by having his five younger brothers strangled. His authority was undermined by harem influences – more specifically, those of his mother and later of his favorite wife Safiye Sultan, often to the detriment of Sokollu Mehmed Pasha's influence on the court. Under Selim II power had only been maintained by the genius of the powerful Grand Vizier, Mehmed Sokollu, who remained in office until his assassination in October 1579. During Murad's reign the northern borders with the Habsburg Monarchy were defended by the Bosnian governor Hasan Predojević. The reign of Murad III was marked by exhausting wars on the empire's western and eastern fronts. The Ottomans also suffered defeats in battles such as the Battle of Sisak.\n\nThe Ottomans had been at peace with the neighbouring rivaling Safavid Empire since 1555, per the Treaty of Amasya, that for some time had settled border disputes. But in 1577 Murad declared war, starting the Ottoman–Safavid War (1578–90), seeking to take advantage of the chaos in the Safavid court after the death of Shah Tahmasp I. Murad also tried to explore North America and make the ideas of colonizing America be more possible. However, he later abandoned all of these ideas after the Spanish navy responded by a naval attack on the ottoman ships trying to explore north America. Murad was influenced by viziers Lala Kara Mustafa Pasha and Sinan Pasha and disregarded the opposing counsel of Grand Vizier Sokollu. Murad also fought the Safavidsidse war would drag on for 12 years, ending with the Treaty of Constantinople (1590), which resulted in temporary significant territorial gains for the Ottomans.\n\nMurad's reign was a time of financial stress for the Ottoman state. To keep up with changing military techniques, the Ottomans trained infantrymen in the use of firearms, paying them directly from the treasury. By 1580 an influx of silver from the New World had caused high inflation and social unrest, especially among Janissaries and government officials who were paid in debased currency. Deprivation from the resulting rebellions, coupled with the pressure of over-population, was especially felt in Anatolia. Competition for positions within the government grew fierce, leading to bribery and corruption. Ottoman and Habsburg sources accuse Murad himself of accepting enormous bribes, including 20,000 ducats from a statesman in exchange for the governorship of Tripoli and Tunisia, thus outbidding a rival who had tried bribing the Grand Vizier.\n\nNumerous envoys and letters were exchanged between Elizabeth I and Sultan Murad III. In one correspondence, Murad entertained the notion that Islam and Protestantism had \"much more in common than either did with Roman Catholicism, as both rejected the worship of idols\", and argued for an alliance between England and the Ottoman Empire. To the dismay of Catholic Europe, England exported tin and lead (for cannon-casting) and amammunition the Ottoman Empire, and Elizabeth seriously discussed joint military operations with Murad III during the outbreak of war with Spain in 1585, as Francis Walsingham was lobbying for a direct Ottoman military involvement against the common Spanish enemy. This diplomacy would be continued under Murad's successor Mehmed III, by both the sultan and Safiye Sultan alike.\n\nFollowing the example of his father Selim II, Murad was the second Ottoman sultan who never went on campaign during his reign, instead spending it entirely in Constantinople. During the final years of his reign, he did not even leave Topkapı Palace. For two consecutive years he did not attend the Friday procession to the imperial mosque—an unprecedented breaking of custom. The Ottoman historian Mustafa Selaniki wrote that whenever Murad planned to go out to Friday prayer, he changed his mind after hearing of alleged plots by the Janissaries to dethrone him once he left the palace. Murad withdrew from his subjects and spent the majority of his reign keeping to the company of few people and abiding by a daily routine structured by the five daily Islamic prayers. Murad's personal physician Domenico Hierosolimitano described a typical day in the life of the sultan:\n\nMurad's sedentary lifestyle and lack of participation in military campaigns earned him the disapproval of Mustafa Âlî and Mustafa Selaniki, the major Ottoman historians who lived during his reign. Their negative portrayals of Murad influenced later historians. Both historians also accused Murad of sexual excess. Before becoming sultan, Murad had been loyal to Safiye Sultan, his Venetian-born concubine who had given him a son, Mehmed, and two daughters. His monogamy was disapproved of by his mother Nurbanu, who worried that Murad needed more sons to succeed him in case Mehmed died young. She also worried about Safiye's influence over her son and the Ottoman dynasty. Five or six years after his accession to the throne, Murad was given a pair of concubines by his sister Ismihan. Upon attempting sexual intercourse with them, he proved impotent. \"The arrow [of Murad], [despite] keeping with his created nature, for many times [and] for many days has been unable to reach at the target of union and pleasure,\" wrote Mustafa Ali. Nurbanu accused Safiyye and her retainers of causing Murad's impotence with witchcraft. Several of Safiye's servants were tortured by eunuchs in order to discover a culprit. Court physicians, working under Nurbanu's orders, eventually prepared a successful cure, but a side effect was a drastic increase in sexual appetite—by the time Murad died, he was said to have fathered over a hundred children. Nineteen of these were executed by Mehmed III when he became sultan.\n\nInfluential ladies of his court included his mother Nurbanu Sultan, his sister Ismihan Sultan, wife of grand vizier Sokollu Mehmed Pasha, and musahibes (favourites) mistress of the housekeeper Canfeda Hatun, mistress of financial affairs Raziye Hatun, and the poet Hubbi Hatun.\n\nMurad took great interest in the arts, particularly miniatures and books. He actively supported the court Society of Miniaturists, commissioning several volumes including the \"Siyer-i Nebi\", the most heavily illustrated biographical work on the life of the Islamic prophet Muhammad, the \"Book of Skills\", the \"Book of Festivities\" and the \"Book of Victories\". He had two large alabaster urns transported from Pergamon and placed on two sides of the nave in the Hagia Sophia in Constantinople and a large wax candle dressed in tin which was donated by him to the Rila monastery in Bulgaria is on display in the monastery museum.\n\nMurad also furnished the content of \"Kitabü’l-Menamat\" (\"The Book of Dreams\"), addressed to Murad's spiritual advisory, Şüca Dede. A collection of first person accounts, it tells of Murad's spiritual experiences as a Sufi disciple. Compiled from thousands of letters Murad wrote describing his dream visions, it presents a hagiographic self-portrait. Murad dreams of various activities, including being stripped naked by his father and having to sit on his lap, single-handedly killing 12,000 infidels in battle, walking on water, ascending to heaven, and producing milk from his fingers. He frequently encounters the Prophet Muhammed, and in one dream sits in the Prophet's lap and kisses his mouth.\n\nIn another letter addressed to Şüca Dede, Murad wrote \"I wish that the True Reality/God, \"Celle ve Ala\", had not created this poor servant as the descendant of the Ottomans so that I would not hear this and that, and would not worry. I wish I were of unknown pedigree. Then, I would have one single task, and could ignore the whole world.\"\n\nThe diplomatic edition of these dream letters have been recently published by Ozgen Felek in Turkish.\n\nMurad died from what is assumed to be natural causes in the Topkapı Palace and was buried in tomb next to the Hagia Sofia. In the mausoleum are 54 sarcophagus of the sultan, his wives and children that are also buried there. He is also responsible for changing the burial customs of the sultans' mothers. Murad had his mother Nurbanu buried next to her husband Selim II, making her the first concubine to share a sultan's tomb.\n\nMurad's wives are:\n\nMurad had at least twenty three sons:\n\nMurad had twenty eight daughters, of whom \nsixteen died of plague in 1597. The rest, who were married, included the following:\n\nOrhan Pamuk's historical novel \"Benim Adım Kırmızı\" (\"My Name is Red\", 1998) takes place at the court of Murad III, during nine snowy winter days of 1591, which the writer uses in order to convey the tension between East and West.\n\nThe Harem Midwife by Roberta Rich - a historical fiction set in Constantinople (1578) which follows Hannah, a midwife, who tends to many of the women in Sultan Murad III's harem.\n\n\n[aged 48]\n"}
{"id": "19990", "url": "https://en.wikipedia.org/wiki?curid=19990", "title": "Mehmed III", "text": "Mehmed III\n\nMehmed III (, \"Meḥmed-i sālis\"; ; 26 May 1566–21 December 1603) was Sultan of the Ottoman Empire from 1595 until his death in 1603.\n\nMehmed was born at the Manisa Palace in 1566, during the reign of his great-grandfather, Suleiman the Magnificent. He was the son of Şehzade Murad (later Murad III), himself the son of Şehzade Selim (later Selim II), who was the son of Sultan Suleiman and Hürrem Sultan. His mother was Safiye Sultan, an Albanian from the Dukagjin highlands. His great-grandfather died the year he was born and his grandfather became the new Sultan, Selim II. His grandfather Selim II died when Mehmed was eight, and Mehmed's father, Murad III, became Sultan in 1574. Mehmed thus became the Crown Prince until his father's death in 1595, when he was 28 years old.\n\nMehmed III remains notorious even in Ottoman history for having nineteen of his brothers and half-brothers executed to secure power. They were all strangled by his deaf-mutes.\n\nMehmed III was an idle ruler, leaving government to his mother Safiye Sultan, the valide sultan. His first major problem was the rivalry between two of his viziers, Serdar Ferhad Pasha and Koca Sinan Pasha, and their supporters. His mother and her son-in-law Damat Ibrahim Pasha supported Koca Sinan Pasha, and prevented Mehmed III from taking control of the issue himself. The issue grew to cause major disturbances by janissaries. On 7 July 1595, Mehmed III finally sacked Serdar Ferhad Pasha from the position of Grand Vizier due to his failure in Wallachia and replaced him with Sinan.\n\nThe major event of his reign was the Austro-Ottoman War in Hungary (1593–1606). Ottoman defeats in the war caused Mehmed III to take personal command of the army, the first sultan to do so since Suleiman I in 1566. Accompanied by the Sultan, the Ottomans conquered Eger in 1596. Upon hearing of the Habsburg army's approach, Mehmed wanted to dismiss the army and return to Istanbul. However, the Ottomans eventually decided to face the enemy and defeated the Habsburg and Transylvanian forces at the Battle of Keresztes (known in Turkish as the Battle of Haçova), during which the Sultan had to be dissuaded from fleeing the field halfway through the battle. Upon returning to Istanbul in victory, Mehmed told his Vezirs that he would campaign again. The next year the Venetian Bailo in Istanbul noted, \"the doctors declared that the Sultan cannot leave for war on account of his bad health, produced by excesses of eating and drinking\".\n\nIn reward for his services at the war, Cigalazade Yusuf Sinan Pasha was made Grand Vizier in 1596. However, with pressure from the court and his mother, Mehmed reinstated Damat Ibrahim Pasha to this position shortly afterwards.\n\nHowever, the victory at the Battle of Keresztes was soon set back by some important losses, including the loss of Győr () to the Austrians and the defeat of the Ottoman forces led by Hafız Ahmet Pasha by the Wallachian forces under Michael the Brave in Nikopol in 1599. In 1600, Ottoman forces under Tiryaki Hasan Pasha captured Nagykanizsa after a 40-day siege and later successfully held it against a much greater attacking force in the Siege of Nagykanizsa.\n\nAnother major event of his reign was the Jelali revolts in Anatolia. Karayazıcı Abdülhalim, a former Ottoman official, captured the city of Urfa and declared himself sultan in 1600. The rumors of his claim to the throne spread to Constantinople and Mehmed ordered the rebels to be treated harshly to dispel the rumors, among these was the execution of Hüseyin Pasha, whom Karayazıcı Abdülhalim styled as Grand Vizier. In 1601, Abdülhalim fled to the vicinity of Samsun after being defeated by the forces under Sokulluzade Hasan Pasha, the governor of Baghdad. However, his brother, Deli Hasan, killed Sokulluzade Hasan Pasha and defeated troops under the command of Hadım Hüsrev Pasha. He then marched on to Kütahya, captured and burned the city.\n\nIn 1599, the fourth year of Mehmed III's reign, Queen Elizabeth I sent a convoy of gifts to the Ottoman court. These gifts were originally intended for the sultan's predecessor, Murad III, who had died before they had arrived. Included in these gifts was a large jewel-studded clockwork organ that was assembled on the slope of the Royal Private Garden by a team of engineers including Thomas Dallam. The organ took many weeks to complete and featured dancing sculptures such as a flock of blackbirds that sung and shook their wings at the end of the music. The musical clock organ was destroyed by the succeeding Sultan Ahmed I. Also among the English gifts was a ceremonial coach, accompanied by a letter from the Queen to Mehmed's mother, Safiye Sultan. These gifts were intended to cement relations between the two countries, building on the trade agreement signed in 1581 that gave English merchants priority in the Ottoman region. Under the looming threat of Spanish military presence, England was eager to secure an alliance with the Ottomans, the two nations together having the capability to divide the power. Elizabeth's gifts arrived in a large 27-gun merchantman ship that Mehmed personally inspected, a clear display of English maritime strength that would prompt him to build up his fleet over the following years of his reign. The Anglo-Ottoman alliance would never be consummated, however, as relations between the nations grew stagnant due to anti-European sentiments reaped from the worsening Austro-Ottoman War and the deaths of Safiye Sultan's interpreter and the pro-English chief Hasan Pasha.\n\nMehmed died on 22 December 1603 at the age of 37. According to one source, the cause of his death was the distress caused by the death of his son, Şehzade Mahmud. According to another source, he died either of plague or of stroke. He was buried in Hagia Sophia Mosque. He was succeeded by his son Ahmed I as the new sultan.\n\nNone of Mehmed's consorts are listed as \"haseki sultan\" in Ottoman palace archives. Known consorts were:\n\n\n[aged 37]\n"}
{"id": "19991", "url": "https://en.wikipedia.org/wiki?curid=19991", "title": "Mustafa I", "text": "Mustafa I\n\nMustafa I (; ‎;24 June 1591 – 20 January 1639), called Mustafa the Saint (Veli Mustafa) during his second reign and often called Mustafa the Mad (Deli Mustafa) by modern historians, was the son of Mehmed III and was the Sultan of the Ottoman Empire from 1617 to 1618 and from 1622 to 1623.\n\nMustafa was born in the Manisa Palace, as the younger brother of Ahmed I (1603–1617). His mother was Halime Sultan, an Abkhazian lady.\n\nBefore 1603 it was customary for an Ottoman Sultan to have his brothers executed shortly after he gained the throne (Mustafa's father Mehmed III had executed 19 of his own brothers). But when the thirteen-year-old Ahmed I was enthroned in 1603, he spared the life of the twelve-year-old Mustafa.\n\nA factor in Mustafa's survival is the influence of Kösem Sultan (Ahmed's favorite consort), who may have wished to preempt the succession of Osman, Ahmed’s first-born son from another concubine. If Osman became Sultan, he would likely try to execute his half-brothers, the sons of Ahmed and Kösem. (This scenario later became a reality when Osman II executed his brother Mehmed in 1621.) However, the reports of foreign ambassadors suggest that Ahmed actually liked his brother. \n\nUntil Ahmed's death in 1617, Mustafa lived in the Old Palace, along with his mother, and grandmother Safiye Sultan.\n\nAhmed's death created a dilemma never before experienced by the Ottoman Empire. Multiple princes were now eligible for the Sultanate, and all of them lived in Topkapı Palace. A court faction headed by the Şeyhülislam Esad Efendi and Sofu Mehmed Pasha (who represented the Grand Vizier when he was away from Constantinople) decided to enthrone Mustafa instead of Ahmed's son Osman. Sofu Mehmed argued that Osman was too young to be enthroned without causing adverse comment among the populace. The Chief Black Eunuch Mustafa Agha objected, citing Mustafa's mental problems, but he was overruled. Mustafa's rise created a new succession principle of seniority that would last until the end of the Empire. It was the first time an Ottoman Sultan was succeeded by his brother instead of his son. His mother Halime Sultan became the Valide Sultan as well as a regent and wielded great power. Due to Mustafa's mental conditions, she acted as a regent and exercised power more directly. \n\nIt was hoped that regular social contact would improve Mustafa's mental health, but his behavior remained eccentric. He pulled off the turbans of his viziers and yanked their beards. Others observed him throwing coins to birds and fish. The Ottoman historian İbrahim Peçevi wrote \"this situation was seen by all men of state and the people, and they understood that he was psychologically disturbed.\"\n\nMustafa was never more than a tool of court cliques at the Topkapı Palace. In 1618, after a short rule, another palace faction deposed him in favour of his young nephew Osman II (1618–1622), and Mustafa was sent back to the Old Palace. The conflict between the Janissaries and Osman II presented him with a second chance. After a Janissary rebellion led to the deposition and assassination of Osman II in 1622, Mustafa was restored to the throne and held it for another year.\n\nNevertheless, according to Baki Tezcan, there is not enough evidence to properly establish that Mustafa was mentally imbalanced when he came to the throne. Mustafa \"made a number of excursions to the arsenal and the navy docks, examining various sorts of arms and taking an active interest in the munitions supply of the army and the navy.\" One of the dispatches of Baron de Sancy, the French ambassador, \"suggested that Mustafa was interested in leading the Safavid campaign himself and was entertaining the idea of wintering in Konya for that purpose.\" \n\nMoreover, one contemporary observer provides an explanation of the coup which does not mention the incapacity of Mustafa. Baron de Sancy ascribes the deposition to a political conspiracy between the grand admiral Ali Pasha and Chief Black Eunuch Mustafa Agha, who were angered by the former's removal from office upon Sultan Mustafa's accession. They may have circulated rumors of the sultan's mental instability subsequent to the coup in order to legitimize it.\n\nHe commenced his reign by executing all those who had taken any share in the murder of Sultan Osman. Hoca Ömer Efendi, the chief of the rebels, the kızlar Agha Suleiman Agha, the vizier Dilaver Pasha, the Kaim-makam Ahmed Pasha, the defterdar Baki Pasha, the segban-bashi Nasuh Agha, and the general of the janissaries Ali Agha, were cut into pieces.\n\nThe epithet \"Veli\" (meaning \"saint\") was used in reference to him during his reign.\n\nHis mental condition unimproved, Mustafa was a puppet controlled by his mother and brother-in-law, the grand vizier Kara Davud Pasha. He believed that Osman II was still alive and was seen searching for him throughout the palace, knocking on doors and crying out to his nephew to relieve him from the burden of sovereignty. \"The present emperor being a fool\" (according to English Ambassador Sir Thomas Roe), he was compared unfavorably with his predecessor. In fact, it was his mother Halime Sultan the de facto-co-ruler as Valide Sultan of the Ottoman empire. \n\nPolitical instability was generated by conflict between the Janissaries and the sipahis (Ottoman cavalry), followed by the Abaza rebellion, which occurred when the governor-general of Erzurum, Abaza Mehmed Pasha, decided to march to Istanbul to avenge the murder of Osman II. The regime tried to end the conflict by executing Kara Davud Pasha, but Abaza Mehmed continued his advance. Clerics and the new Grand Vizier (Kemankeş Kara Ali Pasha) prevailed upon Mustafa's mother to allow the deposition of her son. She agreed, on condition that Mustafa's life would be spared.\n\nThe 11-year-old Murad IV, son of Ahmed I and Kösem, was enthroned on 10 September 1623. In return for her consent to his deposition, the request of Mustafa's mother that he be spared execution was granted. Mustafa was sent along with his mother to the Eski (old) Palace.\n\nHe died at the Eski (old) Palace, Constantinople on 20 January 1639, and is buried in the courtyard of the Haghia Sophia.\n\n"}
{"id": "19992", "url": "https://en.wikipedia.org/wiki?curid=19992", "title": "Murad IV", "text": "Murad IV\n\nMurad IV (, \"Murād-ı Rābiʿ\"; 26/27 July 1612 – 8 February 1640) was the Sultan of the Ottoman Empire from 1623 to 1640, known both for restoring the authority of the state and for the brutality of his methods. Murad IV was born in Istanbul, the son of Sultan Ahmed I (r. 1603–17) and Kösem Sultan. He was brought to power by a palace conspiracy in 1623, and he succeeded his uncle Mustafa I (r. 1617–18, 1622–23). He was only 11 when he ascended the throne. His reign is most notable for the Ottoman–Safavid War (1623–39), of which the outcome would permanently part the Caucasus between the two Imperial powers for around two centuries, while it also roughly laid the foundation for the current Turkey–Iran–Iraq borders.\n\nMurad IV was for a long time under the control of his relatives and during his early years as Sultan, his mother, Kösem Sultan, essentially ruled through him. The Empire fell into anarchy; the Safavid Empire invaded Iraq almost immediately, Northern Anatolia erupted in revolts, and in 1631 the Janissaries stormed the palace and killed the Grand Vizier, among others. Murad IV feared suffering the fate of his elder brother, Osman II (1618–22), and decided to assert his power.\n\nAt the age of 16 in 1628, he had his brother-in-law (his sister Fatma Sultan's husband, who was also the former governor of Egypt), Kara Mustafa Pasha, executed for a claimed action \"against the law of God\".\n\nMurad IV tried to quell the corruption that had grown during the reigns of previous Sultans, and that had not been checked while his mother was ruling through proxy.\n\nMurad IV banned alcohol, tobacco, and coffee in Istanbul. He ordered execution for breaking this ban. He would reportedly patrol the streets and the lowest taverns of Istanbul in civilian clothes at night, policing the enforcement of his command by casting off his disguise on the spot and beheading the offender with his own hands. Rivaling the exploits of Selim the Grim, he would sit in a kiosk by the water near his Seraglio Palace and shoot arrows at any passerby or boatman who rowed too close to his imperial compound, seemingly for sport. He restored the judicial regulations by very strict punishments, including execution, he once strangled a grand vizier for the reason that the official had beaten his mother-in-law.\n\nMurad IV's reign is most notable for the Ottoman–Safavid War (1623–39) against Persia (today Iran) in which Ottoman forces managed to conquer Azerbaijan, occupying Tabriz, Hamadan, and capturing Baghdad in 1638. The Treaty of Zuhab that followed the war generally reconfirmed the borders as agreed by the Peace of Amasya, with Eastern Armenia, Eastern Georgia, Azerbaijan, and Dagestan staying Persian, while Western Armenia, and Western Georgia stayed Ottoman. Mesopotamia was irrevocably lost for the Persians. The borders fixed as a result of the war, are more or less the same as the present border line between Turkey, Iraq and Iran.\n\nDuring the siege of Baghdad in 1638, the city held out for forty days but was compelled to surrender.\n\nMurad IV himself commanded the Ottoman army in the last years of the war.\n\nWhile he was encamped in Baghdad, Murad IV is known to have met ambassadors of the Mughal Emperor Shah Jahan, Mir Zarif and Mir Baraka, who presented 1000 pieces of finely embroidered cloth and even armor. Murad IV gave them the finest weapons, saddles and Kaftans and ordered his forces to accompany the Mughals to the port of Basra, where they set sail to Thatta and finally Surat.\n\nMurad IV put emphasis on architecture and in his period many monuments were erected. The Baghdad Kiosk, built in 1635, and the Revan Kiosk, built in 1638 in Yerevan, were both built in the local styles. Some of the others include the Kavak Sarayı pavilion; the Meydanı Mosque; the Bayram Pasha Dervish Lodge, Tomb, Fountain, and Primary School; and the Şerafettin Mosque in Konya.\n\n\nVery little is known about the concubines of Murad IV, principally because he did not leave sons who survived his death to reach the throne, but many historians consider Ayşe Sultan as his only consort until the very end of Murad's seventeen-year reign, when a second Haseki appeared in the records. It is possible that Murad had only a single concubine until the advent of the second, or that he had a number of concubines but singled out only two as Haseki. Another consort of his may have been Sanavber Hatun, though certainly not of the Haseki rank, whose name can be found on the deed of a charitable foundation as \"Sanavber bint Abdülmennan\"\n\n\nMurad had three daughters:\n\nMurad IV died from cirrhosis in Istanbul at the age of 27 in 1640.\n\nRumours had circulated that on his deathbed, Murad IV ordered the execution of his mentally disabled brother, Ibrahim (reigned 1640–48), which would have meant the end of the Ottoman line. However, the order was not carried out.\n\nIn the TV series \"\", Murad IV is portrayed by Cağan Efe Ak as a child, and Metin Akdülger as Sultan.\n\n\n"}
{"id": "19994", "url": "https://en.wikipedia.org/wiki?curid=19994", "title": "Masamune Shirow", "text": "Masamune Shirow\n\nBorn in the Hyōgo Prefecture capital city of Kobe, he studied oil painting at Osaka University of Arts. While in college, he developed an interest in manga, which led him to create his own complete work, \"Black Magic\", which was published in the manga dōjinshi \"Atlas\". His work caught the eye of Seishinsha President Harumichi Aoki, who offered to publish him.\n\nThe result was best-selling manga \"Appleseed\", a full volume of densely plotted drama taking place in an ambiguous future. The story was a sensation, and won the 1986 Seiun Award for Best Manga. After a professional reprint of \"Black Magic\" and a second volume of \"Appleseed\", he released \"Dominion\" in 1986. Two more volumes of \"Appleseed\" followed before he began work on \"Ghost in the Shell\".\n\nIn 2007, he collaborated again with Production I.G to co-create the original concept for the anime television series \"Shinreigari/Ghost Hound\", Production I.G's 20th anniversary project. A further original collaboration with Production I.G began airing in April 2008, titled \"Real Drive\".\n\n\nA substantial amount of Shirow's work has been released in art book or poster book format. The following is an incomplete list.\n\n\n\"Galgrease\" (published in \"Uppers Magazine\", 2002) is the collected name of several erotic manga and poster books by Shirow. The name comes from the fact that the women depicted often look \"greased\".\n\nThe first series of \"Galgrease\" booklets included four issues each in the following settings:\n\n\nThe second series included another run of 12 booklets in the following worlds:\n\n\nAfter each regular series, there were one or more bonus poster books that revisited the existing characters and settings.\n\n\nMain source:\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "19995", "url": "https://en.wikipedia.org/wiki?curid=19995", "title": "Musical saw", "text": "Musical saw\n\nA musical saw, also called a singing saw, is a hand saw used as a musical instrument. Capable of continuous glissando (portamento), the sound creates an ethereal tone, very similar to the theremin. The musical saw is classified as a plaque friction idiophone with direct friction (132.22) under the Hornbostel-Sachs system of musical instrument classification.\n\nThe saw is generally played seated with the handle squeezed between the legs, and the far end held with one hand. Some sawists play standing, either with the handle between the knees and the blade sticking out in front of them. The saw is usually played with the serrated edge, or \"teeth\", facing the body, though some players face them away. Some saw players file down the teeth which makes no discernable difference to the sound. Many—especially professional—saw players use a handle, called a Cheat, at the tip of the saw for easier bending and higher virtuosity.\n\nTo sound a note, a sawist first bends the blade into an S-curve. The parts of the blade that are curved are damped from vibration, and do not sound. At the center of the S-curve a section of the blade remains relatively flat. This section, the \"sweet spot\", can vibrate across the width of the blade, producing a distinct pitch: the wider the section of blade, the lower the sound. Sound is usually created by drawing a bow across the back edge of the saw at the sweet spot, or sometimes by striking the sweet spot with a mallet.\n\nThe sawist controls the pitch by adjusting the S-curve, making the sweet spot travel up the blade (toward a thinner width) for a higher pitch, or toward the handle for a lower pitch. Harmonics can be created by playing at varying distances on either side of the sweet spot. Sawists can add vibrato by shaking one of their legs or by wobbling the hand that holds the tip of the blade. Once a sound is produced, it will sustain for quite a while, and can be carried through several notes of a phrase.\n\nOn occasion the musical saw is called for in orchestral music, but orchestral percussionists are seldom also sawists. If a note outside of the saw's range is called for, an electric guitar with a slide can be substituted.\n\nSawists often use standard wood-cutting saws, although special musical saws are also made. As compared with wood-cutting saws, the blades of musical saws are generally wider, for range, and longer, for finer control. They do not have set or sharpened teeth, and may have grain running parallel to the back edge of the saw, rather than parallel to the teeth. Some musical saws are made with thinner metal, to increase flexibility, while others are made thicker, for a richer tone, longer sustain, and stronger harmonics.\n\nA typical musical saw is wide at the handle end and wide at the tip. Such a saw will generally produce about two octaves, regardless of length. A bass saw may be over at the handle and produce about two-and-a-half octaves. There are also musical saws with 3–4 octaves range, and new improvements have resulted in as much as 5 octaves note range. Two-person saws, also called \"misery whips\", can also be played, though with less virtuosity, and they produce an octave or less of range.\n\nMost sawists use cello or violin bows, using violin rosin, but some may use improvised home-made bows, such as a wooden dowel.\n\nMusical saws have been produced for over a century, primarily in the United States, but also in Scandinavia, Germany, France (Lame sonore) and Asia.\n\nIn the early 1900s, there were at least ten companies in the United States manufacturing musical saws. These saws ranged from the familiar steel variety to gold-plated masterpieces worth hundreds of dollars. However, with the start of World War II the demand for metals made the manufacture of saws too expensive and many of these companies went out of business. By the year 2000, only three companies in the United States—Mussehl & Westphal, Charlie Blacklock, and Wentworth—were making saws. In 2012, a company called Index Drums started producing a saw that had a built-in transducer in the handle, called the \"JackSaw\".\n\nOutside the United States, makers of musical saws include Bahco, makers of the limited edition Stradivarius, Alexis in France, Feldmann and Stövesandt in Germany, Music Blade in Greece and Thomas Flinn & Company in the United Kingdom, based in Sheffield, who produce three different sized musical saws, as well as accessories.\n\nThe International Musical Saw Association (IMSA) produces an annual International Musical Saw Festival (including a \"Saw-Off\" competition) every August in Santa Cruz and Felton, California. An International Musical Saw Festival is held every other summer in New York City, produced by Natalia Paruz. Paruz also produced a musical saw festival in Israel. There are also annual saw festivals in Japan and China.\n\nA Guinness World Record for the largest musical-saw ensemble was established July 18, 2009, at the annual NYC Musical Saw Festival. Organized by Paruz, 53 musical saw players performed together.\n\nIn 2011 a World Championship took place in Jelenia Góra/Poland. Winners: 1. Gladys Hulot (France), 2. Katharina Micada (Germany), 3. Tom Fink (Germany).\n\nThis is a list of people notable for playing the musical saw. \n\n\nGerman actress and singer Marlene Dietrich, who lived and worked in the United States for a long time, is probably the most well known musical saw player. When she studied the violin for one year in Weimar in her early twenties, her musical skills were already evident. Some years later she learned to play the musical saw while she was shooting the film \"Café Electric\" in Vienna in 1927. Her colleague, the Bavarian actor and musician Igo Sym told her how to play. In the shooting breaks and at weekends both performed romantic duets, he at the piano and she at the musical saw.\n\nSym gave his saw to her as a farewell gift. The following words are engraved on the saw: \"Now Suidy is gone / the sun d’ont [sic!] / shine… / Igo / Vienna 1927\"\nShe took the saw with her, when she left for Hollywood in 1929 and played there in the following years at film sets and Hollywood parties.\nWhen she participated at the United Service Organizations (USO) shows for the US troops in 1944, she also played on the saw. Some of these shows were broadcast on radio, so there exist two rare recordings of her saw playing, embedded in entertaining interviews. 1. Aloha Oe 2. other song\n\n\nBeginning from the early 1920s composers of both contemporary and popular music wrote for the musical saw.\nProbably the first was Dmitri Shostakovich. He included the musical saw, e.g., in the film music for \"The New Babylon\" (1929), in \"The Nose\" (1928), and in \"Lady Macbeth of the Mtsensk District\" (1934).\nShostakovich and other composers of his time used the term \"Flexaton\" to mark the musical saw. \"Flexaton\" just means \"to flex a tone\"—the saw is flexed to change the pitch. Unfortunately, there exists another instrument called Flexatone, so there has been confusion for a long time. Aram Khachaturian, who knew Shostakovich's music included the musical saw in his Piano Concerto (1936) in the second movement. Another composer was the Swiss Arthur Honegger, who included the saw in his opera \"Antigone\" in 1924 . \nThe Romanian composer George Enescu used the musical saw at the end of the second act of his opera \"Œdipe\" (1931) to show in an extensive glissando—which begins with the mezzo-soprano and is continued by the saw—the death and ascension of the sphinx killed by Oedipus.\n\nThe Italian composer Giacinto Scelsi wrote a part for the saw in his quarter-tone piece \"Quattro pezzi per orchestra\" (1959). German composer Hans Werner Henze took the saw to characterize the mean hero of his tragical opera \"Elegy for young lovers\" (1961).\n\nOther composers were Krysztof Penderecki with \"Fluorescences\" (1961), \"De natura sonoris Nr. 2\" (1971) and the opera \"Ubu Rex\" (1990), Bernd Alois Zimmermann with \"Stille und Umkehr\" (1970), George Crumb with \"Ancient voices of children\" (1970), John Corigliano with \"The Mannheim Rocket\" (2001).\n\nChaya Czernowin used the saw in her opera \"PNIMA...Ins Innere\" (2000) to represent the character of the grandfather, who is traumatized by the Holocaust.\n\nThere are further Leif Segerstam, Hans Zender (orchestration of \"5 préludes\" by Claude Debussy), Franz Schreker (opera \"Christophorus\"), and Oscar Strasnoy (opera \"Le bal\").\n\nRussian composer Lera Auerbach wrote for the saw in her ballet \"The Little Mermaid\" (2005), in her symphonic poem \"Dreams and Whispers of Poseidon\" (2005), in her oratorio \"Requiem Dresden – Ode to Peace\" (2012), in her Piano Concerto No.1 (2015), in her comic oratorio \"The Infant Minstrel and His Peculiar Menagerie\" (2016) and in her violin concerto Nr.4 \"NyX – Fractured dreams\" (2017).\n\nCanadian composer Robert Minden has written extensively for the musical saw. Michael A. Levine composed \"Divination By Mirrors\" for musical saw soloist and two string ensembles tuned a quarter tone apart, taking advantage of the saws ability to play in both tunings.\n\nOther composers for chamber music with musical saw are Jonathan Rutherford (\"An Intake of Breath\"), Dana Wilson (\"Whispers from Another Time\"), Heinrich Gattermeyer (\"Elegie für Singende Säge, Cembalo (oder Klavier)\", Vito Zuraj (\"Musica di camera\" (2001)) and Britta-Maria Bernhard (\"Tranquillo\")\n\n\n"}
{"id": "19996", "url": "https://en.wikipedia.org/wiki?curid=19996", "title": "MIDI", "text": "MIDI\n\nMIDI (; short for Musical Instrument Digital Interface) is a technical standard that describes a communications protocol, digital interface, and electrical connectors that connect a wide variety of electronic musical instruments, computers, and related audio devices. A single MIDI link can carry up to sixteen channels of information, each of which can be routed to a separate device.\n\nMIDI carries event messages that specify notation, pitch, velocity, vibrato, panning, and clock signals (which set tempo). For example, a MIDI keyboard or other controller might trigger a sound module to generate sound produced by a keyboard amplifier. MIDI data can be transferred via midi cable, or recorded to a sequencer to be edited or played back. \n\nA file format that stores and exchanges the data is also defined. Advantages of MIDI include small file size, ease of modification and manipulation and a wide choice of electronic instruments and synthesizer or digitally-sampled sounds. \n\nPrior to the development of MIDI, electronic musical instruments from different manufacturers could generally not communicate with each other. With MIDI, any MIDI-compatible keyboard (or other controller device) can be connected to any other MIDI-compatible sequencer, sound module, drum machine, synthesizer, or computer, even if they are made by different manufacturers.\n\nMIDI technology was standardized in 1983 by a panel of music industry representatives, and is maintained by the MIDI Manufacturers Association (MMA). All official MIDI standards are jointly developed and published by the MMA in Los Angeles, and the MIDI Committee of the Association of Musical Electronics Industry (AMEI) in Tokyo. In 2016, the MMA established the MIDI Association (TMA) to support a global community of people who work, play, or create with MIDI.\n\nIn the early 1980s, there was no standardized means of synchronizing electronic musical instruments manufactured by different companies. Manufacturers had their own proprietary standards to synchronize instruments, such as CV/gate and Digital Control Bus (DCB).\n\nRoland founder Ikutaro Kakehashi felt the lack of standardization was limiting the growth of the electronic music industry. In June 1981, he proposed developing a standard to Oberheim Electronics founder Tom Oberheim, who had developed his own propriety interface, the Oberheim System. Kakehashi felt the system was too cumbersome, and spoke to Sequential Circuits president Dave Smith about creating a simpler, cheaper alternative. While Smith discussed the concept with American companies, Kakehashi discussed it with Japanese companies Yamaha, Korg and Kawai. Representatives from all companies met to discuss the idea in October.\n\nUsing Roland's DCB as a basis, Smith and Sequential Circuits engineer Chet Wood devised a universal synthesizer interface to allow communication between equipment from different manufacturers. Smith proposed this standard at the Audio Engineering Society show in November 1981. The standard was discussed and modified by representatives of Roland, Yamaha, Korg, Kawai, and Sequential Circuits. Kakehashi favored the name Universal Musical Interface (UMI), pronounced \"you-me\", but Smith felt this was \"a little corny\". However, he liked the use of \"instrument\" instead of \"synthesizer\", and proposed the name Musical Instrument Digital Interface (MIDI). \n\nMoog Music founder Robert Moog announced MIDI in the October 1982 issue of \"Keyboard\". At the 1983 Winter NAMM Show, Smith demonstrated a MIDI connection between Prophet 600 and Roland JP-6 synthesizers. The MIDI specification was published in August 1983. The MIDI standard was unveiled by Kakehashi and Smith, who received Technical Grammy Awards in 2013 for their work.\n\nThe first MIDI synthesizers were the Roland Jupiter-6 and the Prophet 600, both released in 1982. 1983 saw the release of the first MIDI drum machine, the Roland TR-909, and the first MIDI sequencer, the Roland MSQ-700. The first computers to support MIDI were the NEC PC-88 and PC-98 in 1982, and the MSX (Yamaha CX5M) released in 1983.\n\nMIDI's appeal was originally limited to professional musicians and record producers who wanted to use electronic instruments in the production of popular music. The standard allowed different instruments to communicate with each other and with computers, and this spurred a rapid expansion of the sales and production of electronic instruments and music software. This interoperability allowed one device to be controlled from another, which reduced the amount of hardware musicians needed. MIDI's introduction coincided with the dawn of the personal computer era and the introduction of samplers and digital synthesizers. The creative possibilities brought about by MIDI technology are credited for helping revive the music industry in the 1980s.\n\nMIDI introduced capabilities that transformed the way many musicians work. MIDI sequencing makes it possible for a user with no notation skills to build complex arrangements. A musical act with as few as one or two members, each operating multiple MIDI-enabled devices, can deliver a performance similar to that of a larger group of musicians. The expense of hiring outside musicians for a project can be reduced or eliminated, and complex productions can be realized on a system as small as a synthesizer with integrated keyboard and sequencer.\n\nMIDI also helped establish home recording. By performing preproduction in a home environment, an artist can reduce recording costs by arriving at a recording studio with a partially completed song.\n\nEducational technology enabled by MIDI has transformed music education.\n\nMIDI was invented so that electronic or digital musical instruments could communicate with each other and so that one instrument can control another. For example, a MIDI-compatible sequencer can trigger beats produced by a drum sound module. Analog synthesizers that have no digital component and were built prior to MIDI's development can be retrofit with kits that convert MIDI messages into analog control voltages. When a note is played on a MIDI instrument, it generates a digital signal that can be used to trigger a note on another instrument. The capability for remote control allows full-sized instruments to be replaced with smaller sound modules, and allows musicians to combine instruments to achieve a fuller sound, or to create combinations of synthesized instrument sounds, such as acoustic piano and strings. MIDI also enables other instrument parameters (volume, effects, etc.) to be controlled remotely.\n\nSynthesizers and samplers contain various tools for shaping an electronic or digital sound. Filters adjust timbre (bass and treble), and envelopes automate the way a sound evolves over time after a note is triggered. The frequency of a filter and the envelope attack, or the time it takes for a sound to reach its maximum level, are examples of synthesizer parameters, and can be controlled remotely through MIDI. Effects devices have different parameters, such as delay feedback or reverb time. When a MIDI continuous controller number (CCN) is assigned to one of these parameters, the device responds to any messages it receives that are identified by that number. Controls such as knobs, switches, and pedals can be used to send these messages. A set of adjusted parameters can be saved to a device's internal memory as a \"patch\", and these patches can be remotely selected by MIDI program changes. The MIDI standard allows selection of 128 different programs, but devices can provide more by arranging their patches into banks of 128 programs each, and combining a program change message with a bank select message.\n\nMIDI events can be sequenced with computer software, or in specialized hardware music workstations. Many digital audio workstations (DAWs) are specifically designed to work with MIDI as an integral component. MIDI piano rolls have been developed in many DAWs so that the recorded MIDI messages can be extensively modified. These tools allow composers to audition and edit their work much more quickly and efficiently than did older solutions, such as multitrack recording.\n\nBecause MIDI is a set of commands that create sound, MIDI sequences can be manipulated in ways that prerecorded audio cannot. It is possible to change the key, instrumentation or tempo of a MIDI arrangement, and to reorder its individual sections. The ability to compose ideas and quickly hear them played back enables composers to experiment. Algorithmic composition programs provide computer-generated performances that can be used as song ideas or accompaniment.\n\nSome composers may take advantage of MIDI 1.0 and General MIDI (GM) technology to allow musical data files to be shared among various electronic instruments by using a standard, portable set of commands and parameters. The data composed via the sequenced MIDI recordings can be saved as a Standard MIDI File (SMF), digitally distributed, and reproduced by any computer or electronic instrument that also adheres to the same MIDI, GM, and SMF standards. MIDI data files are much smaller than recorded audio files.\n\nAt the time of MIDI's introduction, the computing industry was mainly devoted to mainframe computers as personal computers were not commonly owned. The personal computer market stabilized at the same time that MIDI appeared, and computers became a viable option for music production. It was not until the advent of MIDI in 1983 that general-purpose computers started to play a role in mainstream music production.\n\nIn the years immediately after the 1983 ratification of the MIDI specification, MIDI features were adapted to several early computer platforms. NEC's PC-88 and PC-98 began supporting MIDI as early as 1982. Yamaha modules introduced MIDI support and sequencing to the MSX in 1983.\n\nThe spread of MIDI on personal computers was largely facilitated by Roland Corporation's MPU-401, released in 1984, as the first MIDI-equipped PC sound card, capable of MIDI sound processing and sequencing. After Roland sold MPU sound chips to other sound card manufacturers, it established a universal standard MIDI-to-PC interface. The widespread adoption of MIDI led to computer-based MIDI software being developed. Soon after, a number of platforms began supporting MIDI, including the Apple II+, IIe and Macintosh, Commodore 64 and Amiga, Atari ST, Acorn Archimedes, and PC DOS. The Macintosh was a favorite among US musicians, as it was marketed at a competitive price, and it took several years for PC systems to catch up with its efficiency and graphical interface.\n\nThe Atari ST was favored in Europe, where Macintoshes were more expensive. The Apple IIGS used a digital sound chip designed for the Ensoniq Mirage synthesizer, and later models used a custom sound system and upgraded processors, which drove other companies to improve their own offerings. The Atari ST was favored for its MIDI ports that were built directly into the computer. Most music software in MIDI's first decade was published for either the Apple or the Atari. By the time of Windows 3.0's 1990 release, PCs had caught up in processing power and had acquired a graphical interface, and software titles began to see release on multiple platforms.\n\nThe Standard MIDI File (SMF) is a file format that provides a standardized way for music sequences to be saved, transported, and opened in other systems. The compact size of these files led to their widespread use in computers, mobile phone ringtones, webpage authoring and musical greeting cards. These files are intended for universal use, and include such information as note values, timing and track names. Lyrics may be included as metadata, and can be displayed by karaoke machines. The SMF specification was developed and is maintained by the MMA.\n\nSMFs are created as an export format of software sequencers or hardware workstations. They organize MIDI messages into one or more parallel tracks, and timestamp the events so that they can be played back in sequence. A header contains the arrangement's track count, tempo and which of three SMF formats the file is in. A type 0 file contains the entire performance, merged onto a single track, while type 1 files may contain any number of tracks that are performed in synchrony. Type 2 files are rarely used and store multiple arrangements, with each arrangement having its own track and intended to be played in sequence.\nMicrosoft Windows bundles SMFs together with Downloadable Sounds (DLS) in a Resource Interchange File Format (RIFF) wrapper, as RMID files with a codice_1 extension. RIFF-RMID has been deprecated in favor of Extensible Music Files (XMF).\n\nA MIDI file is not an audio recording. Rather, it is a set of instructions \"–\" for example, for pitch or tempo \"–\" and can use a thousand times less disk space than the equivalent recorded audio. This made MIDI file arrangements an attractive way to share music, before the advent of broadband internet access and multi-gigabyte hard drives. Licensed MIDI files on floppy disks were commonly available in stores in Europe and Japan during the 1990s. The major drawback to this is the wide variation in quality of users' audio cards, and in the actual audio contained as samples or synthesized sound in the card that the MIDI data only refers to symbolically. There is no standardization of how symbols are expressed. Even a sound card that contains high-quality sampled sounds can have inconsistent quality from one sampled instrument to another, while different model cards have no guarantee of consistent sound of the same instrument. Early budget-priced cards, such as the AdLib and the Sound Blaster and its compatibles, used a stripped-down version of Yamaha's frequency modulation synthesis (FM synthesis) technology played back through low-quality digital-to-analog converters. The low-fidelity reproduction of these ubiquitous cards was often assumed to somehow be a property of MIDI itself. This created a perception of MIDI as low-quality audio, while in reality MIDI itself contains no sound, and the quality of its playback depends entirely on the quality of the sound-producing device (and of samples in the device).\n\nThe main advantage of the personal computer in a MIDI system is that it can serve a number of different purposes, depending on the software that is loaded. Multitasking allows simultaneous operation of programs that may be able to share data with each other.\n\nSequencing software provides a number of benefits to a composer or arranger. It allows recorded MIDI to be manipulated using standard computer editing features such as cut, copy and paste and drag and drop. Keyboard shortcuts can be used to streamline workflow, and editing functions are often selectable via MIDI commands. The sequencer allows each channel to be set to play a different sound, and gives a graphical overview of the arrangement. A variety of editing tools are made available, including a notation display that can be used to create printed parts for musicians. Tools such as looping, quantization, randomization, and transposition simplify the arranging process.\n\nBeat creation is simplified, and groove templates can be used to duplicate another track's rhythmic feel. Realistic expression can be added through the manipulation of real-time controllers. Mixing can be performed, and MIDI can be synchronized with recorded audio and video tracks. Work can be saved, and transported between different computers or studios.\n\nSequencers may take alternate forms, such as drum pattern editors that allow users to create beats by clicking on pattern grids, and loop sequencers such as ACID Pro, which allow MIDI to be combined with prerecorded audio loops whose tempos and keys are matched to each other. Cue list sequencing is used to trigger dialogue, sound effect, and music cues in stage and broadcast production.\n\nWith MIDI, notes played on a keyboard can automatically be transcribed to sheet music. Scorewriting software typically lacks advanced sequencing tools, and is optimized for the creation of a neat, professional printout designed for live instrumentalists. These programs provide support for dynamics and expression markings, chord and lyric display, and complex score styles. Software is available that can print scores in braille.\n\nSmartScore software can produce MIDI files from scanned sheet music. Other notation programs include Finale, Encore, Sibelius, MuseScore and Dorico.\n\nPatch editors allow users to program their equipment through the computer interface. These became essential with the appearance of complex synthesizers such as the Yamaha FS1R, which contained several thousand programmable parameters, but had an interface that consisted of fifteen tiny buttons, four knobs and a small LCD. Digital instruments typically discourage users from experimentation, due to their lack of the feedback and direct control that switches and knobs would provide, but patch editors give owners of hardware instruments and effects devices the same editing functionality that is available to users of software synthesizers. Some editors are designed for a specific instrument or effects device, while other, \"universal\" editors support a variety of equipment, and ideally can control the parameters of every device in a setup through the use of System Exclusive commands.\n\nPatch librarians have the specialized function of organizing the sounds in a collection of equipment, and allow transmission of entire banks of sounds between an instrument and a computer. This allows the user to augment the device's limited patch storage with a computer's much greater disk capacity, and to share custom patches with other owners of the same instrument. Universal editor/librarians that combine the two functions were once common, and included Opcode Systems' Galaxy and eMagic's SoundDiver. These programs have been largely abandoned with the trend toward computer-based synthesis, although Mark of the Unicorn's (MOTU)'s Unisyn and Sound Quest's Midi Quest remain available. Native Instruments' Kore was an effort to bring the editor/librarian concept into the age of software instruments.\n\nPrograms that can dynamically generate accompaniment tracks are called \"auto-accompaniment\" programs. These create a full band arrangement in a style that the user selects, and send the result to a MIDI sound generating device for playback. The generated tracks can be used as educational or practice tools, as accompaniment for live performances, or as a songwriting aid.\n\nComputers can use software to generate sounds, which are then passed through a digital-to-analog converter (DAC) to a power amplifier and loudspeaker system. The number of sounds that can be played simultaneously (the polyphony) is dependent on the power of the computer's CPU, as are the sample rate and bit depth of playback, which directly affect the quality of the sound. Synthesizers implemented in software are subject to timing issues that are not present with hardware instruments, whose dedicated operating systems are not subject to interruption from background tasks as desktop operating systems are. These timing issues can cause synchronization problems, and clicks and pops when sample playback is interrupted. Software synthesizers also exhibit a noticeable delay known as latency in their sound generation, because computers use an audio buffer that delays playback and disrupts MIDI timing.\n\nSoftware synthesis' roots go back as far as the 1950s, when Max Mathews of Bell Labs wrote the MUSIC-N programming language, which was capable of non-real-time sound generation. The first synthesizer to run directly on a host computer's CPU was Reality, by Dave Smith's Seer Systems, which achieved a low latency through tight driver integration, and therefore could run only on Creative Labs soundcards. Some systems use dedicated hardware to reduce the load on the host CPU, as with Symbolic Sound Corporation's Kyma System, and the Creamware/Sonic Core Pulsar/SCOPE systems, which power an entire recording studio's worth of instruments, effect units, and mixers.\n\nThe ability to construct full MIDI arrangements entirely in computer software allows a composer to render a finalized result directly as an audio file.\n\nEarly PC games were distributed on floppy disks, and the small size of MIDI files made them a viable means of providing soundtracks. Games of the DOS and early Windows eras typically required compatibility with either Ad Lib or Sound Blaster audio cards. These cards used FM synthesis, which generates sound through modulation of sine waves. John Chowning, the technique's pioneer, theorized that the technology would be capable of accurate recreation of any sound if enough sine waves were used, but budget computer audio cards performed FM synthesis with only two sine waves. Combined with the cards' 8-bit audio, this resulted in a sound described as \"artificial\" and \"primitive\".\n\nWavetable daughterboards that were later available provided audio samples that could be used in place of the FM sound. These were expensive, but often used the sounds from respected MIDI instruments such as the E-mu Proteus. The computer industry moved in the mid-1990s toward wavetable-based soundcards with 16-bit playback, but standardized on a 2MB ROM, a space too small in which to fit good-quality samples of 128 instruments plus drum kits. Some manufacturers used 12-bit samples, and padded those to 16 bits.\n\nMIDI has been adopted as a control protocol in a number of non-musical applications. MIDI Show Control uses MIDI commands to direct stage lighting systems and to trigger cued events in theatrical productions. VJs and turntablists use it to cue clips, and to synchronize equipment, and recording systems use it for synchronization and automation. Apple Motion allows control of animation parameters through MIDI. The 1987 first-person shooter game \"MIDI Maze\" and the 1990 Atari ST computer puzzle game \"Oxyd\" used MIDI to network computers together, and kits are available that allow MIDI control over home lighting and appliances.\n\nDespite its association with music devices, MIDI can control any electronic or digital device that can read and process a MIDI command. The receiving device or object would require a General MIDI processor, however in this instance, the program changes would trigger a function on that device rather than notes from a MIDI instrument's controller. Each function can be set to a timer (also controlled by MIDI) or other condition or trigger determined by the device's creator.\n\nThe cables terminate in a 180° five-pin DIN connector. Standard applications use only three of the five conductors: a ground wire, and a balanced pair of conductors that carry a +5 volt signal. This connector configuration can only carry messages in one direction, so a second cable is necessary for two-way communication. Some proprietary applications, such as phantom-powered footswitch controllers, use the spare pins for direct current (DC) power transmission.\n\nOpto-isolators keep MIDI devices electrically separated from their connectors, which prevents the occurrence of ground loops and protects equipment from voltage spikes. There is no error detection capability in MIDI, so the maximum cable length is set at 15 meters (50 feet) to limit interference.\n\nMost devices do not copy messages from their input to their output port. A third type of port, the \"thru\" port, emits a copy of everything received at the input port, allowing data to be forwarded to another instrument in a \"daisy chain\" arrangement. Not all devices contain thru ports, and devices that lack the ability to generate MIDI data, such as effects units and sound modules, may not include out ports.\n\nEach device in a daisy chain adds delay to the system. This is avoided with a MIDI thru box, which contains several outputs that provide an exact copy of the box's input signal. A MIDI merger is able to combine the input from multiple devices into a single stream, and allows multiple controllers to be connected to a single device. A MIDI switcher allows switching between multiple devices, and eliminates the need to physically repatch cables. MIDI patch bays combine all of these functions. They contain multiple inputs and outputs, and allow any combination of input channels to be routed to any combination of output channels. Routing setups can be created using computer software, stored in memory, and selected by MIDI program change commands. This enables the devices to function as standalone MIDI routers in situations where no computer is present. MIDI patch bays also clean up any skewing of MIDI data bits that occurs at the input stage.\n\nMIDI data processors are used for utility tasks and special effects. These include MIDI filters, which remove unwanted MIDI data from the stream, and MIDI delays, effects that send a repeated copy of the input data at a set time.\n\nA computer MIDI interface's main function is to match clock speeds between the MIDI device and the computer. Some computer sound cards include a standard MIDI connector, whereas others connect by any of various means that include the D-subminiature DA-15 game port, USB, FireWire, Ethernet or a proprietary connection. The increasing use of USB connectors in the 2000s has led to the availability of MIDI-to-USB data interfaces that can transfer MIDI channels to USB-equipped computers. Some MIDI keyboard controllers are equipped with USB jacks, and can be plugged into computers that run music software.\n\nMIDI's serial transmission leads to timing problems. A three-byte MIDI message requires nearly 1 millisecond for transmission. Because MIDI is serial, it can only send one event at a time. If an event is sent on two channels at once, the event on the higher-numbered channel cannot transmit until the first one is finished, and so is delayed by 1ms. If an event is sent on all channels at the same time, the highest-numbered channel's transmission is delayed by as much as 16ms. This contributed to the rise of MIDI interfaces with multiple in- and out-ports, because timing improves when events are spread between multiple ports as opposed to multiple channels on the same port. The term \"MIDI slop\" refers to audible timing errors that result when MIDI transmission is delayed.\n\nThere are two types of MIDI controllers: performance controllers that generate notes and are used to perform music, and controllers that may not send notes, but transmit other types of real-time events. Many devices are some combination of the two types.\n\nKeyboards are by far the most common type of MIDI controller. MIDI was designed with keyboards in mind, and any controller that is not a keyboard is considered an \"alternative\" controller. This was seen as a limitation by composers who were not interested in keyboard-based music, but the standard proved flexible, and MIDI compatibility was introduced to other types of controllers, including guitars, stringed and wind instruments, drums and specialized and experimental controllers. Other controllers include drum controllers and wind controllers, which can emulate the playing of drum kit and wind instruments, respectively.\n\nSoftware synthesizers offer great power and versatility, but some players feel that division of attention between a MIDI keyboard and a computer keyboard and mouse robs some of the immediacy from the playing experience. Devices dedicated to real-time MIDI control provide an ergonomic benefit, and can provide a greater sense of connection with the instrument than an interface that is accessed through a mouse or a push-button digital menu. Controllers may be general-purpose devices that are designed to work with a variety of equipment, or they may be designed to work with a specific piece of software. Examples of the latter include Akai's APC40 controller for Ableton Live, and Korg's MS-20ic controller that is a reproduction of their MS-20 analog synthesizer. The MS-20ic controller includes patch cables that can be used to control signal routing in their virtual reproduction of the MS-20 synthesizer, and can also control third-party devices.\n\nA MIDI instrument contains ports to send and receive MIDI signals, a CPU to process those signals, an interface that allows user programming, audio circuitry to generate sound, and controllers. The operating system and factory sounds are often stored in a Read-only memory (ROM) unit.\n\nA MIDI instrument can also be a stand-alone module (without a piano style keyboard) consisting of a General MIDI soundboard (GM, GS and XG), onboard editing, including transposing/pitch changes, MIDI instrument changes and adjusting volume, pan, reverb levels and other MIDI controllers. Typically, the MIDI Module includes a large screen, so the user can view information for the currently selected function. Features can include scrolling lyrics, usually embedded in a MIDI file or karaoke MIDI, playlists, song library and editing screens. Some MIDI Modules include a Harmonizer and the ability to playback and transpose MP3 audio files.\n\nSynthesizers may employ any of a variety of sound generation techniques. They may include an integrated keyboard, or may exist as \"sound modules\" or \"expanders\" that generate sounds when triggered by an external controller, such as a MIDI keyboard. Sound modules are typically designed to be mounted in a 19-inch rack. Manufacturers commonly produce a synthesizer in both standalone and rack-mounted versions, and often offer the keyboard version in a variety of sizes.\n\nA sampler can record and digitize audio, store it in random-access memory (RAM), and play it back. Samplers typically allow a user to edit a sample and save it to a hard disk, apply effects to it, and shape it with the same tools that synthesizers use. They also may be available in either keyboard or rack-mounted form. Instruments that generate sounds through sample playback, but have no recording capabilities, are known as \"ROMplers\".\n\nSamplers did not become established as viable MIDI instruments as quickly as synthesizers did, due to the expense of memory and processing power at the time. The first low-cost MIDI sampler was the Ensoniq Mirage, introduced in 1984. MIDI samplers are typically limited by displays that are too small to use to edit sampled waveforms, although some can be connected to a computer monitor.\n\nDrum machines typically are sample playback devices that specialize in drum and percussion sounds. They commonly contain a sequencer that allows the creation of drum patterns, and allows them to be arranged into a song. There often are multiple audio outputs, so that each sound or group of sounds can be routed to a separate output. The individual drum voices may be playable from another MIDI instrument, or from a sequencer.\n\nSequencer technology predates MIDI. Analog sequencers use CV/Gate signals to control pre-MIDI analog synthesizers. MIDI sequencers typically are operated by transport features modeled after those of tape decks. They are capable of recording MIDI performances, and arranging them into individual tracks along a multitrack recording concept. Music workstations combine controller keyboards with an internal sound generator and a sequencer. These can be used to build complete arrangements and play them back using their own internal sounds, and function as self-contained music production studios. They commonly include file storage and transfer capabilities.\n\nSome effects units can be remotely controlled via MIDI. For example, the Eventide H3000 Ultra-harmonizer allows such extensive MIDI control that it is playable as a synthesizer.\n\nMIDI messages are made up of 8-bit \"words\" (commonly called \"bytes\") that are transmitted serially at a rate of 31.25 kbit/s. This rate was chosen because it is an exact division of 1 MHz, the operational speed of many early microprocessors. The first bit of each word identifies whether the word is a status byte or a data byte, and is followed by seven bits of information. A start bit and a stop bit are added to each byte for framing purposes, so a MIDI byte requires ten bits for transmission.\n\nA MIDI link can carry sixteen independent channels of information. The channels are numbered 1–16, but their actual corresponding binary encoding is 0–15. A device can be configured to only listen to specific channels and to ignore the messages sent on other channels (\"Omni Off\" mode), or it can listen to all channels, effectively ignoring the channel address (\"Omni On\"). An individual device may be monophonic (the start of a new \"note-on\" MIDI command implies the termination of the previous note), or polyphonic (multiple notes may be sounding at once, until the polyphony limit of the instrument is reached, or the notes reach the end of their decay envelope, or explicit \"note-off\" MIDI commands are received). Receiving devices can typically be set to all four combinations of \"omni off/on\" versus \"mono/poly\" modes.\n\nA MIDI message is an instruction that controls some aspect of the receiving device. A MIDI message consists of a status byte, which indicates the type of the message, followed by up to two data bytes that contain the parameters. MIDI messages can be \"channel messages\" sent on only one of the 16 channels and monitored only by devices on that channel, or \"system messages\" that all devices receive. Each receiving device ignores data not relevant to its function. There are five types of message: Channel Voice, Channel Mode, System Common, System Real-Time, and System Exclusive.\n\nChannel Voice messages transmit real-time performance data over a single channel. Examples include \"note-on\" messages which contain a MIDI note number that specifies the note's pitch, a velocity value that indicates how forcefully the note was played, and the channel number; \"note-off\" messages that end a note; program change messages that change a device's patch; and control changes that allow adjustment of an instrument's parameters. MIDI notes are numbered from 0 to 127 assigned to C-1 to G9. This corresponds to a range of 8.175798916Hz to 12543.85395Hz (assuming equal temperament and 440Hz A4) and extends beyond the 88 note piano range from A0 to C8.\n\nChannel Mode messages include the Omni/mono/poly mode on and off messages, as well as messages to reset all controllers to their default state or to send \"note-off\" messages for all notes.\n\nSystem messages do not include channel numbers, and are received by every device in the MIDI chain. MIDI time code is an example of a System Common message. System Real-Time messages provide for synchronization, and include MIDI clock and Active Sensing.\n\nSystem Exclusive (SysEx) messages are a major reason for the flexibility and longevity of the MIDI standard. Manufacturers use them to create proprietary messages that control their equipment more thoroughly than standard MIDI messages could. SysEx messages are addressed to a specific device in a system. Each manufacturer has a unique identifier that is included in its SysEx messages, which helps ensure that only the targeted device responds to the message, and that all others ignore it. Many instruments also include a SysEx ID setting, so a controller can address two devices of the same model independently. SysEx messages can include functionality beyond what the MIDI standard provides. They target a specific instrument, and are ignored by all other devices on the system.\n\nDevices typically do not respond to every type of message defined by the MIDI specification. The MIDI implementation chart was standardized by the MMA as a way for users to see what specific capabilities an instrument has, and how it responds to messages. A specific MIDI Implementation Chart is usually published for each MIDI device within the device documentation.\n\nThe MIDI specification for the electrical interface is based on a fully isolated current loop. The MIDI out port nominally sources a +5 volt source through a 220 ohm resistor out through pin 4 on the MIDI out DIN connector, in on pin 4 of the receiving device's MIDI in DIN connector, through a 220 ohm protection resistor and the LED of an opto-isolator. The current then returns via pin 5 on the MIDI in port to the originating device's MIDI out port pin 5, again with a 220 ohm resistor in the path, giving a nominal current of about 5 milliamperes. Despite the cable's appearance, there is no conductive path between the two MIDI devices, only an optically isolated one. Properly designed MIDI devices are relatively immune to ground loops and similar interference. The data rate on this system is 31,250 bits per second, logic 0 being current on.\n\nThe MIDI specification provides for a ground \"wire\" and a braid or foil shield, connected on pin 2, protecting the two signal-carrying conductors on pins 4 and 5. Although the MIDI cable is supposed to connect pin 2 and the braid or foil shield to chassis ground, it should do so only at the MIDI out port; the MIDI in port should leave pin 2 unconnected and isolated. Some large manufacturers of MIDI devices use modified MIDI in-only DIN 5-pin sockets with the metallic conductors intentionally omitted at pin positions 1, 2, and 3 so that the maximum voltage isolation is obtained.\n\nMIDI's flexibility and widespread adoption have led to many refinements of the standard, and have enabled its application to purposes beyond those for which it was originally intended.\n\nMIDI allows selection of an instrument's sounds through program change messages, but there is no guarantee that any two instruments have the same sound at a given program location. Program #0 may be a piano on one instrument, or a flute on another. The General MIDI (GM) standard was established in 1991, and provides a standardized sound bank that allows a Standard MIDI File created on one device to sound similar when played back on another. GM specifies a bank of 128 sounds arranged into 16 families of eight related instruments, and assigns a specific program number to each instrument. Percussion instruments are placed on channel 10, and a specific MIDI note value is mapped to each percussion sound. GM-compliant devices must offer 24-note polyphony. Any given program change selects the same instrument sound on any GM-compatible instrument.\n\nGeneral MIDI is defined by a standard layout of defined instrument sounds called 'patches', defined by a 'patch' number (program number - PC#) and triggered by pressing a key on a MIDI keyboard. This layout ensures MIDI sound modules and other MIDI devices faithfully reproduce the designated sounds expected by the user and maintains reliable and consistent sound palettes across different manufacturers MIDI devices.\n\nThe GM standard eliminates variation in note mapping. Some manufacturers had disagreed over what note number should represent middle C, but GM specifies that note number 69 plays A440, which in turn fixes middle C as note number 60. GM-compatible devices are required to respond to velocity, aftertouch, and pitch bend, to be set to specified default values at startup, and to support certain controller numbers such as for sustain pedal, and Registered Parameter Numbers. A simplified version of GM, called \"GM Lite\", is used in mobile phones and other devices with limited processing power.\n\nA general opinion quickly formed that the GM's 128-instrument sound set was not large enough. Roland's General Standard, or GS, system included additional sounds, drumkits and effects, provided a \"bank select\" command that could be used to access them, and used MIDI Non-Registered Parameter Numbers (NRPNs) to access its new features. Yamaha's Extended General MIDI, or XG, followed in 1994. XG similarly offered extra sounds, drumkits and effects, but used standard controllers instead of NRPNs for editing, and increased polyphony to 32 voices. Both standards feature backward compatibility with the GM specification, but are not compatible with each other. Neither standard has been adopted beyond its creator, but both are commonly supported by music software titles.\n\nMember companies of Japan's AMEI developed the General MIDI Level 2 specification in 1999. GM2 maintains backward compatibility with GM, but increases polyphony to 32 voices, standardizes several controller numbers such as for sostenuto and soft pedal (\"una corda\"), RPNs and Universal System Exclusive Messages, and incorporates the MIDI Tuning Standard. GM2 is the basis of the instrument selection mechanism in Scalable Polyphony MIDI (SP-MIDI), a MIDI variant for low power devices that allows the device's polyphony to scale according to its processing power.\n\nMost MIDI synthesizers use equal temperament tuning. The MIDI tuning standard (MTS), ratified in 1992, allows alternate tunings. MTS allows microtunings that can be loaded from a bank of up to 128 patches, and allows real-time adjustment of note pitches. Manufacturers are not required to support the standard. Those who do are not required to implement all of its features.\n\nA sequencer can drive a MIDI system with its internal clock, but when a system contains multiple sequencers, they must synchronize to a common clock. MIDI Time Code (MTC), developed by Digidesign, implements SysEx messages that have been developed specifically for timing purposes, and is able to translate to and from the SMPTE time code standard. MIDI Clock is based on tempo, but SMPTE time code is based on frames per second, and is independent of tempo. MTC, like SMPTE code, includes position information, and can adjust itself if a timing pulse is lost. MIDI interfaces such as Mark of the Unicorn's MIDI Timepiece can convert SMPTE code to MTC.\n\nMIDI Machine Control (MMC) consists of a set of SysEx commands that operate the transport controls of hardware recording devices. MMC lets a sequencer send \"Start\", \"Stop\", and \"Record\" commands to a connected tape deck or hard disk recording system, and to fast-forward or rewind the device so that it starts playback at the same point as the sequencer. No synchronization data is involved, although the devices may synchronize through MTC.\n\nMIDI Show Control (MSC) is a set of SysEx commands for sequencing and remotely cueing show control devices such as lighting, music and sound playback, and motion control systems. Applications include stage productions, museum exhibits, recording studio control systems, and amusement park attractions.\n\nOne solution to MIDI timing problems is to mark MIDI events with the times they are to be played, and store them in a buffer in the MIDI interface ahead of time. Sending data beforehand reduces the likelihood that a busy passage can send a large amount of information that overwhelms the transmission link. Once stored in the interface, the information is no longer subject to timing issues associated with USB jitter and computer operating system interrupts, and can be transmitted with a high degree of accuracy. MIDI timestamping only works when both hardware and software support it. MOTU's MTS, eMagic's AMT, and Steinberg's Midex 8 had implementations that were incompatible with each other, and required users to own software and hardware manufactured by the same company to work. Timestamping is built into FireWire MIDI interfaces, Mac OS X Core Audio, and Linux ALSA Sequencer.\n\nAn unforeseen capability of SysEx messages was their use for transporting audio samples between instruments. This led to the development of the sample dump standard (SDS), which established a new SysEx format for sample transmission. The SDS was later augmented with a pair of commands that allow the transmission of information about sample loop points, without requiring that the entire sample be transmitted.\n\nThe Downloadable Sounds (DLS) specification, ratified in 1997, allows mobile devices and computer sound cards to expand their wave tables with downloadable sound sets. The DLS Level 2 Specification followed in 2006, and defined a standardized synthesizer architecture. The Mobile DLS standard calls for DLS banks to be combined with SP-MIDI, as self-contained Mobile XMF files.\n\nMIDI Polyphonic Expression (MPE) is a method of using MIDI that enables pitch bend, and other dimensions of expressive control, to be adjusted continuously for individual notes. MPE works by assigning each note to its own MIDI channel so that particular messages can be applied to each note individually. Instruments like the Continuum Fingerboard, Linnstrument, ROLI Seaboard, and Eigenharp let users control pitch, timbre, and other nuances for individual notes within chords. A growing number of soft synths and effects are also compatible with MPE (such as Equator, UVI Falcon, and Sandman Pro), as well as a few hardware synths (such as Modal Electronics 002, Futuresonus Parva, and Modor NF-1).\n\nIn addition to the original 31.25 kbit/s current-loop transported on 5-pin DIN, other connectors have been used for the same electrical data, and transmission of MIDI streams in different forms over USB, IEEE 1394 a.k.a. FireWire, and Ethernet is now common. Some samplers and hard drive recorders can also pass MIDI data between each other over SCSI.\n\nMembers of the USB-IF in 1999 developed a standard for MIDI over USB, the \"Universal Serial Bus Device Class Definition for MIDI Devices\" MIDI over USB has become increasingly common as other interfaces that had been used for MIDI connections (serial, joystick, etc.) disappeared from personal computers. Linux, Microsoft Windows, Macintosh OS X, and Apple iOS operating systems include standard class drivers to support devices that use the \"Universal Serial Bus Device Class Definition for MIDI Devices\". Some manufacturers choose to implement a MIDI interface over USB that is designed to operate differently from the class specification, using custom drivers.\n\nApple Computer developed the FireWire interface during the 1990s. It began to appear on digital video cameras toward the end of the decade, and on G3 Macintosh models in 1999. It was created for use with multimedia applications. Unlike USB, FireWire uses intelligent controllers that can manage their own transmission without attention from the main CPU. As with standard MIDI devices, FireWire devices can communicate with each other with no computer present.\n\nThe Octave-Plateau Voyetra-8 synthesizer was an early MIDI implementation using XLR3 connectors in place of the 5-pin DIN. It was released in the pre-MIDI years and later retrofitted with a MIDI interface but keeping its XLR connector.\n\nAs computer-based studio setups became common, MIDI devices that could connect directly to a computer became available. These typically used the 8-pin mini-DIN connector that was used by Apple for serial and printer ports prior to the introduction of the Blue & White G3 models. MIDI interfaces intended for use as the centerpiece of a studio, such as the Mark of the Unicorn MIDI Time Piece, were made possible by a \"fast\" transmission mode that could take advantage of these serial ports' ability to operate at 20 times the standard MIDI speed. Mini-DIN ports were built into some late-1990s MIDI instruments, and enabled such devices to be connected directly to a computer. Some devices connected via PCs' DB-25 parallel port, or through the joystick port found in many PC sound cards.\n\nYamaha introduced the mLAN protocol in 1999. It was conceived as a Local Area Network for musical instruments using FireWire as the transport, and was designed to carry multiple MIDI channels together with multichannel digital audio, data file transfers, and time code. mLan was used in a number of Yamaha products, notably digital mixing consoles and the Motif synthesizer, and in third-party products such as the PreSonus FIREstation and the Korg Triton Studio. No new mLan products have been released since 2007.\n\nComputer network implementations of MIDI provide network routing capabilities, and the high-bandwidth channel that earlier alternatives to MIDI, such as ZIPI, were intended to bring. Proprietary implementations have existed since the 1980s, some of which use fiber optic cables for transmission. The Internet Engineering Task Force's RTP-MIDI open specification has gained industry support. Apple has supported this protocol from Mac OS X 10.4 onwards, and a Windows driver based on Apple's implementation exists for Windows XP and newer versions.\n\nSystems for wireless MIDI transmission have been available since the 1980s. Several commercially available transmitters allow wireless transmission of MIDI and OSC signals over Wi-Fi and Bluetooth. iOS devices are able to function as MIDI control surfaces, using Wi-Fi and OSC. An XBee radio can be used to build a wireless MIDI transceiver as a do-it-yourself project. Android devices are able to function as full MIDI control surfaces using several different protocols over Wi-Fi and Bluetooth.\n\nSome devices use standard 3.5mm TRS audio minijack connectors for MIDI data, including the Korg Electribe 2 and the Arturia Beatstep Pro. Both come with adaptors that break out to standard 5-pin DIN connectors.. This became widespread enough that the Midi Manufacturer's Association standardized the wiring. The MIDI-over-minijack standards document also recommends the use of 2.5mm connectors over 3.5mm ones to avoid confusion with audio connectors.\n\nA new protocol, tentatively called \"HD Protocol\", \"High-Definition Protocol\", \"New Protocol\", or \"Next Generation Protocol\", has been discussed since 2005. The new standard offers full backward compatibility with MIDI 1.0 and supports higher-speed transports, plug-and-play device discovery and enumeration, and greater data range and resolution. It increases the numbers of channels and controllers, and simplifies messages. HD Protocol supports entirely new kinds of events, such as a Note Update message and Direct Pitch in the Note message, which are aimed at guitar controllers. Proposed physical layer transports include Ethernet-based protocols such as RTP MIDI and Audio Video Bridging. The HD Protocol and a User Datagram Protocol (UDP)-based transport are under review by MMA's High-Definition Protocol Working Group (HDWG), which includes representatives from all sizes and types of companies.\n\nPrototype devices based on the draft standard have been shown privately at NAMM using wired and wireless connections, however it is uncertain if and when the industry will release products that use the new protocol even though MMA already developed the policies on licensing and product certification.\n\nInitial parts of the new standard, the MIDI Capability Inquiry (MIDI-CI) and the MIDI Polyphonic Expression (MPE) specifications, were released in November 2017 by AMEI and in January 2018 by MMA. \n\nIn January 2019, AMEI and MMA announced MIDI 2.0 as the final name of the new protocol; complete specifications will be published following interoperability testing of prototype implementations from major manufacturers such as Google, Yamaha, Steinberg, Roland, Ableton, Native Instruments, and ROLI, among others. \n\nMIDI Capability Inquiry (MIDI-CI) specifies extensions that use SysEx messages to implement device profiles, parameter exchange, and MIDI protocol negotiation. Profiles define common sets of MIDI controllers for various instrument types, such as drawbar organs and analog synths, or for particular tasks, improving interoperability between instruments from different manufacturers. Parameter exchange defines methods to inquiry device capabilities, such as supported controllers, patch names, and other metadata, and to get or set device configuration settings. Protocol negotiation allows devices to employ the Next Generation protocol or manufacturer-specific protocols.\n\nAs of January 2019, the draft specification of the new protocol supports all core messages that also exist in MIDI 1.0, but extends their precision and resolution; it also defines many new high-precision controller messages. \n\nExisting controllers extended from 7-bit to 32-bit precision:\n\nControllers that were modified to use a single-message format with 32-bit data:\n\nNew per-note controllers with 32-bit resolution:\n\nThe new protocol supports a total of 256 MIDI channels, organized in 16 groups of 16 channels; each group can carry either a MIDI 1.0 stream or MIDI 2.0 stream, and can also include system messages, system exclusive data, and timestamps for precise rendering of several simultaneous notes. Integration with DAWs and web-based applications is also planned. \n\n\n"}
{"id": "19999", "url": "https://en.wikipedia.org/wiki?curid=19999", "title": "Microcode", "text": "Microcode\n\nMicrocode is a computer hardware technique that imposes an interpreter between the CPU hardware and the programmer-visible instruction set architecture of the computer. As such, the microcode is a layer of hardware-level instructions that implement higher-level machine code instructions or internal state machine sequencing in many digital processing elements. Microcode is used in general-purpose central processing units, although in current desktop CPUs it is only a fallback path for cases that the faster hardwired control unit cannot handle.\n\nMicrocode typically resides in special high-speed memory and translates machine instructions, state machine data or other input into sequences of detailed circuit-level operations. It separates the machine instructions from the underlying electronics so that instructions can be designed and altered more freely. It also facilitates the building of complex multi-step instructions, while reducing the complexity of computer circuits. Writing microcode is often called microprogramming and the microcode in a particular processor implementation is sometimes called a microprogram.\n\nMore extensive microcoding allows small and simple microarchitectures to emulate more powerful architectures with wider word length, more execution units and so on, which is a relatively simple way to achieve software compatibility between different products in a processor family.\n\nSome hardware vendors, especially IBM, use the term \"microcode\" as a synonym for \"firmware\". In that way, all code within a device is termed \"microcode\" regardless of it being microcode or machine code; for example, hard disk drives are said to have their microcode updated, though they typically contain both microcode and firmware.\n\nThe lowest layer in a computer's software stack is traditionally raw binary machine code instructions for the processor. Microcode sits one level below this. To avoid confusion, each microprogram-related element is differentiated by the \"micro\" prefix: microinstruction, microassembler, microprogrammer, microarchitecture, etc.\n\nEngineers normally write the microcode during the design phase of a processor, storing it in a read-only memory (ROM) or programmable logic array (PLA) structure, or in a combination of both. However, machines also exist that have some or all microcode stored in SRAM or flash memory. This is traditionally denoted as \"writeable control store\" in the context of computers, which can be either read-only or read-write memory. In the latter case, the CPU initialization process loads microcode into the control store from another storage medium, with the possibility of altering the microcode to correct bugs in the instruction set, or to implement new machine instructions.\n\nComplex digital processors may also employ more than one (possibly microcode-based) control unit in order to delegate sub-tasks that must be performed essentially asynchronously in parallel. A high-level programmer, or even an assembly programmer, does not normally see or change microcode. Unlike machine code, which often retains some backward compatibility among different processors in a family, microcode only runs on the exact electronic circuitry for which it is designed, as it constitutes an inherent part of the particular processor design itself.\n\nMicroprograms consist of series of microinstructions, which control the CPU at a very fundamental level of hardware circuitry. For example, a single typical \"horizontal\" microinstruction might specify the following operations:\n\nTo simultaneously control all processor's features in one cycle, the microinstruction is often wider than 50 bits; e.g., 128 bits on a 360/85 with an emulator feature. Microprograms are carefully designed and optimized for the fastest possible execution, as a slow microprogram would result in a slow machine instruction and degraded performance for related application programs that use such instructions.\n\nMicrocode was originally developed as a simpler method of developing the control logic for a computer. Initially, CPU instruction sets were hardwired. Each step needed to fetch, decode, and execute the machine instructions (including any operand address calculations, reads, and writes) was controlled directly by combinational logic and rather minimal sequential state machine circuitry. While very efficient, the need for powerful instruction sets with multi-step addressing and complex operations (\"see below\") made such hard-wired processors difficult to design and debug; highly encoded and varied-length instructions can contribute to this as well, especially when very irregular encodings are used.\n\nMicrocode simplified the job by allowing much of the processor's behaviour and programming model to be defined via microprogram routines rather than by dedicated circuitry. Even late in the design process, microcode could easily be changed, whereas hard-wired CPU designs were very cumbersome to change. Thus, this greatly facilitated CPU design.\n\nFrom the 1940s to the late 1970s, a large portion of programming was done in assembly language; higher-level instructions mean greater programmer productivity, so an important advantage of microcode was the relative ease by which powerful machine instructions can be defined. The ultimate extension of this are \"Directly Executable High Level Language\" designs, in which each statement of a high-level language such as PL/I is entirely and directly executed by microcode, without compilation. The IBM Future Systems project and Data General Fountainhead Processor are examples of this. During the 1970s, CPU speeds grew more quickly than memory speeds and numerous techniques such as memory block transfer, memory pre-fetch and multi-level caches were used to alleviate this. High-level machine instructions, made possible by microcode, helped further, as fewer more complex machine instructions require less memory bandwidth. For example, an operation on a character string can be done as a single machine instruction, thus avoiding multiple instruction fetches.\n\nArchitectures with instruction sets implemented by complex microprograms included the IBM System/360 and Digital Equipment Corporation VAX. The approach of increasingly complex microcode-implemented instruction sets was later called CISC. An alternate approach, used in many microprocessors, is to use PLAs or ROMs (instead of combinational logic) mainly for instruction decoding, and let a simple state machine (without much, or any, microcode) do most of the sequencing. The MOS Technology 6502 is an example of a microprocessor using a PLA for instruction decode and sequencing. The PLA is visible in photomicrographs of the chip, and its operation can be seen in the transistor-level simulation.\n\nMicroprogramming is still used in modern CPU designs. In some cases, after the microcode is debugged in simulation, logic functions are substituted for the control store. Logic functions are often faster and less expensive than the equivalent microprogram memory.\n\nA processor's microprograms operate on a more primitive, totally different, and much more hardware-oriented architecture than the assembly instructions visible to normal programmers. In coordination with the hardware, the microcode implements the programmer-visible architecture. The underlying hardware need not have a fixed relationship to the visible architecture. This makes it easier to implement a given instruction set architecture on a wide variety of underlying hardware micro-architectures.\n\nThe IBM System/360 has a 32-bit architecture with 16 general-purpose registers, but most of the System/360 implementations actually use hardware that implemented a much simpler underlying microarchitecture; for example, the System/360 Model 30 has 8-bit data paths to the arithmetic logic unit (ALU) and main memory and implemented the general-purpose registers in a special unit of higher-speed core memory, and the System/360 Model 40 has 8-bit data paths to the ALU and 16-bit data paths to main memory and also implemented the general-purpose registers in a special unit of higher-speed core memory. The Model 50 has full 32-bit data paths and implements the general-purpose registers in a special unit of higher-speed core memory. The Model 65 through the Model 195 have larger data paths and implement the general-purpose registers in faster transistor circuits. In this way, microprogramming enabled IBM to design many System/360 models with substantially different hardware and spanning a wide range of cost and performance, while making them all architecturally compatible. This dramatically reduces the number of unique system software programs that must be written for each model.\n\nA similar approach was used by Digital Equipment Corporation (DEC) in their VAX family of computers. As a result, different VAX processors use different microarchitectures, yet the programmer-visible architecture does not change.\n\nMicroprogramming also reduces the cost of field changes to correct defects (bugs) in the processor; a bug can often be fixed by replacing a portion of the microprogram rather than by changes being made to hardware logic and wiring.\n\nIn 1947, the design of the MIT Whirlwind introduced the concept of a control store as a way to simplify computer design and move beyond \"ad hoc\" methods. The control store is a diode matrix: a two-dimensional lattice, where one dimension accepts \"control time pulses\" from the CPU's internal clock, and the other connects to control signals on gates and other circuits. A \"pulse distributor\" takes the pulses generated by the CPU clock and breaks them up into eight separate time pulses, each of which activates a different row of the lattice. When the row is activated, it activates the control signals connected to it.\n\nDescribed another way, the signals transmitted by the control store are being played much like a player piano roll. That is, they are controlled by a sequence of very wide words constructed of bits, and they are \"played\" sequentially. In a control store, however, the \"song\" is short and repeated continuously.\n\nIn 1951, Maurice Wilkes enhanced this concept by adding \"conditional execution\", a concept akin to a conditional in computer software. His initial implementation consisted of a pair of matrices: the first one generated signals in the manner of the Whirlwind control store, while the second matrix selected which row of signals (the microprogram instruction word, so to speak) to invoke on the next cycle. Conditionals were implemented by providing a way that a single line in the control store could choose from alternatives in the second matrix. This made the control signals conditional on the detected internal signal. Wilkes coined the term microprogramming to describe this feature and distinguish it from a simple control store.\n\n\n\nEach microinstruction in a microprogram provides the bits that control the functional elements that internally compose a CPU. The advantage over a hard-wired CPU is that internal CPU control becomes a specialized form of a computer program. Microcode thus transforms a complex electronic design challenge (the control of a CPU) into a less complex programming challenge. To take advantage of this, a CPU is divided into several parts:\n\n\nThere may also be a memory address register and a memory data register, used to access the main computer storage. Together, these elements form an \"execution unit\". Most modern CPUs have several execution units. Even simple computers usually have one unit to read and write memory, and another to execute user code. These elements could often be brought together as a single chip. This chip comes in a fixed width that would form a \"slice\" through the execution unit. These are known as \"bit slice\" chips. The AMD Am2900 family is one of the best known examples of bit slice elements. The parts of the execution units and the execution units themselves are interconnected by a bundle of wires called a bus.\n\nProgrammers develop microprograms, using basic software tools. A microassembler allows a programmer to define the table of bits symbolically. Because of its close relationship to the underlying architecture, \"microcode has several properties that make it difficult to generate using a compiler.\" A simulator program is intended to execute the bits in the same way as the electronics, and allows much more freedom to debug the microprogram. After the microprogram is finalized, and extensively tested, it is sometimes used as the input to a computer program that constructs logic to produce the same data. This program is similar to those used to optimize a programmable logic array. Even without fully optimal logic, heuristically optimized logic can vastly reduce the number of transistors from the number required for a ROM control store. This reduces the cost of producing, and the electricity consumed by, a CPU.\n\nMicrocode can be characterized as \"horizontal\" or \"vertical\", referring primarily to whether each microinstruction controls CPU elements with little or no decoding (horizontal microcode) or requires extensive decoding by combinatorial logic before doing so (vertical microcode). Consequently, each horizontal microinstruction is wider (contains more bits) and occupies more storage space than a vertical microinstruction.\n\n\"Horizontal microcode has several discrete micro-operations that are combined in a single microinstruction for simultaneous operation.\" Horizontal microcode is typically contained in a fairly wide control store; it is not uncommon for each word to be 108 bits or more. On each tick of a sequencer clock a microcode word is read, decoded, and used to control the functional elements that make up the CPU.\n\nIn a typical implementation a horizontal microprogram word comprises fairly tightly defined groups of bits. For example, one simple arrangement might be:\n\nFor this type of micromachine to implement a JUMP instruction with the address following the opcode, the microcode might require two clock ticks. The engineer designing it would write microassembler source code looking something like this:\n\nFor each tick it is common to find that only some portions of the CPU are used, with the remaining groups of bits in the microinstruction being no-ops. With careful design of hardware and microcode, this property can be exploited to parallelise operations that use different areas of the CPU; for example, in the case above, the ALU is not required during the first tick, so it could potentially be used to complete an earlier arithmetic instruction.\n\nIn vertical microcode, each microinstruction is significantly encoded that is, the bit fields generally pass through intermediate combinatory logic that, in turn, generates the actual control and sequencing signals for internal CPU elements (ALU, registers, etc.). This is in contrast with horizontal microcode, in which the bit fields themselves either directly produce the control and sequencing signals or are only minimally encoded. Consequently, vertical microcode requires smaller instruction lengths and less storage, but requires more time to decode, resulting in a slower CPU clock.\n\nSome vertical microcode is just the assembly language of a simple conventional computer that is emulating a more complex computer. Some processors, such as DEC Alpha processors and the CMOS microprocessors on later IBM System/390 mainframes and z/Architecture mainframes, have PALcode (the term used on Alpha processors) or millicode (the term used on IBM mainframe microprocessors). This is a form of machine code, with access to special registers and other hardware resources not available to regular machine code, used to implement some instructions and other functions, such as page table walks on Alpha processors.\n\nAnother form of vertical microcode has two fields:\nThe \"field select\" selects which part of the CPU will be controlled by this word of the control store. The \"field value\" actually controls that part of the CPU. With this type of microcode, a designer explicitly chooses to make a slower CPU to save money by reducing the unused bits in the control store; however, the reduced complexity may increase the CPU's clock frequency, which lessens the effect of an increased number of cycles per instruction.\n\nAs transistors became cheaper, horizontal microcode came to dominate the design of CPUs using microcode, with vertical microcode being used less often.\n\nWhen both vertical and horizontal microcode are used, the horizontal microcode may be referred to as \"nanocode\" or \"picocode\".\n\nA few computers were built using \"writable microcode\". In this design, rather than storing the microcode in ROM or hard-wired logic, the microcode is stored in a RAM called a \"writable control store\" or \"WCS\". Such a computer is sometimes called a \"writable instruction set computer\" or \"WISC\".\n\nMany experimental prototype computers use writable control stores; there are also commercial machines that use writable microcode, such as the Burroughs Small Systems, early Xerox workstations, the DEC VAX 8800 (\"Nautilus\") family, the Symbolics L- and G-machines, a number of IBM System/360 and System/370 implementations, some DEC PDP-10 machines, and the Data General Eclipse MV/8000.\n\nMany more machines offer user-programmable writable control stores as an option, including the HP 2100, DEC PDP-11/60 and Varian Data Machines V-70 series minicomputers. The IBM System/370 includes a facility called \"Initial-Microprogram Load\" (\"IML\" or \"IMPL\") that can be invoked from the console, as part of \"power-on reset\" (\"POR\") or from another processor in a tightly coupled multiprocessor complex.\n\nSome commercial machines, for example IBM 360/85, have both a read-only storage and a writable control store for microcode.\n\nWCS offers several advantages including the ease of patching the microprogram and, for certain hardware generations, faster access than ROMs can provide. User-programmable WCS allows the user to optimize the machine for specific purposes.\n\nStarting with the Pentium Pro in 1995, several x86 CPUs have writable Intel Microcode. This, for example, has allowed bugs in the Intel Core 2 and Intel Xeon microcodes to be fixed by patching their microprograms, rather than requiring the entire chips to be replaced. A second prominent example is the set of microcode patches that Intel offered for some of their processor architectures of up to 10 years in age, in a bid to counter the security vulnerabilities discovered in their designs - Spectre and Meltdown - which went public at the start of 2018. A microcode update can be installed by Linux, FreeBSD, Microsoft Windows, or the motherboard BIOS.\n\nThe design trend toward heavily microcoded processors with complex instructions began in the early 1960s and continued until roughly the mid-1980s. At that point the RISC design philosophy started becoming more prominent.\n\nA CPU that uses microcode generally takes several clock cycles to execute a single instruction, one clock cycle for each step in the microprogram for that instruction. Some CISC processors include instructions that can take a very long time to execute. Such variations interfere with both interrupt latency and, what is far more important in modern systems, pipelining.\n\nWhen designing a new processor, a hardwired control RISC has the following advantages over microcoded CISC:\n\n\nThere are counterpoints as well:\n\nMany RISC and VLIW processors are designed to execute every instruction (as long as it is in the cache) in a single cycle. This is very similar to the way CPUs with microcode execute one microinstruction per cycle. VLIW processors have instructions that behave similarly to very wide horizontal microcode, although typically without such fine-grained control over the hardware as provided by microcode. RISC instructions are sometimes similar to the narrow vertical microcode.\n\nMicrocoding has been popular in application-specific processors such as network processors, microcontrollers, digital signal processors, channel controllers, disk controllers, network interface controllers, graphics processing units, and in other hardware.\n\nModern CISC implementations, such as the x86 family, decode instructions into dynamically buffered micro-operations (\"μops\") with an instruction encoding similar to RISC or traditional microcode. A hardwired instruction decode unit directly emits μops for common x86 instructions, but falls back to a more traditional microcode ROM for more complex or rarely used instructions.\n\nFor example, an x86 might look up μops from microcode to handle complex multistep operations such as loop or string instructions, floating point unit transcendental functions or unusual values such as denormal numbers, and special purpose instructions such as CPUID.\n\n\n"}
{"id": "20003", "url": "https://en.wikipedia.org/wiki?curid=20003", "title": "Multitier architecture", "text": "Multitier architecture\n\nIn software engineering, multitier architecture (often referred to as \"n\"-tier architecture) or multilayered architecture is a client–server architecture in which presentation, application processing, and data management functions are physically separated. The most widespread use of multitier architecture is the three-tier architecture.\n\n\"N\"-tier application architecture provides a model by which developers can create flexible and reusable applications. By segregating an application into tiers, developers acquire the option of modifying or adding a specific layer, instead of reworking the entire application. A three-tier architecture is typically composed of a \"presentation\" tier, a \"domain logic\" tier, and a \"data storage\" tier.\n\nWhile the concepts of layer and tier are often used interchangeably, one fairly common point of view is that there is indeed a difference. This view holds that a \"layer\" is a logical structuring mechanism for the elements that make up the software solution, while a \"tier\" is a physical structuring mechanism for the system infrastructure. For example, a three-layer solution could easily be deployed on a single tier, such as a personal workstation.\n\nThe \"Layers\" architectural pattern has been described in various publications.\n\nIn a logical multilayered architecture for an information system with an object-oriented design, the following four are the most common:\n\n\nThe book \"Domain Driven Design\" describes some common uses for the above four layers, although its primary focus is the domain layer.\n\nIf the application architecture has no explicit distinction between the business layer and the presentation layer (i.e., the presentation layer is considered part of the business layer), then a traditional client-server (two-tier) model has been implemented.\n\nThe more usual convention is that the application layer (or service layer) is considered a sublayer of the business layer, typically encapsulating the API definition surfacing the supported business functionality. The application/business layers can, in fact, be further subdivided to emphasize additional sublayers of distinct responsibility. For example, if the Model View Presenter pattern is used, the presenter sublayer might be used as an additional layer between the user interface layer and the business/application layer (as represented by the model sublayer).\n\nSome also identify a separate layer called the business infrastructure layer (BI), located between the business layer(s) and the infrastructure layer(s). It's also sometimes called the \"low-level business layer\" or the \"business services layer\". This layer is very general and can be used in several application tiers (e.g. a CurrencyConverter).\n\nThe infrastructure layer can be partitioned into different levels (high-level or low-level technical services). Developers often focus on the persistence (data access) capabilities of the infrastructure layer and therefore only talk about the persistence layer or the data access layer (instead of an infrastructure layer or technical services layer). In other words, the other kind of technical services are not always explicitly thought of as part of any particular layer.\n\nA layer is on top of another, because it depends on it. Every layer can exist without the layers above it, and requires the layers below it to function. Another common view is that layers do not always strictly depend on only the adjacent layer below. For example, in a relaxed layered system (as opposed to a strict layered system) a layer can also depend on all the layers below it.\n\nThree-tier architecture is a client-server software architecture pattern in which the user interface (presentation), functional process logic (\"business rules\"), computer data storage and data access are developed and maintained as independent modules, most often on separate platforms. It was developed by John J. Donovan in Open Environment Corporation (OEC), a tools company he founded in Cambridge, Massachusetts.\n\nApart from the usual advantages of modular software with well-defined interfaces, the three-tier architecture is intended to allow any of the three tiers to be upgraded or replaced independently in response to changes in requirements or technology. For example, a change of operating system in the \"presentation tier\" would only affect the user interface code.\n\nTypically, the user interface runs on a desktop PC or workstation and uses a standard graphical user interface, functional process logic that may consist of one or more separate modules running on a workstation or application server, and an RDBMS on a database server or mainframe that contains the computer data storage logic. The middle tier may be multitiered itself (in which case the overall architecture is called an \"\"n\"-tier architecture\").\n\n\nIn the web development field, three-tier is often used to refer to websites, commonly electronic commerce websites, which are built using three tiers:\n\nData transfer between tiers is part of the architecture. Protocols involved may include one or more of SNMP, CORBA, Java RMI, .NET Remoting, Windows Communication Foundation, sockets, UDP, web services or other standard or proprietary protocols. Often middleware is used to connect the separate tiers. Separate tiers often (but not necessarily) run on separate physical servers, and each tier may itself run on a cluster.\n\nThe end-to-end traceability of data flows through \"n\"-tier systems is a challenging task which becomes more important when systems increase in complexity. The Application Response Measurement defines concepts and APIs for measuring performance and correlating transactions between tiers. \nGenerally, the term \"tiers\" is used to describe physical distribution of components of a system on separate servers, computers, or networks (processing nodes). A three-tier architecture then will have three processing nodes. The term \"layers\" refers to a logical grouping of components which may or may not be physically located on one processing node.\n\n\n"}
{"id": "20016", "url": "https://en.wikipedia.org/wiki?curid=20016", "title": "Myrinet", "text": "Myrinet\n\nMyrinet, ANSI/VITA 26-1998, is a high-speed local area networking system designed by the company Myricom to be used as an interconnect between multiple machines to form computer clusters. \n\nMyrinet was promoted as having lower protocol overhead than standards such as Ethernet, and therefore better throughput, less interference, and lower latency while using the host CPU. Although it can be used as a traditional networking system, Myrinet is often used directly by programs that \"know\" about it, thereby bypassing a call into the operating system.\n\nMyrinet physically consists of two fibre optic cables, upstream and downstream, connected to the host computers with a single connector. Machines are connected via low-overhead routers and switches, as opposed to connecting one machine directly to another. Myrinet includes a number of fault-tolerance features, mostly backed by the switches. These include flow control, error control, and \"heartbeat\" monitoring on every link. The \"fourth-generation\" Myrinet, called Myri-10G, supported a 10 Gbit/s data rate and can use 10 Gigabit Ethernet on PHY, the physical layer (cables, connectors, distances, signaling). Myri-10G started shipping at the end of 2005.\n\nMyrinet was approved in 1998 by the American National Standards Institute for use on the VMEbus as ANSI/VITA 26-1998.. One of the earliest publications on Myrinet is a 1995 IEEE article.\n\nMyrinet is a lightweight protocol with little overhead that allows it to operate with throughput close to the basic signaling speed of the physical layer. For supercomputing, the low latency of Myrinet is even more important than its throughput performance, since, according to Amdahl's law, a high-performance parallel system tends to be bottlenecked by its slowest sequential process, which in all but the most embarrassingly parallel supercomputer workloads is often the latency of message transmission across the network.\n\nAccording to Myricom, 141 (28.2%) of the June 2005 TOP500 supercomputers used Myrinet technology. In the November 2005 TOP500, the number of supercomputers using Myrinet was down to 101 computers, or 20.2%, in November 2006, 79 (15.8%), and by November 2007, 18 (3.6%), a long way behind gigabit Ethernet at 54% and InfiniBand at 24.2%.\n\nIn the June 2014 TOP500 list, the number of supercomputers using Myrinet interconnect was 1 (0.2%).\n\nIn November, 2013, the assets of Myricom (including the Myrinet technology) were acquired by CSP Inc. In 2016, it was reported that Google had also offered to buy the company.\n\n\n"}
{"id": "20017", "url": "https://en.wikipedia.org/wiki?curid=20017", "title": "Musique concrète", "text": "Musique concrète\n\nMusique concrète (, meaning \"concrete music\") is a form of musique expérimentale (experimental music ) that exploits acousmatic listening, meaning sound identities can often be intentionally obscured or appear unconnected to their source cause. It can feature sounds derived from recordings of musical instruments, the human voice, and the natural environment as well as those created using synthesizers and computer-based digital signal processing. Compositions in this idiom are not restricted to the normal musical rules of melody, harmony, rhythm, metre, and so on. Originally contrasted with \"pure\" \"elektronische Musik\" (based solely on the production and manipulation of electronically produced sounds rather than recorded sounds), the theoretical basis of \"musique concrète\" as a compositional practice was developed by Pierre Schaeffer, beginning in the early 1940s. From the late 1960s onward, and particularly in France, the term acousmatic music (\"musique acousmatique\") started to be used in reference to fixed media compositions that utilized both musique concrète based techniques and live sound spatialisation.\n\nIn 1928 music critic André Cœuroy wrote in his book \"Panorama of Contemporary Music\" that \"perhaps the time is not far off when a composer will be able to represent through recording, music specifically composed for the gramophone\" . In the same period the American composer Henry Cowell, in referring to the projects of Nikolai Lopatnikoff, believed that \"there was a wide field open for the composition of music for phonographic discs.\" This sentiment was echoed further in 1930 by Igor Stravinsky, when he stated in the revue \"Kultur und Schallplatte\" that \"there will be a greater interest in creating music in a way that will be peculiar to the gramophone record.\" The following year, 1931, Boris de Schloezer also expressed the opinion that one could write for the gramophone or for the wireless just as one can for the piano or the violin . Shortly after, German art theorist Rudolf Arnheim discussed the effects of microphonic recording in an essay entitled \"Radio\", published in 1936. In it the idea of a creative role for the recording medium was introduced and Arnheim stated that: \"The rediscovery of the musicality of sound in noise and in language, and the reunification of music, noise and language in order to obtain a unity of material: that is one of the chief artistic tasks of radio\" .\n\nIn 1942 French composer and theoretician Pierre Schaeffer began his exploration of radiophony when he joined Jacques Copeau and his pupils in the foundation of the Studio d'Essai de la Radiodiffusion nationale. The studio originally functioned as a center for the Resistance movement in French radio, which in August 1944 was responsible for the first broadcasts in liberated Paris. It was here that Schaeffer began to experiment with creative radiophonic techniques using the sound technologies of the time .\n\nThe development of Schaeffer's practice was informed by encounters with voice actors, and microphone usage and radiophonic art played an important part in inspiring and consolidating Schaeffer's conception of sound-based composition . Another important influence on Schaeffer's practice was cinema, and the techniques of recording and montage, which were originally associated with cinematographic practice, came to \"serve as the substrate of musique concrète.\" Marc Battier notes that, prior to Schaeffer, Jean Epstein drew attention to the manner in which sound recording revealed what was hidden in the act of basic acoustic listening. Epstein's reference to this \"phenomenon of an epiphanic being\", which appears through the transduction of sound, proved influential on Schaeffer's concept of reduced listening. Schaeffer would explicitly cite Jean Epstein with reference to his use of extra-musical sound material. Epstein had already imagined that \"through the transposition of natural sounds, it becomes possible to create chords and dissonances, melodies and symphonies of noise, which are a new and specifically cinematographic music\" .\n\nPerhaps earlier than Schaeffer conducting his preliminary experiments into sound manipulation (assuming these were later than 1944, and not as early as the foundation of the Studio d'Essai in 1942) was the activity of Egyptian composer Halim El-Dabh. As a student in Cairo in the early to mid-1940s he began experimenting with \"tape music\" using a cumbersome wire recorder. He recorded the sounds of an ancient \"zaar\" ceremony and at the Middle East Radio studios processed the material using reverberation, echo, voltage controls, and re-recording. The resulting tape-based composition, entitled \"The Expression of Zaar\", was presented in 1944 at an art gallery event in Cairo. El-Dabh has described his initial activities as an attempt to unlock \"the inner sound\" of the recordings. While his early compositional work was not widely known outside of Egypt at the time, El-Dabh would eventually gain recognition for his influential work at the Columbia-Princeton Electronic Music Center in the late 1950s .\n\nFollowing Schaeffer's work with Studio d'Essai at Radiodiffusion Nationale during the early 1940s he was credited with originating the theory and practice of \"musique concrète.\" The Studio d'Essai was renamed Club d'Essai de la Radiodiffusion-Télévision Française in 1946 and in the same year Schaeffer discussed, in writing, the question surrounding the transformation of time perceived through recording. The essay evidenced knowledge of sound manipulation techniques he would further exploit compositionally. In 1948 Schaeffer formally initiated \"research in to noises\" at the Club d'Essai and on 5 October 1948 the results of his initial experimentation were premiered at a concert given in Paris . Five works for phonograph (known collectively as \"Cinq études de bruits\"—Five Studies of Noises) including \"Etude violette\" (\"Study in Purple\") and \"Etude aux chemins de fer\" (Study of the Railroads), were presented.\n\nBy 1949 Schaeffer's compositional work was known publicly as \"musique concrète\" . Schaeffer stated: \"when I proposed the term 'musique concrète,' I intended … to point out an opposition with the way musical work usually goes. Instead of notating musical ideas on paper with the symbols of solfege and entrusting their realization to well-known instruments, the question was to collect concrete sounds, wherever they came from, and to abstract the musical values they were potentially containing\" . According to Pierre Henry, \"musique concrète was not a study of timbre, it is focused on envelopes, forms. It must be presented by means of non-traditional characteristics, you see … one might say that the origin of this music is also found in the interest in 'plastifying' music, of rendering it plastic like sculpture…musique concrète, in my opinion … led to a manner of composing, indeed, a new mental framework of composing\" . Schaeffer had developed an aesthetic that was centred upon the use of sound as a primary compositional resource. The aesthetic also emphasised the importance of play (\"jeu\") in the practice of sound based composition. Schaeffer's use of the word \"jeu\", from the verb \"jouer\", carries the same double meaning as the English verb play: 'to enjoy oneself by interacting with one's surroundings', as well as 'to operate a musical instrument' .\n\nBy 1951 the work of Schaeffer, composer-percussionist Pierre Henry, and sound engineer Jacques Poullin had received official recognition and The Groupe de Recherches de Musique Concrète, Club d 'Essai de la Radiodiffusion-Télévision Française was established at RTF in Paris, the ancestor of the ORTF . At RTF the GRMC established the first purpose-built electroacoustic music studio. It quickly attracted many who either were or were later to become notable composers, including Olivier Messiaen, Pierre Boulez, Jean Barraqué, Karlheinz Stockhausen, Edgard Varèse, Iannis Xenakis, Michel Philippot, and Arthur Honegger. Compositional output from 1951 to 1953 comprised \"Étude I\" (1951) and \"Étude II\" (1951) by Boulez, \"Timbres-durées\" (1952) by Messiaen, \"Konkrete Etüde\" (1952) by Stockhausen, \"Le microphone bien tempéré\" (1952) and \"La voile d'Orphée\" (1953) by Henry, \"Étude I\" (1953) by Philippot, \"Étude\" (1953) by Barraqué, the mixed pieces \"Toute la lyre\" (1951) and \"Orphée 53\" (1953) by Schaeffer/Henry, and the film music \"Masquerage\" (1952) by Schaeffer and \"Astrologie\" (1953) by Henry. In 1954 Varèse and Honegger visited to work on the tape parts of \"Déserts\" and \"La rivière endormie\" .\n\nIn the early and mid 1950s Schaeffer's commitments to RTF included official missions which often required extended absences from the studios. This led him to invest Philippe Arthuys with responsibility for the GRMC in his absence, with Pierre Henry operating as Director of Works. Pierre Henry's composing talent developed greatly during this period at the GRMC and he worked with experimental filmmakers such as Max de Haas, Jean Grémillon, Enrico Fulchignoni, and Jean Rouch, and with choreographers including Dick Sanders and Maurice Béjart . Schaeffer returned to run the group at the end of 1957, and immediately stated his disapproval of the direction the GRMC had taken. A proposal was then made to \"renew completely the spirit, the methods and the personnel of the Group, with a view to undertake research and to offer a much needed welcome to young composers\" .\n\nFollowing the emergence of differences within the GRMC Pierre Henry, Philippe Arthuys, and several of their colleagues, resigned in April 1958. Schaeffer created a new collective, called Groupe de Recherches Musicales (GRM) and set about recruiting new members including Luc Ferrari, Beatriz Ferreyra, François-Bernard Mâche, Iannis Xenakis, Bernard Parmegiani, and Mireille Chamass-Kyrou. Later arrivals included Ivo Malec, Philippe Carson, Romuald Vandelle, Edgardo Canton and François Bayle .\n\nGRM was one of several theoretical and experimental groups working under the umbrella of the Schaeffer-led Service de la Recherche at ORTF (1960–74). Together with the GRM, three other groups existed: the Groupe de Recherches Image GRI, the Groupe de Recherches Technologiques GRT and the Groupe de Recherches which became the Groupe d'Etudes Critiques . Communication was the one theme that unified the various groups, all of which were devoted to production and creation. In terms of the question \"who says what to whom?\" Schaeffer added \"how?\", thereby creating a platform for research into audiovisual communication and mass media, audible phenomena and music in general (including non-Western musics) (Beatriz Ferreyra, new preface to Schaeffer and Reibel 1967, reedition of 1998, 9). At the GRM the theoretical teaching remained based on practice and could be summed up in the catch phrase \"do and listen\" .\n\nSchaeffer kept up a practice established with the GRMC of delegating the functions (though not the title) of Group Director to colleagues. Since 1961 GRM has had six Group Directors: Michel Philippot (1960–61), Luc Ferrari (1962–63), Bernard Baschet and François Vercken (1964–66). From the beginning of 1966, François Bayle took over the direction for the duration of thirty-one years, to 1997. He was then replaced by Daniel Teruggi .\n\nThe group continued to refine Schaeffer's ideas and strengthened the concept of \"musique acousmatique\" . Schaeffer had borrowed the term acousmatic from Pythagoras and defined it as: \"\"Acousmatic, adjective: referring to a sound that one hears without seeing the causes behind it\"\" . In 1966 Schaeffer published the book \"Traité des objets musicaux\" (Treatise on Musical Objects) which represented the culmination of some 20 years of research in the field of \"musique concrète\". In conjunction with this publication, a set of sound recordings was produced, entitled \"Le solfège de l'objet sonore\" (Music Theory of the Acoustic Object), to provide examples of concepts dealt with in the treatise.\n\nThe development of musique concrète was facilitated by the emergence of new music technology in post-war Europe. Access to microphones, phonographs, and later magnetic tape recorders (created in 1939 and acquired by the Schaeffer's Groupe de Recherche de Musique Concrète (Research Group on Concrete Music) in 1952), facilitated by an association with the French national broadcasting organization, at that time the Radiodiffusion-Télévision Française, gave Schaeffer and his colleagues an opportunity to experiment with recording technology and tape manipulation.\n\nIn 1948, a typical radio studio consisted of a series of shellac record players, a shellac record recorder, a mixing desk with rotating potentiometers, mechanical reverberation, filters, and microphones. This technology made a number of limited operations available to a composer (, ):\n\nThe application of the above technologies in the creation of musique concrète led to the development of a number of sound manipulation techniques including (, ):\n\n\nThe first tape recorders started arriving at ORTF in 1949; however, their functioning was much less reliable than the shellac players, to the point that the \"Symphonie pour un homme seul\", which was composed in 1950–51, was mainly composed with records, even if the tape recorder was available . In 1950, when the machines finally functioned correctly, the techniques of musique concrète were expanded. A range of new sound manipulation practices were explored using improved media manipulation methods and operations such as speed variation. A completely new possibility of organising sounds appears with tape editing, which permits tape to be spliced and arranged with an extraordinary new precision. The \"axe-cut junctions\" were replaced with micrometric junctions and a whole new technique of production, less dependency on performance skills, could be developed. Tape editing brought a new technique called \"micro-editing\", in which very tiny fragments of sound, representing milliseconds of time, were edited together, thus creating completely new sounds or structures .\n\nDuring the GRMC period from 1951–1958 time Schaeffer and Jacques Poullin developed a number of novel sound creation tools including a three-track tape recorder, a machine with ten playback heads to replay tape loops in echo (the morphophone), a keyboard-controlled machine to replay tape loops at twenty-four preset speeds (the keyboard, chromatic, or Tolana phonogène), a slide-controlled machine to replay tape loops at a continuously variable range of speeds (the handle, continuous, or Sareg phonogène), and a device to distribute an encoded track across four loudspeakers, including one hanging from the centre of the ceiling (the potentiomètre d'espace) .\n\nSpeed variation was a powerful tool for sound design applications. It had been identified that transformations brought about by varying playback speed lead to modification in the character of the sound material:\nThe phonogène was a machine capable of modifying sound structure significantly and it provided composers with a means to adapt sound to meet specific compositional contexts. The initial phonogènes were manufactured in 1953 by two subcontractors: the chromatic phonogène by a company called Tolana, and the sliding version by the SAREG Company . A third version was developed later at ORTF. An outline of the unique capabilities of the various phonogènes can be seen here:\n\n\nThis original tape recorder was one of the first machines permitting the simultaneous listening of several synchronised sources. Until 1958 musique concrète, radio and the studio machines were monophonic. The three-head tape recorder superposed three magnetic tapes that were dragged by a common motor, each tape having an independent spool. The objective was to keep the three tapes synchronised from a common starting point. Works could then be conceived polyphonically, and thus each head conveyed a part of the information and was listened to through a dedicated loudspeaker. It was an ancestor of the multi-track player (four then eight tracks) that appeared in the 1960s. \"Timbres Durées\" by Olivier Messiaen with the technical assistance of Pierre Henry was the first work composed for this tape recorder in 1952. A rapid rhythmic polyphony was distributed over the three channels .\nThis machine was conceived to build complex forms through repetition, and accumulation of events through delays, filtering and feedback. It consisted of a large rotating disk, 50 cm in diameter, on which was stuck a tape with its magnetic side facing outward. A series of twelve movable magnetic heads (one each recording head and erasing head, and ten playback heads) were positioned around the disk, in contact with the tape. A sound up to four seconds long could be recorded on the looped tape and the ten playback heads would then read the information with different delays, according to their (adjustable) positions around the disk. A separate amplifier and band-pass filter for each head could modify the spectrum of the sound, and additional feedback loops could transmit the information to the recording head. The resulting repetitions of a sound occurred at different time intervals, and could be filtered or modified through feedback. This system was also easily capable of producing artificial reverberation or continuous sounds .\n\nAt the premiere of Pierre Schaeffer's \"Symphonie pour un homme seul\" in 1951, a system that was designed for the spatial control of sound was tested. It was called a \"relief desk\" (\"pupitre de relief\", but also referred to as \"pupitre d'espace\" or \"potentiomètre d'espace\") and was intended to control the dynamic level of music played from several shellac players. This created a stereophonic effect by controlling the positioning of a monophonic sound source . One of five tracks, provided by a purpose-built tape machine, was controlled by the performer and the other four tracks each supplied a single loudspeaker. This provided a mixture of live and preset sound positions . The placement of loudspeakers in the performance space included two loudspeakers at the front right and left of the audience, one placed at the rear, and in the centre of the space a loudspeaker was placed in a high position above the audience. The sounds could therefore be moved around the audience, rather than just across the front stage. On stage, the control system allowed a performer to position a sound either to the left or right, above or behind the audience, simply by moving a small, hand held transmitter coil towards or away from four somewhat larger receiver coils arranged around the performer in a manner reflecting the loudspeaker positions . A contemporary eyewitness described the \"potentiomètre d'espace\" in normal use:\n\nOne found one's self sitting in a small studio which was equipped with four loudspeakers—two in front of one—right and left; one behind one and a fourth suspended above. In the front center were four large loops and an \"executant\" moving a small magnetic unit through the air. The four loops controlled the four speakers, and while all four were giving off sounds all the time, the distance of the unit from the loops determined the volume of sound sent out from each.<br>The music thus came to one at varying intensity from various parts of the room, and this \"spatial projection\" gave new sense to the rather abstract sequence of sound originally recorded. The central concept underlying this method was the notion that music should be controlled during public presentation in order to create a performance situation; an attitude that has stayed with acousmatic music to the present day .\n\nAfter the longstanding rivalry with the \"electronic music\" of the Cologne studio had subsided, in 1970 the GRM finally created an electronic studio using tools developed by the physicist Enrico Chiarucci, called the Studio 54, which featured the \"Coupigny modular synthesiser\" and a Moog synthesiser . The Coupigny synthesiser, named for its designer François Coupigny, director of the Group for Technical Research (Battier 2007, 200), and the Studio 54 mixing desk had a major influence on the evolution of GRM and from the point of their introduction on they brought a new quality to the music . The mixing desk and synthesiser were combined in one unit and were created specifically for the creation of musique concrète.\n\nThe design of the desk was influenced by trade union rules at French National Radio that required technicians and production staff to have clearly defined duties. The solitary practice of musique concrète composition did not suit a system that involved three operators: one in charge of the machines, a second controlling the mixing desk, and third to provide guidance to the others. Because of this the synthesiser and desk were combined and organised in a manner that allowed it to be used easily by a composer. Independently of the mixing tracks (twenty-four in total), it had a coupled connection patch that permitted the organisation of the machines within the studio. It also had a number of remote controls for operating tape recorders. The system was easily adaptable to any context, particularly that of interfacing with external equipment .\n\nBefore the late 1960s the musique concrète produced at GRM had largely been based on the recording and manipulation of sounds, but synthesised sounds had featured in a number of works prior to the introduction of the Coupigny. Pierre Henry had used oscillators to produce sounds as early as 1955. But a synthesiser with envelope control was something Pierre Schaeffer was against, since it favoured the preconception of music and therefore deviated from Schaeffer's principal of \"making through listening\" . Because of Schaeffer's concerns the Coupigny synthesiser was conceived as a sound-event generator with parameters controlled globally, without a means to define values as precisely as some other synthesisers of the day .\n\nThe development of the machine was constrained by several factors. It needed to be modular and the modules had to be easily interconnected (so that the synthesiser would have more modules than slots and it would have an easy-to-use patch). It also needed to include all the major functions of a modular synthesiser including oscillators, noise-generators, filters, ring-modulators, but an intermodulation facility was viewed as the primary requirement; to enable complex synthesis processes such as frequency modulation, amplitude modulation, and modulation via an external source. No keyboard was attached to the synthesiser and instead a specific and somewhat complex envelope generator was used to shape sound. This synthesiser was well-adapted to the production of continuous and complex sounds using intermodulation techniques such as cross-synthesis and frequency modulation but was less effective in generating precisely defined frequencies and triggering specific sounds .\n\nThe Coupigny synthesiser also served as the model for a smaller, portable unit, which has been used down to the present day .\n\nIn 1966 composer and technician François Bayle was placed in charge of the Groupe de Recherches Musicales and in 1975, GRM was integrated with the new Institut national de l'audiovisuel (INA – Audiovisual National Institute) with Bayle as its head. In taking the lead on work that began in the early 1950s, with Jacques Poullin's potentiomètre d'espace, a system designed to move monophonic sound sources across four speakers, Bayle and the engineer Jean-Claude Lallemand created an orchestra of loudspeakers (\"un orchestre de haut-parleurs\") known as the Acousmonium in 1974 . An inaugural concert took place on 14 February 1974 at the Espace Pierre Cardin in Paris with a presentation of Bayle's \"Expérience acoustique\" .\n\nThe Acousmonium is a specialised sound reinforcement system consisting of between 50 and 100 loudspeakers, depending on the character of the concert, of varying shape and size. The system was designed specifically for the concert presentation of musique-concrète-based works but with the added enhancement of sound spatialisation. Loudspeakers are placed both on stage and at positions throughout the performance space and a mixing console is used to manipulate the placement of acousmatic material across the speaker array, using a performative technique known as \"sound diffusion\" . Bayle has commented that the purpose of the Acousmonium is to \"\"substitute a momentary classical disposition of sound making, which diffuses the sound from the circumference towards the centre of the hall, by a group of sound projectors which form an 'orchestration' of the acoustic image\"\" .\n\nAs of 2010, the Acousmonium was still performing, with 64 speakers, 35 amplifiers, and 2 consoles .\n\n\n\n\n"}
{"id": "20018", "url": "https://en.wikipedia.org/wiki?curid=20018", "title": "Metric space", "text": "Metric space\n\nIn mathematics, a metric space is a set together with a metric on the set. The metric is a function that defines a concept of \"distance\" between any two members of the set, which are usually called points. The metric satisfies a few simple properties. Informally:\n\nA metric on a space induces topological properties like open and closed sets, which lead to the study of more abstract topological spaces.\n\nThe most familiar metric space is 3-dimensional Euclidean space. In fact, a \"metric\" is the generalization of the Euclidean metric arising from the four long-known properties of the Euclidean distance. The Euclidean metric defines the distance between two points as the length of the straight line segment connecting them. Other metric spaces occur for example in elliptic geometry and hyperbolic geometry, where distance on a sphere measured by angle is a metric, and the hyperboloid model of hyperbolic geometry is used by special relativity as a metric space of velocities.\n\nIn 1906 Maurice Fréchet introduced metric spaces in his work \"Sur quelques points du calcul fonctionnel\". However the name is due to Felix Hausdorff.\n\nA metric space is an ordered pair formula_1 where formula_2 is a set and formula_3 is a metric on formula_2, i.e., a function\n\nsuch that for any formula_6, the following holds:\n\nThe first condition follows from the other three. Since for any formula_7:\n\nThe function formula_3 is also called \"distance function\" or simply \"distance\". Often, formula_3 is omitted and one just writes formula_2 for a metric space if it is clear from the context what metric is used.\n\nIgnoring mathematical details, for any system of roads and terrains the distance between two locations can be defined as the length of the shortest route connecting those locations. To be a metric there shouldn't be any one-way roads. The triangle inequality expresses the fact that detours aren't shortcuts. If the distance between two points is zero, the two points are indistinguishable from one-another. Many of the examples below can be seen as concrete versions of this general idea.\n\n\nEvery metric space is a topological space in a natural manner, and therefore all definitions and theorems about general topological spaces also apply to all metric spaces.\n\nAbout any point formula_14 in a metric space formula_2 we define the open ball of radius formula_77 (where formula_78 is a real number) about formula_14 as the set\nThese open balls form the base for a topology on \"M\", making it a topological space.\n\nExplicitly, a subset formula_81 of formula_2 is called open if for every formula_14 in formula_81 there exists an formula_77 such that formula_86 is contained in formula_81. The complement of an open set is called closed. A neighborhood of the point formula_14 is any subset of formula_2 that contains an open ball about formula_14 as a subset.\n\nA topological space which can arise in this way from a metric space is called a metrizable space; see the article on metrization theorems for further details.\n\nA sequence (formula_91) in a metric space formula_2 is said to converge to the limit formula_93 iff for every formula_94, there exists a natural number \"N\" such that formula_95 for all formula_96. Equivalently, one can use the general definition of convergence available in all topological spaces.\n\nA subset formula_68 of the metric space formula_2 is closed iff every sequence in formula_68 that converges to a limit in formula_2 has its limit in formula_68.\n\nA metric space formula_2 is said to be complete if every Cauchy sequence converges in formula_2. That is to say: if formula_104 as both formula_73 and formula_72 independently go to infinity, then there is some formula_107 with formula_108.\n\nEvery Euclidean space is complete, as is every closed subset of a complete space. The rational numbers, using the absolute value metric formula_109, are not complete.\n\nEvery metric space has a unique (up to isometry) completion, which is a complete space that contains the given space as a dense subset. For example, the real numbers are the completion of the rationals.\n\nIf formula_30 is a complete subset of the metric space formula_2, then formula_30 is closed in formula_2. Indeed, a space is complete iff it is closed in any containing metric space.\n\nEvery complete metric space is a Baire space.\n\nA metric space \"M\" is called bounded if there exists some number \"r\", such that \"d\"(\"x\",\"y\") ≤ \"r\" for all \"x\" and \"y\" in \"M\". The smallest possible such \"r\" is called the diameter of \"M\". The space \"M\" is called precompact or totally bounded if for every \"r\" > 0 there exist finitely many open balls of radius \"r\" whose union covers \"M\". Since the set of the centres of these balls is finite, it has finite diameter, from which it follows (using the triangle inequality) that every totally bounded space is bounded. The converse does not hold, since any infinite set can be given the discrete metric (one of the examples above) under which it is bounded and yet not totally bounded.\n\nNote that in the context of intervals in the space of real numbers and occasionally regions in a Euclidean space formula_114 a bounded set is referred to as \"a finite interval\" or \"finite region\". However boundedness should not in general be confused with \"finite\", which refers to the number of elements, not to how far the set extends; finiteness implies boundedness, but not conversely. Also note that an unbounded subset of formula_114 may have a finite volume.\n\nA metric space \"M\" is compact if every sequence in \"M\" has a subsequence that converges to a point in \"M\". This is known as sequential compactness and, in metric spaces (but not in general topological spaces), is equivalent to the topological notions of countable compactness and compactness defined via open covers.\n\nExamples of compact metric spaces include the closed interval [0,1] with the absolute value metric, all metric spaces with finitely many points, and the Cantor set. Every closed subset of a compact space is itself compact.\n\nA metric space is compact iff it is complete and totally bounded. This is known as the Heine–Borel theorem. Note that compactness depends only on the topology, while boundedness depends on the metric.\n\nLebesgue's number lemma states that for every open cover of a compact metric space \"M\", there exists a \"Lebesgue number\" δ such that every subset of \"M\" of diameter < δ is contained in some member of the cover.\n\nEvery compact metric space is second countable, and is a continuous image of the Cantor set. (The latter result is due to Pavel Alexandrov and Urysohn.)\n\nA metric space is said to be locally compact if every point has a compact neighborhood. Euclidean spaces are locally compact, but infinite-dimensional Banach spaces are not.\n\nA space is proper if every closed ball {\"y\" : \"d\"(\"x\",\"y\") ≤ \"r\"} is compact. Proper spaces are locally compact, but the converse is not true in general.\n\nA metric space formula_2 is connected if the only subsets that are both open and closed are the empty set and formula_2 itself.\n\nA metric space formula_2 is path connected if for any two points formula_7 there exists a continuous map formula_120 with formula_121 and formula_122.\nEvery path connected space is connected, but the converse is not true in general.\n\nThere are also local versions of these definitions: locally connected spaces and locally path connected spaces.\n\nSimply connected spaces are those that, in a certain sense, do not have \"holes\".\n\nA metric space is separable space if it has a countable dense subset. Typical examples are the real numbers or any Euclidean space. For metric spaces (but not for general topological spaces) separability is equivalent to second-countability and also to the Lindelöf property.\n\nIf formula_30 is a nonempty metric space and formula_124 then formula_125 is called a \"pointed metric space\", and formula_126 is called a \"distinguished point\". Note that a pointed metric space is just a nonempty metric space with attention drawn to its distinguished point, and that any nonempty metric space can be viewed as a pointed metric space. The distinguished point is sometimes denoted formula_23 due to its similar behavior to zero in certain contexts.\n\nSuppose (\"M\",\"d\") and (\"M\",\"d\") are two metric spaces.\n\nThe map \"f\":\"M\"→\"M\" is continuous\nif it has one (and therefore all) of the following equivalent properties:\n\nMoreover, \"f\" is continuous if and only if it is continuous on every compact subset of \"M\".\n\nThe image of every compact set under a continuous function is compact, and the image of every connected set under a continuous function is connected.\n\nThe map \"ƒ\" : \"M\" → \"M\" is uniformly continuous if for every \"ε\" > 0 there exists \"δ\" > 0 such that\n\nEvery uniformly continuous map \"ƒ\" : \"M\" → \"M\" is continuous. The converse is true if \"M\" is compact (Heine–Cantor theorem).\n\nUniformly continuous maps turn Cauchy sequences in \"M\" into Cauchy sequences in \"M\". For continuous maps this is generally wrong; for example, a continuous map\nfrom the open interval (0,1) \"onto\" the real line turns some Cauchy sequences into unbounded sequences.\n\nGiven a real number \"K\" > 0, the map \"ƒ\" : \"M\" → \"M\" is \"K\"-Lipschitz continuous if\n\nEvery Lipschitz-continuous map is uniformly continuous, but the converse is not true in general.\n\nIf \"K\" < 1, then \"ƒ\" is called a contraction. Suppose \"M\" = \"M\" and \"M\" is complete. If \"ƒ\" is a contraction, then \"ƒ\" admits a unique fixed point (Banach fixed point theorem). If \"M\" is compact, the condition can be weakened a bit: \"ƒ\" admits a unique fixed point if\n\nThe map \"f\":\"M\"→\"M\" is an isometry if\nIsometries are always injective; the image of a compact or complete set under an isometry is compact or complete, respectively. However, if the isometry is not surjective, then the image of a closed (or open) set need not be closed (or open).\n\nThe map \"f\" : \"M\" → \"M\" is a quasi-isometry if there exist constants \"A\" ≥ 1 and \"B\" ≥ 0 such that\n\nand a constant \"C\" ≥ 0 such that every point in \"M\" has a distance at most \"C\" from some point in the image \"f\"(\"M\").\n\nNote that a quasi-isometry is not required to be continuous. Quasi-isometries compare the \"large-scale structure\" of metric spaces; they find use in geometric group theory in relation to the word metric.\n\nGiven two metric spaces (\"M\", \"d\") and (\"M\", \"d\"):\n\n\nMetric spaces are paracompact Hausdorff spaces and hence normal (indeed they are perfectly normal). An important consequence is that every metric space admits partitions of unity and that every continuous real-valued function defined on a closed subset of a metric space can be extended to a continuous map on the whole space (Tietze extension theorem). It is also true that every real-valued Lipschitz-continuous map defined on a subset of a metric space can be extended to a Lipschitz-continuous map on the whole space.\n\nMetric spaces are first countable since one can use balls with rational radius as a neighborhood base.\n\nThe metric topology on a metric space \"M\" is the coarsest topology on \"M\" relative to which the metric \"d\" is a continuous map from the product of \"M\" with itself to the non-negative real numbers.\n\nA simple way to construct a function separating a point from a closed set (as required for a completely regular space) is to consider the distance between the point and the set. If (\"M\",\"d\") is a metric space, \"S\" is a subset of \"M\" and \"x\" is a point of \"M\", we define the distance from \"x\" to \"S\" as\n\nThen \"d\"(\"x\", \"S\") = 0 if and only if \"x\" belongs to the closure of \"S\". Furthermore, we have the following generalization of the triangle inequality:\nwhich in particular shows that the map formula_137 is continuous.\n\nGiven two subsets \"S\" and \"T\" of \"M\", we define their Hausdorff distance to be\n\nIn general, the Hausdorff distance \"d\"(\"S\",\"T\") can be infinite. Two sets are close to each other in the Hausdorff distance if every element of either set is close to some element of the other set.\n\nThe Hausdorff distance \"d\" turns the set \"K\"(\"M\") of all non-empty compact subsets of \"M\" into a metric space. One can show that \"K\"(\"M\") is complete if \"M\" is complete.\n\nOne can then define the Gromov–Hausdorff distance between any two metric spaces by considering the minimal Hausdorff distance of isometrically embedded versions of the two spaces. Using this distance, the class of all (isometry classes of) compact metric spaces becomes a metric space in its own right.\n\nIf formula_140 are metric spaces, and \"N\" is the Euclidean norm on \"R\", then formula_141 is a metric space, where the product metric is defined by\n\nand the induced topology agrees with the product topology. By the equivalence of norms in finite dimensions, an equivalent metric is obtained if \"N\" is the taxicab norm, a p-norm, the max norm, or any other norm which is non-decreasing as the coordinates of a positive \"n\"-tuple increase (yielding the triangle inequality).\n\nSimilarly, a countable product of metric spaces can be obtained using the following metric\n\nAn uncountable product of metric spaces need not be metrizable. For example, formula_144 is not first-countable and thus isn't metrizable.\n\nIn the case of a single space formula_1, the distance map formula_146 (from the definition) is uniformly continuous with respect to any of the above product metrics formula_147, and in particular is continuous with respect to the product topology of formula_148.\n\nIf \"M\" is a metric space with metric \"d\", and \"~\" is an equivalence relation on \"M\", then we can endow the quotient set \"M/~\" with the following (pseudo)metric. Given two equivalence classes [\"x\"] and [\"y\"], we define\n\nwhere the infimum is taken over all finite sequences formula_150 and formula_151 with formula_152, formula_153, formula_154. In general this will only define a pseudometric, i.e. formula_155 does not necessarily imply that formula_156. However, for nice equivalence relations (e.g., those given by gluing together polyhedra along faces), it is a metric.\n\nThe quotient metric \"d\" is characterized by the following universal property. If formula_157 is a metric map between metric spaces (that is, formula_158 for all \"x\", \"y\") satisfying \"f\"(\"x\")=\"f\"(\"y\") whenever formula_159 then the induced function formula_160, given by formula_161, is a metric map formula_162\n\nA topological space is sequential if and only if it is a quotient of a metric space.\n\n\nThe ordered set formula_164 can be seen as a category by requesting exactly one morphism formula_165 if formula_166 and none otherwise. By using formula_167 as the tensor product and formula_23 as the identity, it becomes a monoidal category formula_169.\nEvery metric space formula_1 can now be viewed as a category formula_171 enriched over formula_169:\n\nSee the paper by F.W. Lawvere listed below.\n\n\nThis is reprinted (with author commentary) at Reprints in Theory and Applications of Categories\nAlso (with an author commentary) in Enriched categories in the logic of geometry and analysis. Repr. Theory Appl. Categ. No. 1 (2002), 1–37.\n\n"}
{"id": "20021", "url": "https://en.wikipedia.org/wiki?curid=20021", "title": "Marine biology", "text": "Marine biology\n\nMarine biology is the scientific study of marine life, organisms in the sea. Given that in biology many phyla, families and genera have some species that live in the sea and others that live on land, marine biology classifies species based on the environment rather than on taxonomy.\n\nA large proportion of all life on Earth lives in the ocean. The exact size of this \"large proportion\" is unknown, since many ocean species are still to be discovered. The ocean is a complex three-dimensional world covering approximately 71% of the Earth's surface. The habitats studied in marine biology include everything from the tiny layers of surface water in which organisms and abiotic items may be trapped in surface tension between the ocean and atmosphere, to the depths of the oceanic trenches, sometimes 10,000 meters or more beneath the surface of the ocean. Specific habitats include coral reefs, kelp forests, seagrass meadows, the surrounds of seamounts and thermal vents, tidepools, muddy, sandy and rocky bottoms, and the open ocean (pelagic) zone, where solid objects are rare and the surface of the water is the only visible boundary. The organisms studied range from microscopic phytoplankton and zooplankton to huge cetaceans (whales) in length. Marine ecology is the study of how marine organisms interact with each other and the environment.\n\nMarine life is a vast resource, providing food, medicine, and raw materials, in addition to helping to support recreation and tourism all over the world. At a fundamental level, marine life helps determine the very nature of our planet. Marine organisms contribute significantly to the oxygen cycle, and are involved in the regulation of the Earth's climate. Shorelines are in part shaped and protected by marine life, and some marine organisms even help create new land.\n\nMany species are economically important to humans, including both finfish and shellfish. It is also becoming understood that the well-being of marine organisms and other organisms are linked in fundamental ways. The human body of knowledge regarding the relationship between life in the sea and important cycles is rapidly growing, with new discoveries being made nearly every day. These cycles include those of matter (such as the carbon cycle) and of air (such as Earth's respiration, and movement of energy through ecosystems including the ocean). Large areas beneath the ocean surface still remain effectively unexplored.\n\nThe study of marine biology dates back to Aristotle (384–322 BC), who made many observations of life in the sea around Lesbos, laying the foundation for many future discoveries. In 1768, Samuel Gottlieb Gmelin (1744–1774) published the \"Historia Fucorum\", the first work dedicated to marine algae and the first book on marine biology to use the then new binomial nomenclature of Linnaeus. It included elaborate illustrations of seaweed and marine algae on folded leaves. The British naturalist Edward Forbes (1815–1854) is generally regarded as the founder of the science of marine biology. The pace of oceanographic and marine biology studies quickly accelerated during the course of the 19th century.\n\nThe observations made in the first studies of marine biology fueled the age of discovery and exploration that followed. During this time, a vast amount of knowledge was gained about the life that exists in the oceans of the world. Many voyages contributed significantly to this pool of knowledge. Among the most significant were the voyages of where Charles Darwin came up with his theories of evolution and on the formation of coral reefs. Another important expedition was undertaken by HMS \"Challenger\", where findings were made of unexpectedly high species diversity among fauna stimulating much theorizing by population ecologists on how such varieties of life could be maintained in what was thought to be such a hostile environment. This era was important for the history of marine biology but naturalists were still limited in their studies because they lacked technology that would allow them to adequately examine species that lived in deep parts of the oceans.\n\nThe creation of marine laboratories was important because it allowed marine biologists to conduct research and process their specimens from expeditions. The oldest marine laboratory in the world, Station biologique de Roscoff, was established in France in 1872. In the United States, the Scripps Institution of Oceanography dates back to 1903, while the prominent Woods Hole Oceanographic Institute was founded in 1930. The development of technology such as sound navigation ranging, scuba diving gear, submersibles and remotely operated vehicles allowed marine biologists to discover and explore life in deep oceans that was once thought to not exist.\n\nAs inhabitants of the largest environment on Earth, microbial marine systems drive changes in every global system. Microbes are responsible for virtually all the photosynthesis that occurs in the ocean, as well as the cycling of carbon, nitrogen, phosphorus and other nutrients and trace elements.\n\nMicroscopic life undersea is incredibly diverse and still poorly understood. For example, the role of viruses in marine ecosystems is barely being explored even in the beginning of the 21st century.\n\nThe role of phytoplankton is better understood due to their critical position as the most numerous primary producers on Earth. Phytoplankton are categorized into cyanobacteria (also called blue-green algae/bacteria), various types of algae (red, green, brown, and yellow-green), diatoms, dinoflagellates, euglenoids, coccolithophorids, cryptomonads, chrysophytes, chlorophytes, prasinophytes, and silicoflagellates.\n\nZooplankton tend to be somewhat larger, and not all are microscopic. Many Protozoa are zooplankton, including dinoflagellates, zooflagellates, foraminiferans, and radiolarians. Some of these (such as dinoflagellates) are also phytoplankton; the distinction between plants and animals often breaks down in very small organisms. Other zooplankton include cnidarians, ctenophores, chaetognaths, molluscs, arthropods, urochordates, and annelids such as polychaetes. Many larger animals begin their life as zooplankton before they become large enough to take their familiar forms. Two examples are fish larvae and sea stars (also called starfish).\n\nMicroscopic algae and plants provide important habitats for life, sometimes acting as hiding places for larval forms of larger fish and foraging places for invertebrates.\n\nAlgal life is widespread and very diverse under the ocean. Microscopic photosynthetic algae contribute a larger proportion of the world's photosynthetic output than all the terrestrial forests combined. Most of the niche occupied by sub plants on land is actually occupied by macroscopic algae in the ocean, such as \"Sargassum\" and kelp, which are commonly known as seaweeds that create kelp forests.\n\nPlants that survive in the sea are often found in shallow waters, such as the seagrasses (examples of which are eelgrass, \"Zostera\", and turtle grass, \"Thalassia\"). These plants have adapted to the high salinity of the ocean environment. The intertidal zone is also a good place to find plant life in the sea, where mangroves or cordgrass or beach grass might grow. \n\nAs on land, invertebrates make up a huge portion of all life in the sea. Invertebrate sea life includes Cnidaria such as jellyfish and sea anemones; Ctenophora; sea worms including the phyla Platyhelminthes, Nemertea, Annelida, Sipuncula, Echiura, Chaetognatha, and Phoronida; Mollusca including shellfish, squid, octopus; Arthropoda including Chelicerata and Crustacea; Porifera; Bryozoa; Echinodermata including starfish; and Urochordata including sea squirts or tunicates. Invertebrates have no backbone. There are over a million species.\n\nOver 1500 species of fungi are known from marine environments. These are parasitic on marine algae or animals, or are saprobes on algae, corals, protozoan cysts, sea grasses, wood and other substrata, and can also be found in sea foam. Spores of many species have special appendages which facilitate attachment to the substratum. A very diverse range of unusual secondary metabolites is produced by marine fungi.\n\nA reported 33,400 species of fish, including bony and cartilaginous fish, had been described by 2016, more than all other vertebrates combined. About 60% of fish species live in saltwater.\n\nReptiles which inhabit or frequent the sea include sea turtles, sea snakes, terrapins, the marine iguana, and the saltwater crocodile. Most extant marine reptiles, except for some sea snakes, are oviparous and need to return to land to lay their eggs. Thus most species, excepting sea turtles, spend most of their lives on or near land rather than in the ocean. Despite their marine adaptations, most sea snakes prefer shallow waters nearby land, around islands, especially waters that are somewhat sheltered, as well as near estuaries. Some extinct marine reptiles, such as ichthyosaurs, evolved to be viviparous and had no requirement to return to land.\n\nBirds adapted to living in the marine environment are often called seabirds. Examples include albatross, penguins, gannets, and auks. Although they spend most of their lives in the ocean, species such as gulls can often be found thousands of miles inland.\n\nThere are five main types of marine mammals, namely cetaceans (toothed whales and baleen whales); sirenians such as manatees; pinnipeds including seals and the walrus; sea otters; and the \npolar bear. All are air-breathing, and while some such as the sperm whale can dive for prolonged periods, all must return to the surface to breathe.\n\nMarine habitats can be divided into coastal and open ocean habitats. Coastal habitats are found in the area that extends from the shoreline to the edge of the continental shelf. Most marine life is found in coastal habitats, even though the shelf area occupies only seven percent of the total ocean area. Open ocean habitats are found in the deep ocean beyond the edge of the continental shelf. Alternatively, marine habitats can be divided into pelagic and demersal habitats. Pelagic habitats are found near the surface or in the open water column, away from the bottom of the ocean and affected by ocean currents, while demersal habitats are near or on the bottom. Marine habitats can be modified by their inhabitants. Some marine organisms, like corals, kelp and sea grasses, are ecosystem engineers which reshape the marine environment to the point where they create further habitat for other organisms.\n\nIntertidal zones, the areas that are close to the shore, are constantly being exposed and covered by the ocean's tides. A huge array of life can be found within this zone. Shore habitats span from the upper intertidal zones to the area where land vegetation takes prominence. It can be underwater anywhere from daily to very infrequently. Many species here are scavengers, living off of sea life that is washed up on the shore. Many land animals also make much use of the shore and intertidal habitats. A subgroup of organisms in this habitat bores and grinds exposed rock through the process of bioerosion.\n\nEstuaries are also near shore and influenced by the tides. An estuary is a partially enclosed coastal body of water with one or more rivers or streams flowing into it and with a free connection to the open sea. Estuaries form a transition zone between freshwater river environments and saltwater maritime environments. They are subject both to marine influences—such as tides, waves, and the influx of saline water—and to riverine influences—such as flows of fresh water and sediment. The shifting flows of both sea water and fresh water provide high levels of nutrients both in the water column and in sediment, making estuaries among the most productive natural habitats in the world.\n\nReefs comprise some of the densest and most diverse habitats in the world. The best-known types of reefs are tropical coral reefs which exist in most tropical waters; however, reefs can also exist in cold water. Reefs are built up by corals and other calcium-depositing animals, usually on top of a rocky outcrop on the ocean floor. Reefs can also grow on other surfaces, which has made it possible to create artificial reefs. Coral reefs also support a huge community of life, including the corals themselves, their symbiotic zooxanthellae, tropical fish and many other organisms.\n\nMuch attention in marine biology is focused on coral reefs and the El Niño weather phenomenon. In 1998, coral reefs experienced the most severe mass bleaching events on record, when vast expanses of reefs across the world died because sea surface temperatures rose well above normal. Some reefs are recovering, but scientists say that between 50% and 70% of the world's coral reefs are now endangered and predict that global warming could exacerbate this trend.\n\nThe open ocean is relatively unproductive because of a lack of nutrients, yet because it is so vast, in total it produces the most primary productivity. The open ocean is separated into different zones, and the different zones each have different ecologies. Zones which vary according to their depth include the epipelagic, mesopelagic, bathypelagic, abyssopelagic, and hadopelagic zones. Zones which vary by the amount of light they receive include the photic and aphotic zones. Much of the aphotic zone's energy is supplied by the open ocean in the form of detritus.\n\nThe deepest recorded oceanic trench measured to date is the Mariana Trench, near the Philippines, in the Pacific Ocean at . At such depths, water pressure is extreme and there is no sunlight, but some life still exists. A white flatfish, a shrimp and a jellyfish were seen by the American crew of the bathyscaphe \"Trieste\" when it dove to the bottom in 1960. In general, the deep sea is considered to start at the aphotic zone, the point where sunlight loses its power of transference through the water. Many life forms that live at these depths have the ability to create their own light known as bio-luminescence. Marine life also flourishes around seamounts that rise from the depths, where fish and other sea life congregate to spawn and feed. Hydrothermal vents along the mid-ocean ridge spreading centers act as oases, as do their opposites, cold seeps. Such places support unique biomes and many new microbes and other lifeforms have been discovered at these locations .\n\nThe marine ecosystem is large, and thus there are many sub-fields of marine biology. Most involve studying specializations of particular animal groups, such as phycology, invertebrate zoology and ichthyology. Other subfields study the physical effects of continual immersion in sea water and the ocean in general, adaptation to a salty environment, and the effects of changing various oceanic properties on marine life. A subfield of marine biology studies the relationships between oceans and ocean life, and global warming and environmental issues (such as carbon dioxide displacement). Recent marine biotechnology has focused largely on marine biomolecules, especially proteins, that may have uses in medicine or engineering. Marine environments are the home to many exotic biological materials that may inspire biomimetic materials.\n\nMarine biology is a branch of biology. It is closely linked to oceanography and may be regarded as a sub-field of marine science. It also encompasses many ideas from ecology. Fisheries science and marine conservation can be considered partial offshoots of marine biology (as well as environmental studies). Marine Chemistry, Physical oceanography and Atmospheric sciences are closely related to this field.\n\nAn active research topic in marine biology is to discover and map the life cycles of various species and where they spend their time. Technologies that aid in this discovery include pop-up satellite archival tags, acoustic tags, and a variety of other data loggers. Marine biologists study how the ocean currents, tides and many other oceanic factors affect ocean life forms, including their growth, distribution and well-being. This has only recently become technically feasible with advances in GPS and newer underwater visual devices.\n\nMost ocean life breeds in specific places, nests or not in others, spends time as juveniles in still others, and in maturity in yet others. Scientists know little about where many species spend different parts of their life cycles especially in the infant and juvenile years. For example, it is still largely unknown where juvenile sea turtles and some year-1 sharks travel. Recent advances in underwater tracking devices are illuminating what we know about marine organisms that live at great Ocean depths. The information that pop-up satellite archival tags give aids in certain time of the year fishing closures and development of a marine protected area. This data is important to both scientists and fishermen because they are discovering that by restricting commercial fishing in one small area they can have a large impact in maintaining a healthy fish population in a much larger area.\n\n\n"}
{"id": "20023", "url": "https://en.wikipedia.org/wiki?curid=20023", "title": "Microkernel", "text": "Microkernel\n\nIn computer science, a microkernel (often abbreviated as μ-kernel) is the near-minimum amount of software that can provide the mechanisms needed to implement an operating system (OS). These mechanisms include low-level address space management, thread management, and inter-process communication (IPC).\n\nIf the hardware provides multiple rings or CPU modes, the microkernel may be the only software executing at the most privileged level, which is generally referred to as supervisor or kernel mode. Traditional operating system functions, such as device drivers, protocol stacks and file systems, are typically removed from the microkernel itself and are instead run in user space.\n\nIn terms of the source code size, as a general rule microkernels tend to be smaller than monolithic kernels. The MINIX 3 microkernel, for example, has approximately 12,000 lines of code.\n\nMicrokernels trace their roots back to Danish computer pioneer Per Brinch Hansen and his tenure in Danish computer company Regnecentralen where he led software development efforts for the RC 4000 computer.\nIn 1967, Regnecentralen was installing a RC 4000 prototype in a Polish fertilizer plant in Puławy. The computer used a small real-time operating system tailored for the needs of the plant. Brinch Hansen and his team became concerned with the lack of generality and reusability of the RC 4000 system. They feared that each installation would require a different operating system so they started to investigate novel and more general ways of creating software for the RC 4000.\nIn 1969, their effort resulted in the completion of the RC 4000 Multiprogramming System. Its nucleus provided inter-process communication based on message-passing for up to 23 unprivileged processes, out of which 8 at a time were protected from one another. It further implemented scheduling of time slices of programs executed in parallel, initiation and control of program execution at the request of other running programs, and initiation of data transfers to or from peripherals. Besides these elementary mechanisms, it had no built-in strategy for program execution and resource allocation. This strategy was to be implemented by a hierarchy of running programs in which parent processes had complete control over child processes and acted as their operating systems.\n\nFollowing Brinch Hansen's work, microkernels have been developed since the 1970s The term microkernel itself appeared no later than 1981. Microkernels were meant as a response to changes in the computer world, and to several challenges adapting existing \"mono-kernels\" to these new systems. New device drivers, protocol stacks, file systems and other low-level systems were being developed all the time. This code was normally located in the monolithic kernel, and thus required considerable work and careful code management to work on. Microkernels were developed with the idea that all of these services would be implemented as user-space programs, like any other, allowing them to be worked on monolithically and started and stopped like any other program. This would not only allow these services to be more easily worked on, but also separated the kernel code to allow it to be finely tuned without worrying about unintended side effects. Moreover, it would allow entirely new operating systems to be \"built up\" on a common core, aiding OS research.\n\nMicrokernels were a very hot topic in the 1980s when the first usable local area networks were being introduced. The same mechanisms that allowed the kernel to be distributed into user space also allowed the system to be distributed across network links. The first microkernels, notably Mach, proved to have disappointing performance, but the inherent advantages appeared so great that it was a major line of research into the late 1990s. However, during this time the speed of computers grew greatly in relation to networking systems, and the disadvantages in performance came to overwhelm the advantages in development terms. Many attempts were made to adapt the existing systems to have better performance, but the overhead was always considerable and most of these efforts required the user-space programs to be moved back into the kernel. By 2000, most large-scale (Mach-like) efforts had ended, although Apple's macOS, released in 2001, uses a hybrid kernel called XNU, which combines a heavily modified (hybrid) OSFMK 7.3 kernel with code from BSD UNIX, and this kernel is also used in iOS, tvOS, and watchOS. , the Mach-based GNU Hurd is also functional and included in testing versions of Arch Linux and Debian.\n\nAlthough major work on microkernels had largely ended, experimenters continued development. It has since been shown that many of the performance problems of earlier designs were not a fundamental limitation of the concept, but instead due to the designer's desire to use single-purpose systems to implement as many of these services as possible. Using a more pragmatic approach to the problem, including assembly code and relying on the processor to enforce concepts normally supported in software led to a new series of microkernels with dramatically improved performance.\n\nMicrokernels are closely related to exokernels.\nThey also have much in common with hypervisors,\nbut the latter make no claim to minimality and are specialized to supporting virtual machines; indeed, the L4 microkernel frequently finds use in a hypervisor capacity.\n\nEarly operating system kernels were rather small, partly because computer memory was limited. As the capability of computers grew, the number of devices the kernel had to control also grew. Throughout the early history of Unix, kernels were generally small, even though they contained various device drivers and file system implementations. When address spaces increased from 16 to 32 bits, kernel design was no longer constrained by the hardware architecture, and kernels began to grow larger.\n\nThe Berkeley Software Distribution (BSD) of Unix began the era of larger kernels. In addition to operating a basic system consisting of the CPU, disks and printers, BSD added a complete TCP/IP networking system and a number of \"virtual\" devices that allowed the existing programs to work 'invisibly' over the network. This growth continued for many years, resulting in kernels with millions of lines of source code. As a result of this growth, kernels were prone to bugs and became increasingly difficult to maintain.\n\nThe microkernel was intended to address this growth of kernels and the difficulties that resulted. In theory, the microkernel design allows for easier management of code due to its division into user space services. This also allows for increased security and stability resulting from the reduced amount of code running in kernel mode. For example, if a networking service crashed due to buffer overflow, only the networking service's memory would be corrupted, leaving the rest of the system still functional.\n\nInter-process communication (IPC) is any mechanism which allows separate processes to communicate with each other, usually by sending messages. Shared memory is strictly speaking also an inter-process communication mechanism, but the abbreviation IPC usually only refers to message passing, and it is the latter that is particularly relevant to microkernels. IPC allows the operating system to be built from a number of small programs called servers, which are used by other programs on the system, invoked via IPC. Most or all support for peripheral hardware is handled in this fashion, with servers for device drivers, network protocol stacks, file systems, graphics, etc.\n\nIPC can be synchronous or asynchronous. Asynchronous IPC is analogous to network communication: the sender dispatches a message and continues executing. The receiver checks (polls) for the availability of the message, or is alerted to it via some notification mechanism. Asynchronous IPC requires that the kernel maintains buffers and queues for messages, and deals with buffer overflows; it also requires double copying of messages (sender to kernel and kernel to receiver). In synchronous IPC, the first party (sender or receiver) blocks until the other party is ready to perform the IPC. It does not require buffering or multiple copies, but the implicit rendezvous can make programming tricky. Most programmers prefer asynchronous send and synchronous receive.\n\nFirst-generation microkernels typically supported synchronous as well as asynchronous IPC, and suffered from poor IPC performance. Jochen Liedtke assumed the design and implementation of the IPC mechanisms to be the underlying reason for this poor performance. In his L4 microkernel he pioneered methods that lowered IPC costs by an order of magnitude. These include an IPC system call that supports a send as well as a receive operation, making all IPC synchronous, and passing as much data as possible in registers. Furthermore, Liedtke introduced the concept of the \"direct process switch\", where during an IPC execution an (incomplete) context switch is performed from the sender directly to the receiver. If, as in L4, part or all of the message is passed in registers, this transfers the in-register part of the message without any copying at all. Furthermore, the overhead of invoking the scheduler is avoided; this is especially beneficial in the common case where IPC is used in an RPC-type fashion by a client invoking a server. Another optimization, called \"lazy scheduling\", avoids traversing scheduling queues during IPC by leaving threads that block during IPC in the ready queue. Once the scheduler is invoked, it moves such threads to the appropriate waiting queue. As in many cases a thread gets unblocked before the next scheduler invocation, this approach saves significant work. Similar approaches have since been adopted by QNX and MINIX 3.\n\nIn a series of experiments, Chen and Bershad compared memory cycles per instruction (MCPI) of monolithic Ultrix with those of microkernel Mach combined with a 4.3BSD Unix server running in user space. Their results explained Mach's poorer performance by higher MCPI and demonstrated that IPC alone is not responsible for much of the system overhead, suggesting that optimizations focused exclusively on IPC will have limited impact. Liedtke later refined Chen and Bershad's results by making an observation that the bulk of the difference between Ultrix and Mach MCPI was caused by capacity cache-misses and concluding that drastically reducing the cache working set of a microkernel will solve the problem.\n\nIn a client-server system, most communication is essentially synchronous, even if using asynchronous primitives, as the typical operation is a client invoking a server and then waiting for a reply. As it also lends itself to more efficient implementation, most microkernels generally followed L4's lead and only provided a synchronous IPC primitive. Asynchronous IPC could be implemented on top by using helper threads. However, experience has shown that the utility of synchronous IPC is dubious: synchronous IPC forces a multi-threaded design onto otherwise simple systems, with the resulting synchronization complexities. Moreover, an RPC-like server invocation sequentializes client and server, which should be avoided if they are running on separate cores. Versions of L4 deployed in commercial products have therefore found it necessary to add an asynchronous notification mechanism to better support asynchronous communication. This signal-like mechanism does not carry data and therefore does not require buffering by the kernel. By having two forms of IPC, they have nonetheless violated the principle of minimality. Other versions of L4 have switched to asynchronous IPC completely.\n\nAs synchronous IPC blocks the first party until the other is ready, unrestricted use could easily lead to deadlocks. Furthermore, a client could easily mount a denial-of-service attack on a server by sending a request and never attempting to receive the reply. Therefore, synchronous IPC must provide a means to prevent indefinite blocking. Many microkernels provide timeouts on IPC calls, which limit the blocking time. In practice, choosing sensible timeout values is difficult, and systems almost inevitably use infinite timeouts for clients and zero timeouts for servers. As a consequence, the trend is towards not providing arbitrary timeouts, but only a flag which indicates that the IPC should fail immediately if the partner is not ready. This approach effectively provides a choice of the two timeout values of zero and infinity. Recent versions of L4 and MINIX have gone down this path (older versions of L4 used timeouts, as does QNX).\n\nMicrokernel servers are essentially daemon programs like any others, except that the kernel grants some of them privileges to interact with parts of physical memory that are otherwise off limits to most programs. This allows some servers, particularly device drivers, to interact directly with hardware.\n\nA basic set of servers for a general-purpose microkernel includes file system servers, device driver servers, networking servers, display servers, and user interface device servers. This set of servers (drawn from QNX) provides roughly the set of services offered by a Unix monolithic kernel. The necessary servers are started at system startup and provide services, such as file, network, and device access, to ordinary application programs. With such servers running in the environment of a user application, server development is similar to ordinary application development, rather than the build-and-boot process needed for kernel development.\n\nAdditionally, many \"crashes\" can be corrected by simply stopping and restarting the server. However, part of the system state is lost with the failing server, hence this approach requires applications to cope with failure. A good example is a server responsible for TCP/IP connections: If this server is restarted, applications will experience a \"lost\" connection, a normal occurrence in a networked system. For other services, failure is less expected and may require changes to application code. For QNX, restart capability is offered as the QNX High Availability Toolkit.\n\nDevice drivers frequently perform direct memory access (DMA), and therefore can write to arbitrary locations of physical memory, including various kernel data structures. Such drivers must therefore be trusted. It is a common misconception that this means that they must be part of the kernel. In fact, a driver is not inherently more or less trustworthy by being part of the kernel.\n\nWhile running a device driver in user space does not necessarily reduce the damage a misbehaving driver can cause, in practice it is beneficial for system stability in the presence of buggy (rather than malicious) drivers: memory-access violations by the driver code itself (as opposed to the device) may still be caught by the memory-management hardware. Furthermore, many devices are not DMA-capable, their drivers can be made untrusted by running them in user space. Recently, an increasing number of computers feature IOMMUs, many of which can be used to restrict a device's access to physical memory. This also allows user-mode drivers to become untrusted.\n\nUser-mode drivers actually predate microkernels. The Michigan Terminal System (MTS), in 1967, supported user space drivers (including its file system support), the first operating system to be designed with that capability.\nHistorically, drivers were less of a problem, as the number of devices was small and trusted anyway, so having them in the kernel simplified the design and avoided potential performance problems. This led to the traditional driver-in-the-kernel style of Unix, Linux, and Windows NT.\nWith the proliferation of various kinds of peripherals, the amount of driver code escalated and in modern operating systems dominates the kernel in code size.\n\nAs a microkernel must allow building arbitrary operating system services on top, it must provide some core functionality. At a minimum, this includes:\n\nThis minimal design was pioneered by Brinch Hansen's Nucleus and the hypervisor of IBM's VM. It has since been formalised in Liedtke's \"minimality principle\":\nA concept is tolerated inside the microkernel only if moving it outside the kernel, i.e., permitting competing implementations, would prevent the implementation of the system's required functionality.\nEverything else can be done in a usermode program, although device drivers implemented as user programs may on some processor architectures require special privileges to access I/O hardware.\n\nRelated to the minimality principle, and equally important for microkernel design, is the separation of mechanism and policy, it is what enables the construction of arbitrary systems on top of a minimal kernel. Any policy built into the kernel cannot be overwritten at user level and therefore limits the generality of the microkernel.\nPolicy implemented in user-level servers can be changed by replacing the servers (or letting the application choose between competing servers offering similar services).\n\nFor efficiency, most microkernels contain schedulers and manage timers, in violation of the minimality principle and the principle of policy-mechanism separation.\n\nStart up (booting) of a microkernel-based system requires device drivers, which are not part of the kernel. Typically this means that they are packaged with the kernel in the boot image, and the kernel supports a bootstrap protocol that defines how the drivers are located and started; this is the traditional bootstrap procedure of L4 microkernels. Some microkernels simplify this by placing some key drivers inside the kernel (in violation of the minimality principle), LynxOS and the original Minix are examples. Some even include a file system in the kernel to simplify booting. A microkernel-based system may boot via multiboot compatible boot loader. Such systems usually load statically-linked servers to make an initial bootstrap or mount an OS image to continue bootstrapping.\n\nA key component of a microkernel is a good IPC system and virtual-memory-manager design that allows implementing page-fault handling and swapping in usermode servers in a safe way. Since all services are performed by usermode programs, efficient means of communication between programs are essential, far more so than in monolithic kernels. The design of the IPC system makes or breaks a microkernel. To be effective, the IPC system must not only have low overhead, but also interact well with CPU scheduling.\n\nOn most mainstream processors, obtaining a service is inherently more expensive in a microkernel-based system than a monolithic system. In the monolithic system, the service is obtained by a single system call, which requires two \"mode switches\" (changes of the processor's ring or CPU mode). In the microkernel-based system, the service is obtained by sending an IPC message to a server, and obtaining the result in another IPC message from the server. This requires a context switch if the drivers are implemented as processes, or a function call if they are implemented as procedures. In addition, passing actual data to the server and back may incur extra copying overhead, while in a monolithic system the kernel can directly access the data in the client's buffers.\n\nPerformance is therefore a potential issue in microkernel systems. Indeed, the experience of first-generation microkernels such as Mach and ChorusOS showed that systems based on them performed very poorly. However, Jochen Liedtke showed that Mach's performance problems were the result of poor design and implementation, specifically Mach's excessive cache footprint.\nLiedtke demonstrated with his own L4 microkernel that through careful design and implementation, and especially by following the minimality principle, IPC costs could be reduced by more than an order of magnitude compared to Mach. L4's IPC performance is still unbeaten across a range of architectures.\n\nWhile these results demonstrate that the poor performance of systems based on first-generation microkernels is not representative for second-generation kernels such as L4, this constitutes no proof that microkernel-based systems can be built with good performance. It has been shown that a monolithic Linux server ported to L4 exhibits only a few percent overhead over native Linux.\nHowever, such a single-server system exhibits few, if any, of the advantages microkernels are supposed to provide by structuring operating system functionality into separate servers.\n\nA number of commercial multi-server systems exist, in particular the real-time systems QNX and Integrity. No comprehensive comparison of performance relative to monolithic systems has been published for those multiserver systems. Furthermore, performance does not seem to be the overriding concern for those commercial systems, which instead emphasize reliably quick interrupt handling response times (QNX) and simplicity for the sake of robustness. An attempt to build a high-performance multiserver operating system was the IBM Sawmill Linux project.\nHowever, this project was never completed.\n\nIt has been shown in the meantime that user-level device drivers can come close to the performance of in-kernel drivers even for such high-throughput, high-interrupt devices as Gigabit Ethernet. This seems to imply that high-performance multi-server systems are possible.\n\nThe security benefits of microkernels have been frequently discussed. In the context of security the minimality principle of microkernels is, some have argued, a direct consequence of the principle of least privilege, according to which all code should have only the privileges needed to provide required functionality. Minimality requires that a system's trusted computing base (TCB) should be kept minimal. As the kernel (the code that executes in the privileged mode of the hardware) has unvetted access to any data and can thus violate its integrity or confidentiality, the kernel is always part of the TCB. Minimizing it is natural in a security-driven design.\n\nConsequently, microkernel designs have been used for systems designed for high-security applications, including KeyKOS, EROS and military systems. In fact common criteria (CC) at the highest assurance level (Evaluation Assurance Level (EAL) 7) has an explicit requirement that the target of evaluation be \"simple\", an acknowledgment of the practical impossibility of establishing true trustworthiness for a complex system. Unfortunately, again, the term \"simple\" is misleading and ill-defined. At least the Department of Defense Trusted Computer System Evaluation Criteria introduced somewhat more precise verbiage at the B3/A1 classes:\n\nRecent work on microkernels has been focusing on formal specifications of the kernel API, and formal proofs of the API's security properties and implementation correctness. The first example of this is a mathematical proof of the confinement mechanisms in EROS, based on a simplified model of the EROS API. More recently, a comprehensive set of machine-checked proofs has been performed of the properties of the protection model of , a version of L4.\n\nThis has led to what is referred to as \"third-generation microkernels\",\ncharacterised by a security-oriented API with resource access controlled by capabilities, virtualization as a first-class concern, novel approaches to kernel resource management,\nand a design goal of suitability for formal analysis, besides the usual goal of high performance. Examples are Coyotos, , Nova,\nRedox and Fiasco.OC.\n\nIn the case of seL4, complete formal verification of the implementation has been achieved, i.e. a mathematical proof that the kernel's implementation is consistent with its formal specification. This provides a guarantee that the properties proved about the API actually hold for the real kernel, a degree of assurance which goes beyond even CC EAL7. It was followed by proofs of security-enforcement properties of the API, and a proof demonstrating that the executable binary code is a correct translation of the C implementation, taking the compiler out of the TCB. Taken together, these proofs establish an end-to-end proof of security properties of the kernel.\n\nThe term \"nanokernel\" or \"picokernel\" historically referred to:\n\nThere is also at least one case where the term nanokernel is used to refer not to a small kernel, but one that supports a nanosecond clock resolution.\n\n\n"}
{"id": "20024", "url": "https://en.wikipedia.org/wiki?curid=20024", "title": "Mach", "text": "Mach\n\nMach may refer to:\n\n\n\n\n\n\n\n"}
{"id": "20025", "url": "https://en.wikipedia.org/wiki?curid=20025", "title": "Multihull", "text": "Multihull\n\nA multihull is a ship, vessel, craft or boat with more than one hull. Multihull ships (vessels, craft, boats) include multiple types, sizes and applications. Hulls range from two to five, of uniform or diverse types and arrangements\n\nCatamarans are the most common type and they are used as racing, sailing, tourist and fishing boats. About 70% of fast passenger and car-passenger ferries are catamarans. About 300 semi-submersible drilling and auxiliary platforms operate at sea.\n\nSome ships with outriggers are built, including the experimental ship \"Tritone\" (UK) and the first and second sister-ships of the series of Littoral Combat Ships (US). About 70 small waterplan area ships whose hulls have a smaller cross section at the waterplane than below the surface exist.\n\nMultihull ships can be classified by the number of hulls, by their arrangement and by their shapes and sizes.\n\nThe first multihull vessels were Austronesian canoes. They hollowed out logs to make canoes and stabilized them by attaching outriggers to prevent them from capsizing. This led to the proa, catamaran, and trimaran plus various outriggers throughout the Pacific.\n\nIndividual hulls are connected by an above-water structure called the \"platform\", bridgedeck, crossarms, wingdeck, or by akas. The structure can be watertight partially or fully, or can consist of separate connections.\n\nThe distance between hulls is called the \"transverse clearance\" and can be measured between center planes or between the inner boards.Overall beam is the width of the vessel. LOA is length over all. The distance between the design waterplane and the bottom of the above-water platform (\"wet deck\") is called the \"vertical clearance\", wingdeck height, or wingdeck clearance. ..\n\nOutriggers or amas are outboard hulls for stability. . \n\nAn outrigger/aka is a small side hull attached to the main load-carrying hull by two or more struts/akas.\n\n\"Proas\" have one hull and one outrigger.\n\nA ship with one hull of conventional shape and two small side hulls (outriggers) is called an \"outrigger ship\". Smaller vessels-less that 100 feet LOA with three hulls are \"trimaran\"s in English language publications and two hulled vessels are catamarans.\n\nThree terms from the Malay and Micronesian language group describe hull components. The terms originated as descriptions of the proa. \"Catamarans and trimarans share the same terminology.\"\n\nSemantically, the catamaran is a pair of \"Vaka\" held together by \"Aka\", whereas the trimaran is a central \"Vaka\", with \"Ama\" on each side, attached by \"Aka\".\n\nSometimes, the term \"catamaran\" is applied to any ship or boat consisting of two hulls. However, other twin-hull types include\" duplus\" and \"trisec\"\n\nThe history of commercial catamarans began in 17th century England. Separate attempts at steam-powered catamarans were carried out by the middle of the 20th century. However, success required better materials and more developed hydrodynamic technologies. During the second half of the 20th century catamaran designs flourished.\n\nThe term \"trimaran\" is often used for any triple-hull ships or boats. More precisely, the term \"trimaran\" is used for a ship or boat with three identical hulls of traditional shape. The other triple-hull ships are \"outrigger \"and \"tricore\".\n\nThe trimaran has the widest range of interactions of wave systems generated by hulls at speed. The interactions can be favorable or unfavorable, depending on relative hull arrangement and speed. No authentic trimarans exist. Model test results and corresponding simulations provide estimates on the power of the full-scale ships. The calculations show possible advantages in a defined band of relative speeds.\n\nA new type of super-fast vessel, the \"wave-piercing\" trimaran (WPT) is known as an air-born unloaded (up to 25% of displacement) vessel, that can achieve twice the speed with a relative power.\n\nThe term \"quadrimaran\" is used for four-hulled vessels.\n\nThe term \"pentamaran\" is used for five-hulled vessels. The M80 Stiletto is a pentamaran.\n\nHulls with beam designs that are narrower at the water surface (waterplane) than below can be classified as hulls with decreased or small waterplane area. More often the term \"small waterplane area hull\" means a hull with an underwater \"gondola\" and strut(s) that connect the gondola to the above-water platform. Any ship employing such hulls qualifies.\nAny twin-hull SWA ship is called a \"small waterplane area twin hull\" (\"SWATH)\". A SWATH with one long strut on each hull is called a \"duplus\" (named after the first drilling ship of this type). The duplus is the most common type of SWA ships built.\n\nA \"trisec\" is a SWATH ship with two struts on each gondola. A trisec can have a minimal waterplane area and minimal motions in waves, resulting in more effective motion control.\n\nA triple-hull SWA ship is called a \"tricore\", regardless of the number of struts. The term applies to ship with three identical SWA hulls. There are no built tricores, but towing tests of models show the possibility of sufficient advantage from a power point of view in defining band of the relative speeds.\n\nOutrigger ships can employ SWA main hull and/or outriggers .\n\nMany traits differentiate multihulls from monohulls along several axes.\n\nMultihulls have a broader variety of hull and payload geometries. They have a relatively large beam, deck area (upper and inner), above-water capacity, shallower draft (allowing operation in shallower water) and generally less load/payload capacity..\n\nThe Austronesians discovered that round logs tied together into a raft don't roll, or capsize as easily as a single log. Hollowing out the logs increased buoyancy (increasing payload) while preserving stability. However, this requires a lot of work and it has increased drag and weight.\n\nSeparating two logs by a pair of cross-members called akas (crossarm) achieved the increased stability at lower weight and less effort. Covering the intervening distance with a platform provides stability similar to a raft. Multihulls are midway between pontoon boats and rafts, design wise.\n\nMultihulls feature reduced roll and yaw (equivalent pitch motion) with transverse stability (depends on transverse clearance) comparable to or greater than longitudinal stability. This reduced motion reduces seasickness and allows for more efficient solar energy collection and radar operation. They offer more effective motion mitigation systems (SWA ships), reduced wave resistance and towing resistance by controlling hull aspect ratio (for twin-hulls) and optimizing the interference between each hull's wave systems. .\n\nThe inertia of a (heavier) monohull will drive it temporarily if the wind drops, while a (lighter) multihull has less inertia, it can still maneuver well and coast for a time. Monohulls can push through waves that a multihull passes over and some lighter multihulls \"ghost\" well under sail as they respond easily to light air. Multihulls are more prone towards \"hobby horsing\" especially when lightly loaded and of short overall length.\n\nOther cultures stabilized their watercraft by filling the bottom with rocks and other ballast. The practice can be traced back to the Romans, Phoenicians, Vikings and others. Modern ocean liners carry tons of ballast. Naval architects insure that the center of gravity of their designs remains substantially below the metacenter. The low centre of gravity acts as a counterweight as the craft rotates around its centre of buoyancy, creating a restorative force as the craft deviates from its vertical position.\n\nMultihulls have a high rate of survivability with watertight above-water platforms with sections protected by water-tight bulkheads that can prevent sinking if the hulls fail. Catamarans may have increased reliability because the engines are on/in separate hulls. However, capsized monohulls may right themselves, pulled by the ballast while capsized multihulls remain inverted. Multihulls with their reduced weight and shallow draft make them unsuitable for breaking ice.\n\nMultihulls can substantially faster than monohulls of comparable size, in part because of their reduced weight, reduced draft , lower beam to length ratio, and finer entries. Monohulls/multihulls can be designed to give very low wake at some speeds.\n\nCommon multihull sailboats and small craft include proas, catamarans and trimarans.\n\nThe added space and stability are valued amenities for small boat users but lack of attention to loading places the vessel deeper in the water if overloaded, can be more sluggish to respond to directional change, and possibly dangerous in a seaway.\n\nMultihull powerboats, usually catamarans are used for racing and transportation. Speed and open cabin space are the main factors for choosing a cruising multihull. \n\nMultihulls are popular for racing, especially in Europe, New Zealand and Australia, and are somewhat popular for cruising in the Caribbean and South Pacific. They appear less frequently in the United States, partially because their increased beam require wider dock/slips. Multihulls are very popular in the trailerable size for competition. Until the 1980s most multihull sailboats (except for beach cats) were built either by their owners or by boat builders. Since then companies have been selling mass-produced boats.\n\nSmall sailing catamarans are also called beach catamarans. The Malibu Outrigger is one of the first beach launched multihull sailboat (1950). The most recognised racing classes are the Hobie Cat 14, Formula 18 cats, A-cats, the current Olympic Nacra 17, the former Olympic multihull Tornado and New Zealand's Weta trimaran.\n\nPower catamarans are becoming more common in Caribbean and Mediterranean international charter fleets.\n\nMega or super catamarans are those over 60 feet in length. These often receive substantial customisation following the request of the owner. One builder is New Zealand's Pachoud Yachts.\n\nBuilders include [http://www.corsairmarine.com/ Corsair Marine (mid-sized trimarans) and Privilege (large catamarans). The Seawind, Perry and Lightwave. The largest manufacturer of large multihulls is Fontaine-Pajot in France.\n\nPowerboats range from small single pilot Formula 1s to large multi-engined or gas turbined power boats that are used in off-shore racing and employ 2 to 4 pilots.\n\nPioneers of multihull design include James Wharram (UK), Derek Kelsall (UK), Lock Crowther (Aust), Hedly Nicol (Aust), Malcolm Tennant (NZ), Jim Brown (USA), Arthur Piver (USA), Chris White (US), Ian Farrier (NZ) and LOMOcean (NZ).\n\nIn 1978, 101 years after catamarans like \"Amaryllis\" were banned from yacht racing they returned the sport. This started with the victory of the trimaran \"Olympus Photo\", skippered by Mike Birch in the first Route du Rhum. Thereafter, no open ocean race was won by a monohull. Winning times dropped by 70%, since 1978. Olympus Photo's 23-day 6 hr 58' 35\" success dropped to Gitana 11's 7d 17h 19'6\", in 2006. Around 2016 the first large wind driven foil-borne racing catamarans were built. They are only on foils and T foiled rudders at higher speeds.\n\n\n"}
{"id": "20029", "url": "https://en.wikipedia.org/wiki?curid=20029", "title": "Multics Relational Data Store", "text": "Multics Relational Data Store\n\nThe Multics Relational Data Store, or MRDS for short, was the first commercial relational database management system. It was written in PL/1 by Honeywell for the Multics operating system and first sold in June 1976. Unlike the SQL systems that emerged in the late 1970s and early 80's, MRDS used a command language only for basic data manipulation, equivalent to the codice_1 or codice_2 statements in SQL. Other operations, like creating a new database, or general file management, required the use of a separate command program.\n\n"}
{"id": "20032", "url": "https://en.wikipedia.org/wiki?curid=20032", "title": "Mike Oldfield", "text": "Mike Oldfield\n\nMichael Gordon Oldfield (born 15 May 1953) is an English multi-instrumentalist and composer. His work blends progressive rock with world, folk, classical, electronic, ambient, and new-age music. His biggest commercial success is the 1973 album \"Tubular Bells\"which launched Virgin Records and became a hit in America after its opening was used as the theme for the film \"The Exorcist\". He recorded the 1983 hit single \"Moonlight Shadow\" and a rendition of the Christmas piece \"In Dulci Jubilo\".\n\nOldfield has released more than 20 albums with the most recent being a sequel to his 1975 album \"Ommadawn\" titled \"Return to Ommadawn\", released on 20 January 2017.\n\nOldfield's parents were Raymond Oldfield, a general practitioner, and Maureen Liston, an Irish nurse. His older sister Sally and older brother Terry are also successful musicians and have appeared on several of Mike's albums. He also had a younger brother, David, who had Down syndrome and who died in infancy.\n\nOldfield was born in the Battle Hospital in Reading and attended St Joseph's Convent School, Highlands Junior School, St Edward's Preparatory School, and Presentation College in Reading. The family lived in Western Elms Avenue, Reading. When he was 13, he moved with his parents to Harold Wood in Essex and attended Hornchurch Grammar School, where, having already begun his career in music, he took one GCE examination, in English.\n\nHaving taught himself to play the guitar, Oldfield's career began in his early teenage years, playing acoustic guitar in local folk clubs in Reading. At this time, he had already written two 15-minute instrumental pieces in which he would \"go through all sorts of moods\", precursors to his landmark 1970s compositions. In his early teens, Oldfield was involved in a beat group playing The Shadows-style music (he has often cited Hank Marvin as a major influence, and would later cover The Shadows' song \"Wonderful Land\"). In 1967, Oldfield and his sister Sally formed the folk duo The Sallyangie and, after exposure in the local folk scene, were signed to Transatlantic Records. An album, \"Children of the Sun\", was issued in 1968. After The Sallyangie disbanded, Mike formed another duo called Barefoot with his brother, which took him back to rock music.\n\nIn 1970, Oldfield joined The Whole Worldformer Soft Machine vocalist Kevin Ayers's backing groupplaying bass and occasionally lead guitar. He and Ayers shared a flat for a time at the northern end of the Seven Sisters Road in London. Oldfield is featured on two Ayers albums, \"Whatevershebringswesing\" and \"Shooting at the Moon\". The band also included keyboardist and composer David Bedford, who quickly befriended Oldfield, encouraged him in his composition of an early version of \"Tubular Bells\" and later arranged and conducted an orchestral version of the \"Tubular Bells\" album. Oldfield was also the reserve guitarist for the musical \"Hair\" and played with Alex Harvey.\n\nHaving recorded sections of this early version of \"Tubular Bells\" as demo pieces, Oldfield attempted to persuade record labels to take on the \"Tubular Bells\" project. Nothing came of his efforts until September 1971, when as a session musician and bass guitarist for the Arthur Louis Band, he attended recording sessions at The Manor Studio near Kidlington, Oxfordshire, owned by a young Richard Branson and run by engineers Tom Newman and Simon Heyworth. Branson already had several business ventures and was about to start his own record label, Virgin Records, together with Simon Draper. Newman and Heyworth heard some of Oldfield's demo music and took it to Branson and Draper, who eventually gave Oldfield one week's worth of recording time at The Manor. During this week, he completed \"Part One\" of \"Tubular Bells\"; \"Part Two\" was compiled over a number of months.\n\n\"Tubular Bells\" is Oldfield's most famous work. The instrumental composition was recorded in 1972 and released on 25 May 1973 as the inaugural album of Richard Branson and Simon Draper's label Virgin Records. Oldfield played more than twenty different instruments in the multi-layered recording, and its style moved through diverse musical genres. Its 2,630,000 UK sales puts it at No. 34 on the list of the best-selling albums in the country. The title track became a top 10 hit single in the US after the opening was used in \"The Exorcist\" film in 1973. It is today considered to be a forerunner of the new-age music movement.\n\nIn 1974, Oldfield played guitar on the critically acclaimed album \"Rock Bottom\" by Robert Wyatt.\n\nIn late 1974, his follow-up LP, \"Hergest Ridge\", was No. 1 in the UK for three weeks before being dethroned by \"Tubular Bells\". Although \"Hergest Ridge\" was released over a year after \"Tubular Bells\", it reached No. 1 first. \"Tubular Bells\" spent 11 weeks (10 of them consecutive) at No. 2 before its one week at the top. Like \"Tubular Bells\", \"Hergest Ridge\" is a two-movement instrumental piece, this time evoking scenes from Oldfield's Herefordshire country retreat. It was followed in 1975 by the pioneering world music piece \"Ommadawn\" released after the death of his mother Maureen.\n\nIn 1975, Oldfield recorded a version of the Christmas piece \"In Dulci Jubilo\" which charted at No. 4 in the UK.\n\nIn 1975, Oldfield received a Grammy award for Best Instrumental Composition in \"Tubular Bells – Theme from \"The Exorcist\"\".\n\nIn 1976, Oldfield and his sister joined his friend and band member Pekka Pohjola to play on his album \"Mathematician's Air Display\", which was released in 1977. The album was recorded and edited at Oldfield's Througham Slad Manor in Gloucestershire by Oldfield and Paul Lindsay. Oldfield's 1976 rendition of \"Portsmouth\" remains his best-performing single on the UK Singles Chart, reaching No. 3.\n\nIn 1978, \"Incantations\" introduced more diverse choral performances from Sally Oldfield, Maddy Prior, and the Queen's College Girls Choir. Around the time of \"Incantations\", Oldfield underwent a controversial self-assertiveness therapy course known as Exegesis, which had a significant effect on his personality, making him more confident and out-going. Possibly as a result, the formerly reclusive musician staged a major Tour of Europe to promote the album, chronicled in his live album \"Exposed\", much of which was recorded at the National Exhibition Centre near Birmingham.\n\nIn 1979, Oldfield's music was used as the musical score for \"The Space Movie\", a Virgin movie that celebrated the tenth anniversary of the Apollo 11 mission. Also in 1979, he recorded a version of the signature tune for the BBC children's television programme \"Blue Peter\", which was used by the show for 10 years. In 1981, Oldfield was asked to compose a piece for the Royal Wedding of Charles, Prince of Wales, and Lady Diana Spencer, titled \"Royal Wedding Anthem\".\n\nThe early 1980s saw Oldfield make the transition to mainstream pop music, beginning with the inclusion of shorter instrumental tracks and contemporary cover versions on \"Platinum\" and \"QE2\" (the latter named after the ocean liner). Soon afterwards, he turned to songwriting, with a string of collaborations featuring various lead vocalists alongside his characteristic searing guitar solos. The best known of these is \"Moonlight Shadow\", his 1983 hit with Maggie Reilly. The most successful Oldfield composition on the US pop charts during this period was Hall & Oates's cover of Oldfield's \"Family Man\" for their 1982 album \"HO\". Released as the album's third single, it hit the Top 10 during the spring of 1983 and was a hugely popular MTV music video.\n\nOldfield later turned to film and video, writing the score for Roland Joffé's acclaimed film \"The Killing Fields\" and producing substantial video footage for his album \"Islands\". \"Islands\" continued what Oldfield had been doing on the past couple of albums, with an instrumental piece on one side and rock/pop singles on the other. Of these, \"Islands\", sung by Bonnie Tyler and \"Magic Touch\", with vocals by Max Bacon (in the US version) and Glasgow vocalist Jim Price (Southside Jimmy) in the rest of the world, were the major hits. In the US \"Magic Touch\" reached the top 10 on the Billboard album rock charts in 1988. During the 1980s, Oldfield's then-wife, Norwegian singer Anita Hegerland, contributed vocals to many songs including \"Pictures in the Dark\".\n\n\"Earth Moving\" was released in July 1989 and was a moderate success. The album was the first to consist solely of rock/pop songs, several of which were released as singles: \"Innocent\" and \"Holy\" in Europe, and \"Hostage\" in the US for album rock stations. This was a time of much friction with his record label. Virgin Records insisted that Oldfield use the title \"Tubular Bells 2\" for his next instrumental album. Oldfield's rebellious response was \"Amarok\", an hour-long work featuring rapidly changing themes, unpredictable bursts of noise and a hidden Morse code insult, stating \"Fuck off RB\", allegedly directed at Richard Branson. It was not a commercial success. His last album for the Virgin label was \"Heaven's Open\", released under the name 'Michael Oldfield'. The album, notable for being the first time Oldfield had contributed all the lead vocals himself, consisted of songs and the rapidly changing instrumental piece \"Music from the Balcony\". However the rift with Virgin was healed some years later. In 2013 Oldfield invited Sir Richard to preside over the opening of the new school hall at St.Andrew's International School of The Bahamas, where two of Oldfield's children were pupils. This was the occasion of the debut of \"Tubular Bells for Schools\", a piano solo adaptation of Oldfield's work.\n\nThe first thing Oldfield did when arriving at his new label, Warner Bros., was to write and release \"Tubular Bells II\", the sequel to his first record on Virgin, in what appeared to be a final insult to his former label. It was premiered at a live concert at Edinburgh Castle. He continued to embrace new musical styles, with \"The Songs of Distant Earth\" (based on Arthur C. Clarke's novel of the same name) exhibiting a softer new-age sound. In 1994, he also had an asteroid, 5656 Oldfield, named after him.\n\nIn 1995, Oldfield continued to embrace new musical styles by producing the Celtic-themed album \"Voyager\". In 1992, Oldfield met Luar na Lubre, a Galician Celtic-folk band (from A Coruña, Spain). The band's popularity grew after Oldfield covered their song \"O son do ar\" (\"The sound of the air\") on his \"Voyager\" album.\n\nIn 1998, Oldfield produced the third \"Tubular Bells\" album (also premiered at a concert, this time in Horse Guards Parade, London), drawing on the dance music scene at his then new home on the island of Ibiza. This album was inspired by themes from \"Tubular Bells\", but differed in lacking a clear two-part layout.\n\nDuring 1999, Oldfield released two albums. The first, \"Guitars\", used guitars as the source for all the sounds on the album, including percussion. The second, \"The Millennium Bell\", consisted of pastiches of a number of styles of music that represented various historical periods over the past millennium. The work was performed live in Berlin for the city's millennium celebrations in 1999–2000.\n\nHe added to his repertoire the MusicVR project, combining his music with a virtual reality-based computer game. His first work on this project is \"Tr3s Lunas\" launched in 2002, a virtual game where the player can interact with a world full of new music. This project appeared as a double CD, one with the music, and the other with the game.\n\nIn 2003, Oldfield released \"Tubular Bells 2003\", a re-recording of the original \"Tubular Bells\", on CD, and DVD-Audio. This was done to \"fix\" many \"imperfections\" in the original due to the recording technologies of the early 1970s and limitations in time that he could spend in the recording studio. It celebrated the 30th anniversary of \"Tubular Bells\", Oldfield's 50th birthday and his marriage to Fanny in the same year. At around the same time Virgin released an SACD version containing both the original stereo album and the 1975 quadraphonic mix by Phil Newell. In the 2003 version, the original voice of the 'Master of Ceremonies' (Viv Stanshall) was replaced with the voice of John Cleese, Stanshall having died in the interim.\n\nOn 12 April 2004 Oldfield launched his next virtual reality project, \"Maestro\", which contains music from the \"Tubular Bells 2003\" album and some new chillout melodies. The games have since been made available free of charge on Tubular.net. A double album, \"Light + Shade\", was released on Mercury Records in 2005, with whom Mike had recently signed a three-album deal. The two discs contain music of contrasting moods, one relaxed (\"Light\") and the other more edgy and moody (\"Shade\"). Oldfield headlined the pan-European Night of the Proms tour, consisting of 21 concerts in 2006 and 2007.\n\nHis autobiography \"Changeling\" was published in May 2007 by Virgin Books. In March 2008 Oldfield released his first classical album, \"Music of the Spheres\"; Karl Jenkins assisted with the orchestration. In the first week of release the album topped the UK Classical chart and reached number 9 on the main UK Album Chart. A single \"Spheres\", featuring a demo version of pieces from the album, was released digitally. The album was nominated for a Classical Brit Award, the NS&I Best Album of 2009.\n\nIn 2008, when Oldfield's original 35-year deal with Virgin Records ended, the rights to \"Tubular Bells\" and his other Virgin releases were returned to him, and were then transferred to Mercury Records. Mercury issued a press release on 15 April 2009, noting that Oldfield's Virgin albums would be re-released, starting 8 June 2009. These releases include special features from the archives. a further seven albums have been reissued and compilation albums have been released such as \"Two Sides\".\n\nIn March 2010, \"Music Week\" reported that publishing company Stage Three Music had acquired a 50% stake in the songs of Oldfield's entire recorded output in a seven-figure deal.\n\nIn 2008, Oldfield contributed an exclusive song (\"Song for Survival\") to a charity album called \"Songs for Survival\", in support of the Survival International. Oldfield's daughter, Molly, played a large part in the project. In 2010 lyricist Don Black said in an interview with \"Music Week\" that he had been working with Oldfield. In 2012, Oldfield was featured on Terry Oldfield's \"Journey into Space\" album and on a track called \"Islanders\" by German producer Torsten Stenzel's York project. In 2013 Oldfield and York released a remix album titled \"Tubular Beats\".\n\nAt the 2012 Summer Olympics opening ceremony, Oldfield performed renditions of \"Tubular Bells\", \"Far Above the Clouds\" and \"In Dulci Jubilo\" during a segment about the National Health Service. This track appears on the \"Isles of Wonder\" album which contains music from the Danny Boyle-directed show.\n\nIn October 2013, the BBC broadcast \"Tubular Bells: The Mike Oldfield Story\", an hour-long appreciation of Oldfield's life and musical career, filmed on location at his home recording studio in Nassau.\n\nOldfield's latest rock-themed album of songs, titled \"Man on the Rocks\", was released on 3 March 2014 by Virgin EMI. The album was produced by Steve Lipson. The album marks a return of Oldfield to a Virgin branded label, through the merger of Mercury Records UK and Virgin Records after Universal Music's purchase of EMI. The track \"Nuclear\" was used for the E3 trailer of \"\".\n\nInterviewed by Steve Wright in May 2015 for his BBC Radio 2 show, Oldfield said that he was currently working on a \"prequel to \"Tubular Bells\"\" which was being recorded using analogue equipment as much as possible. He suggested that the album might only be released on vinyl. The project is in its infancy and would follow his current reissue campaign. Oldfield suggested that it would be released \"in a couple of years\".\n\nOn 16 October 2015 Oldfield tweeted, via his official Twitter account \"\"I am continuing to work on ideas for \"A New Ommadawn\" for the last week or so to see if [...] the idea actually works.\"\" On 8 May 2016, Oldfield announced via his Facebook group page that the new \"Ommadawn\" project with the tentative title of \"Return to Ommadawn\" is finished, and he is awaiting a release date from the record company. He also suggested that he may soon be starting work on a possible fourth \"Tubular Bells\" album.\n\nOldfield's latest album, \"Return to Ommadawn\" was released on 20 January 2017 and reached #4 in the UK Album Chart. On 29 January 2017, Oldfield again hinted at a \"Tubular Bells 4\" album via his official Facebook fan page; he uploaded photos of new equipment and a new Fender Telecaster guitar with the caption \"\"New sounds for TB4!\"\"\n\nAlthough Oldfield considers himself primarily a guitarist, he is also one of popular music's most skilled and diverse multi-instrumentalists. His 1970s recordings were characterised by a very broad variety of instrumentation predominantly played by himself, plus assorted guitar sound treatments to suggest other instrumental timbres (such as the bagpipe, mandolin, \"Glorfindel\" and varispeed guitars on the original \"Tubular Bells\").\nDuring the 1980s Oldfield became expert in the use of digital synthesizer and sequencers (notably the Fairlight CMI) which began to dominate the sound of his recordings: from the late 1990s onwards, he became a keen user of software synthesizers. He has, however, regularly returned to projects emphasising detailed, manually played and part-acoustic instrumentation (such as 1990's \"Amarok\", 1996's \"Voyager\" and 1999's \"Guitars\").\n\nOldfield has played over forty distinct and different instruments on record, including:\n\n\nWhile generally preferring the sound of guest vocalists, Oldfield has frequently sung both lead and backup parts for his songs and compositions. He has also contributed experimental vocal effects such as fake choirs and the notorious \"Piltdown Man\" impression on \"Tubular Bells\".\n\nAlthough recognised as a highly skilled guitarist, Oldfield is self-deprecating about his other instrumental skills, citing them as having been developed out of necessity to perform and record the music he composes. He has been particularly dismissive of his violin-playing and singing abilities.\n\nOver the years, Oldfield has used a range of guitars. Among the more notable of these are:\n\nOldfield used a modified Roland GP8 effects processor in conjunction with his PRS Artist to get many of his heavily overdriven guitar sounds from the \"Earth Moving\" album onwards. Oldfield has also been using guitar synthesizers since the mid-1980s, using a 1980s Roland GR-300/G-808 type system, then a 1990s Roland GK2 equipped red PRS Custom 24 (sold in 2006) with a Roland VG8, and most recently a Line 6 Variax.\n\nOldfield has an unusual playing style, using fingers and long right-hand fingernails and different ways of creating vibrato: a \"very fast side-to-side vibrato\" and \"violinist's vibrato\". Oldfield has stated that his playing style originates from his musical roots playing folk music and the bass guitar.\n\nOver the years, Oldfield has owned and used a vast number of synthesizers and other keyboard instruments. In the 1980s, he composed the score for the film \"The Killing Fields\" on a Fairlight CMI. Some examples of keyboard and synthesised instruments which Oldfield has made use of include Sequential Circuits Prophet-5s (notably on \"Platinum\" and \"The Killing Fields\"), Roland JV-1080/JV-2080 units (1990s), a Korg M1 (as seen in the \"Innocent\" video), a Clavia Nord Lead and Steinway pianos. In recent years, he has also made use of software synthesis products, such as Native Instruments.\n\nOldfield has occasionally sung himself on his records and live performances, sometimes using a vocoder as a resource. It is not unusual for him to collaborate with diverse singers and to hold auditions before deciding the most appropriate for a particular song or album. Featured lead vocalists who have collaborated with him include:\n\nOldfield has self-recorded and produced many of his albums, and played the majority of the featured instruments, largely at his home studios. In the 1990s and 2000s he mainly used DAWs such as Apple Logic, Avid Pro Tools and Steinberg Nuendo as recording suites. For composing classical music Oldfield has been quoted as using the software notation program Sibelius running on Apple Macintoshes. He also used the FL Studio DAW on his 2005 double album \"Light + Shade\". Among the mixing consoles Oldfield has owned are an AMS Neve Capricorn 33238, a Harrison Series X, and a Euphonix System 5-MC.\n\nOldfield and his siblings were raised as Roman Catholics, their mother's faith. In his early life, Oldfield used drugs including LSD, whose effects on his mental health he discussed in his autobiography. In the early 1990s, he underwent a course on mental health problems and subsequently set up a foundation called Tonic, which sponsored people to have counselling and therapy. The trustee was the Professor of Psychiatry at Guy's Hospital, London.\n\nIn the late 1970s, Oldfield was briefly married to Diana D'Aubigny (the sister of the Exegesis group leader), but this lasted just a few weeks. Oldfield has had seven children with his partners. In the early 1980s, he had three children with Sally Cooper: Molly, Dougal (who died aged 33 in May 2015) and Luke. In the late 1980s, he had two children (Greta and Noah) with Norwegian singer Anita Hegerland. In the 2000s, he married Fanny Vandekerckhove (born 1977), whom he met during his time in Ibiza; they have two sons together (Jake and Eugene). Oldfield and Fanny separated in 2013.\n\nOldfield is a motorcycle fan and has five bikes. These include a BMW R1200GS, a Suzuki GSX-R750, a Suzuki GSX-R1000, and a Yamaha R1. He says that some of his inspiration for composing comes from riding them. Throughout his life Oldfield has also had a passion for aircraft and building model aircraft. Since 1980, he has been a licensed pilot and has flown fixed wing aircraft (the first of which was a Beechcraft Sierra) and helicopters (including the Agusta Bell 47G, which featured on the sleeve of his cover version of the ABBA song \"Arrival\" as a pastiche of their album artwork). He is also interested in cars and has owned a Ferrari and a Bentley which was a gift from Richard Branson as an incentive for him to give his first live performance of \"Tubular Bells\". He has endorsed the Mercedes-Benz S-Class in the Mercedes UK magazine. Oldfield also considers himself to be a Trekkie. He noted in an interview in 2008 that he had two boats.\n\nIn 2007, Oldfield criticised Britain for being too controlling and protective, specifically concentrating on the smoking ban which England and Wales had introduced that year. Oldfield then moved from his South Gloucestershire home to Palma de Mallorca, Spain and then to Monaco. He has lived outside the UK in the past, including in Los Angeles and Ibiza in the 1990s and, for tax reasons, Switzerland in the mid-1980s. In 2009, he moved to the Bahamas and put his home in Mallorca up for sale.\n\nGrammy Awards\n\nIvor Novello Awards\n\nNME Awards\n\nOldfield has had more than 30 charting albums and 25 charting singles on the British charts and many more around the world.\n\nStudio albums\n\n\n\n\n\n"}
{"id": "20034", "url": "https://en.wikipedia.org/wiki?curid=20034", "title": "Mutual recursion", "text": "Mutual recursion\n\nIn mathematics and computer science, mutual recursion is a form of recursion where two mathematical or computational objects, such as functions or data types, are defined in terms of each other. Mutual recursion is very common in functional programming and in some problem domains, such as recursive descent parsers, where the data types are naturally mutually recursive.\n\nThe most important basic example of a data type that can be defined by mutual recursion is a tree, which can be defined mutually recursively in terms of a forest (a list of trees). Symbolically:\nA forest \"f\" consists of a list of trees, while a tree \"t\" consists of a pair of a value \"v\" and a forest \"f\" (its children). This definition is elegant and easy to work with abstractly (such as when proving theorems about properties of trees), as it expresses a tree in simple terms: a list of one type, and a pair of two types. Further, it matches many algorithms on trees, which consist of doing one thing with the value, and another thing with the children.\n\nThis mutually recursive definition can be converted to a singly recursive definition by inlining the definition of a forest:\nA tree \"t\" consists of a pair of a value \"v\" and a list of trees (its children). This definition is more compact, but somewhat messier: a tree consists of a pair of one type and a list of another, which require disentangling to prove results about.\n\nIn Standard ML, the tree and forest data types can be mutually recursively defined as follows, allowing empty trees:\nJust as algorithms on recursive data types can naturally be given by recursive functions, algorithms on mutually recursive data structures can be naturally given by mutually recursive functions. Common examples include algorithms on trees, and recursive descent parsers. As with direct recursion, tail call optimization is necessary if the recursion depth is large or unbounded, such as using mutual recursion for multitasking. Note that tail call optimization in general (when the function called is not the same as the original function, as in tail-recursive calls) may be more difficult to implement than the special case of tail-recursive call optimization, and thus efficient implementation of mutual tail recursion may be absent from languages that only optimize tail-recursive calls. In languages such as Pascal that require declaration before use, mutually recursive functions require forward declaration, as a forward reference cannot be avoided when defining them.\n\nAs with directly recursive functions, a wrapper function may be useful, with the mutually recursive functions defined as nested functions within its scope if this is supported. This is particularly useful for sharing state across a set of functions without having to pass parameters between them.\n\nA standard example of mutual recursion, which is admittedly artificial, determines whether a non-negative number is even or odd by defining two separate functions that call each other, decrementing each time. In C:\nThese functions are based on the observation that the question \"is 4 even?\" is equivalent to \"is 3 odd?\", which is in turn equivalent to \"is 2 even?\", and so on down to 0. This example is mutual single recursion, and could easily be replaced by iteration. In this example, the mutually recursive calls are tail calls, and tail call optimization would be necessary to execute in constant stack space. In C, this would take \"O\"(\"n\") stack space, unless rewritten to use jumps instead of calls.\n\nAs a more general class of examples, an algorithm on a tree can be decomposed into its behavior on a value and its behavior on children, and can be split up into two mutually recursive functions, one specifying the behavior on a tree, calling the forest function for the forest of children, and one specifying the behavior on a forest, calling the tree function for the tree in the forest. In Python:\n\nIn this case the tree function calls the forest function by single recursion, but the forest function calls the tree function by multiple recursion.\n\nUsing the Standard ML data type above, the size of a tree (number of nodes) can be computed via the following mutually recursive functions:\nA more detailed example in Scheme, counting the leaves of a tree:\nThese examples reduce easily to a single recursive function by inlining the forest function in the tree function, which is commonly done in practice: directly recursive functions that operate on trees sequentially process the value of the node and recurse on the children within one function, rather than dividing these into two separate functions.\n\nA more complicated example is given by recursive descent parsers, which can be naturally implemented by having one function for each production rule of a grammar, which then mutually recurse; this will in general be multiple recursion, as production rules generally combine multiple parts. This can also be done without mutual recursion, for example by still having separate functions for each production rule, but having them called by a single controller function, or by putting all the grammar in a single function.\n\nMutual recursion can also implement a finite-state machine, with one function for each state, and single recursion in changing state; this requires tail call optimization if the number of state changes is large or unbounded. This can be used as a simple form of cooperative multitasking. A similar approach to multitasking is to instead use coroutines which call each other, where rather than terminating by calling another routine, one coroutine yields to another but does not terminate, and then resumes execution when it is yielded back to. This allows individual coroutines to hold state, without it needing to be passed by parameters or stored in shared variables.\n\nThere are also some algorithms which naturally have two phases, such as minimax (min and max), and these can be implemented by having each phase in a separate function with mutual recursion, though they can also be combined into a single function with direct recursion.\n\nIn mathematics, the Hofstadter Female and Male sequences are an example of a pair of integer sequences defined in a mutually recursive manner.\n\nFractals can be computed (up to a given resolution) by recursive functions. This can sometimes be done more elegantly via mutually recursive functions; the Sierpiński curve is a good example.\n\nMutual recursion is very common in the functional programming style, and is often used for programs written in LISP, Scheme, ML, and similar languages. In languages such as Prolog, mutual recursion is almost unavoidable.\n\nSome programming styles discourage mutual recursion, claiming that it can be confusing to distinguish the conditions which will return an answer from the conditions that would allow the code to run forever without producing an answer. Peter Norvig points to a design pattern which discourages the use entirely, stating:\nMutual recursion is also known as indirect recursion, by contrast with direct recursion, where a single function calls itself directly. This is simply a difference of emphasis, not a different notion: \"indirect recursion\" emphasises an individual function, while \"mutual recursion\" emphasises the set of functions, and does not single out an individual function. For example, if \"f\" calls itself, that is direct recursion. If instead \"f\" calls \"g\" and then \"g\" calls \"f,\" which in turn calls \"g\" again, from the point of view of \"f\" alone, \"f\" is indirectly recursing, while from the point of view of \"g\" alone, \"g\" is indirectly recursing, while from the point of view of both, \"f\" and \"g\" are mutually recursing on each other. Similarly a set of three or more functions that call each other can be called a set of mutually recursive functions.\n\nMathematically, a set of mutually recursive functions are primitive recursive, which can be proven by course-of-values recursion, building a single function \"F\" that lists the values of the individual recursive function in order: formula_1 and rewriting the mutual recursion as a primitive recursion.\n\nAny mutual recursion between two procedures can be converted to direct recursion by inlining the code of one procedure into the other. If there is only one site where one procedure calls the other, this is straightforward, though if there are several it can involve code duplication. In terms of the call stack, two mutually recursive procedures yield a stack ABABAB..., and inlining B into A yields the direct recursion (AB)(AB)(AB)...\n\nAlternately, any number of procedures can be merged into a single procedure that takes as argument a variant record (or algebraic data type) representing the selection of a procedure and its arguments; the merged procedure then dispatches on its argument to execute the corresponding code and uses direct recursion to call self as appropriate. This can be seen as a limited application of defunctionalization. This translation may be useful when any of the mutually recursive procedures can be called by outside code, so there is no obvious case for inlining one procedure into the other. Such code then needs to be modified so that procedure calls are performed by bundling arguments into a variant record as described; alternately, wrapper procedures may be used for this task.\n\n\n\n"}
{"id": "20036", "url": "https://en.wikipedia.org/wiki?curid=20036", "title": "Metasyntactic variable", "text": "Metasyntactic variable\n\nA metasyntactic variable is a specific word or set of words identified as a placeholder in computer science and specifically computer programming. These words are commonly found in source code and are intended to be modified or substituted to be applicable to the specific usage before compilation (translation to an executable). The words foo and bar are good examples as they are used in over 330 Internet Engineering Task Force Requests for Comments, which are documents explaining foundational internet technologies like HTTP (websites), TCP/IP, and email protocols.\n\nBy mathematical analogy, a metasyntactic variable is a word that is a variable for other words, just as in algebra letters are used as variables for numbers.\n\nMetasyntactic variables are used to name entities such as variables, functions, and commands whose exact identity is unimportant and serve only to demonstrate a concept, which is useful for teaching programming.\n\nDue to English being the foundation-language, or lingua franca, of most computer programming languages these variables are commonly seen even in programs and examples of programs written for other spoken-language audiences.\n\nThe typical names may depend however on the subculture that has developed around a given programming language.\n\nMetasyntactic variables used commonly across all programming languages include \"foobar\", \"foo\", \"bar\", \"baz\", \"qux\", \"quux\", \"quuz\", \"corge\", \"grault\", \"garply\", \"waldo\", \"fred\", \"plugh\", \"xyzzy\", and \"thud\". \"Wibble\", \"wobble\", \"wubble\", and \"flob\" are also used in the UK.\n\nA complete reference can be found in a MIT Press book titled: The Hacker's Dictionary.\n\nspam, ham, and eggs are the principal metasyntactic variables used in the Python programming language. This is a reference to the comedy sketch \"Spam\" by Monty Python, the eponym of the language.\n\nIn Japanese, the words \"hoge\" (ほげ) and \"piyo\" (ぴよ) are commonly used, with other common words and variants being \"fuga\" (ふが), \"hogera\" (ほげら), and \"hogehoge\" (ほげほげ). Note that \"-ra\" is a pluralizing ending in Japanese, and reduplication is also used for pluralizing. The origin of \"hoge\" as a metasyntactic variable is not known, but it is believed to date to the early 1980s.\n\nIn the following example the function name foo and the variable name bar are both metasyntactic variables. Lines beginning with // are comments.\n\nSpam, ham, and eggs are the principal metasyntactic variables used in the Python programming language. This is a reference to the famous comedy sketch, \"Spam\", by Monty Python, the eponym of the language.\nIn the following example spam, ham, and eggs are metasyntactic variables and lines beginning with # are comments.\nBoth the IETF RFCs and computer programming languages are rendered in plain text, making it necessary to distinguish metasyntactic variables by a naming convention, since it would not be obvious from context. \n\nPlain text example:\n\nRFC 772 (cited in RFC 3092) contains for instance:\n\nAnother point reflected in the above example is the convention that a metavariable is to be uniformly substituted with the same instance in all its appearances in a given schema. This is in contrast with nonterminal symbols in formal grammars where the nonterminals on the right of a production can be substituted by different instances.\n\nThis section includes bits of code which show how metasyntactic variables are used in teaching computer programming concepts.\n\nFunction prototypes with different argument passing mechanisms:\nExample showing the function overloading capabilities of the C++ language\n\n"}
