{"id": "5698", "url": "https://en.wikipedia.org/wiki?curid=5698", "title": "Charles Babbage", "text": "Charles Babbage\n\nCharles Babbage (; 26 December 1791 – 18 October 1871) was an English polymath. A mathematician, philosopher, inventor and mechanical engineer, Babbage originated the concept of a digital programmable computer.\n\nConsidered by some to be a \"father of the computer\", Babbage is credited with inventing the first mechanical computer that eventually led to more complex electronic designs, though all the essential ideas of modern computers are to be found in Babbage's analytical engine. His varied work in other fields has led him to be described as \"pre-eminent\" among the many polymaths of his century.\n\nParts of Babbage's incomplete mechanisms are on display in the Science Museum in London. In 1991, a functioning difference engine was constructed from Babbage's original plans. Built to tolerances achievable in the 19th century, the success of the finished engine indicated that Babbage's machine would have worked.\n\nBabbage's birthplace is disputed, but according to the \"Oxford Dictionary of National Biography\" he was most likely born at 44 Crosby Row, Walworth Road, London, England. A blue plaque on the junction of Larcom Street and Walworth Road commemorates the event.\n\nHis date of birth was given in his obituary in \"The Times\" as 26 December 1792; but then a nephew wrote to say that Babbage was born one year earlier, in 1791. The parish register of St. Mary's, Newington, London, shows that Babbage was baptised on 6 January 1792, supporting a birth year of 1791.\nBabbage was one of four children of Benjamin Babbage and Betsy Plumleigh Teape. His father was a banking partner of William Praed in founding Praed's & Co. of Fleet Street, London, in 1801. In 1808, the Babbage family moved into the old Rowdens house in East Teignmouth. Around the age of eight, Babbage was sent to a country school in Alphington near Exeter to recover from a life-threatening fever. For a short time he attended King Edward VI Grammar School in Totnes, South Devon, but his health forced him back to private tutors for a time.\n\nBabbage then joined the 30-student Holmwood Academy, in Baker Street, Enfield, Middlesex, under the Reverend Stephen Freeman. The academy had a library that prompted Babbage's love of mathematics. He studied with two more private tutors after leaving the academy. The first was a clergyman near Cambridge; through him Babbage encountered Charles Simeon and his evangelical followers, but the tuition was not what he needed. He was brought home, to study at the Totnes school: this was at age 16 or 17. The second was an Oxford tutor, under whom Babbage reached a level in Classics sufficient to be accepted by Cambridge.\n\nBabbage arrived at Trinity College, Cambridge, in October 1810. He was already self-taught in some parts of contemporary mathematics; he had read in Robert Woodhouse, Joseph Louis Lagrange, and Marie Agnesi. As a result, he was disappointed in the standard mathematical instruction available at the university.\n\nBabbage, John Herschel, George Peacock, and several other friends formed the Analytical Society in 1812; they were also close to Edward Ryan. As a student, Babbage was also a member of other societies such as The Ghost Club, concerned with investigating supernatural phenomena, and the Extractors Club, dedicated to liberating its members from the madhouse, should any be committed to one.\n\nIn 1812 Babbage transferred to Peterhouse, Cambridge. He was the top mathematician there, but did not graduate with honours. He instead received a degree without examination in 1814. He had defended a thesis that was considered blasphemous in the preliminary public disputation; but it is not known whether this fact is related to his not sitting the examination.\n\nConsidering his reputation, Babbage quickly made progress. He lectured to the Royal Institution on astronomy in 1815, and was elected a Fellow of the Royal Society in 1816. After graduation, on the other hand, he applied for positions unsuccessfully, and had little in the way of career. In 1816 he was a candidate for a teaching job at Haileybury College; he had recommendations from James Ivory and John Playfair, but lost out to Henry Walter. In 1819, Babbage and Herschel visited Paris and the Society of Arcueil, meeting leading French mathematicians and physicists. That year Babbage applied to be professor at the University of Edinburgh, with the recommendation of Pierre Simon Laplace; the post went to William Wallace.\n\nWith Herschel, Babbage worked on the electrodynamics of Arago's rotations, publishing in 1825. Their explanations were only transitional, being picked up and broadened by Michael Faraday. The phenomena are now part of the theory of eddy currents, and Babbage and Herschel missed some of the clues to unification of electromagnetic theory, staying close to Ampère's force law.\n\nBabbage purchased the actuarial tables of George Barrett, who died in 1821 leaving unpublished work, and surveyed the field in 1826 in \"Comparative View of the Various Institutions for the Assurance of Lives\". This interest followed a project to set up an insurance company, prompted by Francis Baily and mooted in 1824, but not carried out. Babbage did calculate actuarial tables for that scheme, using Equitable Society mortality data from 1762 onwards.\n\nDuring this whole period Babbage depended awkwardly on his father's support, given his father's attitude to his early marriage, of 1814: he and Edward Ryan wedded the Whitmore sisters. He made a home in Marylebone in London, and founded a large family. On his father's death in 1827, Babbage inherited a large estate (value around £100,000, equivalent to £ or $ today), making him independently wealthy. After his wife's death in the same year he spent time travelling. In Italy he met Leopold II, Grand Duke of Tuscany, foreshadowing a later visit to Piedmont. In April 1828 he was in Rome, and relying on Herschel to manage the difference engine project, when he heard that he had become professor at Cambridge, a position he had three times failed to obtain (in 1820, 1823 and 1826).\n\nBabbage was instrumental in founding the Royal Astronomical Society in 1820, initially known as the Astronomical Society of London. Its original aims were to reduce astronomical calculations to a more standard form, and to circulate data. These directions were closely connected with Babbage's ideas on computation, and in 1824 he won its Gold Medal, cited \"for his invention of an engine for calculating mathematical and astronomical tables\".\n\nBabbage's motivation to overcome errors in tables by mechanisation has been a commonplace since Dionysius Lardner wrote about it in 1834 in the \"Edinburgh Review\" (under Babbage's guidance). The context of these developments is still debated. Babbage's own account of the origin of the difference engine begins with the Astronomical Society's wish to improve \"The Nautical Almanac\". Babbage and Herschel were asked to oversee a trial project, to recalculate some part of those tables. With the results to hand, discrepancies were found. This was in 1821 or 1822, and was the occasion on which Babbage formulated his idea for mechanical computation. The issue of the \"Nautical Almanac\" is now described as a legacy of a polarisation in British science caused by attitudes to Sir Joseph Banks, who had died in 1820.\n\nBabbage studied the requirements to establish a modern postal system, with his friend Thomas Frederick Colby, concluding there should be a uniform rate that was put into effect with the introduction of the Uniform Fourpenny Post supplanted by the Uniform Penny Post in 1839 and 1840. Colby was another of the founding group of the Society. He was also in charge of the Survey of Ireland. Herschel and Babbage were present at a celebrated operation of that survey, the remeasuring of the Lough Foyle baseline.\n\nThe Analytical Society had initially been no more than an undergraduate provocation. During this period it had some more substantial achievements. In 1816 Babbage, Herschel and Peacock published a translation from French of the lectures of Sylvestre Lacroix, which was then the state-of-the-art calculus textbook.\n\nReference to Lagrange in calculus terms marks out the application of what are now called formal power series. British mathematicians had used them from about 1730 to 1760. As re-introduced, they were not simply applied as notations in differential calculus. They opened up the fields of functional equations (including the difference equations fundamental to the difference engine) and operator (D-module) methods for differential equations. The analogy of difference and differential equations was notationally changing Δ to D, as a \"finite\" difference becomes \"infinitesimal\". These symbolic directions became popular, as operational calculus, and pushed to the point of diminishing returns. The Cauchy concept of limit was kept at bay. Woodhouse had already founded this second \"British Lagrangian School\" with its treatment of Taylor series as formal.\n\nIn this context function composition is complicated to express, because the chain rule is not simply applied to second and higher derivatives. This matter was known to Woodhouse by 1803, who took from Louis François Antoine Arbogast what is now called Faà di Bruno's formula. In essence it was known to Abraham De Moivre (1697). Herschel found the method impressive, Babbage knew of it, and it was later noted by Ada Lovelace as compatible with the analytical engine. In the period to 1820 Babbage worked intensively on functional equations in general, and resisted both conventional finite differences and Arbogast's approach (in which Δ and D were related by the simple additive case of the exponential map). But via Herschel he was influenced by Arbogast's ideas in the matter of iteration, i.e. composing a function with itself, possibly many times. Writing in a major paper on functional equations in the \"Philosophical Transactions\" (1815/6), Babbage said his starting point was work of Gaspard Monge.\n\nFrom 1828 to 1839 Babbage was Lucasian Professor of Mathematics at Cambridge. Not a conventional resident don, and inattentive to teaching, he wrote three topical books during this period of his life. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1832. Babbage was out of sympathy with colleagues: George Biddell Airy, his predecessor as Lucasian Professor of Mathematics at Trinity College, Cambridge, thought an issue should be made of his lack of interest in lecturing. Babbage planned to lecture in 1831 on political economy. Babbage's reforming direction looked to see university education more inclusive, universities doing more for research, a broader syllabus and more interest in applications; but William Whewell found the programme unacceptable. A controversy Babbage had with Richard Jones lasted for six years. He never did give a lecture.\n\nIt was during this period that Babbage tried to enter politics. Simon Schaffer writes that his views of the 1830s included disestablishment of the Church of England, a broader political franchise, and inclusion of manufacturers as stakeholders. He twice stood for Parliament as a candidate for the borough of Finsbury. In 1832 he came in third among five candidates, missing out by some 500 votes in the two-member constituency when two other reformist candidates, Thomas Wakley and Christopher Temple, split the vote. In his memoirs Babbage related how this election brought him the friendship of Samuel Rogers: his brother Henry Rogers wished to support Babbage again, but died within days. In 1834 Babbage finished last among four. In 1832, Babbage, Herschel and Ivory were appointed Knights of the Royal Guelphic Order, however they were not subsequently made knights bachelor to entitle them to the prefix \"Sir\", which often came with appointments to that foreign order (though Herschel was later created a baronet).\n\nBabbage now emerged as a polemicist. One of his biographers notes that all his books contain a \"campaigning element\". His \"Reflections on the Decline of Science and some of its Causes\" (1830) stands out, however, for its sharp attacks. It aimed to improve British science, and more particularly to oust Davies Gilbert as President of the Royal Society, which Babbage wished to reform. It was written out of pique, when Babbage hoped to become the junior secretary of the Royal Society, as Herschel was the senior, but failed because of his antagonism to Humphry Davy. Michael Faraday had a reply written, by Gerrit Moll, as \"On the Alleged Decline of Science in England\" (1831). On the front of the Royal Society Babbage had no impact, with the bland election of the Duke of Sussex to succeed Gilbert the same year. As a broad manifesto, on the other hand, his \"Decline\" led promptly to the formation in 1831 of the British Association for the Advancement of Science (BAAS).\n\nThe \"Mechanics' Magazine\" in 1831 identified as Declinarians the followers of Babbage. In an unsympathetic tone it pointed out David Brewster writing in the \"Quarterly Review\" as another leader; with the barb that both Babbage and Brewster had received public money.\n\nIn the debate of the period on statistics (\"qua\" data collection) and what is now statistical inference, the BAAS in its Statistical Section (which owed something also to Whewell) opted for data collection. This Section was the sixth, established in 1833 with Babbage as chairman and John Elliot Drinkwater as secretary. The foundation of the Statistical Society followed. Babbage was its public face, backed by Richard Jones and Robert Malthus.\n\nBabbage published \"On the Economy of Machinery and Manufactures\" (1832), on the organisation of industrial production. It was an influential early work of operational research. John Rennie the Younger in addressing the Institution of Civil Engineers on manufacturing in 1846 mentioned mostly surveys in encyclopaedias, and Babbage's book was first an article in the \"Encyclopædia Metropolitana\", the form in which Rennie noted it, in the company of related works by John Farey, Jr., Peter Barlow and Andrew Ure. From \"An essay on the general principles which regulate the application of machinery to manufactures and the mechanical arts\" (1827), which became the \"Encyclopædia Metropolitana\" article of 1829, Babbage developed the schematic classification of machines that, combined with discussion of factories, made up the first part of the book. The second part considered the \"domestic and political economy\" of manufactures.\n\nThe book sold well, and quickly went to a fourth edition (1836). Babbage represented his work as largely a result of actual observations in factories, British and abroad. It was not, in its first edition, intended to address deeper questions of political economy; the second (late 1832) did, with three further chapters including one on piece rate. The book also contained ideas on rational design in factories, and profit sharing.\n\nIn \"Economy of Machinery\" was described what is now called the \"Babbage principle\". It pointed out commercial advantages available with more careful division of labour. As Babbage himself noted, it had already appeared in the work of Melchiorre Gioia in 1815. The term was introduced in 1974 by Harry Braverman. Related formulations are the \"principle of multiples\" of Philip Sargant Florence, and the \"balance of processes\".\n\nWhat Babbage remarked is that skilled workers typically spend parts of their time performing tasks that are below their skill level. If the labour process can be divided among several workers, labour costs may be cut by assigning only high-skill tasks to high-cost workers, restricting other tasks to lower-paid workers. He also pointed out that training or apprenticeship can be taken as fixed costs; but that returns to scale are available by his approach of standardisation of tasks, therefore again favouring the factory system. His view of human capital was restricted to minimising the time period for recovery of training costs.\n\nAnother aspect of the work was its detailed breakdown of the cost structure of book publishing. Babbage took the unpopular line, from the publishers' perspective, of exposing the trade's profitability. He went as far as to name the organisers of the trade's restrictive practices. Twenty years later he attended a meeting hosted by John Chapman to campaign against the Booksellers Association, still a cartel.\n\nIt has been written that \"what Arthur Young was to agriculture, Charles Babbage was to the factory visit and machinery\". Babbage's theories are said to have influenced the layout of the 1851 Great Exhibition, and his views had a strong effect on his contemporary George Julius Poulett Scrope. Karl Marx argued that the source of the productivity of the factory system was exactly the combination of the division of labour with machinery, building on Adam Smith, Babbage and Ure. Where Marx picked up on Babbage and disagreed with Smith was on the motivation for division of labour by the manufacturer: as Babbage did, he wrote that it was for the sake of profitability, rather than productivity, and identified an impact on the concept of a trade.\n\nJohn Ruskin went further, to oppose completely what manufacturing in Babbage's sense stood for. Babbage also affected the economic thinking of John Stuart Mill. George Holyoake saw Babbage's detailed discussion of profit sharing as substantive, in the tradition of Robert Owen and Charles Fourier, if requiring the attentions of a benevolent captain of industry, and ignored at the time.\n\nWorks by Babbage and Ure were published in French translation in 1830; \"On the Economy of Machinery\" was translated in 1833 into French by Édouard Biot, and into German the same year by Gottfried Friedenberg. The French engineer and writer on industrial organisation Léon Lalanne was influenced by Babbage, but also by the economist Claude Lucien Bergery, in reducing the issues to \"technology\". William Jevons connected Babbage's \"economy of labour\" with his own labour experiments of 1870. The Babbage principle is an inherent assumption in Frederick Winslow Taylor's scientific management.\n\nMary Boole, the wife of Babbage's collaborator George Boole claimed that there was profound influence via her uncle George Everest of Indian thought in general and Indian logic, in particular, on him and on George Boole, as well as on Augustus De Morgan:\n\nThink what must have been the effect of the intense Hinduizing of three such men as Babbage, De Morgan, and George Boole on the mathematical atmosphere of 1830–65. What share had it in generating the Vector Analysis and the mathematics by which investigations in physical science are now conducted?\n\nIn 1837, responding to the series of eight \"Bridgewater Treatises\", Babbage published his \"Ninth Bridgewater Treatise\", under the title \"On the Power, Wisdom and Goodness of God, as manifested in the Creation\". In this work Babbage weighed in on the side of uniformitarianism in a current debate. He preferred the conception of creation in which a God-given natural law dominated, removing the need for continuous \"contrivance\".\n\nThe book is a work of natural theology, and incorporates extracts from related correspondence of Herschel with Charles Lyell. Babbage put forward the thesis that God had the omnipotence and foresight to create as a divine legislator. In this book, Babbage dealt with relating interpretations between science and religion; on the one hand, he insisted that \"\"there exists no fatal collision between the words of Scripture and the facts of nature;\"\" on the one hand, he wrote the Book of Genesis was not meant to be read literally in relation to scientific terms. Against those who said these were in conflict, he wrote \"\"that the contradiction they have imagined can have no real existence, and that whilst the testimony of Moses remains unimpeached, we may also be permitted to confide in the testimony of our senses.\"\"\n\nThe Ninth Bridgewater Treatise was quoted extensively in \"Vestiges of the Natural History of Creation\". The parallel with Babbage's computing machines is made explicit, as allowing plausibility to the theory that transmutation of species could be pre-programmed.\nJonar Ganeri, author of \"Indian Logic\", believes Babbage may have been influenced by Indian thought; one possible route would be through Henry Thomas Colebrooke. Mary Everest Boole argues that Babbage was introduced to Indian thought in the 1820s by her uncle George Everest:\nSome time about 1825, [Everest] came to England for two or three years, and made a fast and lifelong friendship with Herschel and with Babbage, who was then quite young. I would ask any fair-minded mathematician to read Babbage's Ninth Bridgewater Treatise and compare it with the works of his contemporaries in England; and then ask himself whence came the peculiar conception of the nature of miracle which underlies Babbage's ideas of Singular Points on Curves (Chap, viii) – from European Theology or Hindu Metaphysic? Oh! how the English clergy of that day hated Babbage's book!\n\nBabbage was raised in the Protestant form of the Christian faith, his family having inculcated in him an orthodox form of worship. He explained:\n\nRejecting the Athanasian Creed as a \"direct contradiction in terms\", in his youth he looked to Samuel Clarke's works on religion, of which \"Being and Attributes of God\" (1704) exerted a particularly strong influence on him. Later in life, Babbage concluded that \"the true value of the Christian religion rested, not on speculative [theology] … but … upon those doctrines of kindness and benevolence which that religion claims and enforces, not merely in favour of man himself but of every creature susceptible of pain or of happiness.\"\n\nIn his autobiography (1864), Babbage wrote a whole chapter on the topic of religion, where he identified three sources of divine knowledge:\nHe stated, on the basis of the design argument, that studying the works of nature had been the more appealing evidence, and the one which led him to actively profess the existence of God. Advocating for natural theology, he wrote:\nLike Samuel Vince, Babbage also wrote a defence of the belief in divine miracles. Against objections previously posed by David Hume, Babbage advocated for the belief of divine agency, stating \"we must not measure the credibility or incredibility of an event by the narrow sphere of our own experience, nor forget that there is a Divine energy which overrides what we familiarly call the laws of nature.\" He alluded to the limits of human experience, expressing: \"all that we see in a miracle is an effect which is new to our observation, and whose cause is concealed. The cause may be beyond the sphere of our observation, and would be thus beyond the familiar sphere of nature; but this does not make the event a violation of any law of nature. The limits of man's observation lie within very narrow boundaries, and it would be arrogance to suppose that the reach of man's power is to form the limits of the natural world.\"\n\nThe British Association was consciously modelled on the Deutsche Naturforscher-Versammlung, founded in 1822. It rejected romantic science as well as metaphysics, and started to entrench the divisions of science from literature, and professionals from amateurs. Belonging as he did to the \"Wattite\" faction in the BAAS, represented in particular by James Watt the younger, Babbage identified closely with industrialists. He wanted to go faster in the same directions, and had little time for the more gentlemanly component of its membership. Indeed, he subscribed to a version of conjectural history that placed industrial society as the culmination of human development (and shared this view with Herschel). A clash with Roderick Murchison led in 1838 to his withdrawal from further involvement. At the end of the same year he sent in his resignation as Lucasian professor, walking away also from the Cambridge struggle with Whewell. His interests became more focussed, on computation and metrology, and on international contacts.\n\nA project announced by Babbage was to tabulate all physical constants (referred to as \"constants of nature\", a phrase in itself a neologism), and then to compile an encyclopaedic work of numerical information. He was a pioneer in the field of \"absolute measurement\". His ideas followed on from those of Johann Christian Poggendorff, and were mentioned to Brewster in 1832. There were to be 19 categories of constants, and Ian Hacking sees these as reflecting in part Babbage's \"eccentric enthusiasms\". Babbage's paper \"On Tables of the Constants of Nature and Art\" was reprinted by the Smithsonian Institution in 1856, with an added note that the physical tables of Arnold Henry Guyot \"will form a part of the important work proposed in this article\".\n\nExact measurement was also key to the development of machine tools. Here again Babbage is considered a pioneer, with Henry Maudslay, William Sellers, and Joseph Whitworth.\n\nThrough the Royal Society Babbage acquired the friendship of the engineer Marc Brunel. It was through Brunel that Babbage knew of Joseph Clement, and so came to encounter the artisans whom he observed in his work on manufactures. Babbage provided an introduction for Isambard Kingdom Brunel in 1830, for a contact with the proposed Bristol & Birmingham Railway. He carried out studies, around 1838, to show the superiority of the broad gauge for railways, used by Brunel's Great Western Railway.\n\nIn 1838, Babbage invented the pilot (also called a cow-catcher), the metal frame attached to the front of locomotives that clears the tracks of obstacles; he also constructed a dynamometer car. His eldest son, Benjamin Herschel Babbage, worked as an engineer for Brunel on the railways before emigrating to Australia in the 1850s.\n\nBabbage also invented an ophthalmoscope, which he gave to Thomas Wharton Jones for testing. Jones, however, ignored it. The device only came into use after being independently invented by Hermann von Helmholtz.\n\nBabbage achieved notable results in cryptography, though this was still not known a century after his death. Letter frequency was category 18 of Babbage's tabulation project. Joseph Henry later defended interest in it, in the absence of the facts, as relevant to the management of movable type.\n\nAs early as 1845, Babbage had solved a cipher that had been posed as a challenge by his nephew Henry Hollier, and in the process, he made a discovery about ciphers that were based on Vigenère tables. Specifically, he realized that enciphering plain text with a keyword rendered the cipher text subject to modular arithmetic. During the Crimean War of the 1850s, Babbage broke Vigenère's autokey cipher as well as the much weaker cipher that is called Vigenère cipher today. His discovery was kept a military secret, and was not published. Credit for the result was instead given to Friedrich Kasiski, a Prussian infantry officer, who made the same discovery some years later. However, in 1854, Babbage published the solution of a Vigenère cipher, which had been published previously in the \"Journal of the Society of Arts\". In 1855, Babbage also published a short letter, \"Cypher Writing\", in the same journal. Nevertheless, his priority was not established until 1985.\n\nBabbage involved himself in well-publicised but unpopular campaigns against public nuisances. He once counted all the broken panes of glass of a factory, publishing in 1857 a \"Table of the Relative Frequency of the Causes of Breakage of Plate Glass Windows\": Of 464 broken panes, 14 were caused by \"drunken men, women or boys\".\n\nBabbage's distaste for commoners (\"the Mob\") included writing \"Observations of Street Nuisances\" in 1864, as well as tallying up 165 \"nuisances\" over a period of 80 days. He especially hated street music, and in particular the music of organ grinders, against whom he railed in various venues. The following quotation is typical:\n\nBabbage was not alone in his campaign. A convert to the cause was the MP Michael Thomas Bass.\n\nIn the 1860s, Babbage also took up the anti-hoop-rolling campaign. He blamed hoop-rolling boys for driving their iron hoops under horses' legs, with the result that the rider is thrown and very often the horse breaks a leg. Babbage achieved a certain notoriety in this matter, being denounced in debate in Commons in 1864 for \"commencing a crusade against the popular game of tip-cat and the trundling of hoops.\"\n\nBabbage's machines were among the first mechanical computers. That they were not actually completed was largely because of funding problems and clashes of personality, most notably with Airy, the Astronomer Royal.\n\nBabbage directed the building of some steam-powered machines that achieved some modest success, suggesting that calculations could be mechanised. For more than ten years he received government funding for his project, which amounted to £17,000, but eventually the Treasury lost confidence in him.\n\nWhile Babbage's machines were mechanical and unwieldy, their basic architecture was similar to a modern computer. The data and program memory were separated, operation was instruction-based, the control unit could make conditional jumps, and the machine had a separate I/O unit.\n\nIn Babbage's time, printed mathematical tables were calculated by human computers; in other words, by hand. They were central to navigation, science and engineering, as well as mathematics. Mistakes were known to occur in transcription as well as calculation.\n\nAt Cambridge, Babbage saw the fallibility of this process, and the opportunity of adding mechanisation into its management. His own account of his path towards mechanical computation references a particular occasion:\n\nIn 1812 he was sitting in his rooms in the Analytical Society looking at a table of logarithms, which he knew to be full of mistakes, when the idea occurred to him of computing all tabular functions by machinery. The French government had produced several tables by a new method. Three or four of their mathematicians decided how to compute the tables, half a dozen more broke down the operations into simple stages, and the work itself, which was restricted to addition and subtraction, was done by eighty computers who knew only these two arithmetical processes. Here, for the first time, mass production was applied to arithmetic, and Babbage was seized by the idea that the labours of the unskilled computers [people] could be taken over completely by machinery which would be quicker and more reliable.\n\nThere was another period, seven years later, when his interest was aroused by the issues around computation of mathematical tables. The French official initiative by Gaspard de Prony, and its problems of implementation, were familiar to him. After the Napoleonic Wars came to a close, scientific contacts were renewed on the level of personal contact: in 1819 Charles Blagden was in Paris looking into the printing of the stalled de Prony project, and lobbying for the support of the Royal Society. In works of the 1820s and 1830s, Babbage referred in detail to de Prony's project.\n\nBabbage began in 1822 with what he called the difference engine, made to compute values of polynomial functions. It was created to calculate a series of values automatically. By using the method of finite differences, it was possible to avoid the need for multiplication and division.\n\nFor a prototype difference engine, Babbage brought in Joseph Clement to implement the design, in 1823. Clement worked to high standards, but his machine tools were particularly elaborate. Under the standard terms of business of the time, he could charge for their construction, and would also own them. He and Babbage fell out over costs around 1831.\n\nSome parts of the prototype survive in the Museum of the History of Science, Oxford. This prototype evolved into the \"first difference engine.\" It remained unfinished and the finished portion is located at the Science Museum in London. This first difference engine would have been composed of around 25,000 parts, weighed fifteen tons (13,600 kg), and would have been tall. Although Babbage received ample funding for the project, it was never completed. He later (1847–1849) produced detailed drawings for an improved version,\"Difference Engine No. 2\", but did not receive funding from the British government. His design was finally constructed in 1989–1991, using his plans and 19th-century manufacturing tolerances. It performed its first calculation at the Science Museum, London, returning results to 31 digits.\n\nNine years later, in 2000, the Science Museum completed the printer Babbage had designed for the difference engine.\n\nThe Science Museum has constructed two Difference Engines according to Babbage's plans for the Difference Engine No 2. One is owned by the museum. The other, owned by the technology multimillionaire Nathan Myhrvold, went on exhibition at the Computer History Museum in Mountain View, California on 10 May 2008. The two models that have been constructed are not replicas; Myhrvold's engine is the first design by Babbage, and the Science Museum's is a later model.\n\nAfter the attempt at making the first difference engine fell through, Babbage worked to design a more complex machine called the Analytical Engine. He hired C. G. Jarvis, who had previously worked for Clement as a draughtsman. The Analytical Engine marks the transition from mechanised arithmetic to fully-fledged general purpose computation. It is largely on it that Babbage's standing as computer pioneer rests.\n\nThe major innovation was that the Analytical Engine was to be programmed using punched cards: the Engine was intended to use loops of Jacquard's punched cards to control a mechanical calculator, which could use as input the results of preceding computations. The machine was also intended to employ several features subsequently used in modern computers, including sequential control, branching and looping. It would have been the first mechanical device to be, in principle, Turing-complete. The Engine was not a single physical machine, but rather a succession of designs that Babbage tinkered with until his death in 1871.\nAda Lovelace, who corresponded with Babbage during his development of the Analytical Engine, is credited with developing an algorithm that would enable the Engine to calculate a sequence of Bernoulli numbers. Despite documentary evidence in Lovelace's own handwriting, some scholars dispute to what extent the ideas were Lovelace's own. For this achievement, she is often described as the first computer programmer; though no programming language had yet been invented.\n\nLovelace also translated and wrote literature supporting the project. Describing the engine's programming by punch cards, she wrote: \"We may say most aptly that the Analytical Engine weaves algebraical patterns just as the Jacquard loom weaves flowers and leaves.\"\n\nBabbage visited Turin in 1840 at the invitation of Giovanni Plana. In 1842 Charles Wheatstone approached Lovelace to translate a paper of Luigi Menabrea, who had taken notes of Babbage's Turin talks; and Babbage asked her to add something of her own. Fortunato Prandi who acted as interpreter in Turin was an Italian exile and follower of Giuseppe Mazzini.\n\nPer Georg Scheutz wrote about the difference engine in 1830, and experimented in automated computation. After 1834 and Lardner's \"Edinburgh Review\" article he set up a project of his own, doubting whether Babbage's initial plan could be carried out. This he pushed through with his son, Edvard Scheutz. Another Swedish engine was that of Martin Wiberg (1860).\n\nIn 2011, researchers in Britain proposed a multimillion-pound project, \"Plan 28\", to construct Babbage's Analytical Engine. Since Babbage's plans were continually being refined and were never completed, they intended to engage the public in the project and crowd-source the analysis of what should be built. It would have the equivalent of 675 bytes of memory, and run at a clock speed of about 7 Hz. They hope to complete it by the 150th anniversary of Babbage's death, in 2021.\n\nAdvances in MEMs and nanotechnology have led to recent high-tech experiments in mechanical computation. The benefits suggested include operation in high radiation or high temperature environments. These modern versions of mechanical computation were highlighted in \"The Economist\" in its special \"end of the millennium\" black cover issue in an article entitled \"Babbage's Last Laugh\".\n\nDue to his association with the town Babbage was chosen in 2007 to appear on the 5 Totnes pound note. An image of Babbage features in the British cultural icons section of the newly designed British passport in 2015.\n\nOn 25 July 1814, Babbage married Georgiana Whitmore at St. Michael's Church in Teignmouth, Devon; her sister Louisa married Edward Ryan. The couple lived at Dudmaston Hall, Shropshire (where Babbage engineered the central heating system), before moving to 5 Devonshire Street, London in 1815.\n\nCharles and Georgiana had eight children, but only four – Benjamin Herschel, Georgiana Whitmore, Dugald Bromhead and Henry Prevost – survived childhood. Charles' wife Georgiana died in Worcester on 1 September 1827, the same year as his father, their second son (also named Charles) and their newborn son Alexander.\n\nHis youngest surviving son, Henry Prevost Babbage (1824–1918), went on to create six small demonstration pieces for Difference Engine No. 1 based on his father's designs, one of which was sent to Harvard University where it was later discovered by Howard H. Aiken, pioneer of the Harvard Mark I. Henry Prevost's 1910 Analytical Engine Mill, previously on display at Dudmaston Hall, is now on display at the Science Museum.\n\nBabbage lived and worked for over 40 years at 1 Dorset Street, Marylebone, where he died, at the age of 79, on 18 October 1871; he was buried in London's Kensal Green Cemetery. According to Horsley, Babbage died \"of renal inadequacy, secondary to cystitis.\" He had declined both a knighthood and baronetcy. He also argued against hereditary peerages, favouring life peerages instead.\n\nIn 1983 the autopsy report for Charles Babbage was discovered and later published by his great-great-grandson. A copy of the original is also available. Half of Babbage's brain is preserved at the Hunterian Museum in the Royal College of Surgeons in London. The other half of Babbage's brain is on display in the Science Museum, London.\n\nThere is a black plaque commemorating the 40 years Babbage spent at 1 Dorset Street, London. Locations, institutions and other things named after Babbage include:\n\nBabbage frequently appears in steampunk works; he has been called an iconic figure of the genre. Other works in which Babbage appears include:\n\n\n\n"}
{"id": "5700", "url": "https://en.wikipedia.org/wiki?curid=5700", "title": "Cross-dressing", "text": "Cross-dressing\n\nCross-dressing is the act of wearing items of clothing and other accoutrements commonly associated with the opposite sex within a particular society. Cross-dressing has been used for purposes of disguise, comfort, and self-expression in modern times and throughout history.\n\nAlmost every human society throughout history has had expected norms for each gender relating to style, color, or type of clothing they are expected to wear, and likewise most societies have had a set of guidelines, views or even laws defining what type of clothing is appropriate for each gender.\n\nThe term \"cross-dressing\" refers to an action or a behavior, without attributing or implying any specific causes or motives for that behavior. Cross-dressing is not synonymous with being transgender. \n\nThe phenomenon of cross-dressing is not new: it was referred to in the Hebrew Bible. However, the terms to describe it change. The Anglo-Saxon \"cross-dresser\" has largely superseded the Latinate \"transvestite\", which has come to be seen as outdated and derogatory. This is because the latter was historically used to diagnose psychiatric disorders (e.g. transvestic fetishism), but the former was coined by the transgender community.\n\nCross-dressing has been practiced throughout much of recorded history, in many societies, and for many reasons. Examples exist in Greek, Norse, and Hindu mythology. There is a rich history of cross-dressing found in folklore, literature, theater, and music, such as Kabuki and Korean shamanism. In the British and European context, theatrical troupes (\"playing companies\") were all-male, with the female parts undertaken by boy players.\n\nA wide variety of historical figures are known to have cross-dressed to varying degrees. Many women found they had to disguise themselves as men in order to participate in the wider world. For example, Margaret King cross-dressed in the early nineteenth century to attend medical school, as none would accept female students. A century later, Vita Sackville-West dressed as a young soldier in order to \"walk out\" with her girlfriend Violet Keppel, to avoid the street harassment that two women would have faced. The prohibition on women wearing male garb, once strictly applied, still has echoes today in some Western societies which require girls and women to wear skirts, for example as part of school uniform or office dress codes. In some countries, even in casual settings, women are still prohibited from wearing traditionally male clothing. Sometimes all trousers, no matter how loose and long, are automatically considered \"indecent\", which may render their wearer subject to severe punishment, as in the case of Lubna al-Hussein in Sudan in 2009.\n\nThere are many different kinds of cross-dressing and many different reasons why an individual might engage in cross-dressing behavior. Some people cross-dress as a matter of comfort or style, out of personal preference for clothing associated with the opposite sex. In this case, a person's cross-dressing may or may not be apparent to other people. Some people cross-dress to shock others or challenge social norms. Some people attempt to pass as a member of the opposite sex in order to gain access to places or resources they would not otherwise be able to reach.\n\nGender disguise has been used by women and girls to pass as male, and by men and boys to pass as female. Gender disguise has also been used as a plot device in storytelling, particularly in narrative ballads, and is a recurring motif in literature, theater, and film. Historically, some women have cross-dressed to take up male-dominated or male-exclusive professions, such as military service. Conversely, some men have cross-dressed to escape from mandatory military service or as a disguise to assist in political or social protest, as men did in the Rebecca Riots.\n\nUndercover journalism may require cross-dressing, as with Norah Vincent's project \"Self-Made Man\".\n\nSome girls in Afghanistan, long after the fall of the Taliban, are still disguised by their families as boys. This is known as \"bacha posh\".\n\nSingle-sex theatrical troupes often have some performers who cross-dress to play roles written for members of the opposite sex (travesti and trouser roles). Cross-dressing, particularly the depiction of males wearing dresses, is often used for comic effect onstage and on-screen.\n\nDrag is a special form of performance art based on the act of cross-dressing. A drag queen is usually a male-assigned person who performs as an exaggeratedly feminine character, in heightened costuming sometimes consisting of a showy dress, high-heeled shoes, obvious make-up, and wig. A drag queen may imitate famous female film or pop-music stars. A faux queen is a female-assigned person employing the same techniques. A drag king is a counterpart of the drag queen - a female-assigned person who adopts a masculine persona in performance or imitates a male film or pop-music star. Some female-assigned people undergoing gender reassignment therapy also self-identify as \"drag kings\" although this use of \"drag king\" would generally be considered inaccurate.\n\nThe modern activity of battle reenactments has raised the question of women passing as male soldiers. In 1989, Lauren Burgess dressed as a male soldier in a U.S. National Park Service reenactment of the Battle of Antietam, and was ejected after she was discovered to be a woman. Burgess sued the Park Service for sexual discrimination. The case spurred spirited debate among Civil War buffs. In 1993, a federal judge ruled in Burgess's favor.\n\n\"Wigging\" refers to the practice of male stunt doubles taking the place of an actress, parallel to \"paint downs\", where white stunt doubles are made up to resemble black actors. Female stunt doubles have begun to protest this norm of \"historical sexism\", saying that it restricts their already limited job possibilities\n\nA transvestic fetishist is a person who cross-dresses as part of a sexual fetish. According to the fourth edition of \"Diagnostic and Statistical Manual of Mental Disorders\", this fetishism was limited to heterosexual men; however, DSM-5 does not have this restriction, and opens it to women and men, regardless of their sexual orientation.\n\nThe term \"underdressing\" is used by male cross-dressers to describe wearing female undergarments under their male clothes. The famous low-budget film-maker Edward D. Wood, Jr. said he often wore women's underwear under his military uniform during World War II. \"Female masking\" is a form of cross-dressing in which men wear masks that present them as female.\n\nSometimes either member of a heterosexual couple will cross-dress in order to arouse the other. For example, the male might wear skirts or lingerie and/or the female will wear boxers or other male clothing. (See also forced feminization)\n\nSome people who cross-dress may endeavor to project a complete impression of belonging to another gender, including mannerisms, speech patterns, and emulation of sexual characteristics. This is referred to as passing or \"trying to pass\" depending how successful the person is. An observer who sees through the cross-dresser's attempt to pass is said to have \"read\" or \"clocked\" them. There are videos, books, and magazines on how a man may look more like a woman.\n\nOthers may choose to take a mixed approach, adopting some feminine traits and some masculine traits in their appearance. For instance, a man might wear both a dress and a beard. This is sometimes known as \"genderfuck\". In a broader context, cross-dressing may also refer to other actions undertaken to pass as a particular sex, such as packing (accentuating the male crotch bulge) or, the opposite, tucking (concealing the male crotch bulge).\n\nThe actual determination of cross-dressing is largely socially constructed. For example, in Western society, trousers have long been adopted for usage by women, and it is no longer regarded as cross-dressing. In cultures where men have traditionally worn skirt-like garments such as the kilt or sarong, these are not seen as female clothing, and wearing them is not seen as cross-dressing for men. As societies are becoming more global in nature, both men's and women's clothing are adopting styles of dress associated with other cultures.\nCosplaying may also involve cross-dressing, for some females may wish to dress as a male, and vice versa (see Crossplay). Breast binding (for females) is not uncommon and is one of the things likely needed to cosplay a male character.\n\nIn most parts of the world it remains socially disapproved for men to wear clothes traditionally associated with women. Attempts are occasionally made, e.g. by fashion designers, to promote the acceptance of skirts as everyday wear for men. Cross-dressers have complained that society permits women to wear pants or jeans and other masculine clothing, while condemning any man who wants to wear clothing sold for women.\n\nWhile creating a more feminine figure, male cross-dressers will often utilize different types and styles of breast forms, which are silicone prostheses traditionally used by women who have undergone mastectomies to recreate the visual appearance of a breast.\n\nWhile most male cross-dressers utilize clothing associated with modern women, some are involved in subcultures that involve dressing as little girls or in vintage clothing. Some such men have written that they enjoy dressing as femininely as possible, so they wear frilly dresses with lace and ribbons, bridal gowns complete with veils, as well as multiple petticoats, corsets, girdles and/or garter belts with nylon stockings.\n\nCross-dressers may begin wearing clothing associated with the opposite sex in childhood, using the clothes of a sibling, parent, or friend. Some parents have said they allowed their children to cross-dress and, in many cases, the child stopped when they became older. The same pattern often continues into adulthood, where there may be confrontations with a spouse. Married cross-dressers experience considerable anxiety and guilt if their spouse objects to their behavior. Sometimes cross-dressers have periodically disposed of all their clothing, a practice called \"purging\", only to start collecting other gender's clothing again.\n\nCelebrations of cross-dressing occur in wide-spread cultures. The Abissa festival in Cote d’Ivoire, Ofudamaki in Japan, and Kottankulangara Festival in India are all examples of this.\n\nThe historical associations of maleness with power and femaleness with submission and frivolity mean that in the present time a woman dressing in men's clothing and a man dressing in women's clothing evoke very different responses. A woman dressing in men's clothing is considered to be a more acceptable activity.\n\nAdvocacy for social change has done much to relax the constrictions of gender roles on men and women, but they are still subject to prejudice from some people. It is noticeable that as 'transgender' is becoming more socially accepted as a normal human condition, the prejudices against cross-dressing are changing quite quickly, just as the similar prejudices against homosexuals have changed rapidly in recent decades.\n\nThe reason it is so hard to have statistics for female-assigned cross-dressers is that the line where cross-dressing stops and cross-dressing begins has become blurred, whereas the same line for men is as well defined as ever. This is one of the many issues being addressed by third wave feminism as well as the modern-day masculist movement.\n\nThe general culture has very mixed views about cross-dressing. A woman who wears her husband's shirt to bed is considered attractive while a man who wears his wife's nightgown to bed may be considered transgressive. Marlene Dietrich in a tuxedo was considered very erotic; Jack Lemmon in a dress was considered ridiculous. All this may result from an overall gender role rigidity for males; that is, because of the prevalent gender dynamic throughout the world, men frequently encounter discrimination when deviating from masculine gender norms, particularly violations of heteronormativity. A man's adoption of feminine clothing is often considered a going down in the gendered social order whereas a woman's adoption of what are traditionally men's clothing (at least in the English-speaking world) has less of an impact because women have been traditionally subordinate to men, unable to affect serious change through style of dress. Thus when a male cross-dresser puts on his clothes, he transforms into the quasi-female and thereby becomes an embodiment of the conflicted gender dynamic. Following the work of Butler, gender proceeds along through ritualized performances, but in male cross-dressing it becomes a performative \"breaking\" of the masculine and a \"subversive repetition\" of the feminine.\n\nPsychoanalysts today do not regard cross-dressing by itself as a psychological problem, unless it interferes with a person's life. \"For instance,\" said Dr. Joseph Merlino, senior editor of \"Freud at 150: 21st Century Essays on a Man of Genius\", \"[suppose that]...I'm a cross-dresser and I don't want to keep it confined to my circle of friends, or my party circle, and I want to take that to my wife and I don't understand why she doesn't accept it, or I take it to my office and I don't understand why they don't accept it, then it's become a problem because it's interfering with my relationships and environment.\"\n\n\n"}
{"id": "5702", "url": "https://en.wikipedia.org/wiki?curid=5702", "title": "Channel Tunnel", "text": "Channel Tunnel\n\nThe Channel Tunnel (; also nicknamed the Chunnel) is a rail tunnel linking Folkestone, Kent, in the United Kingdom, with Coquelles, Pas-de-Calais, near Calais in northern France, beneath the English Channel at the Strait of Dover. It is the only fixed link between the island of Great Britain and the European mainland. At its lowest point, it is deep below the sea bed and below sea level. At , the tunnel has the longest undersea portion of any tunnel in the world, although the Seikan Tunnel in Japan is both longer overall at and deeper at below sea level. The speed limit for trains in the tunnel is .\n\nThe tunnel carries high-speed Eurostar passenger trains, the Eurotunnel Shuttle for road vehicles—the largest such transport in the world—and international goods trains. The tunnel connects end-to-end with the LGV Nord and High Speed 1 high-speed railway lines. In 2017 through rail services carried 10.3 million passengers and 1.22M tonnes of freight, and the Shuttle carried 10.4M passengers, 2.6M cars, 51,000 coaches, and 1.6M lorries (equivalent to 21.3M tonnes of freight). This compares with 11.7 million passengers, 2.6 million lorries and 2.2 million cars through the Port of Dover.\n\nIdeas for a cross-Channel fixed link appeared as early as 1802, but British political and press pressure over the compromising of national security stalled attempts to construct a tunnel. An early attempt at building a Channel Tunnel was made in the late 19th century, on the English side, \"in the hope of forcing the hand of the English Government\". The eventual successful project, organised by Eurotunnel, began construction in 1988 and opened in 1994. At £5.5 billion (1985 prices), it was at the time the most expensive construction project ever proposed. The cost finally amounted to £9 billion ($21 billion), well over its predicted budget.\n\nSince its construction, the tunnel has faced a few mechanical problems. Both fires and cold weather have temporarily disrupted its operation.\n\nPeople have attempted to use the tunnel to enter the UK illegally since 1997, creating the ongoing issue of the migrants around Calais on the French side, causing both diplomatic disagreement and violence.\n\nIn 1802, Albert Mathieu-Favier, a French mining engineer, put forward a proposal to tunnel under the English Channel, with illumination from oil lamps, horse-drawn coaches, and an artificial island positioned mid-Channel for changing horses. Mathieu-Favier's design envisaged a bored two-level tunnel with the top tunnel used for transport and the bottom one for groundwater flows.\n\nIn 1839, Aimé Thomé de Gamond, a Frenchman, performed the first geological and hydrographical surveys on the Channel, between Calais and Dover. Thomé de Gamond explored several schemes and, in 1856, he presented a proposal to Napoleon III for a mined railway tunnel from Cap Gris-Nez to Eastwater Point with a port/airshaft on the Varne sandbank at a cost of 170 million francs, or less than £7 million.\n\nIn 1865, a deputation led by George Ward Hunt proposed the idea of a tunnel to the Chancellor of the Exchequer of the day, William Ewart Gladstone.\n\nAround 1866, William Low and Sir John Hawkshaw promoted ideas, but apart from preliminary geological studies none were implemented. An official Anglo-French protocol was established in 1876 for a cross-Channel railway tunnel. In 1881, the British railway entrepreneur Sir Edward Watkin and Alexandre Lavalley, a French Suez Canal contractor, were in the Anglo-French Submarine Railway Company that conducted exploratory work on both sides of the Channel. On the English side a diameter Beaumont-English boring machine dug a pilot tunnel from Shakespeare Cliff. On the French side, a similar machine dug from Sangatte. The project was abandoned in May 1882, owing to British political and press campaigns asserting that a tunnel would compromise Britain's national defences. These early works were encountered more than a century later during the TML project.\n\nA 1907 film, \"Tunnelling the English Channel\" by pioneer filmmaker Georges Méliès, depicts King Edward VII and President Armand Fallières dreaming of building a tunnel under the English Channel.\n\nIn 1919, during the Paris Peace Conference, the British prime minister, David Lloyd George, repeatedly brought up the idea of a Channel tunnel as a way of reassuring France about British willingness to defend against another German attack. The French did not take the idea seriously, and nothing came of Lloyd George's proposal.\n\nIn the 1920s Winston Churchill was an advocate for the Channel Tunnel, using that exact nomenclature in an essay entitled \"Should Strategists Veto The Tunnel?\" The essay was published on 27 July 1924 in the \"Weekly Dispatch\", and argued vehemently against those that believed the tunnel could be used by a Continental enemy in an invasion of Britain. Churchill extolled his enthusiasm for the project again in an article for the \"Daily Mail\" on 12 February 1936, \"Why Not A Channel Tunnel?\"\n\nThere was another proposal in 1929, but nothing came of this discussion and the idea was shelved. Proponents estimated construction to be about US$150 million. The engineers had addressed the concerns of both nations' military leaders by designing two sumps—one near the coast of each country—that could be flooded at will to block the tunnel. This design feature did not override the concerns of both nations' military leaders, and other concerns about hordes of undesirable tourists who would disrupt English habits of living. Military fears continued during the Second World War. After the fall of France, as Britain prepared for an expected German invasion, a Royal Navy officer in the Directorate of Miscellaneous Weapons Development calculated that Hitler could use slave labour to build two Channel tunnels in 18 months. The estimate caused rumours that Germany had already begun digging.\n\nA British film from Gaumont Studios, \"The Tunnel\" (also called \"TransAtlantic Tunnel\"), was released in 1935 as a futuristic science fiction project concerning the creation of a transatlantic tunnel. It referred briefly to its protagonist, a Mr. McAllan, as having completed a British Channel tunnel successfully in 1940, five years into the future of the film's release.\n\nBy 1955, defence arguments had become less relevant due to the dominance of air power, and both the British and French governments supported technical and geological surveys. In 1958 the 1881 workings were cleared in preparation for a £100,000 geological survey by the Channel Tunnel Study Group. 30% of the funding came from the Channel Tunnel Co Ltd, the largest shareholder of which was the British Transport Commission, as successor to the South Eastern Railway. A detailed geological survey was carried out in 1964–65.\n\nAlthough the two countries agreed to build a tunnel in 1964, the phase 1 initial studies and signing of a second agreement to cover phase 2 took until 1973. Construction work of this government-funded project to create two tunnels designed to accommodate car shuttle wagons on either side of a service tunnel started on both sides of the Channel in 1974.\n\nOn 20 January 1975, to the dismay of their French partners, the now-governing Labour Party in Britain cancelled the project due to uncertainty about EEC membership, doubling cost estimates and the general economic crisis at the time. By this time the British tunnel boring machine was ready and the Ministry of Transport was able to do a experimental drive. This short tunnel was reused as the starting and access point for tunnelling operations from the British side. The cancellation costs were estimated to be £17 million. On the French side, a tunnel boring machine had been installed underground in a stub tunnel. It lay “moth-balled” for 14 years until 1988, when it was sold, dismantled, refurbished and shipped to Turkey where it was used to drive the Moda tunnel for the Istanbul Sewerage Scheme.\n\nIn 1979, the \"Mouse-hole Project\" was suggested when the Conservatives came to power in Britain. The concept was a single-track rail tunnel with a service tunnel, but without shuttle terminals. The British government took no interest in funding the project, but Margaret Thatcher, the prime minister, said she had no objection to a privately funded project. In 1981 Thatcher and François Mitterrand, the French president, agreed to establish a working group to evaluate a privately funded project. In June 1982 the Franco-British study group favoured a twin tunnel to accommodate conventional trains and a vehicle shuttle service. In April 1985 promoters were invited to submit scheme proposals. Four submissions were shortlisted:\n\n\nThe cross-Channel ferry industry protested under the name \"Flexilink\". In 1975 there was no campaign protesting against a fixed link, with one of the largest ferry operators (Sealink) being state-owned. Flexilink continued rousing opposition throughout 1986 and 1987. Public opinion strongly favoured a drive-through tunnel, but ventilation issues, concerns about accident management, and fear of driver mesmerisation led to the only shortlisted rail submission, CTG/F-M, being awarded the project in January 1986. Among reasons given for the selection was that it caused least disruption to shipping in the Channel, least environmental disruption, was the best protected against terrorism, and was the most likely to attract sufficient private finance.\n\nThe British \"Channel Tunnel Group\" consisted of two banks and five construction companies, while their French counterparts, \"France–Manche\", consisted of three banks and five construction companies. The role of the banks was to advise on financing and secure loan commitments. On 2 July 1985, the groups formed Channel Tunnel Group/France–Manche (CTG/F–M). Their submission to the British and French governments was drawn from the 1975 project, including 11 volumes and a substantial environmental impact statement.\n\nThe design and construction was done by the ten construction companies in the CTG/F-M group. The French terminal and boring from Sangatte was undertaken by the five French construction companies in the joint venture group \"GIE Transmanche Construction\". The English Terminal and boring from Shakespeare Cliff was undertaken by the five British construction companies in the \"Translink Joint Venture\". The two partnerships were linked by TransManche Link (TML), a bi-national project organisation. The Maître d'Oeuvre was a supervisory engineering body employed by Eurotunnel under the terms of the concession that monitored project activity and reported back to the governments and banks.\n\nIn France, with its long tradition of infrastructure investment, the project garnered widespread approval. In April the French National Assembly gave unanimous support and, in June 1987, after a public inquiry, the Senate gave unanimous support. In Britain, select committees examined the proposal, making history by holding hearings away from Westminster, in Kent. In February 1987, the third reading of the Channel Tunnel Bill took place in the House of Commons, and was carried by 94 votes to 22. The Channel Tunnel Act gained Royal assent and passed into law in July. Parliamentary support for the project came partly from provincial members of Parliament on the basis of promises of regional Eurostar through train services that never materialised; the promises were repeated in 1996 when the contract for construction of the Channel Tunnel Rail Link was awarded.\n\nThe tunnel is a build-own-operate-transfer (BOOT) project with a concession.<ref name=\"Flyvbjerg p. 96/97\">Flyvbjerg et al. pp. 96–97</ref> TML would design and build the tunnel, but financing was through a separate legal entity, Eurotunnel. Eurotunnel absorbed CTG/F-M and signed a construction contract with TML, but the British and French governments controlled final engineering and safety decisions, now in the hands of the Channel Tunnel Safety Authority. The British and French governments gave Eurotunnel a 55-year operating concession (from 1987; extended by 10 years to 65 years in 1993) to repay loans and pay dividends. A Railway Usage Agreement was signed between Eurotunnel, British Rail and SNCF guaranteeing future revenue in exchange for the railways obtaining half of the tunnel's capacity.\n\nPrivate funding for such a complex infrastructure project was of unprecedented scale. An initial equity of £45 million was raised by CTG/F-M, increased by £206 million private institutional placement, £770 million was raised in a public share offer that included press and television advertisements, a syndicated bank loan and letter of credit arranged £5 billion. Privately financed, the total investment costs at 1985 prices were £2.6 billion. At the 1994 completion actual costs were, in 1985 prices, £4.65 billion: an 80% cost overrun. The cost overrun was partly due to enhanced safety, security, and environmental demands. Financing costs were 140% higher than forecast.\n\nWorking from both the English side and the French side of the Channel, eleven tunnel boring machines or TBMs cut through chalk marl to construct two rail tunnels and a service tunnel. The vehicle shuttle terminals are at Cheriton (part of Folkestone) and Coquelles, and are connected to the English M20 and French A16 motorways respectively.\n\nTunnelling commenced in 1988, and the tunnel began operating in 1994. In 1985 prices, the total construction cost was £4.65 billion (equivalent to £ billion in 2015), an 80% cost overrun. At the peak of construction 15,000 people were employed with daily expenditure over £3 million. Ten workers, eight of them British, were killed during construction between 1987 and 1993, most in the first few months of boring.\n\nA two-inch (50-mm) diameter pilot hole allowed the service tunnel to break through without ceremony on 30 October 1990. On 1 December 1990, Englishman Graham Fagg and Frenchman Phillippe Cozette broke through the service tunnel with the media watching. Eurotunnel completed the tunnel on time, and it was officially opened, one year later than originally planned, by Queen Elizabeth II and the French president, François Mitterrand, in a ceremony held in Calais on 6 May 1994. The Queen travelled through the tunnel to Calais on a Eurostar train, which stopped nose to nose with the train that carried President Mitterrand from Paris. Following the ceremony President Mitterrand and the Queen travelled on Le Shuttle to a similar ceremony in Folkestone. A full public service did not start for several months. The first freight train, however, ran on 1 June 1994 and carried Rover and Mini cars being exported to Italy.\n\nThe Channel Tunnel Rail Link (CTRL), now called High Speed 1, runs from St Pancras railway station in London to the tunnel portal at Folkestone in Kent. It cost £5.8 billion. On 16 September 2003 the prime minister, Tony Blair, opened the first section of High Speed 1, from Folkestone to north Kent. On 6 November 2007 the Queen officially opened High Speed 1 and St Pancras International station, replacing the original slower link to Waterloo International railway station. High Speed 1 trains travel at up to , the journey from London to Paris taking 2 hours 15 minutes, to Brussels 1 hour 51 minutes.\n\nIn 1994, the American Society of Civil Engineers elected the tunnel as one of the seven modern Wonders of the World. In 1995, the American magazine \"Popular Mechanics\" published the results.\n\nSurveying undertaken in the 20 years before construction confirmed earlier speculations that a tunnel could be bored through a chalk marl stratum. The chalk marl is conducive to tunnelling, with impermeability, ease of excavation and strength. The chalk marl runs along the entire length of the English side of the tunnel, but on the French side a length of has variable and difficult geology. The tunnel consists of three bores: two diameter rail tunnels, apart, in length with a diameter service tunnel in between. The three bores are connected by cross-passages and piston relief ducts. The service tunnel was used as a pilot tunnel, boring ahead of the main tunnels to determine the conditions. English access was provided at Shakespeare Cliff, French access from a shaft at Sangatte. The French side used five tunnel boring machines (TBMs), the English side six. The service tunnel uses Service Tunnel Transport System (STTS) and Light Service Tunnel Vehicles (LADOGS). Fire safety was a critical design issue.\n\nBetween the portals at Beussingue and Castle Hill the tunnel is long, with under land on the French side and on the UK side, and under sea. It is the third-longest rail tunnel in the world, behind the Gotthard Base Tunnel in Switzerland and the Seikan Tunnel in Japan, but with the longest under-sea section. The average depth is below the seabed. On the UK side, of the expected of spoil approximately was used for fill at the terminal site, and the remainder was deposited at Lower Shakespeare Cliff behind a seawall, reclaiming of land. This land was then made into the Samphire Hoe Country Park. Environmental impact assessment did not identify any major risks for the project, and further studies into safety, noise, and air pollution were overall positive. However, environmental objections were raised over a high-speed link to London.\n\nSuccessful tunnelling required a sound understanding of the topography and geology and the selection of the best rock strata through which to dig. The geology of this site generally consists of northeasterly dipping Cretaceous strata, part of the northern limb of the Wealden-Boulonnais dome. Characteristics include:\n\n\nOn the English side, the stratum dip is less than 5°; on the French side this increases to 20°. Jointing and faulting are present on both sides. On the English side, only minor faults of displacement less than exist; on the French side, displacements of up to are present owing to the Quenocs anticlinal fold. The faults are of limited width, filled with calcite, pyrite and remoulded clay. The increased dip and faulting restricted the selection of route on the French side. To avoid confusion, microfossil assemblages were used to classify the chalk marl. On the French side, particularly near the coast, the chalk was harder, more brittle and more fractured than on the English side. This led to the adoption of different tunnelling techniques on the two sides.\n\nThe Quaternary undersea valley Fosse Dangaered, and Castle Hill landslip at the English portal, caused concerns. Identified by the 1964–65 geophysical survey, the Fosse Dangaered is an infilled valley system extending below the seabed, south of the tunnel route in mid-channel. A 1986 survey showed that a tributary crossed the path of the tunnel, and so the tunnel route was made as far north and deep as possible. The English terminal had to be located in the Castle Hill landslip, which consists of displaced and tipping blocks of lower chalk, glauconitic marl and gault debris. Thus the area was stabilised by buttressing and inserting drainage adits. The service tunnel acted as a pilot preceding the main ones, so that the geology, areas of crushed rock, and zones of high water inflow could be predicted. Exploratory probing took place in the service tunnel, in the form of extensive forward probing, vertical downward probes and sideways probing.\n\nMarine soundings and samplings by Thomé de Gamond were carried out during 1833–67, establishing the seabed depth at a maximum of and the continuity of geological strata (layers). Surveying continued over many years, with 166 marine and 70 land-deep boreholes being drilled and over 4,000-line-kilometres of marine geophysical survey completed. Surveys were undertaken in 1958–1959, 1964–1965, 1972–1974 and 1986–1988.\n\nThe surveying in 1958–59 catered for immersed tube and bridge designs as well as a bored tunnel, and thus a wide area was investigated. At this time, marine geophysics surveying for engineering projects was in its infancy, with poor positioning and resolution from seismic profiling. The 1964–65 surveys concentrated on a northerly route that left the English coast at Dover harbour; using 70 boreholes, an area of deeply weathered rock with high permeability was located just south of Dover harbour.\n\nGiven the previous survey results and access constraints, a more southerly route was investigated in the 1972–73 survey, and the route was confirmed to be feasible. Information for the tunnelling project also came from work before the 1975 cancellation. On the French side at Sangatte, a deep shaft with adits was made. On the English side at Shakespeare Cliff, the government allowed of diameter tunnel to be driven. The actual tunnel alignment, method of excavation and support were essentially the same as the 1975 attempt. In the 1986–87 survey, previous findings were reinforced, and the characteristics of the gault clay and the tunnelling medium (chalk marl that made up 85% of the route) were investigated. Geophysical techniques from the oil industry were employed.\n\nTunnelling was a major engineering challenge, with the only precedent being the undersea Seikan Tunnel in Japan, which opened in 1988. A serious risk with underwater tunnels is major water inflow due to the pressure from the sea above, under weak ground conditions. The tunnel also had the challenge of time: being privately funded, early financial return was paramount.\n\nThe objective was to construct two rail tunnels, apart, in length; a service tunnel between the two main ones; pairs of cross-passages linking the rail tunnels to the service one at spacing; piston relief ducts in diameter connecting the rail tunnels apart; two undersea crossover caverns to connect the rail tunnels, with the service tunnel always preceding the main ones by at least to ascertain the ground conditions. There was plenty of experience with excavating through chalk in the mining industry, while the undersea crossover caverns were a complex engineering problem. The French one was based on the Mount Baker Ridge freeway tunnel in Seattle; the UK cavern was dug from the service tunnel ahead of the main ones, to avoid delay.\n\nPrecast segmental linings in the main TBM drives were used, but two different solutions were used. On the French side, neoprene and grout sealed bolted linings made of cast iron or high-strength reinforced concrete were used; on the English side, the main requirement was for speed so bolting of cast-iron lining segments was only carried out in areas of poor geology. In the UK rail tunnels, eight lining segments plus a key segment were used; in the French side, five segments plus a key. On the French side, a diameter deep grout-curtained shaft at Sangatte was used for access. On the English side, a marshalling area was below the top of Shakespeare Cliff, the New Austrian Tunnelling method (NATM) was first applied in the chalk marl here. On the English side, the land tunnels were driven from Shakespeare Cliff – same place as the marine tunnels – not from Folkestone. The platform at the base of the cliff was not large enough for all of the drives and, despite environmental objections, tunnel spoil was placed behind a reinforced concrete seawall, on condition of placing the chalk in an enclosed lagoon, to avoid wide dispersal of chalk fines. Owing to limited space, the precast lining factory was on the Isle of Grain in the Thames estuary, which used Scottish granite aggregate delivered by ship from the Foster Yeoman coastal super quarry at Glensanda in Loch Linnhe on the west coast of Scotland.\n\nOn the French side, owing to the greater permeability to water, earth pressure balance TBMs with open and closed modes were used. The TBMs were of a closed nature during the initial , but then operated as open, boring through the chalk marl stratum. This minimised the impact to the ground, allowed high water pressures to be withstood and it also alleviated the need to grout ahead of the tunnel. The French effort required five TBMs: two main marine machines, one main land machine (the short land drives of allowed one TBM to complete the first drive then reverse direction and complete the other), and two service tunnel machines. On the English side, the simpler geology allowed faster open-faced TBMs. Six machines were used; all commenced digging from Shakespeare Cliff, three marine-bound and three for the land tunnels. Towards the completion of the undersea drives, the UK TBMs were driven steeply downwards and buried clear of the tunnel. These buried TBMs were then used to provide an electrical earth. The French TBMs then completed the tunnel and were dismantled. A gauge railway was used on the English side during construction.\n\nIn contrast to the English machines, which were given alphanumeric names, the French tunnelling machines were all named after women: Brigitte, Europa, Catherine, Virginie, Pascaline, Séverine.\n\nAt the end of the tunnelling, one machine was on display at the side of the M20 motorway in Folkestone until Eurotunnel sold it on eBay for £39,999 to a scrap metal merchant. Another machine (T4 \"Virginie\") still survives on the French side, adjacent to Junction 41 on the A16, in the middle of the D243E3/D243E4 roundabout. On it are the words \"hommage aux bâtisseurs du tunnel\", meaning \"tribute to the builders of the tunnel\".\n\nThe 11 TBM's were designed and manufactured through a joint venture between Robbins Company of Kent, Markham & Co. and Kawasaki Heavy Industries.\n\nThere are three communication systems: concession radio (CR) for mobile vehicles and personnel within Eurotunnel's Concession (terminals, tunnels, coastal shafts); track-to-train radio (TTR) for secure speech and data between trains and the railway control centre; Shuttle internal radio (SIR) for communication between shuttle crew and to passengers over car radios. This service was discontinued within one year of opening because of drivers' difficulty setting their radios to the correct frequency (88.8 MHz).\n\nPower is delivered to the locomotives via an overhead line (catenary) at . All tunnel services run on electricity, shared equally from English and French sources. There are two sub-stations fed at 400 kV at each terminal, but in an emergency the tunnel's lighting (about 20,000 light fittings) and plant can be powered solely from either England or France.\n\nThe traditional railway south of London uses a 750 V DC third rail to deliver electricity, but since the opening of High Speed 1 there is no longer any need for tunnel trains to use the third rail system. High Speed 1, the tunnel and the LGV Nord all have power provided via overhead catenary at 25 kV 50 Hz. The railways on \"classic\" lines in Belgium are also electrified by overhead wires, but at 3000 V DC.\n\nA cab signalling system gives information directly to train drivers on a display. There is a train protection system that stops the train if the speed exceeds that indicated on the in-cab display. TVM430, as used on LGV Nord and High Speed 1, is used in the tunnel. The TVM signalling is interconnected with the signalling on the high-speed lines either side, allowing trains to enter and exit the tunnel system without stopping. The maximum speed is .\n\nSignalling in the tunnel is coordinated from two control centres: The main control centre at the Folkestone terminal, and a backup at the Calais terminal, which is staffed at all times and can take over all operations in the event of a breakdown or emergency.\n\nConventional ballasted tunnel-track was ruled out owing to the difficulty of maintenance and lack of stability and precision. The Sonneville International Corporation's track system was chosen based on reliability and cost-effectiveness based on good performance in Swiss tunnels and worldwide. The type of track used is known as Low Vibration Track (LVT). Like ballasted track the LVT is of the free floating type, held in place by gravity and friction. Reinforced concrete blocks of 100 kg support the rails every 60 cm and are held by 12 mm thick closed cell polymer foam pads placed at the bottom of rubber boots. The latter separate the blocks' mass movements from the lean encasement concrete. Ballastless track provides extra overhead clearance necessary for the passage of larger trains. The corrugated rubber walls of the boots add a degree of isolation of horizontal wheel-rail vibrations, and are insulators of the track signal circuit in the humid tunnel environment. UIC60 (60 kg/m) rails of 900A grade rest on rail pads, which fit the RN/Sonneville bolted dual leaf-springs. The rails, LVT-blocks and their boots with pads were assembled outside the tunnel, in a fully automated process developed by the LVT inventor, Mr. Roger Sonneville. About 334,000 Sonneville blocks were made on the Sangatte site.\n\nMaintenance activities are less than projected. Initially the rails were ground on a yearly basis or after approximately 100MGT of traffic. Ride quality continues to be noticeably smooth and of low noise. Maintenance is facilitated by the existence of two tunnel junctions or crossover facilities, allowing for two-way operation in each of the six tunnel segments thereby created, and thus providing safe access for maintenance of one isolated tunnel segment at a time. The two crossovers are the largest artificial undersea caverns ever built; 150 m long, 10 m high and 18 m wide. The English crossover is from Shakespeare Cliff, and the French crossover is from Sangatte.\n\nThe ventilation system maintains the air pressure in the service tunnel higher than in the rail tunnels, so that in the event of a fire, smoke does not enter the service tunnel from the rail tunnels. Two cooling water pipes in each rail tunnel circulate chilled water to remove heat generated by the rail traffic. Pumping stations remove water in the tunnels from rain, seepage, and so on.\n\nInitially 38 Le Shuttle locomotives were commissioned, with one at each end of a shuttle train. The shuttles have two separate halves: single and double deck. Each half has two loading/unloading wagons and 12 carrier wagons. Eurotunnel's original order was for nine tourist shuttles.\n\nHeavy goods vehicle (HGV) shuttles also have two halves, with each half containing one loading wagon, one unloading wagon and 14 carrier wagons. There is a club car behind the leading locomotive. Eurotunnel originally ordered six HGV shuttle rakes.\n\nForty-six Class 92 locomotives for hauling freight trains and overnight passenger trains (the Nightstar project, which was abandoned) were commissioned, running on both overhead AC and third-rail DC power. However, RFF does not let these run on French railways, so there are plans to certify Alstom Prima II locomotives for use in the tunnel.\n\nThirty-one Eurostar trains, based on the French TGV, built to UK loading gauge with many modifications for safety within the tunnel, were commissioned, with ownership split between British Rail, French national railways (SNCF) and Belgian national railways (SNCB). British Rail ordered seven more for services north of London. Around 2010, Eurostar ordered ten trains from Siemens based on its Velaro product.\n\nGermany (DB) has since around 2005 tried to get permission to run train services to London. At the end of 2009, extensive fire-proofing requirements were dropped and DB received permission to run German Intercity-Express (ICE) test trains through the tunnel. In June 2013 DB was granted access to the tunnel. In June 2014 the plans were shelved, because there are special safety rules that requires custom made trains (DB calls them Class 407).\n\nDiesel locomotives for rescue and shunting work are Eurotunnel Class 0001 and Eurotunnel Class 0031.\n\nThe following chart presents the estimated number of passengers and tonnes of freight, respectively, annually transported through the Channel Tunnel since 1994, in millions:\n\nTransport services offered by the tunnel are as follows:\n\n\nBoth the freight and passenger traffic forecasts that led to the construction of the tunnel were overestimated; in particular, Eurotunnel's commissioned forecasts were over-predictions. Although the captured share of Channel crossings was forecast correctly, high competition (especially from budget airlines which expanded rapidly in the 1990s and 2000s) and reduced tariffs led to low revenue. Overall cross-Channel traffic was overestimated.\n\nWith the EU's liberalisation of international rail services, the tunnel and High Speed 1 have been open to competition since 2010. There have been a number of operators interested in running trains through the tunnel and along High Speed 1 to London. In June 2013, after several years, DB obtained a licence to operate Frankfurt – London trains, not expected to run before 2016 because of delivery delays of the custom-made trains.\n\nCross-tunnel passenger traffic volumes peaked at 18.4 million in 1998, dropped to 14.9 million in 2003, then rose to 21.0 million in 2014.\n\nAt the time of the decision about building the tunnel, 15.9 million passengers were predicted for Eurostar trains in the opening year. In 1995, the first full year, actual numbers were a little over 2.9 million, growing to 7.1 million in 2000, then dropping to 6.3 million in 2003. Eurostar was initially limited by the lack of a high-speed connection on the British side. After the completion of High Speed 1 in two stages in 2003 and 2007, traffic increased. In 2008, Eurostar carried 9,113,371 passengers, a 10% increase over the previous year, despite traffic limitations due to the 2008 Channel Tunnel fire. Eurostar passenger numbers continued to increase, reaching 10,397,894 in 2014.\n\nFreight volumes have been erratic, with a major decrease during 1997 due to a closure caused by a fire in a freight shuttle. Freight crossings increased over the period, indicating the substitutability of the tunnel by sea crossings. The tunnel has achieved a market share close to or above Eurotunnel's 1980s predictions but Eurotunnel's 1990 and 1994 predictions were overestimates.\n\nFor through freight trains, the first year prediction was 7.2 million gross tonnes; the actual 1995 figure was 1.3M gross tonnes. Through freight volumes peaked in 1998 at 3.1M tonnes. This fell back to 1.21M tonnes in 2007, increasing slightly to 1.24M tonnes in 2008. Together with that carried on freight shuttles, freight growth has occurred since opening, with 6.4M tonnes carried in 1995, 18.4M tonnes recorded in 2003 and 19.6M tonnes in 2007. Numbers fell back in the wake of the 2008 fire.\n\nEurotunnel's freight subsidiary is Europorte 2. In September 2006 EWS, the UK's largest rail freight operator, announced that owing to cessation of UK-French government subsidies of £52 million per annum to cover the tunnel \"Minimum User Charge\" (a subsidy of around £13,000 per train, at a traffic level of 4,000 trains per annum), freight trains would stop running after 30 November.\n\nShares in Eurotunnel were issued at £3.50 per share on 9 December 1987. By mid-1989 the price had risen to £11.00. Delays and cost overruns led to the price dropping; during demonstration runs in October 1994 it reached an all-time low. Eurotunnel suspended payment on its debt in September 1995 to avoid bankruptcy. In December 1997 the British and French governments extended Eurotunnel's operating concession by 34 years, to 2086. Financial restructuring of Eurotunnel occurred in mid-1998, reducing debt and financial charges. Despite the restructuring, \"The Economist\" reported in 1998 that to break even Eurotunnel would have to increase fares, traffic and market share for sustainability. A cost benefit analysis of the tunnel indicated that there were few impacts on the wider economy and few developments associated with the project, and that the British economy would have been better off if it had not been constructed.\n\nUnder the terms of the Concession, Eurotunnel was obliged to investigate a cross-Channel road tunnel. In December 1999 road and rail tunnel proposals were presented to the British and French governments, but it was stressed that there was not enough demand for a second tunnel. A three-way treaty between the United Kingdom, France and Belgium governs border controls, with the establishment of \"control zones\" wherein the officers of the other nation may exercise limited customs and law enforcement powers. For most purposes these are at either end of the tunnel, with the French border controls on the UK side of the tunnel and vice versa. For some city-to-city trains, the train is a control zone. A binational emergency plan coordinates UK and French emergency activities.\n\nIn 1999 Eurostar posted its first net profit, having made a loss of £925m in 1995. In 2005 Eurotunnel was described as being in a serious situation. In 2013, operating profits rose 4 per cent from 2012, to £54 million.\n\nThere is a need for full passport controls, since this is the border between the Schengen Area and the Common Travel Area. There are juxtaposed controls, meaning that passports are checked before boarding first by officials belonging to departing country and then officials of the destination country. These are only placed at the main Eurostar stations: French officials operate at London St Pancras, Ebbsfleet International and Ashford International, while British officials operate at Calais-Fréthun, Lille-Europe, Brussels-South and Paris-Gare du Nord. There are security checks before boarding as well. For the shuttle road-vehicle trains, there are juxtaposed passport controls before boarding the trains.\n\nFor Eurostar trains travelling from places south of Paris, there is no passport and security check before departure, and those trains must stop in Lille at least 30 minutes to allow all passengers to be checked. No checks are done on board. There have been plans for services from Amsterdam, Frankfurt and Cologne to London, but a major reason to cancel them was the need for a stop in Lille.\n\nThe reason for juxtaposed controls is a wish to prevent illegal immigration before reaching British soil, and because a check of all passengers on a train can take 30 minutes, which creates long queues if done at arrival.\n\nThe terminals' sites are at Cheriton (near Folkestone in the United Kingdom) and Coquelles (near Calais in France). The terminals are designed to transfer vehicles from the motorway onto trains at a rate of 700 cars and 113 heavy vehicles per hour. The UK site uses the M20 motorway for access. The terminals are organised with the frontier controls juxtaposed with the entry to the system to allow travellers to go onto the motorway at the destination country immediately after leaving the shuttle. The area of the UK site was severely constrained and the design was challenging.\n\nThe French layout was achieved more easily. To achieve design output, the shuttles accept cars on double-deck wagons; for flexibility, ramps were placed inside the shuttles to provide access to the top decks. At Folkestone there are of main-line track, 45 turnouts and eight platforms. At Calais there are of track and 44 turnouts. At the terminals the shuttle trains traverse a figure eight to reduce uneven wear on the wheels. There is a freight marshalling yard west of Cheriton at Dollands Moor Freight Yard.\n\nA 1996 report from the European Commission predicted that Kent and Nord-Pas de Calais had to face increased traffic volumes due to general growth of cross-Channel traffic and traffic attracted by the tunnel. In Kent, a high-speed rail line to London would transfer traffic from road to rail. Kent's regional development would benefit from the tunnel, but being so close to London restricts the benefits. Gains are in the traditional industries and are largely dependent on the development of Ashford International passenger station, without which Kent would be totally dependent on London's expansion. Nord-Pas-de-Calais enjoys a strong internal symbolic effect of the Tunnel which results in significant gains in manufacturing.\n\nThe removal of a bottleneck by means like the tunnel does not necessarily induce economic gains in all adjacent regions. The image of a region being connected to the European high-speed transport and active political response are more important for regional economic development. Some small-medium enterprises located in the immediate vicinity of the terminal have used the opportunity to re-brand the profile of their business with positive effect, such as \"The New Inn\" at Etchinghill which was able to commercially exploit its unique selling point as being 'the closest pub to the Channel Tunnel'. Tunnel-induced regional development is small compared to general economic growth. The South East of England is likely to benefit developmentally and socially from faster and cheaper transport to continental Europe, but the benefits are unlikely to be equally distributed throughout the region. The overall environmental impact is almost certainly negative.\n\nSince the opening of the tunnel, small positive impacts on the wider economy have been felt, but it is difficult to identify major economic successes directly attributed to the tunnel.<ref name=\"Flyvbjerg p. 68/69\">Flyvbjerg et al. p. 68–69</ref> The Eurotunnel does operate profitably, offering an alternative transportation mode unaffected by poor weather. High costs of construction did delay profitability, however, and companies involved in the tunnel's construction and operation early in operation relied on government aid to deal with debts amounted.\n\nIllegal immigrants and would-be asylum seekers have used the tunnel to attempt to enter Britain. By 1997, the problem had attracted international press attention, and by 1999, the French Red Cross opened the first migrant centre at Sangatte, using a warehouse once used for tunnel construction; by 2002, it housed up to 1,500 people at a time, most of them trying to get to the UK. In 2001, most came from Afghanistan, Iraq, and Iran, but African countries were also represented.\n\nEurotunnel, the company that operates the crossing, said that more than 37,000 migrants were intercepted between January and July 2015. Approximately 3,000 migrants, mainly from Ethiopia, Eritrea, Sudan and Afghanistan, were living in the temporary camps erected in Calais at the time of an official count in July 2015. An estimated 3,000 to 5,000 migrants were waiting in Calais for a chance to get to England.\n\nBritain and France operate a system of Juxtaposed controls on immigration and customs, where investigations happen before travel. France is part of the Schengen Agreement, which has largely abolished border checks between member nations, but the United Kingdom is not.\n\nMost illegal immigrants and would-be asylum seekers who got into Britain found some way to ride a freight train. Trucks are loaded onto freight trains. In a few instances, groups of migrants were able to stow away in the cargo area of a tanker truck carrying liquid chocolate and managed to survive, though they did not enter the UK in one attempt. Although the facilities were fenced, airtight security was deemed impossible; migrants would even jump from bridges onto moving trains. In several incidents people were injured during the crossing; others tampered with railway equipment, causing delays and requiring repairs. Eurotunnel said it was losing £5m per month because of the problem.\n\nIn 2001 and 2002, several riots broke out at Sangatte, and groups of migrants (up to 550 in a December 2001 incident) stormed the fences and attempted to enter \"en masse\".\n\nOther migrants use the Eurostar passenger train. They arrive as legitimate Eurostar passengers, but without proper entry papers.\n\nLocal authorities in both France and the UK called for the closure of the Sangatte migrant camp, and Eurotunnel twice sought an injunction against the centre. The United Kingdom blamed France for allowing Sangatte to open, and France blamed both the UK for its lax asylum rules, and the EU for not having a uniform immigration policy. The \"cause célèbre\" nature of the problem even included journalists detained as they followed migrants onto railway property.\n\nIn 2002, after the European Commission told France that it was in breach of European Union rules on the free transfer of goods because of the delays and closures as a result of its poor security, a double fence was built at a cost of £5 million, reducing the numbers of migrants detected each week reaching Britain on goods trains from 250 to almost none. Other measures included CCTV cameras and increased police patrols. At the end of 2002, the Sangatte centre was closed after the UK agreed to absorb some migrants.\n\nOn 23 and 30 June 2015, striking workers associated with MyFerryLink damaged the sections of track by burning car tires, leading to all trains being cancelled and a backlog of vehicles. Hundreds seeking to reach Britain made use of the situation to attempt to stow away inside and underneath transport trucks destined for the United Kingdom. Extra security measures included a £2 million upgrade of detection technology, £1 million extra for dog searches, and £12 million (over three years) towards a joint fund with France for security surrounding the Port of Calais.\n\nMigrants take great risks to evade security precautions. In 2002, a dozen migrants died in crossing attempts. In the two months from June to July 2015, ten migrants died near the French tunnel terminal, during a period when 1,500 attempts to evade security precautions were being made each day.\n\nOn 6 July 2015, a migrant died while attempting to climb onto a freight train while trying to reach Britain from the French side of the Channel. The previous month an Eritrean man was killed under similar circumstances.\n\nDuring the night of 28 July 2015, one person, aged 25–30, was found dead after a night in which 1,500–2,000 migrants had attempted to enter the Eurotunnel terminal.\n\nOn 4 August 2015, a Sudanese migrant walked nearly the entire length of one of the tunnels. He was arrested close to the British side, after having walked about through the tunnel.\n\nOn 20 June 2017, a van driver was killed when migrants stopped vehicles on the A16 autoroute with a tree trunk, in order to stow away in the cargo area. The van, registered in Poland, hit a lorry and burst into flames, killing the van driver. Nine migrants from Eritrea have been arrested in connection with this incident.\n\nThere have been three fires in the tunnel, all on the heavy goods vehicle (HGV) shuttles, that were significant enough to close the tunnel, as well as other more minor incidents.\n\nOn 9 December 1994, during an \"invitation only\" testing phase, a fire broke out in a Ford Escort car whilst its owner was loading it onto the upper deck of a tourist shuttle. The fire started at about 10:00, with the shuttle train stationary in the Folkestone terminal and was put out about 40 minutes later with no passenger injuries.\n\nOn 18 November 1996, a fire broke out on an HGV shuttle wagon in the tunnel, but nobody was seriously hurt. The exact cause is unknown, although it was neither a Eurotunnel equipment nor rolling stock problem; it may have been due to arson of a heavy goods vehicle. It is estimated that the heart of the fire reached , with the tunnel severely damaged over , with some affected to some extent. Full operation recommenced six months after the fire.\n\nOn 21 August 2006, the tunnel was closed for several hours when a truck on an HGV shuttle train caught fire.\n\nOn 11 September 2008, a fire occurred in the Channel Tunnel at 13:57 GMT. The incident started on an HGV shuttle train travelling towards France. The event occurred from the French entrance to the tunnel. No one was killed but several people were taken to hospitals suffering from smoke inhalation, and minor cuts and bruises. The tunnel was closed to all traffic, with the undamaged South Tunnel reopening for limited services two days later. Full service resumed on 9 February 2009 after repairs costing €60 million.\n\nOn 29 November 2012, the tunnel was closed for several hours after a truck on an HGV shuttle caught fire.\n\nOn 17 January 2015, both tunnels were closed following a lorry fire which filled the midsection of Running Tunnel North with smoke. Eurostar cancelled all services. The shuttle train had been heading from Folkestone to Coquelles and stopped adjacent to cross-passage CP 4418 just before 12:30 UTC. Thirty-eight passengers and four members of Eurotunnel staff were evacuated into the service tunnel, and then transported to France using special STTS road vehicles in the Service Tunnel. The passengers and crew were taken to the Eurotunnel Fire/Emergency Management Centre close to the French portal.\n\nOn the night of 19/20 February 1996, about 1,000 passengers became trapped in the Channel Tunnel when Eurostar trains from London broke down owing to failures of electronic circuits caused by snow and ice being deposited and then melting on the circuit boards.\n\nOn 3 August 2007, an electrical failure lasting six hours caused passengers to be trapped in the tunnel on a shuttle.\n\nOn the evening of 18 December 2009, during the December 2009 European snowfall, five London-bound Eurostar trains failed inside the tunnel, trapping 2,000 passengers for approximately 16 hours, during the coldest temperatures in eight years. A Eurotunnel spokesperson explained that snow had evaded the train's winterisation shields, and the transition from cold air outside to the tunnel's warm atmosphere had melted the snow, resulting in electrical failures. One train was turned back before reaching the tunnel; two trains were hauled out of the tunnel by Eurotunnel Class 0001 diesel locomotives. The blocking of the tunnel led to the implementation of Operation Stack, the transformation of the M20 motorway into a linear car park.\n\nThe occasion was the first time that a Eurostar train was evacuated inside the tunnel; the failing of four at once was described as \"unprecedented\". The Channel Tunnel reopened the following morning. Nirj Deva, Member of the European Parliament for South East England, had called for Eurostar chief executive Richard Brown to resign over the incidents. An independent report by Christopher Garnett (former CEO of Great North Eastern Railway) and Claude Gressier (a French transport expert) on the 18/19 December 2009 incidents was issued in February 2010, making 21 recommendations.\n\nOn 7 January 2010, a Brussels–London Eurostar broke down in the tunnel. The train had 236 passengers on board and was towed to Ashford; other trains that had not yet reached the tunnel were turned back.\n\nThe Channel Tunnel Safety Authority is responsible for some aspects of safety regulation in the tunnel; it reports to the IGC.\nThe service tunnel is used for access to technical equipment in cross-passages and equipment rooms, to provide fresh-air ventilation and for emergency evacuation. The Service Tunnel Transport System (STTS) allows fast access to all areas of the tunnel. The service vehicles are rubber-tyred with a buried wire guidance system. The 24 STTS vehicles are used mainly for maintenance but also for firefighting and in emergencies. \"Pods\" with different purposes, up to a payload of , are inserted into the side of the vehicles. The vehicles cannot turn around within the tunnel, and are driven from either end. The maximum speed is when the steering is locked. A fleet of 15 Light Service Tunnel Vehicles (LADOGS) was introduced to supplement the STTSs. The LADOGS have a short wheelbase with a turning circle, allowing two-point turns within the service tunnel. Steering cannot be locked like the STTS vehicles, and maximum speed is . Pods up to can be loaded onto the rear of the vehicles. Drivers in the tunnel sit on the right, and the vehicles drive on the left. Owing to the risk of French personnel driving on their native right side of the road, sensors in the vehicles alert the driver if the vehicle strays to the right side.\n\nThe three tunnels contain of air that needs to be conditioned for comfort and safety. Air is supplied from ventilation buildings at Shakespeare Cliff and Sangatte, with each building capable of providing 100% standby capacity. Supplementary ventilation also exists on either side of the tunnel. In the event of a fire, ventilation is used to keep smoke out of the service tunnel and move smoke in one direction in the main tunnel to give passengers clean air. The tunnel was the first main-line railway tunnel to have special cooling equipment. Heat is generated from traction equipment and drag. The design limit was set at , using a mechanical cooling system with refrigeration plants on both sides that run chilled water circulating in pipes within the tunnel.\n\nTrains travelling at high speed create piston-effect pressure changes that can affect passenger comfort, ventilation systems, tunnel doors, fans and the structure of the trains, and which drag on the trains. Piston relief ducts of diameter were chosen to solve the problem, with 4 ducts per kilometre to give close to optimum results. Unfortunately this design led to unacceptable lateral forces on the trains so a reduction in train speed was required and restrictors were installed in the ducts.\n\nThe safety issue of a possible fire on a passenger-vehicle shuttle garnered much attention, with Eurotunnel noting that fire was the risk attracting the most attention in a 1994 safety case for three reasons: the opposition of ferry companies to passengers being allowed to remain with their cars; Home Office statistics indicating that car fires had doubled in ten years; and the long length of the tunnel. Eurotunnel commissioned the UK Fire Research Station – now part of the Building Research Establishment – to give reports of vehicle fires, and liaised with Kent Fire Brigade to gather vehicle fire statistics over one year. Fire tests took place at the French Mines Research Establishment with a mock wagon used to investigate how cars burned. The wagon door systems are designed to withstand fire inside the wagon for 30 minutes, longer than the transit time of 27 minutes. Wagon air conditioning units help to purge dangerous fumes from inside the wagon before travel. Each wagon has a fire detection and extinguishing system, with sensing of ions or ultraviolet radiation, smoke and gases that can trigger halon gas to quench a fire. Since the HGV wagons are not covered, fire sensors are located on the loading wagon and in the tunnel. A water main in the service tunnel provides water to the main tunnels at intervals. The ventilation system can control smoke movement. Special arrival sidings accept a train that is on fire, as the train is not allowed to stop whilst on fire in the tunnel, unless continuing its journey would lead to a worse outcome. Eurotunnel has banned a wide range of hazardous goods from travelling in the tunnel. Two STTS (Service Tunnel Transportation System) vehicles with firefighting pods are on duty at all times, with a maximum delay of 10 minutes before they reach a burning train.\n\nIn 1999, the \"Kosovo Train for Life\" passed through the tunnel en route to Pristina, in Kosovo.\n\nIn 2009, former F1 racing champion John Surtees drove a Ginetta G50 EV electric sports car prototype from England to France, using the service tunnel, as part of a charity event. He was required to keep to the speed limit. To celebrate the 2014 Tour de France's transfer from its opening stages in Britain to France in July of that year, Chris Froome of Team Sky rode a bicycle through the service tunnel, becoming the first solo rider to do so. The Crossing took under an hour, reaching speeds of –faster than most cross-channel ferries.\n\nSince 2012, French operators Bouygues Telecom, Orange and SFR have covered Running Tunnel South, the tunnel bore normally used for travel from France to Britain.\n\nIn January 2014, UK operators EE and Vodafone signed ten-year contracts with Eurotunnel for Running Tunnel North. The agreements will enable both operators' subscribers to use 2G and 3G services. Both EE and Vodafone planned to offer LTE services on the route; EE said it expected to cover the route with LTE connectivity by summer 2014. EE and Vodafone will offer Channel Tunnel network coverage for travellers from the UK to France. Eurotunnel said it also held talks with Three UK but has yet to reach an agreement with the operator.\n\nIn May 2014, Eurotunnel announced that they had installed equipment from Alcatel-Lucent to cover Running Tunnel North and simultaneously to provide mobile service (GSM 900/1800 MHz and UMTS 2100 MHz) by EE, O and Vodafone. The service of EE and Vodafone commenced on the same date as the announcement. O service was expected to be available soon afterwards.\n\nIn November 2014, EE announced that it had previously switched on LTE earlier in September 2014. O turned on 2G, 3G and 4G services in November 2014, whilst Vodafone's 4G was due to go live later.\n\nAnother usage of the Channel Tunnel is the 1,000 MW high-voltage direct current ElecLink connecting the electrical grids of the two countries, scheduled for 2019 at a cost of €580m.\nThe foundation stone of the Folkestone Converter Station was laid in February 2017, by Jesse Norman, Minister for Industry and Energy.\n\n\n"}
{"id": "5703", "url": "https://en.wikipedia.org/wiki?curid=5703", "title": "Cyberpunk", "text": "Cyberpunk\n\nCyberpunk is a subgenre of science fiction in a futuristic setting that tends to focus on a \"combination of lowlife and high tech\" featuring advanced technological and scientific achievements, such as artificial intelligence and cybernetics, juxtaposed with a degree of breakdown or radical change in the social order.\n\nMuch of cyberpunk is rooted in the New Wave science fiction movement of the 1960s and 1970s, when writers like Philip K. Dick, Roger Zelazny, J. G. Ballard, Philip José Farmer and Harlan Ellison examined the impact of drug culture, technology and the sexual revolution while avoiding the utopian tendencies of earlier science fiction. Released in 1984, William Gibson's influential debut novel \"Neuromancer\" would help solidify cyberpunk as a genre, drawing influence from punk subculture and early hacker culture. Other influential cyberpunk writers included Bruce Sterling and Rudy Rucker. The Japanese cyberpunk subgenre began in 1982 with the debut of Katsuhiro Otomo's manga series \"Akira\", with its 1988 anime film adaptation later popularizing the subgenre.\n\nEarly films in the genre include Ridley Scott's 1982 film \"Blade Runner\", one of several of Philip K. Dick's works that have been adapted into films. The films \"Johnny Mnemonic\" and \"New Rose Hotel\", both based upon short stories by William Gibson, flopped commercially and critically. More recent additions to this genre of filmmaking include the 2017 release of \"Blade Runner 2049\", sequel to the original 1982 film, the 2018 body horror film \"Upgrade\", and the 2018 Netflix TV series \"Altered Carbon\".\n\nLawrence Person has attempted to define the content and ethos of the cyberpunk literary movement stating: \nCyberpunk plots often center on conflict among artificial intelligences, hackers, and megacorporations, and tend to be set in a near-future Earth, rather than in the far-future settings or galactic vistas found in novels such as Isaac Asimov's \"Foundation\" or Frank Herbert's \"Dune\". The settings are usually post-industrial dystopias but tend to feature extraordinary cultural ferment and the use of technology in ways never anticipated by its original inventors (\"the street finds its own uses for things\"). Much of the genre's atmosphere echoes film noir, and written works in the genre often use techniques from detective fiction. There are sources who view that cyberpunk has shifted from a literary movement to a mode of science fiction due to the limited number of writers and its transition to a more generalized cultural formation.\n\nThe origins of cyberpunk are rooted in the New Wave science fiction movement of the 1960s and 70s, where \"New Worlds\", under the editorship of Michael Moorcock, began inviting and encouraging stories that examined new writing styles, techniques, and archetypes. Reacting to conventional storytelling, New Wave authors attempted to present a world where society coped with a constant upheaval of new technology and culture, generally with dystopian outcomes. Writers like Roger Zelazny, J.G. Ballard, Philip Jose Farmer, and Harlan Ellison often examined the impact of drug culture, technology, and the sexual revolution with an avant-garde style influenced by the Beat Generation (especially William S. Burroughs' own SF), Dadaism, and their own ideas. Ballard attacked the idea that stories should follow the \"archetypes\" popular since the time of Ancient Greece, and the assumption that these would somehow be the same ones that would call to modern readers, as Joseph Campbell argued in \"The Hero with a Thousand Faces\". Instead, Ballard wanted to write a new myth for the modern reader, a style with \"more psycho-literary ideas, more meta-biological and meta-chemical concepts, private time systems, synthetic psychologies and space-times, more of the sombre half-worlds one glimpses in the paintings of schizophrenics.\"\n\nThis had a profound influence on a new generation of writers, some of whom would come to call their movement \"Cyberpunk\". One, Bruce Sterling, later said:\nBallard, Zelazny, and the rest of New Wave was seen by the subsequent generation as delivering more \"realism\" to science fiction, and they attempted to build on this.\n\nSimilarly influential, and generally cited as proto-cyberpunk, is the Philip K. Dick novel \"Do Androids Dream of Electric Sheep\", first published in 1968. Presenting precisely the general feeling of dystopian post-economic-apocalyptic future as Gibson and Sterling later deliver, it examines ethical and moral problems with cybernetic, artificial intelligence in a way more \"realist\" than the Isaac Asimov \"Robot\" series that laid its philosophical foundation. This novel was made into the seminal movie \"Blade Runner\", released in 1982. This was one year after another story, \"Johnny Mnemonic\" helped move proto-cyberpunk concepts into the mainstream. This story, which also became a film years later, involves another dystopian future, where human couriers deliver computer data, stored cybernetically in their own minds.\n\nIn 1983 a short story written by Bruce Bethke, called \"Cyberpunk\", was published in \"Amazing Stories\". The term was picked up by Gardner Dozois, editor of \"Isaac Asimov's Science Fiction Magazine\" and popularized in his editorials. Bethke says he made two lists of words, one for technology, one for troublemakers, and experimented with combining them variously into compound words, consciously attempting to coin a term that encompassed both punk attitudes and high technology.\n\nHe described the idea thus:\nAfterward, Dozois began using this term in his own writing, most notably in a \"Washington Post\" article where he said \"About the closest thing here to a self-willed esthetic “school” would be the purveyors of bizarre hard-edged, high-tech stuff, who have on occasion been referred to as “cyberpunks” — Sterling, Gibson, Shiner, Cadigan, Bear.\"\n\nAbout that time, William Gibson's novel \"Neuromancer\" was published, delivering a glimpse of a future encompassed by what became an archetype of cyberpunk \"virtual reality\", with the human mind being fed light-based worldscapes through a computer interface. Some, perhaps ironically including Bethke himself, argued at the time that the writers whose style Gibson's books epitomized should be called \"Neuromantics\", a pun on the name of the novel plus \"New Romantics\", a term used for a New Wave pop music movement that had just occurred in Britain, but this term did not catch on. Bethke later paraphrased Michael Swanwick's argument for the term: \"the movement writers should properly be termed neuromantics, since so much of what they were doing was clearly Imitation \"Neuromancer\"\".\n\nSterling was another writer who played a central role, often consciously, in the cyberpunk genre, variously seen as keeping it on track, or distorting its natural path into a stagnant formula. In 1986 he edited a volume of cyberpunk stories called \"\", an attempt to establish what cyberpunk was, from Sterling's perspective.\n\nIn the subsequent decade, the motifs of Gibson's \"Neuromancer\" became formulaic, climaxing in the satirical extremes of Neal Stephenson's \"Snow Crash\" in 1992.\n\nBookending the Cyberpunk era, Bethke himself published a novel in 1995 called \"Headcrash\", like \"Snow Crash\" a satirical attack on the genre's excesses. Fittingly, it won an honor named after cyberpunk's spiritual founder, the Philip K. Dick Award.\n\nIt satirized the genre in this way:\nThe impact of cyberpunk, though, has been long-lasting. Elements of both the setting and storytelling have become normal in science fiction in general, and a slew of sub-genres now have -punk tacked onto their names, most obviously Steampunk, but also a host of other Cyberpunk derivatives.\n\nPrimary figures in the cyberpunk movement include William Gibson, Neal Stephenson, Bruce Sterling, Bruce Bethke, Pat Cadigan, Rudy Rucker, and John Shirley. Philip K. Dick (author of \"Do Androids Dream of Electric Sheep?\", from which the film \"Blade Runner\" was adapted) is also seen by some as prefiguring the movement.\n\n\"Blade Runner\" can be seen as a quintessential example of the cyberpunk style and theme. Video games, board games, and tabletop role-playing games, such as \"Cyberpunk 2020\" and \"Shadowrun\", often feature storylines that are heavily influenced by cyberpunk writing and movies. Beginning in the early 1990s, some trends in fashion and music were also labeled as cyberpunk. Cyberpunk is also featured prominently in anime and manga: \"Akira\", \"Ghost in the Shell\" and \"Cowboy Bebop\" being among the most notable.\n\nCyberpunk writers tend to use elements from hardboiled detective fiction, film noir, and postmodernist prose to describe an often nihilistic underground side of an electronic society. The genre's vision of a troubled future is often called the antithesis of the generally utopian visions of the future popular in the 1940s and 1950s. Gibson defined cyberpunk's antipathy towards utopian SF in his 1981 short story \"The Gernsback Continuum,\" which pokes fun at and, to a certain extent, condemns utopian science fiction.\n\nIn some cyberpunk writing, much of the action takes place online, in cyberspace, blurring the line between actual and virtual reality. A typical trope in such work is a direct connection between the human brain and computer systems. Cyberpunk settings are dystopias with corruption, computers and internet connectivity. Giant, multinational corporations have for the most part replaced governments as centers of political, economic, and even military power.\n\nThe economic and technological state of Japan is a regular theme in the Cyberpunk literature of the '80s. Of Japan's influence on the genre, William Gibson said, \"Modern Japan simply was cyberpunk.\" Cyberpunk is often set in urbanized, artificial landscapes, and \"city lights, receding\" was used by Gibson as one of the genre's first metaphors for cyberspace and virtual reality. The cityscapes of Hong Kong and Shanghai have had major influences in the urban backgrounds, ambiance and settings in many cyberpunk works such as \"Blade Runner\" and \"Shadowrun\". Ridley Scott envisioned the landscape of cyberpunk Los Angeles in \"Blade Runner\" to be \"Hong Kong on a very bad day\". The streetscapes of \"Ghost in the Shell\" were based on Hong Kong. Its director Mamoru Oshii felt that Hong Kong's strange and chaotic streets where \"old and new exist in confusing relationships\", fit the theme of the film well. Hong Kong's Kowloon Walled City is particularly notable for its disorganized hyper-urbanization and breakdown in traditional urban planning to be an inspiration to cyberpunk landscapes.\n\nOne of the cyberpunk genre's prototype characters is Case, from Gibson's \"Neuromancer\". Case is a \"console cowboy,\" a brilliant hacker who has betrayed his organized criminal partners. Robbed of his talent through a crippling injury inflicted by the vengeful partners, Case unexpectedly receives a once-in-a-lifetime opportunity to be healed by expert medical care but only if he participates in another criminal enterprise with a new crew.\n\nLike Case, many cyberpunk protagonists are manipulated, placed in situations where they have little or no choice, and although they might see things through, they do not necessarily come out any further ahead than they previously were. These anti-heroes—\"criminals, outcasts, visionaries, dissenters and misfits\"—call to mind the private eye of detective fiction. This emphasis on the misfits and the malcontents is the \"punk\" component of cyberpunk.\n\nCyberpunk can be intended to disquiet readers and call them to action. It often expresses a sense of rebellion, suggesting that one could describe it as a type of culture revolution in science fiction. In the words of author and critic David Brin:\n\n...a closer look [at cyberpunk authors] reveals that they nearly always portray future societies in which governments have become wimpy and pathetic ...Popular science fiction tales by Gibson, Williams, Cadigan and others \"do\" depict Orwellian accumulations of power in the next century, but nearly always clutched in the secretive hands of a wealthy or corporate elite.\n\nCyberpunk stories have also been seen as fictional forecasts of the evolution of the Internet. The earliest descriptions of a global communications network came long before the World Wide Web entered popular awareness, though not before traditional science-fiction writers such as Arthur C. Clarke and some social commentators such as James Burke began predicting that such networks would eventually form.\n\nSome observers cite that cyberpunk tends to marginalize sectors of society such as women and Africans. For instance, it is claimed that cyberpunk depicts fantasies that ultimately empower masculinity using fragmentary and decentered aesthetic that culminate in a masculine genre populated by male outlaws. Critics also note the absence of any reference to Africa or an African-American character in the quintessential cyberpunk film \"Blade Runner\" while other films reinforce stereotypes.\n\nMinnesota writer Bruce Bethke coined the term in 1980 for his short story \"Cyberpunk,\" which was published in the November 1983 issue of \"Amazing Science Fiction Stories\". The term was quickly appropriated as a label to be applied to the works of William Gibson, Bruce Sterling, Pat Cadigan and others. Of these, Sterling became the movement's chief ideologue, thanks to his fanzine \"Cheap Truth.\" John Shirley wrote articles on Sterling and Rucker's significance. John Brunner's 1975 novel \"The Shockwave Rider\" is considered by many to be the first cyberpunk novel with many of the tropes commonly associated with the genre, some five years before the term was popularized by Dozois.\n\nWilliam Gibson with his novel \"Neuromancer\" (1984) is arguably the most famous writer connected with the term cyberpunk. He emphasized style, a fascination with surfaces, and atmosphere over traditional science-fiction tropes. Regarded as ground-breaking and sometimes as \"the archetypal cyberpunk work,\" \"Neuromancer\" was awarded the Hugo, Nebula, and Philip K. Dick Awards. \"Count Zero\" (1986) and \"Mona Lisa Overdrive\" (1988) followed after Gibson's popular debut novel. According to the Jargon File, \"Gibson's near-total ignorance of computers and the present-day hacker culture enabled him to speculate about the role of computers and hackers in the future in ways hackers have since found both irritatingly naïve and tremendously stimulating.\"\n\nEarly on, cyberpunk was hailed as a radical departure from science-fiction standards and a new manifestation of vitality. Shortly thereafter, however, some critics arose to challenge its status as a revolutionary movement. These critics said that the SF New Wave of the 1960s was much more innovative as far as narrative techniques and styles were concerned. Furthermore, while \"Neuromancer\"<nowiki>'</nowiki>s narrator may have had an unusual \"voice\" for science fiction, much older examples can be found: Gibson's narrative voice, for example, resembles that of an updated Raymond Chandler, as in his novel \"The Big Sleep\" (1939). Others noted that almost all traits claimed to be uniquely cyberpunk could in fact be found in older writers' works—often citing J. G. Ballard, Philip K. Dick, Harlan Ellison, Stanisław Lem, Samuel R. Delany, and even William S. Burroughs. For example, Philip K. Dick's works contain recurring themes of social decay, artificial intelligence, paranoia, and blurred lines between objective and subjective realities. The influential cyberpunk movie \"Blade Runner\" (1982) is based on his book, \"Do Androids Dream of Electric Sheep?\". Humans linked to machines are found in Pohl and Kornbluth's \"Wolfbane\" (1959) and Roger Zelazny's \"Creatures of Light and Darkness\" (1968).\n\nIn 1994, scholar Brian Stonehill suggested that Thomas Pynchon's 1973 novel \"Gravity's Rainbow\" \"not only curses but precurses what we now glibly dub cyberspace.\" Other important predecessors include Alfred Bester's two most celebrated novels, \"The Demolished Man\" and \"The Stars My Destination\", as well as Vernor Vinge's novella \"True Names\".\n\nScience-fiction writer David Brin describes cyberpunk as \"the finest free promotion campaign ever waged on behalf of science fiction.\" It may not have attracted the \"real punks,\" but it did ensnare many new readers, and it provided the sort of movement that postmodern literary critics found alluring. Cyberpunk made science fiction more attractive to academics, argues Brin; in addition, it made science fiction more profitable to Hollywood and to the visual arts generally. Although the \"self-important rhetoric and whines of persecution\" on the part of cyberpunk fans were irritating at worst and humorous at best, Brin declares that the \"rebels did shake things up. We owe them a debt.\"\n\nFredric Jameson considers cyberpunk the \"supreme literary expression if not of postmodernism, then of late capitalism itself\".\n\nCyberpunk further inspired many professional writers who were not among the \"original\" cyberpunks to incorporate cyberpunk ideas into their own works, such as George Alec Effinger's \"When Gravity Fails\". \"Wired\" magazine, created by Louis Rossetto and Jane Metcalfe, mixes new technology, art, literature, and current topics in order to interest today's cyberpunk fans, which Paula Yoo claims \"proves that hardcore hackers, multimedia junkies, cyberpunks and cellular freaks are poised to take over the world.\"\n\nThe film \"Blade Runner\" (1982)—adapted from Philip K. Dick's \"Do Androids Dream of Electric Sheep?\"—is set in 2019 in a dystopian future in which manufactured beings called replicants are slaves used on space colonies and are legal prey on Earth to various bounty hunters who \"retire\" (kill) them. Although \"Blade Runner\" was largely unsuccessful in its first theatrical release, it found a viewership in the home video market and became a cult film. Since the movie omits the religious and mythical elements of Dick's original novel (e.g. empathy boxes and Wilbur Mercer), it falls more strictly within the cyberpunk genre than the novel does. William Gibson would later reveal that upon first viewing the film, he was surprised at how the look of this film matched his vision for \"Neuromancer\", a book he was then working on. The film's tone has since been the staple of many cyberpunk movies, such as \"The Matrix trilogy\" (1999-2003), which uses a wide variety of cyberpunk elements.\n\nThe number of films in the genre or at least using a few genre elements has grown steadily since \"Blade Runner\". Several of Philip K. Dick's works have been adapted to the silver screen. The films \"Johnny Mnemonic\" and \"New Rose Hotel\", both based upon short stories by William Gibson, flopped commercially and critically. These box offices misses significantly slowed the development of cyberpunk as a literary or cultural form although a sequel to the 1982 film \"Blade Runner\" was released in October 2017 with Harrison Ford reprising his role from the original film.\n\nIn addition, \"tech-noir\" film as a hybrid genre, means a work of combining neo-noir and science fiction or cyberpunk. It includes many cyberpunk films such as \"Blade Runner\", \"Burst City\", \"Robocop\", \"12 Monkeys\", \"The Lawnmower Man\", \"Hackers\", \"Hardware\", and \"Strange Days.\"\n\nThe Japanese cyberpunk subgenre began in 1982 with the debut of Katsuhiro Otomo's manga series \"Akira\", with its 1988 anime film adaptation later popularizing the subgenre. \"Akira\" inspired a wave of Japanese cyberpunk works, including manga and anime series such as \"Ghost in the Shell\", \"Battle Angel Alita\", \"Cowboy Bebop\", and \"Serial Experiments Lain\". Other early Japanese cyberpunk works include the 1982 film \"Burst City\", the 1985 original video animation \"Megazone 23\", and the 1989 film \"\".\n\nCyberpunk themes are widely visible in anime and manga. In Japan, where cosplay is popular and not only teenagers display such fashion styles, cyberpunk has been accepted and its influence is widespread. William Gibson's \"Neuromancer,\" whose influence dominated the early cyberpunk movement, was also set in Chiba, one of Japan's largest industrial areas, although at the time of writing the novel Gibson did not know the location of Chiba and had no idea how perfectly it fit his vision in some ways. The exposure to cyberpunk ideas and fiction in the 1980s has allowed it to seep into the Japanese culture.\n\nCyberpunk anime and manga draw upon a futuristic vision which has elements in common with western science fiction and therefore have received wide international acceptance outside Japan. \"The conceptualization involved in cyberpunk is more of forging ahead, looking at the new global culture. It is a culture that does not exist right now, so the Japanese concept of a cyberpunk future, seems just as valid as a Western one, especially as Western cyberpunk often incorporates many Japanese elements.\" William Gibson is now a frequent visitor to Japan, and he came to see that many of his visions of Japan have become a reality:\nModern Japan simply was cyberpunk. The Japanese themselves knew it and delighted in it. I remember my first glimpse of Shibuya, when one of the young Tokyo journalists who had taken me there, his face drenched with the light of a thousand media-suns—all that towering, animated crawl of commercial information—said, \"You see? You see? It is \"Blade Runner\" town.\" And it was. It so evidently was.\n\nCyberpunk themes have appeared in many anime and manga, including the ground-breaking \"Akira\", \"Appleseed\", \"Ghost in the Shell\", \"Ergo Proxy\", \"Battle Angel Alita\", \"Megazone 23\", \"Neo Tokyo\", \"Goku Midnight Eye\", \"Cyber City Oedo 808\", \"Bubblegum Crisis\", \"\", \"Angel Cop\", \"Extra\", \"Blame!\", \"Armitage III\", \"Texhnolyze\", \"Serial Experiments Lain\", \"Neon Genesis Evangelion\" and \"Psycho-Pass\".\n\n\"Akira\" (1982 manga) and its 1988 anime film adaptation have influenced numerous works in animation, comics, film, music, television and video games. \"Akira\" has been cited as a major influence on Hollywood films such as \"The Matrix\", \"Chronicle\", \"Looper\", \"Midnight Special\", and \"Inception\", as well as cyberpunk-influenced video games such as Hideo Kojima's \"Snatcher\" and \"Metal Gear Solid\", Valve's \"Half-Life\" series and Dontnod Entertainment's \"Remember Me\". \"Akira\" has also influenced the work of musicians such as Kanye West, who paid homage to \"Akira\" in the \"Stronger\" music video, and Lupe Fiasco, whose album \"Tetsuo & Youth\" is named after Tetsuo Shima. The popular bike from the film, Kaneda's Motorbike, appears in \"Steven Spielberg\"'s film \"Ready Player One\", and CD Projekt's video game \"Cyberpunk 2077\".\n\n\"Ghost in the Shell\" (1989) influenced a number of prominent filmmakers. Its 1995 anime film adaptation inspired The Wachowskis to create \"The Matrix\" (1999) and its sequels. \"The Matrix\" series took several concepts from the film, including the Matrix digital rain, which was inspired by the opening credits of \"Ghost in the Shell\", and the way characters access the Matrix through holes in the back of their necks. Other parallels have been drawn to James Cameron's \"Avatar\", Steven Spielberg's \"A.I. Artificial Intelligence\", and Jonathan Mostow's \"Surrogates\".\n\nThe original video animation \"Megazone 23\" (1985) has a number of similarities to \"The Matrix\". \"Battle Angel Alita\" (1990) has had a notable influence on filmmaker James Cameron, who was planning to adapt it into a film since 2000. It was an influence on his TV series \"Dark Angel\", and he is the producer of the 2018 film adaptation \"\".\n\nThere are cyberpunk video games. Popular series include the \"Megami Tensei\" series, \"Deus Ex\" series, \"Syndicate\" series, and \"System Shock\" and its sequel. Other games, like \"Blade Runner\", \"Ghost in the Shell\", and the \"Matrix\" series, are based upon genre movies, or role-playing games (for instance the various \"Shadowrun\" games).\n\nSeveral RPGs called \"Cyberpunk\" exist: \"Cyberpunk\", \"Cyberpunk 2020\" and \"Cyberpunk v3\", by R. Talsorian Games, and \"GURPS Cyberpunk\", published by Steve Jackson Games as a module of the GURPS family of RPGs. \"Cyberpunk 2020\" was designed with the settings of William Gibson's writings in mind, and to some extent with his approval, unlike the approach taken by FASA in producing the transgenre \"Shadowrun\" game. Both are set in the near future, in a world where cybernetics are prominent. In addition, Iron Crown Enterprises released an RPG named \"Cyberspace\", which was out of print for several years until recently being re-released in online PDF form. CD Projekt Red is currently developing \"Cyberpunk 2077,\" a cyberpunk first-person open world RPG video-game based on the tabletop RPG \"Cyberpunk 2020\".\nIn 1990, in a convergence of cyberpunk art and reality, the United States Secret Service raided Steve Jackson Games's headquarters and confiscated all their computers. Officials denied that the target had been the \"GURPS Cyberpunk\" sourcebook, but Jackson would later write that he and his colleagues \"were never able to secure the return of the complete manuscript; [...] The Secret Service at first flatly refused to return anything – then agreed to let us copy files, but when we got to their office, restricted us to one set of out-of-date files – then agreed to make copies for us, but said \"tomorrow\" every day from March 4 to March 26. On March 26 we received a set of disks which purported to be our files, but the material was late, incomplete and well-nigh useless.\" Steve Jackson Games won a lawsuit against the Secret Service, aided by the new Electronic Frontier Foundation. This event has achieved a sort of notoriety, which has extended to the book itself as well. All published editions of \"GURPS Cyberpunk\" have a tagline on the front cover, which reads \"The book that was seized by the U.S. Secret Service!\" Inside, the book provides a summary of the raid and its aftermath.\n\nCyberpunk has also inspired several tabletop, miniature and board games such as \"Necromunda\" by Games Workshop. \"Netrunner\" is a collectible card game introduced in 1996, based on the \"Cyberpunk 2020\" role-playing game. \"Tokyo NOVA\", debuting in 1993, is a cyberpunk role-playing game that uses playing cards instead of dice.\n\nSome musicians and acts have been classified as cyberpunk due to their aesthetic style and musical content. Often dealing with dystopian visions of the future or biomechanical themes, some fit more squarely in the category than others. Bands whose music has been classified as cyberpunk include Psydoll, Front Line Assembly, Clock DVA and Sigue Sigue Sputnik. Some musicians not normally associated with cyberpunk have at times been inspired to create concept albums exploring such themes. Albums such as Gary Numan's \"Replicas\", \"The Pleasure Principle\" and \"Telekon\" were heavily inspired by the works of Philip K. Dick. Kraftwerk's \"The Man-Machine\" and \"Computer World\" albums both explored the theme of humanity becoming dependent on technology. Nine Inch Nails' concept album \"Year Zero\" also fits into this category. Fear Factory concept albums are heavily based upon future dystopia, cybernetics, clash between man and machines, virtual worlds. Billy Idol's \"Cyberpunk\" drew heavily from cyberpunk literature and the cyberdelic counter culture in its creation. \"1. Outside\", a cyberpunk narrative fueled concept album by David Bowie, was warmly met by critics upon its release in 1995. Many musicians have also taken inspiration from specific cyberpunk works or authors, including Sonic Youth, whose albums \"Sister\" and \"Daydream Nation\" take influence from the works of Philip K. Dick and William Gibson respectively.\n\nVaporwave and synthwave are also influenced by cyberpunk. The former has been interpreted as a dystopian critique of capitalism in the vein of cyberpunk and the latter as a nostalgic retrofuturistic revival of aspects of cyberpunk's origins.\n\nSome Neo-Futurism artworks and cityscapes have been influenced by cyberpunk, such as the Sony Center in the Potsdamer Platz public square of Berlin, Germany.\n\nSeveral subcultures have been inspired by cyberpunk fiction. These include the cyberdelic counter culture of the late 1980s and early 90s. Cyberdelic, whose adherents referred to themselves as \"cyberpunks\", attempted to blend the psychedelic art and drug movement with the technology of cyberculture. Early adherents included Timothy Leary, Mark Frauenfelder and R. U. Sirius. The movement largely faded following the dot-com bubble implosion of 2000.\n\nCybergoth is a fashion and dance subculture which draws its inspiration from cyberpunk fiction, as well as rave and Gothic subcultures. In addition, a distinct cyberpunk fashion of its own has emerged in recent years which rejects the raver and goth influences of cybergoth, and draws inspiration from urban street fashion, \"post apocalypse\", functional clothing, high tech sports wear, tactical uniform and multifunction. This fashion goes by names like \"tech wear\", \"goth ninja\" or \"tech ninja\". Important designers in this type of fashion are ACRONYM, Demobaza, Boris Bidjan Saberi, Rick Owens and Alexander Wang.\n\nThe Kowloon Walled City in Hong Kong (demolished in 1994) is often referenced as the model cyberpunk/dystopian slum as, given its poor living conditions at the time coupled with the city's political, physical, and economic isolation has caused many in academia to be fascinated by the ingenuity of its spawning.\n\nAs a wider variety of writers began to work with cyberpunk concepts, new subgenres of science fiction emerged, some of which could be considered as playing off the cyberpunk label, others which could be considered as legitimate explorations into newer territory. These focused on technology and its social effects in different ways. One prominent subgenre is \"steampunk,\" which is set in an alternate history Victorian era that combines anachronistic technology with cyberpunk's bleak film noir world view. The term was originally coined around 1987 as a joke to describe some of the novels of Tim Powers, James P. Blaylock, and K.W. Jeter, but by the time Gibson and Sterling entered the subgenre with their collaborative novel \"The Difference Engine\" the term was being used earnestly as well.\n\nAnother subgenre is \"biopunk\" (cyberpunk themes dominated by biotechnology) from the early 1990s, a derivative style building on biotechnology rather than informational technology. In these stories, people are changed in some way not by mechanical means, but by genetic manipulation. Paul Di Filippo is seen as the most prominent biopunk writer, including his half-serious ribofunk. Bruce Sterling's Shaper/Mechanist cycle is also seen as a major influence. In addition, some people consider works such as Neal Stephenson's \"The Diamond Age\" to be postcyberpunk.\n\nCyberpunk works have been described as well-situated within postmodern literature.\n\nRole playing game publisher R. Talsorian Games, owner of the \"Cyberpunk 2020\" franchise, trademarked the word \"Cyberpunk\" in the United States in 2012. Video game developer CD Projekt, which is developing \"Cyberpunk 2077\", bought the U.S. trademark from R. Talsorian Games, and has filed a trademark in the European Union.\n\n"}
{"id": "5704", "url": "https://en.wikipedia.org/wiki?curid=5704", "title": "Comic strip", "text": "Comic strip\n\nA comic strip is a sequence of drawings arranged in interrelated panels to display brief humor or form a narrative, often serialized, with text in balloons and captions. Traditionally, throughout the 20th century and into the 21st, these have been published in newspapers and magazines, with horizontal strips printed in black-and-white in daily newspapers, while Sunday newspapers offered longer sequences in special color comics sections. With the development of the internet, they began to appear online as webcomics. There were more than 200 different comic strips and daily cartoon panels in South Korea alone each day for most of the 20th century, for a total of at least 7,300,000 episodes.\n\nStrips are written and drawn by a comics artist or cartoonist. As the name implies, comic strips can be humorous (for example, \"gag-a-day\" strips such as \"Blondie\", \"Bringing Up Father\", \"Marmaduke\", and \"Pearls Before Swine\").\nStarting in the late 1920s, comic strips expanded from their mirthful origins to feature adventure stories, as seen in \"Popeye\", \"Captain Easy\", \"Buck Rogers\", \"Tarzan\", and \"The Adventures of Tintin\". Soap-opera continuity strips such as \"Judge Parker\" and \"Mary Worth\" gained popularity in the 1940s. All are called, generically, comic strips, though cartoonist Will Eisner has suggested that \"sequential art\" would be a better genre-neutral name.\n\nIn the UK and the rest of Europe, comic strips are also serialized in comic book magazines, with a strip's story sometimes continuing over three pages or more. Comic strips have appeared in American magazines such as \"Liberty\" and \"Boys' Life\" and also on the front covers of magazines, such as the \"Flossy Frills\" series on \"The American Weekly\" Sunday newspaper supplement.\n\nStorytelling using a sequence of pictures has existed through history. One medieval European example in textile form is the Bayeux Tapestry. Printed examples emerged in 19th-century Germany and in 18th-century England, where some of the first satirical or humorous sequential narrative drawings were produced. William Hogarth's 18th century English cartoons include both narrative sequences, such as \"A Rake's Progress\", and single panels.\n\nThe \"Biblia pauperum\" (\"Paupers' Bible\"), a tradition of picture Bibles beginning in the later Middle Ages, sometimes depicted Biblical events with words spoken by the figures in the miniatures written on scrolls coming out of their mouths—which makes them to some extent ancestors of the modern cartoon strips.\n\nIn China, with its traditions of block printing and of the incorporation of text with image, experiments with what became \"lianhuanhua\" date back to 1884.\n\nThe first newspaper comic strips appeared in North America in the late 19th century. \"The Yellow Kid\" is usually credited as one of the first newspaper strips. However, the art form combining words and pictures developed gradually and there are many examples which led up to the comic strip.\n\nSwiss author and caricature artist Rodolphe Töpffer (Geneva, 1799–1846) is considered the father of the modern comic strips. His illustrated stories such as \"Histoire de M. Vieux Bois\" (1827), first published in the USA in 1842 as \"The Adventures of Obadiah Oldbuck\" or \"Histoire de Monsieur Jabot\" (1831), inspired subsequent generations of German and American comic artists. In 1865, German painter, author, and caricaturist Wilhelm Busch created the strip \"Max and Moritz\", about two trouble-making boys, which had a direct influence on the American comic strip. \"Max and Moritz\" was a series of severely moralistic tales in the vein of German children's stories such as \"Struwwelpeter\" (\"Shockheaded Peter\"); in one, the boys, after perpetrating some mischief, are tossed into a sack of grain, run through a mill, and consumed by a flock of geese. \"Max and Moritz\" provided an inspiration for German immigrant Rudolph Dirks, who created the \"Katzenjammer Kids\" in 1897. Familiar comic-strip iconography such as stars for pain, sawing logs for snoring, speech balloons, and thought balloons originated in Dirks' strip.\n\nHugely popular, \"Katzenjammer Kids\" occasioned one of the first comic-strip copyright ownership suits in the history of the medium. When Dirks left William Randolph Hearst for the promise of a better salary under Joseph Pulitzer, it was an unusual move, since cartoonists regularly deserted Pulitzer for Hearst. In a highly unusual court decision, Hearst retained the rights to the name \"Katzenjammer Kids\", while creator Dirks retained the rights to the characters. Hearst promptly hired Harold Knerr to draw his own version of the strip. Dirks renamed his version \"Hans and Fritz\" (later, \"The Captain and the Kids\"). Thus, two versions distributed by rival syndicates graced the comics pages for decades. Dirks' version, eventually distributed by United Feature Syndicate, ran until 1979.\n\nIn the United States, the great popularity of comics sprang from the . \"The Little Bears\" (1893–96) was the first American comic strip with recurring characters, while the first color comic supplement was published by the \"Chicago Inter-Ocean\" sometime in the latter half of 1892, followed by the \"New York Journal\"'s first color Sunday comic pages in 1897. On January 31, 1912, Hearst introduced the nation's first full daily comic page in his \"New York Evening Journal\". The history of this newspaper rivalry and the rapid appearance of comic strips in most major American newspapers is discussed by Ian Gordon. Numerous events in newspaper comic strips have reverberated throughout society at large, though few of these events occurred in recent years, owing mainly to the declining role of the newspaper comic strip as an entertainment form.\n\nThe longest-running American comic strips are:\n\nMost newspaper comic strips are syndicated; a syndicate hires people to write and draw a strip and then distributes it to many newspapers for a fee. Some newspaper strips begin or remain exclusive to one newspaper. For example, the \"Pogo\" comic strip by Walt Kelly originally appeared only in the \"New York Star\" in 1948 and was not picked up for syndication until the following year.\n\nNewspaper comic strips come in two different types: daily strips and Sunday strips. In the United States, a daily strip appears in newspapers on weekdays, Monday through Saturday, as contrasted with a Sunday strip, which typically only appears on Sundays. Daily strips usually are printed in black and white, and Sunday strips are usually in color. However, a few newspapers have published daily strips in color, and some newspapers have published Sunday strips in black and white.\n\nThe popularity and accessibility of strips meant they were often clipped and saved; authors including John Updike and Ray Bradbury have written about their childhood collections of clipped strips. Often posted on bulletin boards, clipped strips had an ancillary form of distribution when they were faxed, photocopied or mailed. The \"Baltimore Sun\"'s Linda White recalled, \"I followed the adventures of \"Winnie Winkle\", \"Moon Mullins\" and \"Dondi\", and waited each fall to see how Lucy would manage to trick Charlie Brown into trying to kick that football. (After I left for college, my father would clip out that strip each year and send it to me just to make sure I didn’t miss it.)\"\n\nThe two conventional formats for newspaper comics are strips and single gag panels. The strips are usually displayed horizontally, wider than they are tall. Single panels are square, circular or taller than they are wide. Strips usually, but not always, are broken up into several smaller panels with continuity from panel to panel. A horizontal strip can also be used for a single panel with a single gag, as seen occasionally in Mike Peters' \"Mother Goose and Grimm\".\n\nEarly daily strips were large, often running the entire width of the newspaper, and were sometimes three or more inches high. Initially, a newspaper page included only a single daily strip, usually either at the top or the bottom of the page. By the 1920s, many newspapers had a comics page on which many strips were collected together. During the 1930s, the original art for a daily strip could be drawn as large as 25 inches wide by six inches high. Over decades, the size of daily strips became smaller and smaller, until by 2000, four standard daily strips could fit in an area once occupied by a single daily strip. As strips have become smaller, the number of panels have been reduced.\n\nProof sheets were the means by which syndicates provided newspapers with black-and-white line art for the reproduction of strips (which they arranged to have colored in the case of Sunday strips). Michigan State University Comic Art Collection librarian Randy Scott describes these as \"large sheets of paper on which newspaper comics have traditionally been distributed to subscribing newspapers. Typically each sheet will have either six daily strips of a given title or one Sunday strip. Thus, a week of \"Beetle Bailey\" would arrive at the \"Lansing State Journal\" in two sheets, printed much larger than the final version and ready to be cut apart and fitted into the local comics page.\" Comic strip historian Allan Holtz described how strips were provided as mats (the plastic or cardboard trays in which molten metal is poured to make plates) or even plates ready to be put directly on the printing press. He also notes that with electronic means of distribution becoming more prevalent printed sheets \"are definitely on their way out.\"\n\nNEA Syndicate experimented briefly with a two-tier daily strip, \"Star Hawks\", but after a few years, \"Star Hawks\" dropped down to a single tier.\n\nIn Flanders, the two-tier strip is the standard publication style of most daily strips like \"Spike and Suzy\" and \"Nero\". They appear Monday through Saturday; until 2003 there were no Sunday papers in Flanders. In the last decades, they have switched from black and white to color.\n\nSingle panels usually, but not always, are not broken up and lack continuity. The daily \"Peanuts\" is a strip, and the daily \"Dennis the Menace\" is a single panel. J. R. Williams' long-run \"Out Our Way\" continued as a daily panel even after it expanded into a Sunday strip, \"Out Our Way with the Willets\". Jimmy Hatlo's \"They'll Do It Every Time\" was often displayed in a two-panel format with the first panel showing some deceptive, pretentious, unwitting or scheming human behavior and the second panel revealing the truth of the situation.\n\nSunday newspapers traditionally included a special color section. Early Sunday strips (known colloquially as \"the funny papers\", shortened to \"the funnies\"), such as \"Thimble Theatre\" and \"Little Orphan Annie\", filled an entire newspaper page, a format known to collectors as full page. Sunday pages during the 1930s and into the 1940s often carried a secondary strip by the same artist as the main strip. No matter whether it appeared above or below a main strip, the extra strip was known as the topper, such as \"The Squirrel Cage\" which ran along with \"Room and Board\", both drawn by Gene Ahern.\n\nDuring the 1930s, the original art for a Sunday strip was usually drawn quite large. For example, in 1930, Russ Westover drew his \"Tillie the Toiler\" Sunday page at a size of 17\" × 37\". In 1937, the cartoonist Dudley Fisher launched the innovative \"Right Around Home\", drawn as a huge single panel filling an entire Sunday page.\n\nFull-page strips were eventually replaced by strips half that size. Strips such as \"The Phantom\" and \"Terry and the Pirates\" began appearing in a format of two strips to a page in full-size newspapers, such as the \"New Orleans Times Picayune\", or with one strip on a tabloid page, as in the \"Chicago Sun-Times\". When Sunday strips began to appear in more than one format, it became necessary for the cartoonist to allow for rearranged, cropped or dropped panels. During World War II, because of paper shortages, the size of Sunday strips began to shrink. After the war, strips continued to get smaller and smaller because of increased paper and printing costs. The last full-page comic strip was the \"Prince Valiant\" strip for 11 April 1971.\n\nComic strips have also been published in Sunday newspaper magazines. Russell Patterson and Carolyn Wells' \"New Adventures of Flossy Frills\" was a continuing strip series seen on Sunday magazine covers. Beginning January 26, 1941, it ran on the front covers of Hearst's \"American Weekly\" newspaper magazine supplement, continuing until March 30 of that year. Between 1939 and 1943, four different stories featuring Flossy appeared on \"American Weekly\" covers.\n\nSunday comics sections employed offset color printing with multiple print runs imitating a wide range of colors. Printing plates were created with four or more colors—traditionally, the CMYK color model: cyan, magenta, yellow and \"K\" for black. With a screen of tiny dots on each printing plate, the dots allowed an image to be printed in a halftone that appears to the eye in different gradations. The semi-opaque property of ink allows halftone dots of different colors to create an optical effect of full-color imagery.\n\nThe decade of the 1960s saw the rise of underground newspapers, which often carried comic strips, such as \"Fritz the Cat\" and \"The Fabulous Furry Freak Brothers\". \"Zippy the Pinhead\" initially appeared in underground publications in the 1970s before being syndicated. \"Bloom County\" and \"Doonesbury\" began as strips in college newspapers under different titles, and later moved to national syndication. Underground comic strips covered subjects that are usually taboo in newspaper strips, such as sex and drugs. Many underground artists, notably Vaughn Bode, Dan O'Neill, Gilbert Shelton, and Art Spiegelman went on to draw comic strips for magazines such as \"Playboy\", \"National Lampoon\", and Pete Millar's \"CARtoons\". Jay Lynch graduated from undergrounds to alternative weekly newspapers to \"Mad\" and children's books.\n\n\"Webcomics\", also known as \"online comics\" and \"internet comics\", are comics that are available to read on the Internet. Many are exclusively published online, but the majority of traditional newspaper comic strips have some Internet presence. King Features Syndicate and other syndicates often provide archives of recent strips on their websites. Some, such as Scott Adams, creator of \"Dilbert\", include an email address in each strip.\n\nMost comic strip characters do not age throughout the strip's life, but in some strips, like Lynn Johnston's award-winning \"For Better or For Worse\", the characters age as the years pass. The first strip to feature aging characters was \"Gasoline Alley\".\n\nThe history of comic strips also includes series that are not humorous, but tell an ongoing dramatic story. Examples include \"The Phantom\", \"Prince Valiant\", \"Dick Tracy\", \"Mary Worth\", \"Modesty Blaise\", \"Little Orphan Annie\", \"Flash Gordon\", and \"Tarzan\". Sometimes these are spin-offs from comic books, for example \"Superman\", \"Batman\", and \"The Amazing Spider-Man\".\n\nA number of strips have featured animals ('funny animals') as main characters. Some are non-verbal (\"Marmaduke\", \"The Angriest Dog in the World\"), some have verbal thoughts but are not understood by humans, (\"Garfield\", Snoopy in \"Peanuts\"), and some can converse with humans (\"Bloom County\", \"Calvin and Hobbes\", \"Mutts\", \"Citizen Dog\", \"Buckles\", \"Get Fuzzy\", \"Pearls Before Swine\", and \"Pooch Cafe\"). Other strips are centered entirely on animals, as in \"Pogo\" and \"Donald Duck\". Gary Larson's \"The Far Side\" was unusual, as there were no central characters. Instead \"The Far Side\" used a wide variety of characters including humans, monsters, aliens, chickens, cows, worms, amoebas, and more. John McPherson's \"Close to Home\" also uses this theme, though the characters are mostly restricted to humans and real-life situations. Wiley Miller not only mixes human, animal, and fantasy characters, but also does several different comic strip continuities under one umbrella title, \"Non Sequitur\". Bob Thaves's \"Frank & Ernest\" began in 1972 and paved the way for some of these strips, as its human characters were manifest in diverse forms — as animals, vegetables, and minerals.\n\nThe comics have long held a distorted mirror to contemporary society, and almost from the beginning have been used for political or social commentary. This ranged from the conservative slant of \"Little Orphan Annie\" to the unabashed liberalism of \"Doonesbury\". \"Pogo\" used animals to particularly devastating effect, caricaturing many prominent politicians of the day as animal denizens of Pogo's Okeefenokee Swamp. In a fearless move, Pogo's creator Walt Kelly took on Joseph McCarthy in the 1950s, caricaturing him as a bobcat named Simple J. Malarkey, a megalomaniac who was bent on taking over the characters' birdwatching club and rooting out all undesirables. Kelly also defended the medium against possible government regulation in the McCarthy era. At a time when comic books were coming under fire for supposed sexual, violent, and subversive content, Kelly feared the same would happen to comic strips. Going before the Congressional subcommittee, he proceeded to charm the members with his drawings and the force of his personality. The comic strip was safe for satire.\n\nDuring the early 20th century, comic strips were widely associated with publisher William Randolph Hearst, whose papers had the largest circulation of strips in the United States. Hearst was notorious for his practice of yellow journalism, and he was frowned on by readers of \"The New York Times\" and other newspapers which featured few or no comic strips. Hearst's critics often assumed that all the strips in his papers were fronts for his own political and social views. Hearst did occasionally work with or pitch ideas to cartoonists, most notably his continued support of George Herriman's \"Krazy Kat\". An inspiration for Bill Watterson and other cartoonists, \"Krazy Kat\" gained a considerable following among intellectuals during the 1920s and 1930s.\n\nSome comic strips, such as \"Doonesbury\" and \"The Boondocks\", may be printed on the editorial or op-ed page rather than the comics page because of their regular political commentary. For example, the August 12, 1974 \"Doonesbury\" strip was awarded a 1975 Pulitzer Prize for its depiction of the Watergate scandal. \"Dilbert\" is sometimes found in the business section of a newspaper instead of the comics page because of the strip's commentary about office politics, and Tank McNamara often appears on the sports page because of its subject matter. Lynn Johnston's \"For Better or for Worse\" created an uproar when one of its supporting characters came out of the closet and announced he was gay.\n\nThe world's longest comic strip is long and on display at Trafalgar Square as part of the London Comedy Festival. The London Cartoon Strip was created by 15 of Britain's best known cartoonists and depicts the history of London.\n\nThe Reuben, named for cartoonist Rube Goldberg, is the most prestigious award for U.S. comic strip artists. Reuben awards are presented annually by the National Cartoonists Society (NCS).\n\nIn 1995, the United States Postal Service issued a series of commemorative stamps, Comic Strip Classics, marking the comic-strip centennial.\n\nToday's strip artists, with the help of the NCS, enthusiastically promote the medium, which is considered to be in decline due to fewer markets (today few strips are published in newspapers outside the United States, the United Kingdom, and Canada, mainly because of the smaller interest there, with translated versions of popular strips - particularly in Spanish - are primarily read over the internet) and ever-shrinking newspaper space. One particularly humorous example of such promotional efforts is the Great Comic Strip Switcheroonie, held in 1997 on April Fool's Day, an event in which dozens of prominent artists took over each other's strips. \"Garfield\"’s Jim Davis, for example, switched with \"Blondie\"’s Stan Drake, while Scott Adams (\"Dilbert\") traded strips with Bil Keane (\"The Family Circus\").\n\nWhile the 1997 Switcheroonie was a one-time publicity stunt, an artist taking over a feature from its originator is an old tradition in newspaper cartooning (as it is in the comic book industry). In fact, the practice has made possible the longevity of the genre's more popular strips. Examples include \"Little Orphan Annie\" (drawn and plotted by Harold Gray from 1924 to 1944 and thereafter by a succession of artists including Leonard Starr and Andrew Pepoy), and \"Terry and The Pirates\", started by Milton Caniff in 1934 and picked up by George Wunder.\n\nA business-driven variation has sometimes led to the same feature continuing under a different name. In one case, in the early 1940s, Don Flowers' \"Modest Maidens\" was so admired by William Randolph Hearst that he lured Flowers away from the Associated Press and to King Features Syndicate by doubling the cartoonist's salary, and renamed the feature \"Glamor Girls\" to avoid legal action by the AP. The latter continued to publish \"Modest Maidens\", drawn by Jay Allen in Flowers' style.\n\nAs newspapers have declined, the changes have affected comic strips. Jeff Reece, lifestyle editor of \"The Florida Times-Union\", wrote, \"Comics are sort of the 'third rail' of the newspaper.\"\n\nIn the early decades of the 20th century, all Sunday comics received a full page, and daily strips were generally the width of the page. The competition between papers for having more cartoons than the rest from the mid-1920s, the growth of large-scale newspaper advertising during most of the thirties, paper rationing during World War II, the decline on news readership (as television newscasts began to be more common) and inflation (which has caused higher printing costs) beginning during the fifties and sixties led to Sunday strips being published on smaller and more diverse formats. Daily strips have suffered as well, in 1910 the strips had an unlimited amount of panels, covering the entire width page, while by 1930 most \"dailies\" had four or five panels covering six of the eight columns occupied by a traditional broadsheet paper, by 1958 those four panels would be narrower, and those would have half of the space a 1910 daily strip had, and by 1998 most strips would have three panels only (with a few exceptions), or even two or one on an occasional basis, apart from strips being smaller, as most papers became slightly narrower. While most cartoonist decided to follow the tide, some cartoonists have complained about this, with \"Pogo\" ending in 1975 as a form of protest from its creators against the practice. Since then \"Calvin and Hobbes\" creator Bill Watterson has written extensively on the issue, arguing that size reduction and dropped panels reduce both the potential and freedom of a cartoonist. After a lengthy battle with his syndicator, Watterson won the privilege of making half page-sized Sunday strips where he could arrange the panels any way he liked. Many newspaper publishers and a few cartoonists objected to this, and some papers continued to print \"Calvin and Hobbes\" at small sizes. Opus won that same privilege years after \"Calvin and Hobbes\" ended, while Wiley Miller circumvented further downsizings by making his \"Non Sequitur\" Sunday strip available only in an extremely vertical (near-page-long) arrangement. Few newspapers still run half-page strips, as with \"Prince Valiant\" and \"Hägar the Horrible\" in the front page of the \"Reading Eagle\" Sunday comics section. Actually Universal Uclick and United Media practically have no half-page comics, with the remaining strips from both syndicates in this format are published only as \"thirds\", \"fourths\", and \"sixths\" (also called \"third tabs\").\n\nIn an issue related to size limitations, Sunday comics are often bound to rigid formats that allow their panels to be rearranged in several different ways while remaining readable. Such formats usually include throwaway panels at the beginning, which some newspapers will omit for space. As a result, cartoonists have less incentive to put great efforts into these panels. \"Garfield\" and \"Mutts\" were known during the mid-to-late 80s and 1990s respectively for their throwaways on their Sunday strips, however both strips now run \"generic\" title panels.\n\nWith the success of \"The Gumps\" during the 1920s, it became commonplace for strips (comedy- and adventure-laden alike) to have lengthy stories spanning weeks or months. The \"Monarch of Medioka\" story in Floyd Gottfredson's \"Mickey Mouse\" comic strip ran from September 8, 1937 to May 2, 1938. Between the 1960s and the late 1980s, as television news relegated newspaper reading to an occasional basis rather than daily, syndicators were abandoning long stories and urging cartoonists to switch to simple daily gags, or week-long \"storylines\" (with six consecutive (mostly unrelated) strips following a same subject), with longer storylines being used mainly on adventure-based and dramatic strips. Strips begun during the mid-1980s or after (such as \"Get Fuzzy\", \"Over the Hedge\", \"Monty\", and others) are known for their heavy use of storylines, lasting between one and three weeks in most cases.\n\nThe writing style of comic strips changed as well after World War II. With an increase in the number of college-educated readers, there was a shift away from slapstick comedy and towards more cerebral humor. Slapstick and visual gags became more confined to Sunday strips, because as \"Garfield\" creator Jim Davis put it, \"Children are more likely to read Sunday strips than dailies.\"\n\nMany older strips are no longer drawn by the original cartoonist, who has either died or retired. Such strips are known as \"zombie strips\". A cartoonist, paid by the syndicate or sometimes a relative of the original cartoonist, continues writing the strip, a tradition that became commonplace in the early half of the 20th century. \"Hägar the Horrible\" and \"Frank and Ernest\" are both drawn by the sons of the creators. Some strips which are still in affiliation with the original creator are produced by small teams or entire companies, such as Jim Davis' \"Garfield\", however there is some debate if these strips fall in this category.\n\nThis act is commonly criticized by modern cartoonists including Watterson and \"Pearls Before Swine\"'s Stephan Pastis. The issue was addressed in six consecutive \"Pearls\" strips in 2005. Charles Schulz, of \"Peanuts\" fame, requested that his strip not be continued by another cartoonist after his death. He also rejected the idea of hiring an inker or letterer, comparing it to a golfer hiring a man to make his putts. Schulz's family has honored his wishes and refused numerous proposals by syndicators to continue \"Peanuts\" with a new author.\n\nSince the consolidation of newspaper comics by the first quarter of the 20th century, most cartoonists have used a group of assistants (with usually one of them credited). However, quite a few cartoonists (e.g.: George Herriman and Charles Schulz, among others) have done their strips almost completely by themselves; often criticizing the use of assistants for the same reasons most have about their editors hiring anyone else to continue their work after their retirement.\n\nHistorically, syndicates owned the creators' work, enabling them to continue publishing the strip after the original creator retired, left the strip, or died. This practice led to the term \"legacy strips,\" or more pejoratively \"zombie strips\"). Most syndicates signed creators to 10- or even 20-year contracts. (There have been exceptions, however, such as Bud Fisher's \"Mutt and Jeff\" being an early — if not the earliest — case in which the creator retained ownership of his work.) Both these practices began to change with the 1970 debut of Universal Press Syndicate, as the company gave cartoonists a 50-percent ownership share of their work. Creators Syndicate, founded in 1987, granted artists full rights to the strips, something that Universal Press did in 1990, followed by King Features in 1995. By 1999 both Tribune Media Services and United Feature had begun granting ownership rights to creators (limited to new and/or hugely popular strips).\n\nStarting in the late 1940s, the national syndicates which distributed newspaper comic strips subjected them to very strict censorship. \"Li'l Abner\" was censored in September 1947 and was pulled from the Pittsburg Press by Scripps-Howard. The controversy, as reported in \"Time\", centered on Capp's portrayal of the U.S. Senate. Said Edward Leech of Scripps, \"We don't think it is good editing or sound citizenship to picture the Senate as an assemblage of freaks and crooks... boobs and undesirables.\"\n\nAs comics are easier for children to access compared to other types of media, they have a significantly more rigid censorship code than other media. Stephan Pastis has lamented that the \"unwritten\" censorship code is still \"stuck somewhere in the 1950s.\" Generally, comics are not allowed to include such words as \"damn\", \"sucks\", \"screwed\", and \"hell\", although there have been exceptions such as the September 22, 2010 \"Mother Goose and Grimm\" in which an elderly man says, \"This nursing home food sucks,\" and a pair of \"Pearls Before Swine\" comics from January 11, 2011 with a character named Ned using the word \"crappy\". Naked backsides and shooting guns cannot be shown, according to \"Dilbert\" cartoonist Scott Adams. Such comic strip taboos were detailed in Dave Breger's book \"But That's Unprintable\" (Bantam, 1955).\n\nMany issues such as sex, narcotics, and terrorism cannot or can very rarely be openly discussed in strips, although there are exceptions, usually for satire, as in \"Bloom County\". This led some cartoonists to resort to double entendre or dialogue children do not understand, as in Greg Evans' \"Luann\". Young cartoonists have claimed commonplace words, images, and issues should be allowed in the comics. Some of the taboo words and topics are mentioned daily on television and other forms of visual media. Webcomics and comics distributed primarily to college newspapers are much freer in this respect.\n\n\n\n\n"}
{"id": "5705", "url": "https://en.wikipedia.org/wiki?curid=5705", "title": "Continuum hypothesis", "text": "Continuum hypothesis\n\nIn mathematics, the continuum hypothesis (abbreviated CH) is a hypothesis about the possible sizes of infinite sets. It states:\nThe continuum hypothesis was advanced by Georg Cantor in 1878, and\nestablishing its truth or falsehood is the first of Hilbert's 23 problems presented in 1900. Τhe answer to this problem is independent of ZFC set theory (that is, Zermelo–Fraenkel set theory with the axiom of choice included), so that either the continuum hypothesis or its negation can be added as an axiom to ZFC set theory, with the resulting theory being consistent if and only if ZFC is consistent. This independence was proved in 1963 by Paul Cohen, complementing earlier work by Kurt Gödel in 1940.\n\nThe name of the hypothesis comes from the term \"the continuum\" for the real numbers.\nCantor believed the continuum hypothesis to be true and tried for many years to prove it, in vain . It became the first on David Hilbert's list of important open questions that was presented at the International Congress of Mathematicians in the year 1900 in Paris. Axiomatic set theory was at that point not yet formulated. \nKurt Gödel proved in 1940 that the negation of the continuum hypothesis, i.e., the existence of a set with intermediate cardinality, could not be proved in standard set theory. The second half of the independence of the continuum hypothesis – i.e., unprovability of the nonexistence of an intermediate-sized set – was proved in 1963 by Paul Cohen.\n\nTwo sets are said to have the same \"cardinality\" or \"cardinal number\" if there exists a bijection (a one-to-one correspondence) between them. Intuitively, for two sets \"S\" and \"T\" to have the same cardinality means that it is possible to \"pair off\" elements of \"S\" with elements of \"T\" in such a fashion that every element of \"S\" is paired off with exactly one element of \"T\" and vice versa. Hence, the set {banana, apple, pear} has the same cardinality as {yellow, red, green}.\n\nWith infinite sets such as the set of integers or rational numbers, the existence of a bijection between two sets becomes more difficult to demonstrate. The rational numbers seemingly form a counterexample to the continuum hypothesis: the integers form a proper subset of the rationals, which themselves form a proper subset of the reals, so intuitively, there are more rational numbers than integers and more real numbers than rational numbers. However, this intuitive analysis is flawed; it does not take proper account of the fact that all three sets are infinite. It turns out the rational numbers can actually be placed in one-to-one correspondence with the integers, and therefore the set of rational numbers is the same size (\"cardinality\") as the set of integers: they are both countable sets.\n\nCantor gave two proofs that the cardinality of the set of integers is strictly smaller than that of the set of real numbers (see Cantor's first uncountability proof and Cantor's diagonal argument). His proofs, however, give no indication of the extent to which the cardinality of the integers is less than that of the real numbers. Cantor proposed the continuum hypothesis as a possible solution to this question.\n\nThe continuum hypothesis states that the set of real numbers has minimal possible cardinality which is greater than the cardinality of the set of integers. That is, every set, \"S\", of real numbers can either be mapped one-to-one into the integers or the real numbers can be mapped one-to-one into \"S\". Using the fact that the real numbers are equinumerous with the powerset of the integers, the continuum hypothesis says that there is no set formula_1 for which formula_2\n\nAssuming the axiom of choice, there is a smallest cardinal number formula_3 greater than formula_4, and the continuum hypothesis is in turn equivalent to the equality formula_5\n\nThere is also a generalization of the continuum hypothesis called the generalized continuum hypothesis (GCH) which says that for every ordinal α, formula_6 That is, GCH asserts that the cardinality of the power set of each infinite set is the smallest cardinality greater than that of the set.\n\nThe independence of the continuum hypothesis (CH) from Zermelo–Fraenkel set theory (ZF) follows from combined work of Kurt Gödel and Paul Cohen.\n\nCohen (1963, 1964) showed that CH cannot be proven from the ZFC axioms, completing the overall independence proof. To prove his result, Cohen developed the method of forcing, which has become a standard tool in set theory. Essentially, this method begins with a model of ZF in which CH holds, and constructs another model which contains more sets than the original, in a way that CH does not hold in the new model. Cohen was awarded the Fields Medal in 1966 for his proof.\n\nThe independence proof just described shows that CH is independent of ZFC. Further research has shown that CH is independent of all known \"large cardinal axioms\" in the context of ZFC. () Moreover, it has been shown that the cardinality of the continuum can be any cardinal consistent with König's theorem. A result of Solovay, proved shortly after Cohen's result on the independence of the continuum hypothesis, shows that in any model of ZFC, if formula_7 is a cardinal of uncountable cofinality, then there is a forcing extension in which formula_8. However, per König's theorem, it is not consistent to assume formula_9 is formula_10 or formula_11 or any cardinal with cofinality formula_12.\n\nThe continuum hypothesis is closely related to many statements in analysis, point set topology and measure theory. As a result of its independence, many substantial conjectures in those fields have subsequently been shown to be independent as well.\n\nThe independence from ZFC means that proving or disproving the CH within ZFC is impossible. However, Gödel and Cohen's negative results are not universally accepted as disposing of all interest in the continuum hypothesis. Hilbert's problem remains an active topic of research; see and for an overview of the current research status.\n\nThe continuum hypothesis was not the first statement shown to be independent of ZFC. An immediate consequence of Gödel's incompleteness theorem, which was published in 1931, is that there is a formal statement (one for each appropriate Gödel numbering scheme) expressing the consistency of ZFC that is independent of ZFC, assuming that ZFC is consistent. The continuum hypothesis and the axiom of choice were among the first mathematical statements shown to be independent of ZF set theory.\n\nGödel believed that CH is false, and that his proof that CH is consistent with ZFC only shows that the Zermelo–Fraenkel axioms do not adequately characterize the universe of sets. Gödel was a platonist and therefore had no problems with asserting the truth and falsehood of statements independent of their provability. Cohen, though a formalist , also tended towards rejecting CH.\n\nHistorically, mathematicians who favored a \"rich\" and \"large\" universe of sets were against CH, while those favoring a \"neat\" and \"controllable\" universe favored CH. Parallel arguments were made for and against the axiom of constructibility, which implies CH. More recently, Matthew Foreman has pointed out that ontological maximalism can actually be used to argue in favor of CH, because among models that have the same reals, models with \"more\" sets of reals have a better chance of satisfying CH (Maddy 1988, p. 500).\n\nAnother viewpoint is that the conception of set is not specific enough to determine whether CH is true or false. This viewpoint was advanced as early as 1923 by Skolem, even before Gödel's first incompleteness theorem. Skolem argued on the basis of what is now known as Skolem's paradox, and it was later supported by the independence of CH from the axioms of ZFC since these axioms are enough to establish the elementary properties of sets and cardinalities. In order to argue against this viewpoint, it would be sufficient to demonstrate new axioms that are supported by intuition and resolve CH in one direction or another. Although the axiom of constructibility does resolve CH, it is not generally considered to be intuitively true any more than CH is generally considered to be false (Kunen 1980, p. 171).\n\nAt least two other axioms have been proposed that have implications for the continuum hypothesis, although these axioms have not currently found wide acceptance in the mathematical community. In 1986, Chris Freiling presented an argument against CH by showing that the negation of CH is equivalent to Freiling's axiom of symmetry, a statement about probabilities. Freiling believes this axiom is \"intuitively true\" but others have disagreed. A difficult argument against CH developed by W. Hugh Woodin has attracted considerable attention since the year 2000 (Woodin 2001a, 2001b). Foreman (2003) does not reject Woodin's argument outright but urges caution.\n\nSolomon Feferman (2011) has made a complex philosophical argument that CH is not a definite mathematical problem. He proposes a theory of \"definiteness\" using a semi-intuitionistic subsystem of ZF that accepts classical logic for bounded quantifiers but uses intuitionistic logic for unbounded ones, and suggests that a proposition formula_13 is mathematically \"definite\" if the semi-intuitionistic theory can prove formula_14. He conjectures that CH is not definite according to this notion, and proposes that CH should, therefore, be considered not to have a truth value. Peter Koellner (2011b) wrote a critical commentary on Feferman's article.\n\nJoel David Hamkins proposes a multiverse approach to set theory and argues that \"the continuum hypothesis is settled on the multiverse view by our extensive knowledge about how it behaves in the multiverse, and, as a result, it can no longer be settled in the manner formerly hoped for.\" (Hamkins 2012). In a related vein, Saharon Shelah wrote that he does \"not agree with the pure Platonic view that the interesting problems in set theory can be decided, that we just have to discover the additional axiom. My mental picture is that we have many possible set theories, all conforming to ZFC.\" (Shelah 2003).\n\nThe \"generalized continuum hypothesis\" (GCH) states that if an infinite set's cardinality lies between that of an infinite set \"S\" and that of the power set of \"S\", then it either has the same cardinality as the set \"S\" or the same cardinality as the power set of \"S\". That is, for any infinite cardinal formula_15 there is no cardinal formula_7 such that formula_17 GCH is equivalent to:\nThe beth numbers provide an alternate notation for this condition: formula_20 for every ordinal formula_19\n\nThis is a generalization of the continuum hypothesis since the continuum has the same cardinality as the power set of the integers. It was first suggested by . (For the early history of GCH, see ).\n\nLike CH, GCH is also independent of ZFC, but Sierpiński proved that ZF + GCH implies the axiom of choice (AC) (and therefore the negation of the axiom of determinacy, AD), so choice and GCH are not independent in ZF; there are no models of ZF in which GCH holds and AC fails. To prove this, Sierpiński showed GCH implies that every cardinality n is smaller than some Aleph number, and thus can be ordered. This is done by showing that n is smaller than formula_22 which is smaller than its own Hartogs number—this uses the equality formula_23; for the full proof, see Gillman (2002).\n\nKurt Gödel showed that GCH is a consequence of ZF + V=L (the axiom that every set is constructible relative to the ordinals), and is therefore consistent with ZFC. As GCH implies CH, Cohen's model in which CH fails is a model in which GCH fails, and thus GCH is not provable from ZFC. W. B. Easton used the method of forcing developed by Cohen to prove Easton's theorem, which shows it is consistent with ZFC for arbitrarily large cardinals formula_24 to fail to satisfy formula_25 Much later, Foreman and Woodin proved that (assuming the consistency of very large cardinals) it is consistent that formula_26 holds for every infinite cardinal formula_27 Later Woodin extended this by showing the consistency of formula_28 for every formula_7. showed that, for each \"n\" ≥ 1, it is consistent with ZFC that for each κ, 2 is the \"n\"th successor of κ. On the other hand, proved, that if γ is an ordinal and for each infinite cardinal κ, 2 is the γth successor of κ, then γ is finite.\n\nFor any infinite sets A and B, if there is an injection from A to B then there is an injection from subsets of A to subsets of B. Thus for any infinite cardinals A and B, formula_30 . If A and B are finite, the stronger inequality formula_31 holds. GCH implies that this strict, stronger inequality holds for infinite cardinals as well as finite cardinals.\n\nAlthough the generalized continuum hypothesis refers directly only to cardinal exponentiation with 2 as the base, one can deduce from it the values of cardinal exponentiation formula_32 in all cases. GCH implies that (see: Hayden & Kennison (1968), page 147, exercise 76):\n\n\n\n"}
{"id": "5706", "url": "https://en.wikipedia.org/wiki?curid=5706", "title": "Çevik Bir", "text": "Çevik Bir\n\nÇevik Bir (born 1939) is a retired Turkish army general. He was a member of the Turkish General Staff in the 1990s. He took a major part in several important international missions in the Middle East and North Africa. He was born in Buca, Izmir Province, in 1939 and is married with one child. Çevik Bir is of Albanian origin.\n\nHe graduated from the Turkish Military Academy as an engineer officer in 1958, from the Army Staff College in 1970 and from the Armed Forces College in 1971. He graduated from NATO Defense College, Rome, Italy in 1973.\n\nFrom 1973 to 1985, he served at SHAPE, NATO's headquarters in Belgium. He was promoted to brigadier general and commanded an armed brigade and division in Turkey. From 1987 to 1991, he served as major general, and then was promoted to lieutenant general.\n\nAfter the dictator Siad Barre’s ousting, conflicts between the General Mohammed Farah Aidid party and other clans in Somalia had led to famine and lawlessness throughout the country. An estimated 300,000 people had died from starvation. A combined military force of United States and United Nations (under the name \"UNOSOM\") were deployed to Mogadishu, to monitor the ceasefire and deliver food and supplies to the starving people of Somali. Çevik Bir, who was then a lieutenant-general of Turkey, became the force commander of UNOSOM II in April 1993. Despite the retreat of US and UN forces after several deaths due to local hostilities mainly led by Aidid, the introduction of a powerful military force opened the transportation routes, enabling the provision of supplies and ended the famine quickly. He was succeeded as Force Commander by a Malaysian general in January 1994.\n\nHe became a four-star general and served three years as vice chairman of the Turkish Armed Forces, then appointed commander of the Turkish First Army, in Istanbul. While he was vice chairman of the TAF, he signed the Turkish-Israeli Military Coordination agreement in 1996.\n\nÇevik Bir became the Turkish army's deputy chief of general staff shortly after the Somali operation and played a vital role in establishing a Turkish-Israeli entente.\nÇevik Bir retired from the army on August 30, 1999. He is a former member of the Association for the Study of the Middle East and Africa (ASMEA).\n\nOn April 12, 2012, Bir and 30 other officers were taken in custody for their role in the 1997 military memorandum that forced the then Turkish government, led by the Refah Partisi (Welfare Party), to step down.\n\nÇevik Bir, one of the generals who planned the process, said \"In Turkey we have a marriage of Islam and democracy. (…) The child of this marriage is secularism. Now this child gets sick from time to time. The Turkish Armed Forces is the doctor which saves the child. Depending on how sick the kid is, we administer the necessary medicine to make sure the child recuperates\".\n\n\n \n"}
{"id": "5708", "url": "https://en.wikipedia.org/wiki?curid=5708", "title": "Collectivism", "text": "Collectivism\n\nCollectivism is a cultural value that is characterized by emphasis on cohesiveness among individuals and prioritization of the group over self. Individuals or groups that subscribe to a collectivistic worldview tend to find common values and goals as particularly salient and demonstrate greater orientation toward in-group than toward out-group. The term “in-group” is thought to be more diffusely defined for collectivistic individuals to include societal units ranging from the nuclear family to a religious or racial/ethnic group. Meta-analytic findings support that collectivism shows a consistent association with discrete values, interpersonal patterns of interaction, cognition, perception and self-construal. Collectivism is often discussed alongside the cultural value of individualism, but these are two distinct concepts and are not considered to be opposites.\n\nThe German sociologist Tönnies described an early model of collectivism and individualism using the terms \"Gemeinschaft\" (community) and \"Gesellschaft\" (society). \"Gemeinschaft\" relationships, in which communalism is prioritized, were thought to be characteristic of small, rural village communities. An anthropologist, Redfield (1941) echoed this notion in work contrasting folk society with urban society.\n\nMax Weber (1930) contrasted collectivism and individualism through the lens of religion, believing that Protestants were more individualistic and self-reliant compared to Catholics, who endorsed hierarchical, interdependent relationships among people.\n\nHofstede (1980) was highly influential in ushering in an era of cross-cultural research making comparisons along the dimension of collectivism versus individualism. Hofstede conceptualized collectivism and individualism as part of a single continuum, with each cultural construct representing an opposite pole. The author characterized individuals that endorsed a high degree of collectivism as being embedded in their social contexts and prioritizing communal goals over individual goals. The notion that collectivism-individualism is unidimensional has been challenged by contemporary theorists.\n\nCollectivism was an important part of Marxist–Leninist ideology in the Soviet Union, where it played a key part in forming the New Soviet man, willingly sacrificing his or her life for the good of the collective. Terms such as \"collective\" and \"the masses\" were frequently used in the official language and praised in agitprop literature, for example by Vladimir Mayakovsky (\"Who needs a \"1\"\") and Bertolt Brecht (The Decision, Man Equals Man).\n\nThe construct of collectivism is represented in empirical literature under several different names. Most commonly, the term interdependent self-construal is used. Other phrases used to describe the concept of collectivism-individualism include allocentrism-idiocentrism, collective-private self, as well as subtypes of collectivism-individualism (meaning, vertical and horizontal subtypes). Inconsistent terminology is thought to account for some of the difficulty in effectively synthesizing the empirical literature on collectivism.\n\nTypically, collectivism is measured via self-report questionnaire. Meta-analytic findings suggest that there are six instruments that have been used to measure collectivism (and the related construct of individualism) in a manner that best reflects current theoretical thinking.\n\nIn one critical model of collectivism, Markus and Kitayama describe the interdependent (i.e., collectivistic) self as fundamentally connected to the social context. As such, one's sense of self depends on and is defined in part by those around them and is primarily manifested in public, overt behavior. As such, the organization of the self is guided by using others as a reference. That is, an interdependent individual uses the unexpressed thoughts, feelings, and beliefs of another person with whom they have a relationship with, as well as the other person's behaviors, to make decisions about their own internal attributes and actions.\n\nMarkus and Kitayama also contributed to the literature by challenging Hofstede's unidimensional model of collectivism-individualism. The authors conceptualized these two constructs bidimensionally, such that both collectivism and individualism can be endorsed independently and potentially to the same degree. This notion has been echoed by other prominent theorists in the field.\n\nSome researchers have expanded the collectivism-individualism framework to include a more comprehensive view. Specifically, Triandis and colleagues introduced a theoretical model in which incorporates the notion of relational contexts. The authors argues that the domains of collectivism and individualism can be further described by horizontal and vertical relationships. Horizontal relationships are believed to be status-equal whereas vertical relationships are characterized as hierarchical and status-unequal. As such, horizontal collectivism is manifested as an orientation in which group harmony is highly valued and in-group members are perceived to experience equal standing. Vertical collectivism involves the prioritization of group goals over individual goals, implying a hierarchical positioning of the self in relation to the overarching in-group. The horizontal-vertical individualism-collectivism model has received empirical support and has been used to explore patterns within cultures.\n\nOriginated by W. E. B. DuBois, some researchers have adopted a historical perspective on the emergence of collectivism among some cultural groups. DuBois and others argued that oppressed minority groups contend with internal division, meaning that the development of self-identity for individuals from these groups involves the integration of one's own perceptions of their group as well as typically negative, societal views of their group. This division is thought to impact goal formation such that people from marginalized groups tend to emphasize collectivistic over individualistic values.\n\nSome organizational research has found different variations of collectivism. These include institutional collectivism and in-group collectivism. Institutional collectivism is the idea that a work environment creates a sense of collectivist nature due to similar statuses and similar rewards, such as earning the same salary. In-group collectivism is the idea that an individual's chosen group of people, such as family or friend groups, create a sense of collectivist nature. In-group collectivism can be referred to as family collectivism.\n\nA number of classic studies have demonstrated that there is a relationship between collectivism and cognition. These studies support the notion that people from collectivistic cultures tend to demonstrate a holistic cognitive style, which is reflected in processes such as memory, visual perception, attributional style, and categorization schemas. This effect has been replicated extensively by independent research groups, supporting its robustness.\n\n\nAn individual's self-concept can be fundamentally shaped by cultural values. Such processes typically begin in childhood and adolescence and parents are often one of the first critical inputs that shape a child's sense of self-concept. Parents with more collectivistic world views have been shown to speak and interact with their children in a manner that conveys the core tenets of collectivism, such as emphasis on the relationships between objects and interpersonal connections. As such, youth who are parented in this manner tend to develop a sense of self that is defined in relation to others. This sense of self also has been found to be reflected in patterns of structural and functional connectivity in the brain. For example, generally the medial prefrontal cortex (MPFC) is more active when adults think about themselves compared to when they think about someone else. However, for adults who endorse collectivism, the MPFC actually shows greater response when they think about themselves in the context of their close relationships.\n\nCultural views are believed to have a reciprocal relationship with macro-level processes such as economics, social change, and politics. The collectivism-individualism dimension of culture influences economic development: collectivistic culture theoretically promotes growth. For instance, the influence of the collectivist dimension of culture can be observed among the European Union economies: countries which societies are less individualistic record faster economic development, at the expense of individual liberty. Societal changes in China exemplifies this well. Beginning in the early 1980s, China experienced dramatic expansion of economic and social structures, resulting in greater income inequality between families, less involvement of the government in social welfare programs, and increased competition for employment. Corresponding with these changes was a shift in ideology among Chinese citizens, especially among those who were younger, away from collectivism (the prevailing cultural ideology) toward individualism. China also saw this shift reflected in educational policies, such that teachers were encouraged to promote the development of their students’ individual opinions and self-efficacy, which prior to the aforementioned economic changes, was not emphasized in Chinese culture.\n\nAttempts to study the association of collectivism and political views and behaviors has largely occurred at the aggregate national level. However, more isolated political movements have also adopted a collectivistic framework. For example, Collectivist anarchism (also known as anarcho-collectivism) is a revolutionary anarchist doctrine that advocates the abolition of both the state and private ownership of the means of production. It instead envisions the means of production being owned collectively and controlled and managed by the producers themselves.\n"}
{"id": "5711", "url": "https://en.wikipedia.org/wiki?curid=5711", "title": "Nepeta", "text": "Nepeta\n\nNepeta is a genus of flowering plants in the family Lamiaceae. The genus name is reportedly in reference to Nepete, an ancient Etruscan city. There are about 250 species.\n\nThe genus is native to Europe, Asia, and Africa, and has also naturalized in North America.\n\nSome members of this group are known as catnip or catmint because of their effect on house cats – the nepetalactone contained in some \"Nepeta\" species binds to the olfactory receptors of cats, typically resulting in temporary euphoria.\n\nMost of the species are herbaceous perennial plants, but some are annuals. They have sturdy stems with opposite heart-shaped, green to gray-green leaves. \"Nepeta\" plants are usually aromatic in foliage and flowers.\n\nThe tubular flowers can be lavender, blue, white, pink, or lilac, and spotted with tiny lavender-purple dots. The flowers are located in verticillasters grouped on spikes; or the verticillasters are arranged in opposite cymes, racemes, or panicles – toward the tip of the stems.\n\nThe calyx is tubular or campanulate, they are slightly curved or straight, and the limbs are often 2-lipped with five teeth. The lower lip is larger, with 3-lobes, and the middle lobe is the largest. The flowers have 4 hairless stamens that are nearly parallel, and they ascend under the upper lip of the corolla. Two stamen are longer and stamens of pistillate flowers are rudimentary. The style protrudes outside of the mouth of the flowers.\n\nThe fruits are nutlets, which are oblong-ovoid, ellipsoid, ovoid, or obovoid in shape. The surfaces of the nutlets can be slightly ribbed, smooth or warty.\n\n\nSpecies include:\n\nSome \"Nepeta\" species are cultivated as ornamental plants. They can be drought tolerant – water conserving, often deer repellent, with long bloom periods from late spring to autumn. Some species also have repellent properties to insect pests, including aphids and squash bugs, when planted in a garden.\n\n\"Nepeta\" species are used as food plants by the larvae of some Lepidoptera (butterfly and moth) species including \"Coleophora albitarsella\", and as nectar sources for pollinators, such as honey bees and hummingbirds.\n\n\n\n"}
{"id": "5714", "url": "https://en.wikipedia.org/wiki?curid=5714", "title": "Cornish Nationalist Party", "text": "Cornish Nationalist Party\n\nThe Cornish Nationalist Party (CNP; ) is a political party, founded by Dr James Whetter, who campaigned for independence for Cornwall. It was formed by people who left Cornwall's main nationalist party Mebyon Kernow on 28 May 1975, but it is no longer for independence.\n\nA separate party with a similar name (Cornish National Party) existed from 1969.\n\nThe split with Mebyon Kernow was based on the same debate that was occurring in most of the other political parties campaigning for autonomy from the United Kingdom at the time (such as the Scottish National Party and Plaid Cymru): whether to be a centre-left party, appealing to the electorate on a social democratic line, or whether to appeal emotionally on a centre-right cultural line. Originally, another subject of the split was whether to embrace devolution as a first step to full independence (or as the sole step if this was what the electorate wished) or for it to be \"all or nothing\".\n\nThe CNP essentially represented a more right-wing outlook from those who disagree that economic arguments were more likely to win votes than cultural. The CNP worked to preserve the identity of Cornwall and improve its economy, and encouraged links with Cornish people overseas and with other regions with distinct identities. It also gave support to the Cornish language and commemorated Thomas Flamank, a leader of the Cornish Rebellion in 1497, at an annual ceremony at Bodmin on 27 June each year.\n\nWhile the CNP is not a racist organisation, there was a perceived image problem from the similarly-styled BNP and NF (the nativist British National Party and National Front). The CNP was for some time seen as more of a pressure group, as it did not put up candidates for any elections, although its visibility and influence within Cornwall is negligible. , it is now registered on the UK political parties register, and so Mebyon Kernow is no longer the only registered political party based in Cornwall. In April 2009, a news story reported that the CNP had re-formed following a conference in Bodmin; however, it did not contest any elections that year.\n\nWhetter and the CNP still publish a quarterly journal, \"The Cornish Banner\" (\"An Baner Kernewek\"), within the actions of the Roseland Institute.\n\nA newspaper article and a revamp of the party website in October 2014 state that the party is now to contest elections once more.\nJohn Le Bretton, vice-chairman of the party, said: \"The CNP supports the retention of Cornwall council as a Cornwall-wide authority running Cornish affairs and we call for the British government in Westminster to devolve powers to the council so that decisions affecting Cornwall can be made in Cornwall\".\n\nThe party's policies include the following:\n\nThe CNP has one parish councillor, CNP leader Androw Hawke who was elected to Polperro Community Council for the second time on 4 May 2017.\n\n\n"}
{"id": "5715", "url": "https://en.wikipedia.org/wiki?curid=5715", "title": "Cryptanalysis", "text": "Cryptanalysis\n\nCryptanalysis (from the Greek \"kryptós\", \"hidden\", and \"analýein\", \"to loosen\" or \"to untie\") is the study of analyzing information systems in order to study the hidden aspects of the systems. Cryptanalysis is used to breach cryptographic security systems and gain access to the contents of encrypted messages, even if the cryptographic key is unknown.\n\nIn addition to mathematical analysis of cryptographic algorithms, cryptanalysis includes the study of side-channel attacks that do not target weaknesses in the cryptographic algorithms themselves, but instead exploit weaknesses in their implementation.\n\nEven though the goal has been the same, the methods and techniques of cryptanalysis have changed drastically through the history of cryptography, adapting to increasing cryptographic complexity, ranging from the pen-and-paper methods of the past, through machines like the British Bombes and Colossus computers at Bletchley Park in World War II, to the mathematically advanced computerized schemes of the present. Methods for breaking modern cryptosystems often involve solving carefully constructed problems in pure mathematics, the best-known being integer factorization.\n\nGiven some encrypted data (\"\"ciphertext\"\"), the goal of the \"cryptanalyst\" is to gain as much information as possible about the original, unencrypted data (\"\"plaintext\"\"). It is useful to consider two aspects of achieving this. The first is \"breaking\" the system — that is discovering how the encipherment process works. The second is \"solving\" the key that is unique for a particular encrypted message or group of messages.\n\nAttacks can be classified based on what type of information the attacker has available. As a basic starting point it is normally assumed that, for the purposes of analysis, the general algorithm is known; this is Shannon's Maxim \"the enemy knows the system\" — in its turn, equivalent to Kerckhoffs' principle. This is a reasonable assumption in practice — throughout history, there are countless examples of secret algorithms falling into wider knowledge, variously through espionage, betrayal and reverse engineering. (And on occasion, ciphers have been broken through pure deduction; for example, the German Lorenz cipher and the Japanese Purple code, and a variety of classical schemes):\n\nAttacks can also be characterised by the resources they require. Those resources include:\n\n\nIt's sometimes difficult to predict these quantities precisely, especially when the attack isn't practical to actually implement for testing. But academic cryptanalysts tend to provide at least the estimated \"order of magnitude\" of their attacks' difficulty, saying, for example, \"SHA-1 collisions now 2.\"\n\nBruce Schneier notes that even computationally impractical attacks can be considered breaks: \"Breaking a cipher simply means finding a weakness in the cipher that can be exploited with a complexity less than brute force. Never mind that brute-force might require 2 encryptions; an attack requiring 2 encryptions would be considered a break...simply put, a break can just be a certificational weakness: evidence that the cipher does not perform as advertised.\"\n\nThe results of cryptanalysis can also vary in usefulness. For example, cryptographer Lars Knudsen (1998) classified various types of attack on block ciphers according to the amount and quality of secret information that was discovered:\n\nAcademic attacks are often against weakened versions of a cryptosystem, such as a block cipher or hash function with some rounds removed. Many, but not all, attacks become exponentially more difficult to execute as rounds are added to a cryptosystem, so it's possible for the full cryptosystem to be strong even though reduced-round variants are weak. Nonetheless, partial breaks that come close to breaking the original cryptosystem may mean that a full break will follow; the successful attacks on DES, MD5, and SHA-1 were all preceded by attacks on weakened versions.\n\nIn academic cryptography, a \"weakness\" or a \"break\" in a scheme is usually defined quite conservatively: it might require impractical amounts of time, memory, or known plaintexts. It also might require the attacker be able to do things many real-world attackers can't: for example, the attacker may need to choose particular plaintexts to be encrypted or even to ask for plaintexts to be encrypted using several keys related to the secret key. Furthermore, it might only reveal a small amount of information, enough to prove the cryptosystem imperfect but too little to be useful to real-world attackers. Finally, an attack might only apply to a weakened version of cryptographic tools, like a reduced-round block cipher, as a step towards breaking of the full system.\n\nCryptanalysis has coevolved together with cryptography, and the contest can be traced through the history of cryptography—new ciphers being designed to replace old broken designs, and new cryptanalytic techniques invented to crack the improved schemes. In practice, they are viewed as two sides of the same coin: secure cryptography requires design against possible cryptanalysis.\n\nSuccessful cryptanalysis has undoubtedly influenced history; the ability to read the presumed-secret thoughts and plans of others can be a decisive advantage. For example, in England in 1587, Mary, Queen of Scots was tried and executed for treason as a result of her involvement in three plots to assassinate Elizabeth I of England. The plans came to light after her coded correspondence with fellow conspirators was deciphered by Thomas Phelippes.\n\nIn World War I, the breaking of the Zimmermann Telegram was instrumental in bringing the United States into the war. In World War II, the Allies benefitted enormously from their joint success cryptanalysis of the German ciphers — including the Enigma machine and the Lorenz cipher — and Japanese ciphers, particularly 'Purple' and JN-25. 'Ultra' intelligence has been credited with everything between shortening the end of the European war by up to two years, to determining the eventual result. The war in the Pacific was similarly helped by 'Magic' intelligence.\n\nGovernments have long recognized the potential benefits of cryptanalysis for intelligence, both military and diplomatic, and established dedicated organizations devoted to breaking the codes and ciphers of other nations, for example, GCHQ and the NSA, organizations which are still very active today. In 2004, it was reported that the United States had broken Iranian ciphers. (It is unknown, however, whether this was pure cryptanalysis, or whether other factors were involved:).\n\nAlthough the actual word \"\"cryptanalysis\"\" is relatively recent (it was coined by William Friedman in 1920), methods for breaking codes and ciphers are much older. The first known recorded explanation of cryptanalysis was given by 9th-century Arab polymath, Al-Kindi (also known as \"Alkindus\" in Europe), in \"A Manuscript on Deciphering Cryptographic Messages\". This treatise includes a description of the method of frequency analysis (Ibrahim Al-Kadi, 1992- ref-3). Italian scholar Giambattista della Porta was author of a seminal work on cryptanalysis \"\"De Furtivis Literarum Notis\".\"\n\nFrequency analysis is the basic tool for breaking most classical ciphers. In natural languages, certain letters of the alphabet appear more often than others; in English, \"E\" is likely to be the most common letter in any sample of plaintext. Similarly, the digraph \"TH\" is the most likely pair of letters in English, and so on. Frequency analysis relies on a cipher failing to hide these statistics. For example, in a simple substitution cipher (where each letter is simply replaced with another), the most frequent letter in the ciphertext would be a likely candidate for \"E\". Frequency analysis of such a cipher is therefore relatively easy, provided that the ciphertext is long enough to give a reasonably representative count of the letters of the alphabet that it contains.\n\nIn Europe during the 15th and 16th centuries, the idea of a polyalphabetic substitution cipher was developed, among others by the French diplomat Blaise de Vigenère (1523–96). For some three centuries, the Vigenère cipher, which uses a repeating key to select different encryption alphabets in rotation, was considered to be completely secure (\"le chiffre indéchiffrable\"—\"the indecipherable cipher\"). Nevertheless, Charles Babbage (1791–1871) and later, independently, Friedrich Kasiski (1805–81) succeeded in breaking this cipher. During World War I, inventors in several countries developed rotor cipher machines such as Arthur Scherbius' Enigma, in an attempt to minimise the repetition that had been exploited to break the Vigenère system.\n\nCryptanalysis of enemy messages played a significant part in the Allied victory in World War II. F. W. Winterbotham, quoted the western Supreme Allied Commander, Dwight D. Eisenhower, at the war's end as describing Ultra intelligence as having been \"decisive\" to Allied victory. Sir Harry Hinsley, official historian of British Intelligence in World War II, made a similar assessment about Ultra, saying that it shortened the war \"by not less than two years and probably by four years\"; moreover, he said that in the absence of Ultra, it is uncertain how the war would have ended.\n\nIn practice, frequency analysis relies as much on linguistic knowledge as it does on statistics, but as ciphers became more complex, mathematics became more important in cryptanalysis. This change was particularly evident before and during World War II, where efforts to crack Axis ciphers required new levels of mathematical sophistication. Moreover, automation was first applied to cryptanalysis in that era with the Polish Bomba device, the British Bombe, the use of punched card equipment, and in the Colossus computers — the first electronic digital computers to be controlled by a program.\n\nWith reciprocal machine ciphers such as the Lorenz cipher and the Enigma machine used by Nazi Germany during World War II, each message had its own key. Usually, the transmitting operator informed the receiving operator of this message key by transmitting some plaintext and/or ciphertext before the enciphered message. This is termed the \"indicator\", as it indicates to the receiving operator how to set his machine to decipher the message.\n\nPoorly designed and implemented indicator systems allowed first Polish cryptographers and then the British cryptographers at Bletchley Park to break the Enigma cipher system. Similar poor indicator systems allowed the British to identify \"depths\" that led to the diagnosis of the Lorenz SZ40/42 cipher system, and the comprehensive breaking of its messages without the cryptanalysts seeing the cipher machine.\n\nSending two or more messages with the same key is an insecure process. To a cryptanalyst the messages are then said to be \"\"in depth.\"\" This may be detected by the messages having the same \"indicator\" by which the sending operator informs the receiving operator about the key generator initial settings for the message.\n\nGenerally, the cryptanalyst may benefit from lining up identical enciphering operations among a set of messages. For example, the Vernam cipher enciphers by bit-for-bit combining plaintext with a long key using the \"exclusive or\" operator, which is also known as \"modulo-2 addition\" (symbolized by ⊕ ):\nDeciphering combines the same key bits with the ciphertext to reconstruct the plaintext:\n(In modulo-2 arithmetic, addition is the same as subtraction.) When two such ciphertexts are aligned in depth, combining them eliminates the common key, leaving just a combination of the two plaintexts:\nThe individual plaintexts can then be worked out linguistically by trying \"probable words\" (or phrases), also known as \"\"cribs,\"\" at various locations; a correct guess, when combined with the merged plaintext stream, produces intelligible text from the other plaintext component:\nThe recovered fragment of the second plaintext can often be extended in one or both directions, and the extra characters can be combined with the merged plaintext stream to extend the first plaintext. Working back and forth between the two plaintexts, using the intelligibility criterion to check guesses, the analyst may recover much or all of the original plaintexts. (With only two plaintexts in depth, the analyst may not know which one corresponds to which ciphertext, but in practice this is not a large problem.) When a recovered plaintext is then combined with its ciphertext, the key is revealed:\nKnowledge of a key of course allows the analyst to read other messages encrypted with the same key, and knowledge of a set of related keys may allow cryptanalysts to diagnose the system used for constructing them.\n\nEven though computation was used to great effect in Cryptanalysis of the Lorenz cipher and other systems during World War II, it also made possible new methods of cryptography orders of magnitude more complex than ever before. Taken as a whole, modern cryptography has become much more impervious to cryptanalysis than the pen-and-paper systems of the past, and now seems to have the upper hand against pure cryptanalysis. The historian David Kahn notes:\n\nKahn goes on to mention increased opportunities for interception, bugging, side channel attacks, and quantum computers as replacements for the traditional means of cryptanalysis. In 2010, former NSA technical director Brian Snow said that both academic and government cryptographers are \"moving very slowly forward in a mature field.\"\n\nHowever, any postmortems for cryptanalysis may be premature. While the effectiveness of cryptanalytic methods employed by intelligence agencies remains unknown, many serious attacks against both academic and practical cryptographic primitives have been published in the modern era of computer cryptography:\n\n\nThus, while the best modern ciphers may be far more resistant to cryptanalysis than the Enigma, cryptanalysis and the broader field of information security remain quite active.\n\n\nAsymmetric cryptography (or public key cryptography) is cryptography that relies on using two (mathematically related) keys; one private, and one public. Such ciphers invariably rely on \"hard\" mathematical problems as the basis of their security, so an obvious point of attack is to develop methods for solving the problem. The security of two-key cryptography depends on mathematical questions in a way that single-key cryptography generally does not, and conversely links cryptanalysis to wider mathematical research in a new way.\n\nAsymmetric schemes are designed around the (conjectured) difficulty of solving various mathematical problems. If an improved algorithm can be found to solve the problem, then the system is weakened. For example, the security of the Diffie–Hellman key exchange scheme depends on the difficulty of calculating the discrete logarithm. In 1983, Don Coppersmith found a faster way to find discrete logarithms (in certain groups), and thereby requiring cryptographers to use larger groups (or different types of groups). RSA's security depends (in part) upon the difficulty of integer factorization — a breakthrough in factoring would impact the security of RSA.\n\nIn 1980, one could factor a difficult 50-digit number at an expense of 10 elementary computer operations. By 1984 the state of the art in factoring algorithms had advanced to a point where a 75-digit number could be factored in 10 operations. Advances in computing technology also meant that the operations could be performed much faster, too. Moore's law predicts that computer speeds will continue to increase. Factoring techniques may continue to do so as well, but will most likely depend on mathematical insight and creativity, neither of which has ever been successfully predictable. 150-digit numbers of the kind once used in RSA have been factored. The effort was greater than above, but was not unreasonable on fast modern computers. By the start of the 21st century, 150-digit numbers were no longer considered a large enough key size for RSA. Numbers with several hundred digits were still considered too hard to factor in 2005, though methods will probably continue to improve over time, requiring key size to keep pace or other methods such as elliptic curve cryptography to be used.\n\nAnother distinguishing feature of asymmetric schemes is that, unlike attacks on symmetric cryptosystems, any cryptanalysis has the opportunity to make use of knowledge gained from the public key.\n\n\n\nQuantum computers, which are still in the early phases of research, have potential use in cryptanalysis. For example, Shor's Algorithm could factor large numbers in polynomial time, in effect breaking some commonly used forms of public-key encryption.\n\nBy using Grover's algorithm on a quantum computer, brute-force key search can be made quadratically faster. However, this could be countered by doubling the key length.\n\n\n\n\n\n"}
{"id": "5716", "url": "https://en.wikipedia.org/wiki?curid=5716", "title": "Chicano", "text": "Chicano\n\nChicano or Chicana is a chosen identity of some Mexican Americans in the United States. The term \"Chicano\" is sometimes used interchangeably with \"Mexican-American\". Both names are chosen identities within the Mexican-American community in the United States; however, these terms have a wide range of meanings in various parts of the Southwest. The term became widely used during the Chicano Movement by Mexican Americans to express pride in a shared cultural, ethnic and community identity.\n\nThe term \"Chicano\" had negative connotations before the Chicano Movement, and still is viewed negatively and archaic by more conservative members of this community. Over time, it has gained some acceptance as an identity of pride within the Mexican-American community in the United States.\n\nThe pro-indigenous/Mestizo nature of Chicano nationalism is cemented in the idea of mestizaje. Ultimately, it was the experience of Mexican Americans in the United States which culminated in the creation of a Chicano identity.\n\nThe Chicano poet and writer Tino Villanueva traced the first documented use of the term as an ethnonym to 1911, as referenced in a then-unpublished essay by University of Texas anthropologist José Limón. Linguists Edward R. Simmen and Richard F. Bauerle report the use of the term in an essay by Mexican-American writer, Mario Suárez, published in the \"Arizona Quarterly\" in 1947.\n\nIn 1857, a gunboat, the \"Chicana\", was sold to Jose Maria Carvajal to ship arms on the Rio Grande. The King and Kenedy firm submitted a voucher to the Joint Claims Commission of the United States in 1870 to cover the costs of this gunboat's conversion from a passenger steamer. No particular explanation of the boat's name is known.\n\nThe origin of the word \"chicano\" is disputed. Some claim it is a shortened form of \"Mexicano\" (from the Nahuatl name for a member of the Mexica, the indigenous Aztec people of Anahuac, the Valley of Mexico). The name \"Mexica\" as spoken in its original Nahuatl, and \"Mexico\" by the Spaniards at the time of the Conquistadors, was pronounced originally with a [] and was transcribed with an \"x\" during this time period. According to this etymological hypothesis, the difference between the pronunciation and spelling of \"chicano\" and \"mexicano\" stems from the fact that the modern-day Spanish language experienced a change in pronunciation regarding a majority of words containing the \"x\" (for example: México, Ximenez, Xavier, Xarabe). In most cases the has been replaced with [] and a change of spelling (\"x\" to \"j\", though this has not been done to \"Mexico\" and various other proper names). The word \"Chicano\" would have also been affected by this change. Many Chicanos replace the \"ch\" with the letter \"x\", forming \"Xicano\", due to the original spelling of the Mexica Empire. In the United States, some Mexican-Americans choose the \"Xicano\" spelling to emphasize their indigenous ancestry.\n\nIn Mexico's indigenous regions, mestizos and Westernized natives are referred to as \"mexicanos\", referring to the modern nation, rather than the ' (village or tribal) identification of the speaker, be it Mayan, Zapotec, Mixtec, Huasteco, or any of hundreds of other indigenous groups. Thus, a newly emigrated Nahuatl speaker in an urban center might referred to his cultural relatives in this country, different from himself, as ', shortened to \"chicanos\".\n\nThe \"Handbook of Texas\" combines the two ideas:\nSome believe that the early 20th-century Hispanic Texan epithet \"chicamo\" shifted into \"chicano\" to reflect the grammatical conventions of Spanish-language ethno- and demonyms, such as \"americano\", \"castellano\", and \"peruano\". However, Chicanos generally do not agree that \"chicamo\" was ever a word used within the culture, as its assertion is thus far entirely unsubstantiated. Therefore, most self-identifying Chicanos do not agree that \"Chicano\" was ever derived from the word \"chicamo\".\n\nAnother hypothesis is that \"chicano\" derives from the indigenous population of Guanajuato, the Chichimecas, combined with the word \"Mexicano\". An alternative idea is that it is an altered form of \"Chilango\", meaning someone from Mexico City or Central Mexico (i.e. the highland states of México, Sinaloa, Jalisco, Puebla and Michoacán). A similar notion is that the word derives from Chichen Itza, the Mayan temple ruin and its associated culture in Mexico's Yucatán Peninsula. \"Chicano\" would thus be a Hispanized word for \"Chichen\" and Mayans, rather than the Aztec or Nahua people.\n\nChicanos, like many Mexicans, are Mestizos who have heritage of both indigenous American cultures and European, mainly Spanish, through colonization and immigration. The term \"Latino\" refers to a native or inhabitant of Latin America or a person of Latin American origin living in the U.S.\n\n\"Hispanic\" refers literally to Spain, but, in effect, to those of Spanish-speaking descent; therefore, the two terms are misnomers inasmuch as they apply only by extension to Chicanos, who may identify primarily as Amerindian or simply Mexican, and who may speak Amerindian languages (and English) as well as Spanish. The term was first brought up in the 1970s but it was not until the 1990s that the term was used on the U.S. Census. Since then it has widely been used by politicians and the media. The correct amalgamation is Latin American or Latin Americans, as coined by the Portuguese in the 17th century.\n\nThe term's meanings are highly debatable, but self-described Chicanos view the term as a positive, self-identifying social construction. Outside of Mexican-American communities, and even within them, \"Chicano\" has sometimes been considered pejorative by those who do not prefer the term. Regardless, its implications are subjective, but usually consist of one or more of the following elements.\n\nFrom a popular perspective, the term \"Chicano\" became widely visible outside of Chicano communities during the American civil rights movement. It was commonly used during the mid-1960s by Mexican-American activists, who, in attempts to assert their civil rights, tried to rid the word of its polarizing negative connotation by reasserting a unique ethnic identity and political consciousness, proudly identifying themselves as \"Chicanos\".\n\nAlthough the U.S. Federal Census Bureau provided no way for Mexican Americans or other Latinos to officially identify as a racial/ethnic category prior to 1980, when the broader-than-Mexican term \"Hispanic\" was first available as a self-identification in census forms, there is ample literary evidence to substantiate that \"Chicano\" is a long-standing endonym, as a large body of Chicano literature pre-dates the 1950s.\n\nAccording to the \"Handbook of Texas\":\n\nAt certain points in the 1970s, \"Chicano\" was the preferred term for reference to Mexican Americans, particularly in the scholarly literature. However, even though the term is politicized, its use fell out of favor as a means of referring to the entire population due to ignorance and due to the majority's attempt to impose Latino and Hispanic as misnomers. Because of this, \"Chicano\" has tended to refer to participants in Mexican-American activism. Sabine Ulibarrí, an author from Tierra Amarilla, New Mexico, once labeled \"Chicano\" as a politically \"loaded\" term, though later recanted that assessment.\n\nThe identity may be seen as uncertain. For example, in the 1991 Culture Clash play \"A Bowl of Beings\", in response to Che Guevara's demand for a definition of \"Chicano\", an \"armchair activist\" cries out, \"I still don't know!\". Juan Bruce-Novoa, a professor of Spanish and Portuguese at University of California, Irvine, wrote in 1990: \"A Chicano lives in the space between the hyphen in Mexican-American\".\n\nFor Chicanos, the term usually implies being \"neither from here, nor from there\" in reference to the US and Mexico. As a mixture of cultures from both countries, being Chicano represents the struggle of being institutionally acculturated into the Anglo-dominated society of the United States, while maintaining the cultural sense developed as a Latin-American cultured, US-born Mexican child.\n\nThe identity may be seen as native to the land, and distinct from a European identity, despite partial European descent. As exemplified through its extensive use within el Plan de Santa Bárbara, one of the primary documents responsible for the genesis of M.E.Ch.A. (Movimiento Estudiantil Chicanx de Aztlán), Chicano is used by many as a reference to their indigenous ancestry and roots. The last word in M.E.Ch.A., Aztlán, is a Mexica reference to an ancestral homeland which historians have speculated is somewhere in northern Mexico or the southwest of the US. M.E.Ch.A. is one example of how people have self-identified as Chicano as a means to identify with indigenous roots.\n\nAs Rubén Salazar put it in \"Who is a Chicano? And what is it the Chicanos want?\", a 1970 \"Los Angeles Times\" piece: \"A Chicano is a Mexican-American with a non-Anglo image of himself.\" According to Leo Limón: \"...a Chicano is ... an indigenous Mexican American\".\n\nChicano also has variations such as Chicanx, Xicano/a, Xicanx. The term Chicano because widely used during the Chicano Movement to express pride in cultural, ethnic and community identity. By replacing the 'ch' with the letter 'x', i.e. Xicano, is representing the original spelling of the Mexica Empire. Xicano is a First Generation way of spelling Chicano.\n\nThe Nahuatl language used hieroglyphics, the sound 'ch' written in Greco-Roman alphabet was put in place by the Spanish. Therefore, changing how the 'x' was used.\n\nReies Tijerina (who died on January 19, 2015) was a vocal claimant to the rights of Latin-Americans and Mexican Americans, and he remains a major figure of the early Chicano Movement. Of the term, he wrote: \"The Anglo press degradized the word 'Chicano'. They use it to divide us. We use it to unify ourselves with our people and with Latin America.\"\n\nLong a disparaging term in Mexico, the term \"Chicano\" gradually transformed from a class-based label of derision to one of ethnic pride and general usage within Mexican-American communities, beginning with the rise of the Chicano Movement in the 1960s. In their \"Latinas in the United States: A Historical Encyclopedia\", Vicki Ruíz and Virginia Sánchez report that demographic differences in the adoption of the term existed; because of the prior vulgar connotations, it was more likely to be used by males than females, and as well, less likely to be used among those in a higher socioeconomic status. Usage was also generational, with the more assimilated third-generation members (again, more likely male) likely to adopt the usage. This group was also younger, of more radical persuasion, and less-connected to a Mexican cultural heritage.\n\nIn his essay \"Chicanismo\" in \"The Oxford Encyclopedia of Mesoamerican Cultures\" (2002), José Cuéllar, a professor of Chicano studies at San Francisco State University, dates the transition from derisive to positive to the late 1950s, with a usage by young Mexican-American high school students.\n\nOutside of Mexican-American communities, the term might assume a negative meaning if it is used in a manner that embodies the prejudices and bigotries long directed at Mexican and Mexican-American people in the United States. For example, in one case, a prominent Chicana feminist writer and poet has indicated the following subjective meaning through her creative work.\n\nAna Castillo has referred to herself as a Chicana, and her literary work reflects that she primarily considers the term to be a positive one of self-determination and political solidarity.\n\nThe Mexican archeologist and anthropologist Manuel Gamio reported in 1930 that the term \"chicamo\" (with an \"m\") was used as a derogatory term used by Hispanic Texans for recently arrived Mexican immigrants displaced during the Mexican Revolution in the beginning of the early 20th century. At this time, the term \"Chicano\" began to reference those who resisted total assimilation, while the term \"Pochos\" referred (often pejoratively) to those who strongly advocated assimilation.\n\nIn Mexico, which by American standards would be considered class discrimination or racist, the term is associated with a Mexican-American person of low importance class and poor morals (similarly to the Spanish terms Cholo, Chulo and Majo). The term \"Chicano\" is widely known and used in Mexico.\n\nWhile some Mexican Americans may embrace the term \"Chicano\", others prefer to identify themselves as:\n\nWhen it comes to the use of loanwords, Romance-language orthographies, unlike French for example, do not use uppercase for non-name nouns, such as those used for nationalities or ethnic groups, of whatever sort – even Chicano/Chicana are best written with lowercase as \" in Spanish and related languages suchs Portuguese, Galician, and Catalan.\n\nSome of them might be used more commonly in English and others in Spanish: e.g. one might identify as a \"Mexican\" in a mixed American context, in which English would generally be expected, but to identify as part of the white/Euro-American demographic segment of the ethnic Mexican populations, in a strictly Mexican or Mexican-American context, in which one might be speaking Spanish.\n\nAnyone from the United States is referred to in Spanish as ' or '. Romance languages conserved the original standard (formerly shared with English) of counting the entire New World as a single America, as was the consensus in the Age of Discovery; to Spanish- and Portuguese-speakers in the Americas, they are just as \"americano\" as someone from Belgium would be European. Geological validation of the current English norm is bound by controversies and potential inconsistency, so the best explanation for both cases is mere tradition.\n\n' refers to the Mexicans of Northern Mexico as opposed to '. Mexican Americans do not refer to their shared identity as '. The only people who identify themselves as such are Mexicans from Northern Mexico which represents the whiter and relatively wealthier half of Mexico, compared to ' or southern Mexicans, more related in descent to the original Indigenous peoples of the continent and thus being the ones to actually have greater likelihood for an identity a bit closer to the militant Chicano one. Mainstream Spanish-language discourse does not treat the American Southwest as a contemporary part of Mexico (cultural, identitarian or otherwise), and the indigenist Chicano nationalism is hardly related at all to non-American Mexican desire for reconquering, an irredentist narrative of what might be perceived as a colonial state and collective mentality.\n\nMilitant Chicanos, regardless of their generational status, tend to connect their culture to the indigenous peoples of North America and to a nation of Aztlán, ignoring their European heritage. According to the Aztec legend, Aztlán is a region; Chicano nationalists have equated it with the Southwestern United States.\nSome historians may place Aztlán in Nayarit or the Caribbean while other historians entirely disagree, and make a distinction between legend and the contemporary socio-political ideology.\n\nMany currents came together to produce the revived Chicano political movement of the 1960s and 1970s. Early struggles were against school segregation, but the Mexican-American cause, or \" as it was called, soon came under the banner of the United Farm Workers and César Chávez. However, Corky Gonzales and Reies Tijerina stirred up old tensions about New Mexican land claims with roots going back to before the Mexican–American War. Simultaneous movements like the Young Lords, to empower youth, question patriarchy, democratize the Church, end police brutality, and end the Vietnam War, all intersected with other ethnic nationalist, peace, countercultural, and feminist movements.\n\nSince Chicanismo covers a wide array of political, religious and ethnic beliefs, and not everybody agrees with what exactly a Chicano is, most new Latino immigrants see it as a lost cause, as a lost culture, because Chicanos do not identify with Mexico or wherever their parents migrated from as new immigrants do. Chicanoism is an appreciation of a historical movement, but also is used by many to bring a new revived politicized feeling to voters young and old in the defense of Mexican and Mexican-American rights. People descended from Aztlan (both in the contemporary U.S. and in Mexico) use the Chicano ideology to create a platform for fighting for immigration reform and equality for all people.\n\nFor some, Chicano ideals involve a rejection of borders. The 1848 Treaty of Guadalupe Hidalgo transformed the Rio Grande region from a rich cultural center to a rigid border poorly enforced by the United States government. At the end of the Mexican–American War, 80,000 Spanish-Mexican-Indian people were forced into sudden U.S. habitation. As a result, Chicano identification is aligned with the idea of Aztlán, which extends to the Aztec period of Mexico, celebrating a time preceding land division.\n\nPaired with the dissipation of militant political efforts of the Chicano movement in the 1960s was the emergence of the Chicano generation. Like their political predecessors, the Chicano generation rejects the \"immigrant/foreigner\" categorization status. Chicano identity has expanded from its political origins to incorporate a broader community vision of social integration and nonpartisan political participation.\n\nThe shared Spanish language, Catholic faith, close contact with their political homeland (Mexico) to the south, a history of labor segregation, ethnic exclusion and racial discrimination encourage a united \"Chicano\" or Mexican folkloric tradition in the United States. Ethnic cohesiveness is a resistance strategy to assimilation and the accompanying cultural dissolution.\n\nMexican nationalists in Mexico, however, condemn the advocates of Chicanoism for attempting to create a new identity for the Mexican-American population, distinct from that of the Mexican nation. Chicanoism is embraced through personal identity especially within small rural communities that integrate the American culture connected to the Mexican heritage practiced in different parts of Mexico.\n\nThe term \"Chicano\" is also used to describe the literary, artistic, and musical movements that emerged with the Chicano Movement.\n\nChicano literature tends to focus on themes of identity, discrimination, and culture, with an emphasis on validating Mexican-American and Chicano culture in the United States. Rodolfo \"Corky\" Gonzales's \"Yo Soy Joaquin\" is one of the first examples of explicitly Chicano poetry, while José Antonio Villarreal's \"Pocho\" is widely recognized as the first major Chicano novel.\n\nThe novel \"Chicano\", by Richard Vasquez, was the first novel about Mexican Americans to be released by a major publisher (Doubleday, 1970). It was widely read in high schools and universities during the 1970s, and is now recognized as a breakthrough novel. Vasquez's social themes have been compared with those found in the work of Upton Sinclair and John Steinbeck.\n\nOther major names include Norma Elia Cantú, Rudolfo Anaya, Sandra Cisneros, Gary Soto, Sergio Troncoso, Rigoberto González, Raul Salinas, Oscar Zeta Acosta, Daniel Olivas, John Rechy, Ana Castillo, Denise Chávez, Benjamin Alire Sáenz, Luís Alberto Urrea, Dagoberto Gilb, Alicia Gaspar de Alba, Luis J. Rodriguez, Pat Mora, and Gloria Anzaldúa.\n\nIn the visual arts, works by Chicanos address similar themes as works in literature. The preferred media for Chicano art are murals and graphic arts. San Diego's Chicano Park, home to the largest collection of murals in the world, was created as an outgrowth of the city's political movement by Chicanos. Rasquache art is a unique style subset of the Chicano Arts movement.\n\nChicano art emerged in the mid-60s as a necessary component to the urban and agrarian civil rights movement in the Southwest, known as ', ', or the Chicano Renaissance. The artistic spirit, based on historical and traditional cultural evolution, within the movement has continued into the present millennium. There are artists, for example, who have chosen to do work within ancestral/historical references or who have mastered traditional techniques. Some artists and crafters have transcended the motifs, forms, functions, and context of Chicano references in their work but still acknowledge their identity as Chicano. These emerging artists are incorporating new materials to present mixed-media, digital media, and transmedia works.\n\nChicano performance art blends humor and pathos for tragicomic effect as shown by Los Angeles' comedy troupe Culture Clash and Mexican-born performance artist Guillermo Gómez-Peña and Nao Bustamante is a Chicana artist known internationally for her conceptual art pieces and as a participant in \"\", produced by Sarah Jessica Parker. Lalo Alcaraz often depicts the issues of Chicanos in his cartoon series called \"La Cucaracha\".\n\nOne of the most powerful and far-reaching cultural aspects of Chicano culture is the indigenous current that strongly roots Chicano culture to the American continent. It also unifies ' within the larger Pan-Indian Movement. Since its arrival in 1974, an art movement known as ' in the U.S., (and known by several names in its homeland of the central States of Mexico: ', ', \"\", and so on.) has had a deep impact in Chicano muralism, graphic design, tattoo art (flash), poetry, music, and literature. Lowrider cars also figure prominently as functional art in the Chicano community.\n\nLalo Guerrero has been lauded as the \"father of Chicano music\". Beginning in the 1930s, he wrote songs in the big band and swing genres that were popular at the time. He expanded his repertoire to include songs written in traditional genres of Mexican music, and during the farmworkers' rights campaign, wrote music in support of César Chávez and the United Farm Workers.\n\nJeffrey Lee Pierce of The Gun Club often spoke about being half Mexican and growing up with the Chicano culture.\n\nOther Chicano/Mexican-American singers include Selena, who sang a mixture of Mexican, Tejano, and American popular music, but died in 1995 at the age of 23; Zack de la Rocha, lead vocalist of Rage Against the Machine and social activist; and Los Lonely Boys, a Texas-style country rock band who have not ignored their Mexican-American roots in their music. In recent years, a growing Tex-Mex polka band trend influenced by the ' and ' music of Mexican immigrants, has in turn influenced much new Chicano folk music, especially on large-market Spanish language radio stations and on television music video programs in the U.S. Some of these artists, like the band Quetzal, are known for the political content of political songs.\n\nIn the 1950s, 1960s and 1970s, a wave of Chicano pop music surfaced through innovative musicians Carlos Santana, Johnny Rodriguez, Ritchie Valens and Linda Ronstadt. Joan Baez, who was also of Mexican-American descent, included Hispanic themes in some of her protest folk songs. Chicano rock is rock music performed by Chicano groups or music with themes derived from Chicano culture.\n\nThere are two undercurrents in Chicano rock. One is a devotion to the original rhythm and blues roots of Rock and roll including Ritchie Valens, Sunny and the Sunglows, and ? and the Mysterians. Groups inspired by this include Sir Douglas Quintet, Thee Midniters, Los Lobos, War, Tierra, and El Chicano, and, of course, the Chicano Blues Man himself, the late Randy Garribay.\n\nThe second theme is the openness to Latin American sounds and influences. Trini Lopez, Santana, Malo, Azteca, Toro, Ozomatli and other Chicano Latin rock groups follow this approach. Chicano rock crossed paths of other Latin rock genres (Rock en español) by Cubans, Puerto Ricans, such as Joe Bataan and Ralphi Pagan and South America (Nueva canción). Rock band The Mars Volta combines elements of progressive rock with traditional Mexican folk music and Latin rhythms along with Cedric Bixler-Zavala's Spanglish lyrics.\n\nChicano punk is a branch of Chicano rock. There were many bands that emerged from the California punk scene, including The Zeros, Bags, Los Illegals, The Brat, The Plugz, Manic Hispanic, and the Cruzados; as well as others from outside of California including Mydolls from Houston, Texas and Los Crudos from Chicago, Illinois. Some music historians argue that Chicanos of Los Angeles in the late 1970s might have independently co-founded punk rock along with the already-acknowledged founders from British-European sources when introduced to the US in major cities. The rock band ? and the Mysterians, which was composed primarily of Mexican-American musicians, was the first band to be described as punk rock. The term was reportedly coined in 1971 by rock critic Dave Marsh in a review of their show for \"Creem\" magazine.\n\nAlthough Latin jazz is most popularly associated with artists from the Caribbean (particularly Cuba) and Brazil, young Mexican Americans have played a role in its development over the years, going back to the 1930s and early 1940s, the era of the zoot suit, when young Mexican-American musicians in Los Angeles and San Jose, such as Jenni Rivera, began to experiment with \"\", a jazz-like fusion genre that has grown recently in popularity among Mexican Americans.\n\nChicano rap is a unique style of hip hop music which started with Kid Frost, who saw some mainstream exposure in the early 1990s. While Mellow Man Ace was the first mainstream rapper to use Spanglish, Frost's song \"La Raza\" paved the way for its use in American hip hop. Chicano rap tends to discuss themes of importance to young urban Chicanos. Some of today's Chicano artists include A.L.T., Lil Rob, Psycho Realm, Baby Bash, Serio, A Lighter Shade of Brown, and Funky Aztecs.\n\nPaula DeAnda, Frankie J, and Victor Ivan Santos (early member of the Kumbia Kings and associated with Baby Bash).\n\n\n\n"}
{"id": "5717", "url": "https://en.wikipedia.org/wiki?curid=5717", "title": "Canary Islands", "text": "Canary Islands\n\nThe Canary Islands (; , ) is a Spanish archipelago and the southernmost autonomous community of Spain located in the Atlantic Ocean, west of Morocco at the closest point. The Canaries are among the outermost regions (OMR) of the European Union proper. It is also one of the eight regions with special consideration of historical nationality recognized as such by the Spanish Government. The Canary Islands belong to the African Plate like the Spanish cities of Ceuta and Melilla, the two on the African mainland.\n\nThe seven main islands are (from largest to smallest in area) Tenerife, Fuerteventura, Gran Canaria, Lanzarote, La Palma, La Gomera and El Hierro. The archipelago includes much smaller islands and islets: La Graciosa, Alegranza, Isla de Lobos, Montaña Clara, Roque del Oeste and Roque del Este. It also includes a series of adjacent roques (those of Salmor, Fasnia, Bonanza, Garachico and Anaga). In ancient times, the island chain was often referred to as \"the Fortunate Isles\". The Canary Islands are the most southerly region of Spain and the largest and most populated archipelago of the Macaronesia region. Historically, the Canary Islands has been considered a bridge between four continents: Africa, North America, South America and Europe.\n\nThe archipelago's beaches, climate and important natural attractions, especially Maspalomas in Gran Canaria and Teide National Park and Mount Teide (a World Heritage Site) in Tenerife (the third tallest volcano in the world measured from its base on the ocean floor), make it a major tourist destination with over 12 million visitors per year, especially Gran Canaria, Tenerife, Fuerteventura and Lanzarote. The islands have a subtropical climate, with long hot summers and moderately warm winters. The precipitation levels and the level of maritime moderation vary depending on location and elevation. Green areas as well as desert exist on the archipelago. Due to their location above the temperature inversion layer, the high mountains of these islands are ideal for astronomical observation. For this reason, two professional observatories, Teide Observatory on the island of Tenerife and Roque de los Muchachos Observatory on the island of La Palma, have been built on the islands.\n\nIn 1927, the Province of Canary Islands was split into two provinces. The autonomous community of the Canary Islands was established in 1982. Its capital is shared by the cities of Santa Cruz de Tenerife and Las Palmas de Gran Canaria, which in turn are the capitals of the provinces of Santa Cruz de Tenerife and Las Palmas. Las Palmas de Gran Canaria has been the largest city in the Canaries since 1768, except for a brief period in the 1910s. Between the 1833 territorial division of Spain and 1927 Santa Cruz de Tenerife was the sole capital of the Canary Islands. In 1927 a decree ordered that the capital of the Canary Islands be shared, as it remains at present. The third largest city of the Canary Islands is San Cristóbal de La Laguna (a World Heritage Site) on Tenerife. This city is also home to the \"Consejo Consultivo de Canarias\", which is the supreme consultative body of the Canary Islands.\n\nDuring the time of the Spanish Empire, the Canaries were the main stopover for Spanish galleons on their way to the Americas, which came south to catch the prevailing northeasterly trade winds.\n\nThe name \"Islas Canarias\" is likely derived from the Latin name \"Canariae Insulae\", meaning \"Islands of the Dogs\", a name that was applied only to Gran Canaria. According to the historian Pliny the Elder, the Mauretanian king Juba II named the island \"Canaria\" because it contained \"vast multitudes of dogs of very large size\".\n\nAlternatively, it is said that the original inhabitants of the island, Guanches, used to worship dogs, mummified them and treated dogs generally as holy animals. The ancient Greeks also knew about a people, living far to the west, who are the \"dog-headed ones\", who worshipped dogs on an island. Some hypothesize that the Canary Islands dog-worship and the ancient Egyptian cult of the dog-headed god, Anubis are closely connected but there is no explanation given as to which one was first.\n\nOther theories speculate that the name comes from the Nukkari Berber tribe living in the Moroccan Atlas, named in Roman sources as \"Canarii\", though Pliny again mentions the relation of this term with dogs.\n\nThe connection to dogs is retained in their depiction on the islands' coat-of-arms.\n\nIt is considered that the aborigines of Gran Canaria called themselves \"Canarios\". It is possible that after being conquered, this name was used in plural in Spanish, i.e., as to refer to all of the islands as the Canarii-as.\n\nWhat is certain is that the name of the islands does not derive from the canary bird; rather, the birds are named after the islands.\n\nTenerife is the largest and most populous island of the archipelago. Gran Canaria, with 865,070 inhabitants, is both the Canary Islands' second most populous island, and the third most populous one in Spain after Majorca. The island of Fuerteventura is the second largest in the archipelago and located from the African coast.\n\nThe islands form the Macaronesia ecoregion with the Azores, Cape Verde, Madeira, and the Savage Isles. The Canary Islands is the largest and most populated archipelago of the Macaronesia region. The archipelago consists of seven large and several smaller islands, all of which are volcanic in origin.\n\nAccording to the position of the islands with respect to the north-east trade winds, the climate can be mild and wet or very dry. Several native species form laurisilva forests.\n\nAs a consequence, the individual islands in the Canary archipelago tend to have distinct microclimates. Those islands such as El Hierro, La Palma and La Gomera lying to the west of the archipelago have a climate which is influenced by the moist Canary Current. They are well vegetated even at low levels and have extensive tracts of sub-tropical laurisilva forest. As one travels east toward the African coast, the influence of the current diminishes, and the islands become increasingly arid. Fuerteventura and Lanzarote, the islands which are closest to the African mainland, are effectively desert or semi desert. Gran Canaria is known as a \"continent in miniature\" for its diverse landscapes like Maspalomas and Roque Nublo. In terms of its climate Tenerife is particularly interesting. The north of the island lies under the influence of the moist Atlantic winds and is well vegetated, while the south of the island around the tourist resorts of Playa de las Americas and Los Cristianos is arid. The island rises to almost above sea level, and at altitude, in the cool relatively wet climate, forests of the endemic pine \"Pinus canariensis\" thrive. Many of the plant species in the Canary Islands, like the Canary Island pine and the dragon tree, \"Dracaena draco\" are endemic, as noted by Sabin Berthelot and Philip Barker Webb in their epic work, \"L'Histoire Naturelle des Îles Canaries\" (1835–50).\n\nThe climate is subtropical and desertic, moderated by the sea and in summer by the trade winds. There are a number of microclimates and the classifications range mainly from semi-arid to desert. According to the Köppen climate classification, the majority of the Canary Islands have a hot desert climate represented as BWh. There also exists a subtropical humid climate which is very influenced by the ocean in the middle of the islands of La Gomera, Tenerife and La Palma; where the laurisilva forests grow.\n\nThe seven major islands, one minor island, and several small islets were originally volcanic islands, formed by the Canary hotspot. The Canary Islands is the only place in Spain where volcanic eruptions have been recorded during the Modern Era, with some volcanoes still active (El Hierro, 2011).\nVolcanic islands such as those in the Canary chain often have steep ocean cliffs caused by catastrophic debris avalanches and landslides.\n\nThe Teide volcano on Tenerife is the highest mountain in Spain, and the third tallest volcano on Earth on a volcanic ocean island. All the islands except La Gomera have been active in the last million years; four of them (Lanzarote, Tenerife, La Palma and El Hierro) have historical records of eruptions since European discovery. The islands rise from Jurassic oceanic crust associated with the opening of the Atlantic. Underwater magmatism commenced during the Cretaceous, and reached the ocean's surface during the Miocene. The islands are considered as a distinct physiographic section of the Atlas Mountains province, which in turn is part of the larger African Alpine System division.\n\nIn the summer of 2011 a series of low-magnitude earthquakes occurred beneath El Hierro. These had a linear trend of northeast-southwest. In October a submarine eruption occurred about south of Restinga. This eruption produced gases and pumice, but no explosive activity was reported.\n\nThe following table shows the highest mountains in each of the islands:\n\nThe official natural symbols associated with Canary Islands are the bird \"Serinus canaria\" (canary) and the \"Phoenix canariensis\" palm.\n\nFour of Spain's thirteen national parks are located in the Canary Islands, more than any other autonomous community. Teide National Park is the most visited in Spain, and the oldest and largest within the Canary Islands. The parks are:\n\nThe Canary Islands have four national parks, of which two have been declared World Heritage Site by UNESCO, and the other two declared a World Biosphere Reserve, these national parks are:\n\nThe islands have 13 seats in the Spanish Senate. Of these, 11 seats are directly elected, 3 for Gran Canaria, 3 for Tenerife, 1 for each other island; 2 seats are indirectly elected by the regional Autonomous Government. The local government is presided over by Fernando Clavijo, the current President of the Canary Islands.\n\nThere are some pro-independence political parties, like the National Congress of the Canaries (CNC) and the Popular Front of the Canary Islands, but these parties are non-violent, and their popular support is almost insignificant, with no presence in either the autonomous parliament or the \"cabildos insulares\".\n\nAccording to \"Centro de Investigaciones Sociológicas\" (Sociological Research Center) in 2010, 43.5% of the population of the Canary Islands feels more Canarian than Spanish (37.6%), of which 7.6% only Canarian, compared to 5.4% that feels more Spanish than Canarian (2.4%) or only Spanish (3%). The most popular choice was of those who feel equally Spanish and Canarian, with 49.9%. With these data, one of the Canary recorded levels of identification with higher autonomy from Spain.\n\nThe Autonomous Community of the Canary Islands consists of two provinces, Las Palmas and Santa Cruz de Tenerife, whose capitals (Las Palmas de Gran Canaria and Santa Cruz de Tenerife) are capitals of the autonomous community. Each of the seven major islands is ruled by an island council named \"Cabildo Insular\".\n\nThe international boundary of the Canaries is the subject of dispute between Spain and Morocco. Morocco's official position is that international laws regarding territorial limits do not authorise Spain to claim seabed boundaries based on the territory of the Canaries, since the Canary Islands enjoy a large degree of autonomy. In fact, the islands do not enjoy any special degree of autonomy as each one of the Spanish regions is considered an autonomous community with equal status to the European ones. \n\nThe boundary determines the ownership of seabed oil deposits and other ocean resources. Morocco and Spain have been unable to agree on a compromise regarding the territorial boundary, since neither nation wants to cede its claimed right to the vast resources whose ownership depends upon the boundary. In 2002, for example, Morocco rejected a unilateral Spanish proposal.\n\nBefore the arrival of humans, the Canaries were inhabited by prehistoric animals; for example, the giant lizard (\"Gallotia goliath\"), the Tenerife and Gran Canaria giant rats, and giant prehistoric tortoises, \"Geochelone burchardi\" and \"Geochelone vulcanica\".\n\nThe islands may have been visited by the Phoenicians, the Greeks, and the Carthaginians. King Juba II, Caesar Augustus's Numidian protégé, is credited with discovering the islands for the Western world. According to Pliny the Elder, Juba found the islands uninhabited, but found \"a small temple of stone\" and \"some traces of buildings\". Juba dispatched a naval contingent to re-open the dye production facility at Mogador in what is now western Morocco in the early first century AD. That same naval force was subsequently sent on an exploration of the Canary Islands, using Mogador as their mission base.\n\nWhen the Europeans began to explore the islands in the late Middle Ages, they encountered several indigenous peoples living at a Neolithic level of technology. Although the prehistory of the settlement of the Canary Islands is still unclear, linguistic and genetic analyses seem to indicate that at least some of these inhabitants shared a common origin with the Berbers on the nearby North African coast. The precolonial inhabitants came to be known collectively as the Guanches, although \"Guanches\" had been the name for only the indigenous inhabitants of Tenerife. From the 14th century onward, numerous visits were made by sailors from Majorca, Portugal and Genoa. Lancelotto Malocello settled on Lanzarote in 1312. The Majorcans established a mission with a bishop in the islands that lasted from 1350–1400.\n\nIn 1402, the Castilian conquest of the islands began, with the expedition of the French explorers Jean de Béthencourt and Gadifer de la Salle, nobles and vassals of Henry III of Castile, to Lanzarote. From there, they went on to conquer Fuerteventura (1405) and El Hierro. Béthencourt received the title King of the Canary Islands, but still recognised King Henry III as his overlord. It was not a simple military enterprise, given the aboriginal resistance on some islands. Neither was it politically, since the particular interests of the nobility (determined to strengthen their economic and political power through the acquisition of the islands) conflicted with those of the states, particularly Castile, which were in the midst of territorial expansion and in a process of strengthening of the Crown against the nobility.\nHistorians distinguish two periods in the conquest of the Canary Islands:\n\nAristocratic conquest (\"Conquista señorial\"). This refers to the early conquests carried out by the nobility, for their own benefit and without the direct participation of the Crown of Castile, which merely granted rights of conquest in exchange for pacts of vassalage between the noble conqueror and the Crown. One can identify within this period an early phase known as the Betancurian or Norman Conquest, carried out by Jean de Bethencourt (who was originally from Normandy) and Gadifer de la Salle between 1402 and 1405, which involved the islands of Lanzarote, El Hierro and Fuerteventura. The subsequent phase is known as the Castilian Conquest, carried out by Castilian nobles who acquired, through purchases, assignments and marriages, the previously conquered islands and also incorporated the island of La Gomera around 1450.\n\nRoyal conquest (\"Conquista realenga\"). This defines the conquest between 1478 and 1496, carried out directly by the Crown of Castile, during the reign of the Catholic Monarchs, who armed and partly financed the conquest of those islands which were still to unconquered: Gran Canaria, La Palma and Tenerife. This phase of the conquest came to an end in the year 1496, with the dominion of the island of Tenerife, bringing the entire Canarian Archipelago under the control of the Crown of Castile.\nBéthencourt also established a base on the island of La Gomera, but it would be many years before the island was fully conquered. The natives of La Gomera, and of Gran Canaria, Tenerife, and La Palma, resisted the Castilian invaders for almost a century. In 1448 Maciot de Béthencourt sold the lordship of Lanzarote to Portugal's Prince Henry the Navigator, an action that was accepted by neither the natives nor the Castilians. Despite Pope Nicholas V ruling that the Canary Islands were under Portuguese control, the crisis swelled to a revolt which lasted until 1459 with the final expulsion of the Portuguese. In 1479, Portugal and Castile signed the Treaty of Alcáçovas, which settled disputes between Castile and Portugal over the control of the Atlantic. This treaty recognized Castilian control of the Canary Islands but also confirmed Portuguese possession of the Azores, Madeira, and the Cape Verde islands, and gave the Portuguese rights to any further islands or lands in the Atlantic that might be discovered.\n\nThe Castilians continued to dominate the islands, but due to the topography and the resistance of the native Guanches, they did not achieve complete control until 1496, when Tenerife and La Palma were finally subdued by Alonso Fernández de Lugo. After that, the Canaries were incorporated into the Kingdom of Castile.\n\nAfter the conquest, the Castilians imposed a new economic model, based on single-crop cultivation: first sugarcane; then wine, an important item of trade with England. In this era, the first institutions of colonial government were founded. Gran Canaria, a colony of the Crown of Castile since 6 March 1480 (from 1556, of Spain), and Tenerife, a Spanish colony since 1495, each had its own governor.\n\nThe cities of Santa Cruz de Tenerife and Las Palmas de Gran Canaria became a stopping point for the Spanish conquerors, traders, and missionaries on their way to the New World. This trade route brought great prosperity to some of the social sectors of the islands. The islands became quite wealthy and soon were attracting merchants and adventurers from all over Europe. Magnificent palaces and churches were built on La Palma during this busy, prosperous period. The Church of El Salvador survives as one of the island's finest examples of the architecture of the 16th century.\n\nThe Canaries' wealth invited attacks by pirates and privateers. Ottoman Turkish admiral and privateer Kemal Reis ventured into the Canaries in 1501, while Murat Reis the Elder captured Lanzarote in 1585.\n\nThe most severe attack took place in 1599, during the Dutch Revolt. A Dutch fleet of 74 ships and 12,000 men, commanded by Pieter van der Does, attacked the capital Las Palmas de Gran Canaria (the city had 3,500 of Gran Canaria's 8,545 inhabitants). The Dutch attacked the Castillo de la Luz, which guarded the harbor. The Canarians evacuated civilians from the city, and the Castillo surrendered (but not the city). The Dutch moved inland, but Canarian cavalry drove them back to Tamaraceite, near the city.\n\nThe Dutch then laid siege to the city, demanding the surrender of all its wealth. They received 12 sheep and 3 calves. Furious, the Dutch sent 4,000 soldiers to attack the Council of the Canaries, who were sheltering in the village of Santa Brígida. 300 Canarian soldiers ambushed the Dutch in the village of Monte Lentiscal, killing 150 and forcing the rest to retreat. The Dutch concentrated on Las Palmas de Gran Canaria, attempting to burn it down. The Dutch pillaged Maspalomas, on the southern coast of Gran Canaria, San Sebastián on La Gomera, and Santa Cruz on La Palma, but eventually gave up the siege of Las Palmas and withdrew.\n\nIn 1618 the Barbary pirates attacked Lanzarote and La Gomera taking 1000 captives to be sold as slaves. Another noteworthy attack occurred in 1797, when Santa Cruz de Tenerife was attacked by a British fleet under Horatio Nelson on 25 July. The British were repulsed, losing almost 400 men. It was during this battle that Nelson lost his right arm.\n\nThe sugar-based economy of the islands faced stiff competition from Spain's American colonies. Low prices in the sugar market in the 19th century caused severe recessions on the islands. A new cash crop, cochineal (\"cochinilla\"), came into cultivation during this time, saving the islands' economy. During this time the Canarian-American trade was developed, in which Canarian products such as cochineal, sugarcane and rum are sold in American ports, such as Veracruz, Campeche, La Guaira and Havana among others.\n\nBy the end of the 18th century, Canary Islanders had already emigrated to Spanish American territories, such as Havana, Veracruz, Santo Domingo, San Antonio, Texas and St. Bernard Parish, Louisiana. These economic difficulties spurred mass emigration, primarily to the Americas, during the 19th and first half of the 20th century. Between 1840 and 1890 as many as 40,000 Canary Islanders emigrated to Venezuela. Also, thousands of Canarians moved to Puerto Rico where the Spanish monarchy felt that Canarians would adapt to island life better than other immigrants from the mainland of Spain. Deeply entrenched traditions, such as the Mascaras Festival in the town of Hatillo, Puerto Rico, are an example of Canarian culture still preserved in Puerto Rico. Similarly, many thousands of Canarians emigrated to the shores of Cuba. During the Spanish–American War of 1898, the Spanish fortified the islands against a possible American attack, but no such event took place.\n\nSirera and Renn (2004) distinguish two different types of expeditions, or voyages, during the period 1770–1830, which they term \"the Romantic period\":\n\nFirst are \"expeditions financed by the States, closely related with the official scientific Institutions. characterised by having strict scientific objectives (and inspired by) the spirit of Illustration and progress\". In this type of expedition, Sirera and Renn include the following travellers:\n\nThe second type of expedition identified by Sirera and Renn is one that took place starting from more or less private initiatives. Among these, the key exponents were the following:\n\nSirera and Renn identify the period 1770–1830 as one in which \"In a panorama dominated until that moment by France and England enters with strength and brio Germany of the Romantic period whose presence in the islands will increase\".\n\nAt the beginning of the 20th century, the British introduced a new cash-crop, the banana, the export of which was controlled by companies such as Fyffes.\n\nThe rivalry between the elites of the cities of Las Palmas de Gran Canaria and Santa Cruz de Tenerife for the capital of the islands led to the division of the archipelago into two provinces in 1927. This has not laid to rest the rivalry between the two cities, which continues to this day.\n\nDuring the time of the Second Spanish Republic, Marxist and anarchist workers' movements began to develop, led by figures such as Jose Miguel Perez and Guillermo Ascanio. However, outside of a few municipalities, these organisations were a minority and fell easily to Nationalist forces during the Spanish Civil War.\n\nIn 1936, Francisco Franco was appointed General Commandant of the Canaries. He joined the military revolt of 17 July which began the Spanish Civil War. Franco quickly took control of the archipelago, except for a few points of resistance on La Palma and in the town of Vallehermoso, on La Gomera. Though there was never a war in the islands, the post-war suppression of political dissent on the Canaries was most severe.\n\nDuring the Second World War, Winston Churchill prepared plans for the British seizure of the Canary Islands as a naval base, in the event of Gibraltar being invaded from the Spanish mainland.\n\nOpposition to Franco's regime did not begin to organise until the late 1950s, which experienced an upheaval of parties such as the Communist Party of Spain and the formation of various nationalist, leftist parties.\n\nAfter the death of Franco, there was a pro-independence armed movement based in Algeria, the Movement for the Independence and Self-determination of the Canaries Archipelago (MAIAC). In 1968, the Organisation of African Unity recognized the MAIAC as a legitimate African independence movement, and declared the Canary Islands as an African territory still under foreign rule.\n\nAfter the establishment of a democratic constitutional monarchy in Spain, autonomy was granted to the Canaries via a law passed in 1982, with a newly established autonomous devolved government and parliament. In 1983, the first autonomous elections were held. The Spanish Socialist Workers' Party (PSOE) won. In the 2007 elections, the PSOE gained a plurality of seats, but the nationalist Canarian Coalition and the conservative Partido Popular (PP) formed a ruling coalition government.\n\nAt present, the Canary Islands is the only autonomous community in Spain that has two capitals: Santa Cruz de Tenerife and Las Palmas de Gran Canaria, since the Statute of Autonomy of the Canary Islands was created in 1982.\n\nThe political capital of the archipelago does not exist as such until the nineteenth century. The first cities founded by the Europeans at the time of the conquest of the Canary Islands in the 15th century were: Telde (in Gran Canaria), San Marcial del Rubicón (in Lanzarote) and Betancuria (in Fuerteventura). These cities boasted the first European institutions present in the archipelago, including Catholic bishoprics. Although, because the period of splendor of these cities developed before the total conquest of the archipelago and its incorporation into the Crown of Castile never had a political and real control of the entire Canary archipelago.\n\nThe function of a Canarian city with full jurisdiction for the entire archipelago only exists after the conquest of the Canary Islands, although originally \"De facto\", that is, without legal and real meaning and linked to the headquarters of the Canary Islands General Captaincy.\n\nLas Palmas de Gran Canaria was the first city that exercised this function. This is because the residence of the Captain General of the Canary Islands was in this city during part of the sixteenth and seventeenth centuries. In May 1661, the Captain General of the Canary Islands, Jerónimo de Benavente y Quiñones, moved the headquarters of the captaincy to the city of San Cristóbal de La Laguna on the island of Tenerife. This was due to the fact that this island since the conquest was the most populated, productive and with the highest economic expectations. La Laguna would be considered the \"De facto\" capital of the archipelago until mid the official status of the capital of Canary Islands in the city of Santa Cruz de Tenerife was confirmed in the 19th century, due in part to the constant controversies and rivalries between the bourgeoisies of San Cristóbal de La Laguna and Las Palmas de Gran Canaria for the economic, political and institutional hegemony of the archipelago.\n\nAlready in 1723 the Captain General of the Canary Islands Lorenzo Fernandez de Villavicencio had moved the headquarters of the General Captaincy of the Canary Islands from San Cristóbal de La Laguna to Santa Cruz de Tenerife. This decision continued without pleasing the society of the island of Gran Canaria. It would be after the creation of the Province of Canary Islands in November of 1833 in which Santa Cruz would become the first fully official capital of the Canary Islands (\"De jure\" and not of \"De facto\" as happened previously). Santa Cruz de Tenerife would be the capital of the Canary archipelago until during the Government of General Primo de Rivera in 1927 the Province of Canary Islands was split in two provinces: Las Palmas with capital in Las Palmas de Gran Canaria, and Santa Cruz de Tenerife with capital in the homonymous city.\n\nFinally, with the Statute of Autonomy of the Canary Islands in 1982 and the creation of the Autonomous Community of the Canary Islands, the capital of the archipelago between Las Palmas de Gran Canaria and Santa Cruz de Tenerife is fixed, which is how it remains today.\n\nThe Canary Islands have a population of 2,127,685 inhabitants (2018), making it the eighth most populous of Spain's autonomous communities, with a density of 282.6 inhabitants per square kilometre. The total area of the archipelago is .\n\nThe Canarian population includes long-tenured residents and new waves of mainland Spanish immigrants, as well as Moroccan, Colombian, Venezuelan and Italian. Of the total Canarian population in 2009 (2,098,593) 1,799,373 were Spanish and 299,220 foreigners. Of these, the majority from the Americas, mainly Colombians (34,210), Venezuelans (31,468), Cubans (11,098) and Dominicans (10,159). There are also 51,136 African residents, mostly Moroccans (39,847).\n\nThe population of the islands according to the 2018 data are:\n\nThe Roman Catholic branch of Christianity has been the majority religion in the archipelago for more than five centuries, ever since the Conquest of the Canary Islands. However, there are other religious communities.\n\nThe overwhelming majority of native Canarians are Roman Catholic (85%) with various smaller foreign-born populations of other Christian beliefs such as Protestants from northern Europe.\n\nThe appearance of the Virgin of Candelaria (Patron of Canary Islands) was credited with moving the Canary Islands toward Christianity. Two Catholic saints were born in the Canary Islands: Peter of Saint Joseph de Betancur and José de Anchieta. Both born on the island of Tenerife, they were respectively missionaries in Guatemala and Brazil.\n\nThe Canary Islands are divided into two Catholic dioceses, each governed by a bishop:\n\nSeparate from the overwhelming Christian majority are a minority of Muslims. Among the followers of Islam, the Islamic Federation of the Canary Islands exists to represent the Islamic community in the Canary Islands as well as to provide practical support to members of the Islamic community.\n\nOther religious faiths represented include Jehovah’s Witnesses, The Church of Jesus Christ of Latter-day Saints as well as Hinduism. Minority religions are also present such as the Church of the Guanche People which is classified as a neo-pagan native religion. Also present are Buddhism, Judaism, Baha'i, Afro-American religion, and Chinese religions.\n\nThe distribution of beliefs in 2012 according to the CIS Barometer Autonomy was as follows:\n\nOrdered from west to east, the Canary Islands are El Hierro, La Palma, La Gomera, Tenerife, Gran Canaria, Fuerteventura and Lanzarote. In addition, north of the latter are the island of La Graciosa, the islets of Montaña Clara, Alegranza, Roque del Este and Roque del Oeste, belonging to the Chinijo Archipelago, and northeast of Fuerteventura is the islet of Lobos. There are also a series of small adjacent rocks in the Canary Islands: the Roques de Anaga, Garachico and Fasnia in Tenerife, and those of Salmor and Bonanza in El Hierro.\n\nEl Hierro, the westernmost island, covers , making it the smallest of the major islands, and the least populous with 10,798 inhabitants. The whole island was declared Reserve of the Biosphere in 2000. Its capital is Valverde. Also known as Ferro, it was once believed to be the westernmost land in the world.\n\nFuerteventura, with a surface of , is the second-most extensive island of the archipelago. It has been declared a Biosphere reserve by Unesco. It has a population of 113,275. Being also the most ancient of the islands, it is the one that is more eroded: its highest point is the Peak of the Bramble, at a height of . Its capital is Puerto del Rosario.\n\nGran Canaria has 846,717 inhabitants. The capital, Las Palmas de Gran Canaria (377,203 inhabitants), is the most populous city and shares the status of capital of the Canaries with Santa Cruz de Tenerife. Gran Canaria's surface area is . In center of the island lie the Roque Nublo and Pico de las Nieves (\"Peak of Snow\") . In the south of island are the Maspalomas Dunes (Gran Canaria), these are the biggest tourist attractions.\n\nLa Gomera has an area of and is the second least populous island with 21,136 inhabitants. Geologically it is one of the oldest of the archipelago. The insular capital is San Sebastian de La Gomera. Garajonay's National Park is located on the island.\n\nLanzarote is the easternmost island and one of the most ancient of the archipelago, and it has shown evidence of recent volcanic activity. It has a surface of , and a population of 149,183 inhabitants, including the adjacent islets of the Chinijo Archipelago. The capital is Arrecife, with 56,834 inhabitants.\n\nThe Chinijo Archipelago includes the islands La Graciosa, Alegranza, Montaña Clara, Roque del Este and Roque del Oeste. It has a surface of , and a population of 658 inhabitants all of them on La Graciosa. With , La Graciosa, is the smallest inhabited island of the Canaries, and the major island of the Chinijo Archipelago.\n\nLa Palma, with 81,863 inhabitants covering an area of , is in its entirety a biosphere reserve. It shows no recent signs of volcanic activity, even though the volcano Teneguía entered into eruption last in 1971. In addition, it is the second-highest island of the Canaries, with the Roque de los Muchachos as highest point. Santa Cruz de La Palma (known to those on the island as simply \"Santa Cruz\") is its capital.\n\nTenerife is, with its area of , the most extensive island of the Canary Islands. In addition, with 904,713 inhabitants it is the most populated island of the archipelago and Spain. Two of the islands' principal cities are located on it: The capital, Santa Cruz de Tenerife and San Cristóbal de La Laguna (a World Heritage Site). San Cristóbal de La Laguna, the second city of the island is home to the oldest university in the Canary Islands, the University of La Laguna. The Teide, with its is the highest peak of Spain and also a World Heritage Site. Tenerife is the site of the worst air disaster in the history of aviation, in which 583 people were killed in the collision of two Boeing 747s on 27 March 1977.\n\nThe economy is based primarily on tourism, which makes up 32% of the GDP. The Canaries receive about 12 million tourists per year. Construction makes up nearly 20% of the GDP and tropical agriculture, primarily bananas and tobacco, are grown for export to Europe and the Americas. Ecologists are concerned that the resources, especially in the more arid islands, are being overexploited but there are still many agricultural resources like tomatoes, potatoes, onions, cochineal, sugarcane, grapes, vines, dates, oranges, lemons, figs, wheat, barley, maize, apricots, peaches and almonds.\n\nThe economy is € 25 billion (2001 GDP figures). The islands experienced continuous growth during a 20-year period, up until 2001, at a rate of approximately 5% annually. This growth was fueled mainly by huge amounts of Foreign Direct Investment, mostly to develop tourism real estate (hotels and apartments), and European Funds (near €11 billion euro in the period from 2000 to 2007), since the Canary Islands are labelled Region Objective 1 (eligible for euro structural funds). Additionally, the EU allows the Canary Islands Government to offer special tax concessions for investors who incorporate under the Zona Especial Canaria (ZEC) regime and create more than five jobs.\n\nSpain gave permission in August 2014 for Repsol and its partners to explore oil and gas prospects off the Canary Islands, involving an investment of €7.5 billion over four years, to commence at the end of 2016. Repsol at the time said the area could ultimately produce 100,000 barrels of oil a day, which would meet 10 percent of Spain's energy needs. However, the analysis of samples obtained did not show the necessary volume nor quality to consider future extraction, and the project was scrapped.\n\nThe Canary Islands have great natural attractions, climate and beaches make the islands a major tourist destination, being visited each year by about 12 million people (11,986,059 in 2007, noting 29% of Britons, 22% of Spanish, not residents of the Canaries, and 21% of Germans). Among the islands, Tenerife has the largest number of tourists received annually, followed by Gran Canaria and Lanzarote. The archipelago's principal tourist attraction is the Teide National Park (in Tenerife) where the highest mountain in Spain and third largest volcano in the world (Mount Teide), receives over 2.8 million visitors annually.\n\nThe combination of high mountains, proximity to Europe, and clean air has made the Roque de los Muchachos peak (on La Palma island) a leading location for telescopes like the Grantecan.\n\nThe islands, as an autonomous region of Spain, are in the European Union and the Schengen Area. They are in the European Union Customs Union but outside the VAT area, Instead of VAT there is a local Sales Tax (IGIC) which has a general rate of 7%, an increased tax rate of 13.5%, a reduced tax rate of 3% and a zero tax rate for certain basic need products and services. Consequently, some products are subject to additional VAT if being exported from the islands into mainland Spain or the rest of the EU.\n\nCanarian time is Western European Time (WET) (or GMT; in summer one hour ahead of GMT). So Canarian time is one hour behind that of mainland Spain and the same as that of the UK, Ireland and Portugal all year round.\n\nIn 2017, 15,976,000 tourists visited the Canary Islands. The statistics per destination island were:\nThe Canary Islands have eight airports altogether, two of the main ports of Spain, and an extensive network of autopistas (highways) and other roads. For a road map see multimap.\n\nThere are large ferry boats that link islands as well as fast ferries linking most of the islands. Both types can transport large numbers of passengers and cargo (including vehicles). Fast ferries are made of aluminium and powered by modern and efficient diesel engines, while conventional ferries have a steel hull and are powered by heavy oil. Fast ferries travel relatively quickly (in excess of ) and are a faster method of transportation than the conventional ferry (some ). A typical ferry ride between La Palma and Tenerife may take up to eight hours or more while a fast ferry takes about two and a half hours and between Tenerife and Gran Canaria can be about one hour.\n\nThe largest airport is the Gran Canaria Airport. Tenerife has two airports, Tenerife North Airport and Tenerife South Airport. The island of Tenerife gathers the highest passenger movement of all the Canary Islands through its two airports. The two main islands (Tenerife and Gran Canaria) receive the greatest number of passengers. Tenerife 6,204,499 passengers and Gran Canaria 5,011,176 passengers.\n\nThe port of Las Palmas is first in freight traffic in the islands, while the port of Santa Cruz de Tenerife is the first fishing port with approximately 7,500 tons of fish caught, according to the Spanish government publication Statistical Yearbook of State Ports. Similarly, it is the second port in Spain as regards ship traffic, only surpassed by the Port of Algeciras Bay. The port's facilities include a border inspection post (BIP) approved by the European Union, which is responsible for inspecting all types of imports from third countries or exports to countries outside the European Economic Area. The port of Los Cristianos (Tenerife) has the greatest number of passengers recorded in the Canary Islands, followed by the port of Santa Cruz de Tenerife. The Port of Las Palmas is the third port in the islands in passengers and first in number of vehicles transported.\n\nThe SS America was beached at the Canary islands, in the nineties. However, the ocean liner broke apart after the course of several years and eventually sank beneath the surface.\n\nThe Tenerife Tram opened in 2007 and the only one in the Canary Islands, travelling between the cities of Santa Cruz de Tenerife and San Cristóbal de La Laguna. It is currently planned to have three lines in the Canary Islands (two in Tenerife and one in Gran Canaria). The planned Tren de Gran Canaria railway line will be from Las Palmas de Gran Canaria to Maspalomas (south).\n\n\n\nThe \"Servicio Canario de Salud\" is an autonomous body of administrative nature attached to the Ministry responsible for Health of the Government of the Canary Islands. The majority of the archipelago's hospitals belong to this organization:\n\n\nBefore the arrival of the Aborigines, the Canary Islands was inhabited by endemic animals, such as some extinct; giant lizards (\"Gallotia goliath\"), giant rats (\"Canariomys bravoi\" and \"Canariomys tamarani\") and giant tortoises (\"Geochelone burchardi\" and \"Geochelone vulcanica\"), among others.\n\nWith a range of habitats, the Canary Islands exhibit diverse plant species. The bird life includes European and African species, such as the black-bellied sandgrouse; and a rich variety of endemic (local) taxa including the:\n\nTerrestrial fauna includes geckos, wall lizards, and three endemic species of recently rediscovered and critically endangered giant lizard: the El Hierro giant lizard (or Roque Chico de Salmor giant lizard), La Gomera giant lizard, and La Palma giant lizard. Mammals include the Canarian shrew, Canary big-eared bat, the Algerian hedgehog (which may have been introduced) and the more recently introduced mouflon. Some endemic mammals, the lava mouse, Tenerife giant rat and Gran Canaria giant rat, are extinct, as are the Canary Islands quail, long-legged bunting, the eastern Canary Islands chiffchaff and the giant prehistoric tortoises; Geochelone burchardi and Geochelone vulcanica.\n\nThe marine life found in the Canary Islands is also varied, being a combination of North Atlantic, Mediterranean and endemic species. In recent years, the increasing popularity of both scuba diving and underwater photography have provided biologists with much new information on the marine life of the islands.\n\nFish species found in the islands include many species of shark, ray, moray eel, bream, jack, grunt, scorpionfish, triggerfish, grouper, goby, and blenny. In addition, there are many invertebrate species, including sponge, jellyfish, anemone, crab, mollusc, sea urchin, starfish, sea cucumber and coral.\n\nThere are a total of five different species of marine turtle that are sighted periodically in the islands, the most common of these being the endangered loggerhead sea turtle. The other four are the green sea turtle, hawksbill sea turtle, leatherback sea turtle and Kemp's ridley sea turtle. Currently, there are no signs that any of these species breed in the islands, and so those seen in the water are usually migrating. However, it is believed that some of these species may have bred in the islands in the past, and there are records of several sightings of leatherback sea turtle on beaches in Fuerteventura, adding credibility to the theory.\n\nMarine mammals include the large varieties of cetaceans including rare and not well-known species (see more details in the \"Marine life of the Canary Islands\"). Hooded seals have also been known to be vagrant in the Canary Islands every now and then. The Canary Islands were also formerly home to a population of the rarest pinniped in the world, the Mediterranean monk seal.\n\nSome holidays of those celebrated in the Canary Islands are international and national, others are regional holidays and others are of insular character. The official day of the autonomous community is Canary Islands Day on 30 May. The anniversary of the first session of the Parliament of the Canary Islands, based in the city of Santa Cruz de Tenerife, held on 30 May 1983, is commemorated with this day.\n\nThe common festive calendar throughout the Canary Islands is as follows:\n\nIn addition, each of the islands has an island festival in which it is a holiday only on that island in question. These are the festivities of island patrons saints of each island. Organized chronologically are:\n\nThe most famous festivals of the Canary Islands is the carnival. It is the most famous and international festival of the archipelago. The carnival is celebrated in all the islands and all its municipalities, perhaps the two busiest are those of the two Canarian capitals; the Carnival of Santa Cruz de Tenerife (\"Tourist Festival of International Interest\") and the Carnival of Las Palmas de Gran Canaria. It is celebrated on the streets between the months of February and March. But the rest of the islands of the archipelago have their carnivals with their own traditions among which stand out: The Festival of the Carneros of El Hierro, the Festival of the Diabletes of Teguise in Lanzarote, Los Indianos de La Palma, the Carnival of San Sebastián de La Gomera and the Carnival of Puerto del Rosario in Fuerteventura.\n\nA unique form of wrestling known as Canarian wrestling (\"lucha canaria\") has opponents stand in a special area called a \"terrero\" and try to throw each other to the ground using strength and quick movements.\n\nAnother sport is the \"game of the sticks\" where opponents fence with long sticks. This may have come about from the shepherds of the islands who would challenge each other using their long walking sticks.\n\nFurthermore, there is the shepherd's jump (\"salto del pastor\"). This involves using a long stick to vault over an open area. This sport possibly evolved from the shepherd's need to occasionally get over an open area in the hills as they were tending their sheep.\n\nThe two main football teams in the archipelago are: the CD Tenerife (founded in 1912) and UD Las Palmas (founded in 1949). As if the 2018/2019 season, both Tenerife and Las Palmas play in Liga Adelante. When in the same division, the clubs contest the Canary Islands derby. There are smaller clubs also playing in the mainland Spanish football league system, most notably UD Lanzarote and CD Laguna, although no other Canarian clubs have played in the top flight.\n\nThe mountainous terrain of the Canary Islands also caters to the growing popularity of ultra running and ultramarathons as host of annual competitive long-distance events including Transvulcania on La Palma, Transgrancanaria on Gran Canaria and the Half Marathon des Sables on Fuerteventura.\n\n\n\n\n\n\n\n"}
{"id": "5718", "url": "https://en.wikipedia.org/wiki?curid=5718", "title": "Chuck D", "text": "Chuck D\n\nCarlton Douglas Ridenhour (born August 1, 1960), known professionally as Chuck D, is an American rapper, author, and producer. As the leader of the rap group Public Enemy, he helped create politically and socially conscious hip hop music in the mid-1980s. \"The Source\" ranked him at No. 12 on their list of the Top 50 Hip-Hop Lyricists of All Time.\nRidenhour was born in Queens, New York. He began writing rhymes after the New York City blackout of 1977. After graduating from Roosevelt Junior-Senior High School, he went to Adelphi University on Long Island to study graphic design, where he met William Drayton (Flavor Flav). He received a B.F.A. from Adelphi in 1984 and later received an honorary doctorate from Adelphi in 2013.\n\nWhile at Adelphi, Ridenhour co-hosted hip hop radio show the \"Super Spectrum Mix Hour\" as Chuck D on Saturday nights at Long Island rock radio station WLIR, designed flyers for local hip-hop events, and drew a cartoon called \"Tales of the Skind\" for Adelphi student newspaper \"The Delphian\".\n\nUpon hearing Ridenhour's demo track \"Public Enemy Number One\", fledgling producer/upcoming music-mogul Rick Rubin insisted on signing him to his Def Jam label.\n\nTheir major label albums were \"Yo! Bum Rush the Show\" (1987), \"It Takes a Nation of Millions to Hold Us Back\" (1988), \"Fear of a Black Planet\" (1990), \"Apocalypse 91... The Enemy Strikes Black\" (1991), \"Greatest Misses\" (1992), and \"Muse Sick-n-Hour Mess Age\" (1994). They also released a full-length album soundtrack for the film \"He Got Game\" in 1998. Ridenhour also contributed (as Chuck D) to several episodes of the PBS documentary series \"The Blues\". He has appeared as a featured artist on many other songs and albums, having collaborated with artists such as Janet Jackson, Kool Moe Dee, The Dope Poet Society, Run–D.M.C., Ice Cube, Boom Boom Satellites, Rage Against the Machine, Anthrax, John Mellencamp and many others. In 1990, he appeared on \"Kool Thing\", a song by the alternative rock band Sonic Youth, and along with Flavor Flav, he sang on George Clinton's song \"Tweakin'\", which appears on his 1989 album \"The Cinderella Theory\". In 1993, he executive produced \"Got 'Em Running Scared\", an album by Ichiban Records group Chief Groovy Loo and the Chosen Tribe.\n\nIn 1996, Ridenhour released \"Autobiography of Mistachuck\" on Mercury Records. Chuck D made a rare appearance at the 1998 MTV Video Music Awards, presenting the Video Vanguard Award to the Beastie Boys, whilst commending their musicianship. In November 1998, he settled out of court with Christopher \"The Notorious B.I.G.\" Wallace's estate over the latter's sampling of his voice in the song \"Ten Crack Commandments\". The specific sampling is Ridenhour counting off the numbers one to nine on the track \"Shut 'Em Down\". He later described the decision to sue as \"stupid.\"\n\nIn September 1999, he launched a multi-format \"supersite\" on the web site Rapstation.com. A home for the vast global hip hop community, the site boasts a TV and radio station with original programming, many of hip hop's most prominent DJs, celebrity interviews, free MP3 downloads (the first was contributed by multi-platinum rapper Coolio), downloadable ringtones by ToneThis, social commentary, current events, and regular features on turning rap careers into a viable living. Since 2000, he has been one of the most vocal supporters of peer-to-peer file sharing in the music industry.\n\nHe loaned his voice to \"\" as DJ Forth Right MC for the radio station Playback FM. In 2000, he collaborated with Public Enemy's Gary G-Whiz and MC Lyte on the theme music to the television show \"Dark Angel\". He appeared with Henry Rollins in a cover of Black Flag's \"Rise Above\" for the album \"\". He was also featured on Z-Trip's album \"Shifting Gears\" on a track called \"Shock and Awe\"; a 12-inch of the track was released featuring artwork by Shepard Fairey. In 2008 he contributed a chapter to \"Sound Unbound: Sampling Digital Music and Culture\" (The MIT Press, 2008) edited by Paul D. Miller a.k.a. DJ Spooky, and also turned up on The Go! Team's album \"Proof of Youth\" on the track \"Flashlight Fight.\" He also fulfilled his childhood dreams of being a sports announcer by performing the play-by-play commentary in the video game \"NBA Ballers: Chosen One\" on Xbox 360 and PlayStation 3.\n\nIn 2009, Ridenhour wrote the foreword to the book \"The Love Ethic: The Reason Why You Can't Find and Keep Beautiful Black Love\" by Kamau and Akilah Butler. He also appeared on Brother Ali's album, \"Us\".\n\nIn March 2011, Chuck D re-recorded vocals with The Dillinger Escape Plan for a cover of \"Fight the Power\".\n\nChuck D duetted with Rock singer Meat Loaf on his 2011 album \"Hell in a Handbasket\" on the song \"Mad Mad World/The Good God Is a Woman and She Don't Like Ugly\".\n\nIn 2016 Chuck D joined the band Prophets of Rage along with B-Real and former members of Rage Against the Machine.\n\nChuck D is known for his powerful rapping voice - \"How to Rap\" says, \"Chuck D of Public Enemy has a powerful, resonant voice that is often acclaimed as one of the most distinct and impressive in hip-hop\". Chuck D says this was based on listening to Melle Mel and sportscasters such as Marv Albert.\n\nChuck D often comes up with a title for a song first and that he writes on paper, though he sometimes edits using a computer. He also prefers to not punch in vocals, and he prefers to not overdub vocals.\n\nRidenhour is politically active; he co-hosted \"Unfiltered\" on Air America Radio, testified before Congress in support of peer-to-peer MP3 sharing, and was involved in a 2004 rap political convention. He continues to be an activist, publisher, lecturer, and producer. Addressing the negative views associated with rap music, he co-wrote the essay book \"Fight the Power: Rap, Race, and Reality\", along with Yusuf Jah. He argues that \"music and art and culture is escapism, and escapism sometimes is healthy for people to get away from reality\", but sometimes the distinction is blurred and that's when \"things could lead a young mind in a direction.\" He also founded the record company Slam Jamz and acted as narrator in Kareem Adouard's short film \"Bling: Consequences and Repercussions\", which examines the role of conflict diamonds in bling fashion. Despite Chuck D and Public Enemy's success, Chuck D claims that popularity or public approval was never a driving motivation behind their work. He is admittedly skeptical of celebrity status, revealing in a 1999 interview with BOMB Magazine that, \"The key for the record companies is to just keep making more and more stars, and make the ones who actually challenge our way of life irrelevant. The creation of celebrity has clouded the minds of most people in America, Europe and Asia. It gets people off the path they need to be on as individuals.\" \n\nIn an interview with \"Le Monde\" published January 29, 2008, Chuck D stated that rap is devolving so much into a commercial enterprise, that the relationship between the rapper and the record label is that of slave to a master. He believes that nothing has changed for African-Americans since the debut of Public Enemy and, although he thinks that an Obama-Clinton alliance is great, he does not feel that the establishment will allow anything of substance to be accomplished. He also stated that French President Sarkozy is like any other European elite: he has profited through the murder, rape, and pillaging of those less fortunate and he refuses to allow equal opportunity for those men and women from Africa. In this article, he also defended a comment made by Professor Griff in the past that he says was taken out of context by the media. The real statement was a critique of the Israeli government and its treatment of the Palestinian people. Chuck D stated that it is Public Enemy's belief that all human beings are equal.\n\nIn an interview with the magazine \"N'Digo\" published in late June 2008, he spoke of today's mainstream urban music seemingly relishing the addictive euphoria of materialism and sexism, perhaps being the primary cause of many people harboring resentment towards the genre and its future. However he has expressed hope for its resurrection, saying \"It's only going to be dead if it doesn't talk about the messages of life as much as the messages of death and non-movement\", citing artists such as NYOil, M.I.A. and The Roots as socially conscious artists who push the envelope creatively. \"A lot of cats are out there doing it, on the Web and all over. They're just not placing their career in the hands of some major corporation.\"\n\nIn 2010, Chuck D released a track entitled \"Tear Down That Wall\". He said, \"I talked about the wall not only just dividing the U.S. and Mexico but the states of California, New Mexico and Texas. But Arizona, it's like, come on. Now they're going to enforce a law that talks about basically racial profiling.\"\n\nHe is on the board of the TransAfrica Forum, a Pan African organization that is focused on African, Caribbean and Latin American issues.\n\nHe is a pescetarian.\n\n\n\n\nStudio albums\n\nStudio albums\n\nStudio albums\n\nStudio EPs\n\nStudio albums\n\nCompilation albums\n\n"}
{"id": "5719", "url": "https://en.wikipedia.org/wiki?curid=5719", "title": "Cutaway (filmmaking)", "text": "Cutaway (filmmaking)\n\nIn film and video, a cutaway shot is the interruption of a continuously filmed action by inserting a view of something else. It is usually, although not always, followed by a cut back to the first shot, when the cutaway avoids a jump cut. The cutaway shot does not necessarily contribute any dramatic content of its own, but is used to help the editor assemble a longer sequence. For this reason, editors choose cutaway shots related to the main action, such as another action or object in the same location. For example, if the main shot is of a man walking down an alley, possible cutaways may include a shot of a cat on a nearby dumpster or a shot of a person watching from a window overhead.\n\nSimilarly, a cutaway scene is the interruption of a scene with the insertion of another scene, generally unrelated or only peripherally related to the original scene. The interruption is usually quick, and is usually, although not always, ended by a return to the original scene. The effect is of commentary to the original scene, frequently comic in nature.\n\nThe most common use of cutaway shots in dramatic films is to adjust the pace of the main action, to conceal the deletion of some unwanted part of the main shot, or to allow the joining of parts of two versions of that shot. For example, a scene may be improved by cutting a few frames out of an actor's pause; a brief view of a listener can help conceal the break. Or the actor may fumble some of his lines in a group shot; rather than discarding a good version of the shot, the director may just have the actor repeat the lines for a new shot, and cut to that alternate view when necessary.\n\nCutaways are also used often in older horror films in place of special effects. For example, a shot of a zombie getting its head cut off may, for instance, start with a view of an axe being swung through the air, followed by a close-up of the actor swinging it, then followed by a cut back to the now severed head. George A. Romero, creator of the Dead Series, and Tom Savini pioneered effects that removed the need for cutaways in horror films. 30 Rock would often use cutaway scenes to create visual humor, the Werewolf Bar Mitzvah scene taking three days to create for only five seconds of screen time. The animated television show \"Family Guy\" often uses cutaway gags as humor.\n\nIn news broadcasting and documentary work, the cutaway is used much as it would be in fiction. On location, there is usually just one camera to film an interview, and it's usually trained on the interviewee. Often there is also only one microphone. After the interview, the interviewer will usually repeat his questions while he himself is being filmed, with pauses as they act as if to listen to the answers. These shots can be used as cutaways. Cutaways to the interviewer, called noddies, can also be used to cover cuts.\n\n"}
{"id": "5721", "url": "https://en.wikipedia.org/wiki?curid=5721", "title": "Coma", "text": "Coma\n\nComa is a state of unconsciousness in which a person cannot be awakened; fails to respond normally to painful stimuli, light, or sound; lacks a normal wake-sleep cycle; and does not initiate voluntary actions. A person in a state of coma is described as being \"comatose\". A comatose state may derive from natural causes, or may be medically induced.\n\nA comatose person exhibits a complete absence of wakefulness and is unable to consciously feel, speak, hear, or move. Clinically, a coma can be defined as the inability to consistently follow a one-step command.  It can also be defined as a score of ≤ 8 on the Glasgow Coma Scale lasting ≥ 6 hours. For a patient to maintain consciousness, two important neurological components must function. The first is the cerebral cortex—the gray matter that forms the outer layer of the brain. The other is a structure located in the brainstem, called reticular activating system (RAS).\n\nInjury to either or both of these components is sufficient to cause a patient to experience a coma. The cerebral cortex is a group of tight, dense, \"gray matter\" composed of the nuclei of the neurons whose axons then form the \"white matter,\" and is responsible for perception, relay of the sensory input via the thalamic pathway, and many other neurological functions, including complex thinking.\n\nRAS, on the other hand, is a more primitive structure in the brainstem which includes the reticular formation (RF). The RAS area of the brain has two tracts, the ascending and descending tract. Made up of a system of acetylcholine-producing neurons, the ascending track, or ascending reticular activating system (ARAS), works to arouse and wake up the brain, from the RF, through the thalamus, and then finally to the cerebral cortex. A failure in ARAS functioning may thus lead to a coma.\n\nThe word is from the Greek \"koma\", meaning \"deep sleep\".\n\nGenerally, a person who is unable to voluntarily open the eyes, does not have a sleep-wake cycle, is unresponsive in spite of strong tactile (painful) or verbal stimuli, and who generally scores between 3 and 8 on the Glasgow Coma Scale is considered in a coma. Coma may have developed in humans as a response to injury to allow the body to pause bodily actions and heal the most immediate injuries before waking. It therefore could be a compensatory state in which the body's expenditure of energy is not superfluous. The severity and mode of onset of coma depends on the underlying cause. For instance, severe hypoglycemia (low blood sugar) or hypercapnia (increased carbon dioxide levels in the blood) initially cause mild agitation and confusion, but progress to obtundation, stupor, and finally, complete unconsciousness. In contrast, coma resulting from a severe traumatic brain injury or subarachnoid hemorrhage can be instantaneous. The mode of onset may therefore be indicative of the underlying cause.\n\nComa may result from a variety of conditions, including intoxication (such as drug abuse, overdose or misuse of over the counter medications, prescribed medication, or controlled substances), metabolic abnormalities, central nervous system diseases, acute neurologic injuries such as strokes or herniations, hypoxia, hypothermia, hypoglycemia, eclampsia or traumatic injuries such as head trauma caused by falls, drowning accidents, or vehicle collisions. It may also be deliberately induced by pharmaceutical agents during major neurosurgery, to preserve higher brain functions following brain trauma, or to save the patient from extreme pain during healing of injuries or diseases.\n\nForty percent of comatose states result from drug poisoning. Drugs damage or weaken the synaptic functioning in the ARAS and keep the system from properly functioning to arouse the brain. Secondary effects of drugs, which include abnormal heart rate and blood pressure, as well as abnormal breathing and sweating, may also indirectly harm the functioning of the ARAS and lead to a coma. Seizures and hallucinations have shown to also play a major role in ARAS malfunction. Given that drug poisoning is the cause for a large portion of patients in a coma, hospitals first test all comatose patients by observing pupil size and eye movement, through the vestibular-ocular reflex.\n\nThe second most common cause of coma, which makes up about 25% of comatose patients, is lack of oxygen, generally resulting from cardiac arrest. The Central Nervous System (CNS) requires a great deal of oxygen for its neurons. Oxygen deprivation in the brain, also known as hypoxia, causes neuronal extracellular sodium and calcium to decrease and intracellular calcium to increase, which harms neuron communication. Lack of oxygen in the brain also causes ATP exhaustion and cellular breakdown from cytoskeleton damage and nitric oxide production.\n\nTwenty percent of comatose states result from the side effects of a stroke. During a stroke, blood flow to part of the brain is restricted or blocked. An ischemic stroke, brain hemorrhage, or tumor may cause such cessation of blood flow. Lack of blood to cells in the brain prevents oxygen from getting to the neurons, and consequently causes cells to become disrupted and eventually die. As brain cells die, brain tissue continues to deteriorate, which may affect functioning of the ARAS.\n\nThe remaining 15% of comatose cases result from trauma, excessive blood loss, malnutrition, hypothermia, hyperthermia, abnormal glucose levels, and many other biological disorders.\n\nDiagnosis of coma is simple, but diagnosing the cause of the underlying disease process is often challenging. The first priority in treatment of a comatose patient is stabilization following the basic ABCs (standing for airway, breathing, and circulation). Once a person in a coma is stable, investigations are performed to assess the underlying cause. Investigative methods are divided into physical examination findings and imaging (such as CAT scan, MRI, etc.) and special studies (EEG, etc.)\n\nWhen an unconscious patient enters a hospital, the hospital utilizes a series of diagnostic steps to identify the cause of unconsciousness. According to Young, the following steps should be taken when dealing with a patient possibly in a coma:\n\nIn the initial assessment of coma, it is common to gauge the level of consciousness by spontaneously exhibited actions, response to vocal stimuli (\"Can you hear me?\"), and painful stimuli; this is known as the AVPU (alert, vocal stimuli, painful stimuli, unresponsive) scale. More elaborate scales, such as the Glasgow Coma Scale, quantify an individual's reactions such as eye opening, movement and verbal response on a scale; Glasgow Coma Scale (GCS) is an indication of the extent of brain injury varying from 3 (indicating severe brain injury and death) to a maximum of 15 (indicating mild or no brain injury).\n\nIn those with deep unconsciousness, there is a risk of asphyxiation as the control over the muscles in the face and throat is diminished. As a result, those presenting to a hospital with coma are typically assessed for this risk (\"airway management\"). If the risk of asphyxiation is deemed high, doctors may use various devices (such as an oropharyngeal airway, nasopharyngeal airway or endotracheal tube) to safeguard the airway.\n\nPhysical examination is critical after stabilization. It should include vital signs, a general portion dedicated to making observations about the patient's respiration (breathing pattern), body movements (if any), and of the patient's body habitus (physique); it should also include assessment of the brainstem and cortical function through special reflex tests such as the oculocephalic reflex test (doll's eyes test), oculovestibular reflex test (cold caloric test), nasal tickle, corneal reflex, and the gag reflex.\n\nVital signs in medicine are temperature (rectal is most accurate), blood pressure, heart rate (pulse), respiratory rate, and oxygen saturation. It should be easy to evaluate these vitals quickly to gain insight into a patient's metabolism, fluid status, heart function, vascular integrity, and tissue oxygenation.\n\nRespiratory pattern (breathing rhythm) is significant and should be noted in a comatose patient. Certain stereotypical patterns of breathing have been identified including Cheyne–Stokes, a form of breathing in which the patient's breathing pattern is described as alternating episodes of hyperventilation and apnea. This is a dangerous pattern and is often seen in pending herniations, extensive cortical lesions, or brainstem damage. Another pattern of breathing is apneustic breathing, which is characterized by sudden pauses of Inhalation and is due to a lesion of the pons. Ataxic breathing is irregular and is due to a lesion (damage) of the medulla.\n\nAssessment of posture and body habitus is the next step. It involves general observation about the patient's positioning. There are often two stereotypical postures seen in comatose patients. Decorticate posturing is a stereotypical posturing in which the patient has arms flexed at the elbow, and arms adducted toward the body, with both legs extended. Decerebrate posturing is a stereotypical posturing in which the legs are similarly extended (stretched), but the arms are also stretched (extended at the elbow). The posturing is critical since it indicates where the damage is in the central nervous system. A decorticate posturing indicates a lesion (a point of damage) at or above the red nucleus, whereas a decerebrate posturing indicates a lesion at or below the red nucleus. In other words, a decorticate lesion is closer to the cortex, as opposed to a decerebrate cortex that is closer to the brainstem.\n\nOculocephalic reflex also known as the doll's eye is performed to assess the integrity of the brainstem. Patient's eyelids are gently elevated and the cornea is visualized. The patient's head is then moved to the patient's left, to observe if the eyes stay or deviate toward the patient's right; same maneuver is attempted on the opposite side. If the patient's eyes move in a direction opposite to the direction of the rotation of the head, then the patient is said to have an intact brainstem. However, failure of both eyes to move to one side, can indicate damage or destruction of the affected side. In special cases, where only one eye deviates and the other does not, this often indicates a lesion (or damage) of the medial longitudinal fasciculus (MLF), which is a brainstem nerve tract. Caloric reflex test also evaluates both cortical and brainstem function; cold water is injected into one ear and the patient is observed for eye movement; if the patient's eyes slowly deviate toward the ear where the water was injected, then the brainstem is intact, however failure to deviate toward the injected ear indicates damage of the brainstem on that side. Cortex is responsible for a rapid nystagmus away from this deviated position and is often seen in patients who are conscious or merely lethargic.\n\nAn important part of the physical exam is also assessment of the cranial nerves. Due to the unconscious status of the patient, only a limited number of the nerves can be assessed. These include the cranial nerves number 2 (CN II), number 3 (CN III), number 5 (CN V), number 7 (CN VII), and cranial nerves 9 and 10 (CN IX, CN X). Gag reflex helps assess cranial nerves 9 and 10. Pupil reaction to light is important because it shows an intact retina, and cranial nerve number 2 (CN II); if pupils are reactive to light, then that also indicates that the cranial nerve number 3 (CN III) (or at least its parasympathetic fibers) are intact. Corneal reflex assess the integrity of cranial nerve number 7 (CN VII), and cranial nerve number 5 (CN V). Cranial nerve number 5 (CN V), and its ophthalmic branch (V) are responsible for the afferent arm of the reflex, and the cranial nerve number 7 (CN VII) also known a facial nerve, is responsible for the efferent arm, causing contraction of the muscle orbicularis oculi resulting in closing of the eyes.\n\nPupil assessment is often a critical portion of a comatose examination, as it can give information as to the cause of the coma; the following table is a technical, medical guideline for common pupil findings and their possible interpretations:\n\nImaging basically encompasses computed tomography (CAT or CT) scan of the brain, or MRI for example, and is performed to identify specific causes of the coma, such as hemorrhage in the brain or herniation of the brain structures. Special tests such as an EEG can also show a lot about the activity level of the cortex such as semantic processing, presence of seizures, and are important available tools not only for the assessment of the cortical activity but also for predicting the likelihood of the patient's awakening. The autonomous responses such as the skin conductance response may also provide further insight on the patient's emotional processing.\n\nWhen diagnosing any neurological condition, history and examination are fundamental. History is obtained by family, friends or EMS. The Glasgow Coma Scale is a helpful system used to examine and determine the depth of coma, track patients progress and predict outcome as best as possible. In general a correct diagnosis can be achieved by combining findings from physical exam, imaging, and history components and directs the appropriate therapy.\n\nA coma can be classified as (1) supratentorial (above Tentorium cerebelli), (2) infratentorial (below Tentorium cerebelli), (3) metabolic or (4) diffused. This classification is merely dependent on the position of the original damage that caused the coma, and does not correlate with severity or the prognosis.\nThe severity of coma impairment however is categorized into several levels. Patients may or may not progress through these levels. In the first level, the brain responsiveness lessens, normal reflexes are lost, the patient no longer responds to pain and cannot hear.\n\nThe Rancho Los Amigos Scale is a complex scale that has eight separate levels, and is often used in the first few weeks or months of coma while the patient is under closer observation, and when shifts between levels are more frequent.\n\nThe treatment hospitals use on comatose patients depends on both the severity and cause of the comatose state. Although the best treatment for comatose patients remains unknown, hospitals usually place comatose patients in an Intensive Care Unit (ICU) immediately. Attention must first be directed to maintaining the patient's respiration and circulation, using intubation and ventilation, administration of intravenous fluids or blood and other supportive care as needed. Once a patient is stable and no longer in immediate danger, the medical staff may concentrate on maintaining the health of patient’s physical state. The concentration is directed to preventing infections such as pneumonias, bedsores (decubitus ulcers), and providing balanced nutrition. Infections may appear from the patient not being able to move around, and being confined to the bed. The nursing staff moves the patient every 2–3 hours from side to side and depending on the state of consciousness sometimes to a chair. The goal is to move the patient as much as possible to try to avoid bedsores, atelectasis and pneumonia. Pneumonia can occur from the person’s inability to swallow leading to aspiration, lack of gag reflex or from feeding tube, (aspiration pneumonia). Physical therapy may also be used to prevent contractures and orthopedic deformities that would limit recovery for those patients who awaken from coma.\n\nA person in a coma may become restless, or seize and need special care to prevent them from hurting themselves. Medicine may be given to calm such individuals. Patients who are restless may also try to pull on tubes or dressings so soft cloth wrist restraints may be put on. Side rails on the bed should be kept up to prevent the patient from falling.\n\nMethods to wake comatose patients include reversing the cause of the coma (e.g., glucose shock if low sugar), giving medication to stop brain swelling, or inducing hypothermia. Inducing hypothermia on comatose patients provides one of the main treatments for patients after suffering from cardiac arrest. In this treatment, medical personnel expose patients to “external or intravascular cooling” at 32-34 °C for 24 hours; this treatment cools patients down about 2-3 °C less than normal body temperature. In 2002, Baldursdottir and her coworkers found that in the hospital, more comatose patients survived after induced hypothermia than patients that remained at normal body temperature. For this reason, the hospital chose to continue the induced hypothermia technique for all of its comatose patients that suffered from cardiac arrest.\n\nComa has a wide variety of emotional reactions from the family members of the affected patients, as well as the primary care givers taking care of the patients. Common reactions, such as desperation, anger, frustration, and denial are possible. The focus of the patient care should be on creating an amicable relationship with the family members or dependents of a comatose patient as well as creating a rapport with the medical staff.\n\nComas can last from several days to several weeks. In more severe cases a coma may last for over five weeks, while some have lasted as long as several years. After this time, some patients gradually come out of the coma, some progress to a vegetative state, and others die. Some patients who have entered a vegetative state go on to regain a degree of awareness. Others remain in a vegetative state for years or even decades (the longest recorded period being 42 years).\n\nThe outcome for coma and vegetative state depends on the cause, location, severity and extent of neurological damage. A deeper coma alone does not necessarily mean a slimmer chance of recovery, because some people in deep coma recover well while others in a so-called milder coma sometimes fail to improve.\n\nPeople may emerge from a coma with a combination of physical, intellectual, and psychological difficulties that need special attention. Recovery usually occurs gradually—patients acquire more and more ability to respond. Some patients never progress beyond very basic responses, but many recover full awareness. Regaining consciousness is not instant: in the first days, patients are only awake for a few minutes, and duration of time awake gradually increases. This is unlike the situation in many movies where people who awake from comas are instantly able to continue their normal lives. In reality, the coma patient awakes sometimes in a profound state of confusion, not knowing how they got there and sometimes suffering from dysarthria, the inability to articulate any speech, and with many other disabilities.\n\nPredicted chances of recovery are variable owing to different techniques used to measure the extent of neurological damage. All the predictions are based on statistical rates with some level of chance for recovery present: a person with a low chance of recovery may still awaken. Time is the best general predictor of a chance of recovery: after four months of coma caused by brain damage, the chance of partial recovery is less than 15%, and the chance of full recovery is very low.\n\nThe most common cause of death for a person in a vegetative state is secondary infection such as pneumonia, which can occur in patients who lie still for extended periods.\n\nThere are reports of patients coming out of coma after long periods of time. After 19 years in a minimally conscious state, Terry Wallis spontaneously began speaking and regained awareness of his surroundings.\n\nA brain-damaged man, trapped in a coma-like state for six years, was brought back to consciousness in 2003 by doctors who planted electrodes deep inside his brain. The method, called deep brain stimulation (DBS) successfully roused communication, complex movement and eating ability in the 38-year-old American man who suffered a traumatic brain injury. His injuries left him in a minimally conscious state (MCS), a condition akin to a coma but characterized by occasional, but brief, evidence of environmental and self-awareness that coma patients lack.\n\nComas lasting seconds to minutes result in post-traumatic amnesia (PTA) that lasts hours to days; recovery plateau occurs over days to weeks.\nComas that last hours to days result in PTA lasting days to weeks; recovery plateau occurs over months.\nComas lasting weeks result in PTA that lasts months; recovery plateau occurs over months to years.\n\nResearch by Dr. Eelco Wijdicks on the depiction of comas in movies was published in Neurology in May 2006. Dr. Wijdicks studied 30 films (made between 1970 and 2004) that portrayed actors in prolonged comas, and he concluded that only two films accurately depicted the state of a coma victim and the agony of waiting for a patient to awaken: \"Reversal of Fortune\" (1990) and \"The Dreamlife of Angels\" (1998). The remaining 28 were criticized for portraying miraculous awakenings with no lasting side effects, unrealistic depictions of treatments and equipment required, and comatose patients remaining muscular and tanned.\n\n"}
{"id": "5722", "url": "https://en.wikipedia.org/wiki?curid=5722", "title": "Call of Cthulhu (role-playing game)", "text": "Call of Cthulhu (role-playing game)\n\nCall of Cthulhu is a horror fiction role-playing game based on H. P. Lovecraft's story of the same name and the associated Cthulhu Mythos. The game, often abbreviated as \"CoC\", is published by Chaosium; it was first released in 1981 and is currently in its seventh edition, with many different versions released. It makes use of Chaosium's Basic Role-Playing (BRP) system, with special rules for Sanity.\n\nThe setting of \"Call of Cthulhu\" is a darker version of our world, based on H. P. Lovecraft's observation (from his essay, \"Supernatural Horror in Literature\") that \"The oldest and strongest emotion of mankind is fear, and the strongest kind of fear is fear of the unknown.\" The original game, first published in 1981, uses mechanics from Basic Role-Playing, and is set in the 1920s, the setting of many of Lovecraft's stories. Additional settings were developed in the \"Cthulhu by Gaslight\" supplement, a blend of occult and Holmesian mystery and mostly set in England during the 1890s, and modern/1980s conspiracy with \"Cthulhu Now\" and \"Delta Green\". More recent additions include 1000 AD (\"Cthulhu: Dark Ages\"), 23rd century (\"Cthulhu Rising\") and Ancient Roman times (\"Cthulhu Invictus\"). The protagonists may also travel to places that are not of this earth, represented in the Dreamlands (which can be accessed through dreams as well as being physically connected to the earth), to other planets, or into the voids of space. In keeping with the Lovecraftian theme, the gamemaster is called the Keeper of Arcane Lore, or simply the keeper, while player characters are called \"investigators\".\n\n\"CoC\" uses the Basic Role-Playing system first developed for \"RuneQuest\" and used in other Chaosium games. It is skill-based, with player characters getting better with their skills by succeeding at using them for as long as they stay functionally healthy and sane. They do not, however, gain hit points and do not become significantly harder to kill. The game does not use levels. \n\n\"CoC\" uses percentile dice (with a results ranging from 1 to 100) to determine success or failure. Every player statistic is intended to be compatible with the notion that there is a probability of success for a particular action given what the player is capable of doing. For example, an artist may have a 75% chance of being able to draw something (represented by having 75 in Art skill), and thus rolling a number under 75 would yield a success. Rolling or less of the skill level (1-15 in the example) would be a \"special success\" (or an \"impale\" for combat skills) and would yield some extra bonus to be determined by the keeper. For example, the artist character might draw especially well or especially fast, or catch some unapparent detail in the drawing.\n\nThe players take the roles of ordinary people drawn into the realm of the mysterious: detectives, criminals, scholars, artists, war veterans, etc. Often, happenings begin innocently enough, until more and more of the workings behind the scenes are revealed. As the characters learn more of the true horrors of the world and the irrelevance of humanity, their sanity (represented by \"Sanity Points\", abbreviated SAN) inevitably withers away. The game includes a mechanism for determining how damaged a character's sanity is at any given point; encountering the horrific beings usually triggers a loss of SAN points. To gain the tools they need to defeat the horrors – mystic knowledge and magic – the characters may end up losing some of their sanity, though other means such as pure firepower or simply outsmarting one's opponents also exist. \"CoC\" has a reputation as a game in which it is quite common for a player character to die in gruesome circumstances or end up in a mental institution. Eventual triumph of the players is not assumed.\n\nThe original conception of \"Call of Cthulhu\" was \"Dark Worlds\", a game commissioned by the publisher Chaosium but never published. Sandy Petersen contacted them regarding writing a supplement for their popular fantasy game \"RuneQuest\" set in Lovecraft's Dreamlands. He took over the writing of \"Call of Cthulhu\", and the game was released in 1981.\n\nSince Petersen's departure from Chaosium, continuing development of \"Call of Cthulhu\" passed to Lynn Willis, credited as co-author in the fifth and sixth editions, and more recently to Paul Fricker and Mike Mason. The game system underwent only minor rules changes in its first six editions (between 1981 and 2011); the current seventh edition, released 2014, includes more significant rules alterations than in any previous release.\n\nFor those grounded in the RPG tradition, the very first release of \"Call of Cthulhu\" created a brand new framework for table-top gaming. Rather than the traditional format established by \"Dungeons & Dragons\", which often involved the characters wandering through caves or tunnels and fighting different types of monsters, Sandy Petersen introduced the concept of the \"Onion Skin\": Interlocking layers of information and nested clues that lead the player characters from seemingly minor investigations into a missing person to discovering mind-numbingly awful, global conspiracies to destroy the world. Unlike its predecessor games, \"CoC\" assumed that most investigators would not survive, alive or sane, and that the only safe way to deal with the vast majority of nasty things described in the rule books was to run away. A well-run \"CoC\" campaign should engender a sense of foreboding and inevitable doom in its players. The style and setting of the game, in a relatively modern time period, created an emphasis on real-life settings, character research, and thinking one's way around trouble.\n\nThe first book of \"Call of Cthulhu\" adventures was \"Shadows of Yog-Sothoth\". In this work, the characters come upon a secret society's foul plot to destroy mankind, and pursue it first near to home and then in a series of exotic locations. This template was to be followed in many subsequent campaigns, including \"Fungi from Yuggoth\" (later known as \"Curse of Cthulhu\" and \"Day of the Beast\"), \"Spawn of Azathoth\", and possibly the most highly acclaimed, \"Masks of Nyarlathotep\".\n\n\"Shadows of Yog-Sothoth\" is important not only because it represents the first published addition to the boxed first edition of \"Call of Cthulhu\", but because its format defined a new way of approaching a campaign of linked RPG scenarios involving actual clues for the would-be detectives amongst the players to follow and link in order to uncover the dastardly plots afoot. Its format has been used by every other campaign-length \"Call of Cthulhu\" publication. The standard of \"CoC\" scenarios was well received by independent reviewers. \"The Asylum and Other Tales\", a series of stand alone articles released in 1983, rated an overall 9/10 in Issue 47 of \"White Dwarf\" magazine.\n\nThe standard of the included 'clue' material varies from scenario to scenario, but reached its zenith in the original boxed versions of the \"Masks of Nyarlathotep\" and \"Horror on the Orient Express\" campaigns. Inside these one could find matchbooks and business cards apparently defaced by non-player characters, newspaper cuttings and (in the case of \"Orient Express\") period passports to which players could attach their photographs, increasing the sense of immersion. Indeed, during the period that these supplements were produced, third party campaign publishers strove to emulate the quality of the additional materials, often offering separately-priced 'deluxe' clue packages for their campaigns.\n\nAdditional milieux were provided by Chaosium with the release of \"Dreamlands\", a boxed supplement containing additional rules needed for playing within the Lovecraft Dreamlands, a large map and a scenario booklet, and \"Cthulhu By Gaslight\", another boxed set which moved the action from the 1920s to the 1890s.\n\n\nIn 1987, Chaosium issued the supplement titled \"Cthulhu Now\", a collection of rules, supplemental source materials and scenarios for playing \"Call of Cthulhu\" in the present day. This proved to be a very popular alternative milieu, so much so that much of the supplemental material is now included in the core rule book.\n\nPagan Publishing released \"Delta Green\", a series of supplements originally set in the 1990s, although later supplements add support for playing closer to the present day. In these, player characters are agents of a secret agency known as Delta Green, which fights against creatures from the Mythos and conspiracies related to them. Arc Dream Publishing released a new version of \"Delta Green\" in 2016 as a standalone game, partially using the mechanics from \"Call of Cthulhu\".\n\n\"Lovecraft Country\" was a line of supplements for \"Call of Cthulhu\" released in 1990. These supplements were overseen by Keith Herber and provided backgrounds and adventures set in Lovecraft's fictional towns of Arkham, Kingsport, Innsmouth, Dunwich, and their environs. The intent was to give investigators a common base, as well as to center the action on well-drawn characters with clear motivations.\n\nIn the years since the collapse of the \"Mythos\" collectible card game (production ceased in 1997), the release of \"CoC\" books has been very sporadic with up to a year between releases. Chaosium struggled with near bankruptcy for many years before finally starting their upward climb again.\n\n2005 was Chaosium's busiest year for many years with ten releases for the game. Chaosium took to marketing \"monographs\"—short books by individual writers with editing and layout provided out-of-house—directly to the consumer, allowing the company to gauge market response to possible new works. The range of times and places in which the horrors of the Mythos can be encountered was also expanded in late 2005 onwards with the addition of \"Cthulhu Dark Ages\" by Stéphane Gesbert, which gives a framework for playing games set in 11th century Europe, \"Secrets of Japan\" by Michael Dziesinski for gaming in modern-day Japan, and \"Secrets of Kenya\" by David Conyers for gaming in interwar period Africa.\n\nIn July 2011, Chaosium announced it would re-release a 30th anniversary edition of the \"CoC\" 6th edition role-playing game. This 320-page book features thick (3 mm) leatherette hardcovers with the front cover and spine stamped with gold foil. The interior pages are printed in black ink, on 90 gsm matte art paper. The binding is thread sewn, square backed. Chaosium offered a one-time printing of this Collector's Edition.\n\nOn May 28, 2013, a crowdfunding campaign on Kickstarter for the 7th Edition of \"Call of Cthulhu\" was launched with a goal of $40,000; it ended on June 29 of the same year having collected $561,836. It included many more major revisions than any previous edition, and also split the core rules into two books, a Player's Guide and Keeper's Guide. Problems and delays fulfilling the Kickstarters for the 7th edition of \"Call of Cthulhu\" led Greg Stafford and Sandy Petersen (who had both left in 1998) to return to an active role at Chaosium in June 2015.\n\nChaosium has licensed other publishers to create supplements using their rule system, notably including \"Delta Green\" by Pagan Publishing. Other licensees have included Infogrames, Miskatonic River Press, Theater of the Mind Enterprises, Triad Entertainment, Games Workshop, Fantasy Flight Games, RAFM, Goodman Games, Grenadier Models Inc. and Yog-Sothoth.com. These supplements may be set in different time frames or even different game universes from the original game.\n\n\"Shadow of the Comet\" (later repackaged as \"Call of Cthulhu: Shadow of the Comet\") is an adventure game developed and released by Infogrames in 1993. The game is based on H. P. Lovecraft's Cthulhu Mythos and uses many elements from Lovecraft's \"The Dunwich Horror\" and \"The Shadow Over Innsmouth\". A follow-up game, \"Prisoner of Ice\", is not a direct sequel.\n\n\"Prisoner of Ice\" (also \"Call of Cthulhu: Prisoner of Ice\") is an adventure game developed and released by Infogrames for the PC and Macintosh computers in 1995 in America and Europe. It is based on H. P. Lovecraft's Cthulhu Mythos, particularly \"At the Mountains of Madness\", and is a follow-up to Infogrames' earlier \"Shadow of the Comet\". In 1997, the game was ported to the Sega Saturn and PlayStation exclusively in Japan.\n\nIn 2001, a stand-alone version of \"Call of Cthulhu\" was released by Wizards of the Coast, for the d20 system. Intended to preserve the feeling of the original game, the d20 conversion of the game rules were supposed to make the game more accessible to the large \"D&D\" player base. The d20 system also made it possible to use \"Dungeons & Dragons\" characters in \"Call of Cthulhu\", as well as to introduce the Cthulhu Mythos into \"Dungeons & Dragons\" games. The d20 version of the game is no longer supported by Wizards as per their contract with Chaosium. Chaosium included d20 stats as an appendix in three releases (see Lovecraft Country), but have since dropped the \"dual stat\" idea.\n\nA licensed first-person shooter adventure game by Headfirst Productions, based on \"Call of Cthulhu\" campaign \"Escape from Innsmouth\" and released by Bethesda Softworks in 2005/2006 for the PC and Xbox.\n\nIn February 2008, Pelgrane Press published \"Trail of Cthulhu\", a stand-alone game created by Kenneth Hite using the GUMSHOE System developed by Robin Laws. \"Trail of Cthulhu\"s system is more mystery oriented and focuses mostly on interpreting clues.\n\nIn September 2008, Reality Deviant Publications published \"Shadows of Cthulhu\", a supplement that brings Lovecraftian gaming to Green Ronin's True20 system.\n\nIn October 2009, Reality Blurs published \"Realms of Cthulhu\", a supplement for Pinnacle Entertainment's Savage Worlds system.\n\nIn 2010, Cubicle 7 published an official role-playing game, \"The Laundry\" and a number of supplements, all based on Charles Stross's \"Laundry Files\" series.\n\nIn April 2011, Chaosium and new developer Red Wasp Design announced a joint project to produce a mobile video game based on the \"Call of Cthulhu\" RPG, entitled \"Call of Cthulhu: The Wasted Land\". The game was released on January 30, 2012.\n\nIn 2018, Metarcade produced \"Cthulhu Chronicles\", a game for iOS with a campaign of nine mobile interactive fiction stories set in 1920s England based on \"Call of Cthulhu\". The first five stories were released on July 10, 2018.\n\n\"Mythos\" was a collectible card game (CCG) based on the Cthulhu Mythos that Chaosium produced and marketed during the mid-1990s. While generally praised for its fast gameplay and unique mechanics, it ultimately failed to gain a very large market presence. It bears mention because its eventual failure brought the company to hard times that affected its ability to produce material for \"Call of Cthulhu\". \"Call of Cthulhu: The Card Game\" is a second collectible card game, produced by Fantasy Flight Games.\n\nThe first licensed \"Call of Cthulhu\" gaming miniatures were sculpted by Andrew Chernack and released by Grenadier Models in boxed sets and blister packs in 1983. The license was later transferred to RAFM. As of 2011, RAFM still produce licensed C\"all of Cthulhu\" models sculpted by Bob Murch. Both lines include investigator player character models and the iconic monsters of the Cthulhu mythos.\nAs of July 2015, Reaper Miniatures started its third \"Bones Kickstarter\", a Kickstarter intended to help the company migrate some miniatures from metal to plastic, and introducing some new ones. Among the stretch goals was the second $50 expansion, devoted to the Mythos, with miniatures such as Cultists, Deep Ones, Mi'Go, and an extra $15 Shub-Niggurath \"miniature\" (it is, at least, 6x4 squares). It is expected for those miniatures to remain in the Reaper Miniatures catalogue after the Kickstarter project finishes.\n\nCall of Cthulhu: The Official Video Game is a survival horror role-playing video game developed by Cyanide and published by Focus Home Interactive for PlayStation 4, Xbox One and Windows. The game features a semi-open world environment and incorporates themes of Lovecraftian and psychological horror into a story which includes elements of investigation and stealth. It is inspired by H. P. Lovecraft's short story \"The Call of Cthulhu\".\n\nWilliam A. Barton reviewed \"Call of Cthulhu\" in \"The Space Gamer\" No. 49. Barton calls the game \"an excellent piece of work.\" He noted that there were some shortcomings resulting from assumption of rules details from \"RuneQuest\" that are not in \"CoC\" itself, but praised it overall, saying \"the worlds of H. P. Lovecraft are truly open for the fantasy gamer., and calling the game a satisfactory experience.\n\nDavid Cook reviewed \"Call of Cthulhu\" for \"Dragon\" magazine #61 (May 1982). He commented: \"It is a good game for experienced role-playing gamers and ambitious judges, especially if they like Lovecraft’s type of story.\"\n\n\"Call of Cthulhu\" was ranked 1st in the 1996 reader poll of \"Arcane\" magazine to determine the 50 most popular roleplaying games of all time. The UK magazine's editor Paul Pettengale commented: \"\"Call of Cthulhu\" is fully deserved of the title as the most popular roleplaying system ever - it's a game that doesn't age, is eminently playable, and which hangs together perfectly. The system, even though it's over ten years old, it still one of the very best you'll find in any roleplaying game. Also, there's not a referee in the land who could say they've read every Lovecraft inspired book or story going, so there's a pretty-well endless supply of scenario ideas. It's simply marvellous.\"\n\nThe game won several major awards in the following years:\n\n\n\n"}
{"id": "5723", "url": "https://en.wikipedia.org/wiki?curid=5723", "title": "Constellations (journal)", "text": "Constellations (journal)\n\nConstellations: An International Journal of Critical and Democratic Theory is a quarterly peer-reviewed academic journal of critical and democratic theory and successor of \"Praxis International\". It is edited by Jean L. Cohen, Amy Allen, and Andreas Kalyvas. Seyla Benhabib is a co-founding former editor and Nancy Fraser a former co-editor. With a broad and international editorial contribution, it is based at the New School in New York.\n"}
{"id": "5724", "url": "https://en.wikipedia.org/wiki?curid=5724", "title": "Cape Breton Island", "text": "Cape Breton Island\n\nCape Breton Island (—formerly '; or '; ; or simply \"Cape Breton\") is an island on the Atlantic coast of North America and part of the province of Nova Scotia, Canada.\n\nThe island accounts for 18.7% of Nova Scotia's total area. Although the island is physically separated from the Nova Scotia peninsula by the Strait of Canso, the long rock-fill Canso Causeway connects it to mainland Nova Scotia. The island is east-northeast of the mainland with its northern and western coasts fronting on the Gulf of Saint Lawrence; its western coast also forms the eastern limits of the Northumberland Strait. The eastern and southern coasts front the Atlantic Ocean; its eastern coast also forms the western limits of the Cabot Strait. Its landmass slopes upward from south to north, culminating in the highlands of its northern cape. One of the world's larger salt water lakes, (\"Arm of Gold\" in French), dominates the island's centre.\n\nThe island is divided into four of Nova Scotia's eighteen counties: Cape Breton, Inverness, Richmond, and Victoria. Their total population at the 2016 census numbered 132,010 \"Cape Bretoners\"; this is approximately 15% of the provincial population. Cape Breton Island has experienced a decline in population of approximately 2.9% since the 2011 census. Approximately 75% of the island's population is in the Cape Breton Regional Municipality (CBRM) which includes all of Cape Breton County and is often referred to as Industrial Cape Breton, given the history of coal mining and steel manufacturing in this area, which was Nova Scotia's industrial heartland throughout the 20th century.\n\nThe island has five reserves of the Mi'kmaq Nation: , , , , and /Chapel Island. is the largest in both population and land area.\n\nIts name may derive from near Bayonne, or more probably from \"Cape\" and the word ', the French demonym for ', the French historical region.\n\nCape Breton Island's first residents were likely Archaic maritime natives, ancestors of the Mi'kmaq. These peoples and their progeny inhabited the island (known as Unama'ki) for several thousand years and continue to live there to this day. Their traditional lifestyle centred around hunting and fishing because of the unfavourable agricultural conditions of their maritime home. This ocean-centric lifestyle did, however, make them among the first indigenous peoples to discover European explorers and sailors fishing in the St Lawrence Estuary. John Cabot reportedly visited the island in 1497. However, European histories and maps of the period are of too poor quality to be sure whether Cabot first visited Newfoundland or Cape Breton Island. This discovery is commemorated by Cape Breton's Cabot Trail, and by the Cabot's Landing Historic Site & Provincial Park, near the village of Dingwall.\n\nThe local Mi'kmaq peoples began trading with European fishermen when the fishermen began landing in their territories as early as the 1520s. In about 1521–22, the Portuguese under João Álvares Fagundes established a fishing colony on the island. As many as two hundred settlers lived in a village, the name of which is not known, located according to some historians at what is now Ingonish on the island's northeastern peninsula. These fishermen traded with the local population but did not maintain a permanent settlement. This Portuguese colony's fate is unknown, but it is mentioned as late as 1570.\n\nDuring the Anglo-French War of 1627 to 1629, under Charles I, the Kirkes took Quebec City; Sir James Stewart of Killeith, Lord Ochiltree planted a colony on Unama'ki at Baleine, Nova Scotia; and Alexander's son, William Alexander, 1st Earl of Stirling, established the first incarnation of \"New Scotland\" at Port Royal. These claims, and larger European ideals of native conquest were the first time the island was incorporated as European territory, though it would be several decades later that treaties would actually be signed (no copies of these treaties exist).\n\nThese Scottish triumphs, which left Cape Sable as the only major French holding in North America, did not last. Charles I's haste to make peace with France on the terms most beneficial to him meant the new North American gains would be bargained away in the Treaty of Saint-Germain-en-Laye (1632), which established which European power had claim over the territories, but did not in fact establish that Europeans had any claim to begin with.\n\nThe French quickly defeated the Scots at Baleine, and established the first European settlements on Île Royale: present day Englishtown (1629) and St. Peter's (1630). These settlements lasted only one generation, until Nicolas Denys left in 1659. The island did not have any European settlers for another fifty years before those communities along with Louisbourg were re-established in 1713, after which point European settlement was permanently established on the island.\n\nKnown as \"\"Île Royale\"\" (\"Royal Island\") to the French, the island also saw active settlement by France. After the French ceded their claims to Newfoundland and the Acadian mainland to the British by the Treaty of Utrecht in 1713, the French relocated the population of Plaisance, Newfoundland, to Île Royale and the French garrison was established in the central eastern part at Sainte Anne. As the harbour at Sainte Anne experienced icing problems, it was decided to build a much larger fortification at Louisbourg to improve defences at the entrance to the Gulf of Saint Lawrence and to defend France's fishing fleet on the Grand Banks. The French also built the Louisbourg Lighthouse in 1734, the first lighthouse in Canada and one of the first in North America. In addition to Cape Breton Island, the French colony of Île Royale also included Île Saint-Jean, today called Prince Edward Island, and Les Îles-de-la-Madeleine.\n\nLouisbourg itself was one of the most important commercial and military centres in New France. Louisbourg was captured by New Englanders with British naval assistance in 1745 and by British forces in 1758. The French population of Île Royale was deported to France after each siege. While French settlers returned to their homes in Île Royale after the Treaty of Aix-la-Chapelle was signed in 1748, the fortress was demolished after the second siege. Île Royale remained formally part of New France until it was ceded to Great Britain by the Treaty of Paris in 1763. It was then merged with the adjacent, British colony of Nova Scotia (present day peninsular Nova Scotia and New Brunswick). Acadians who had been expelled from Nova Scotia and Île Royale were permitted to settle in Cape Breton beginning in 1764, and established communities in north-western Cape Breton, near Cheticamp, and southern Cape Breton, on and near Isle Madame.\n\nSome of the first British-sanctioned settlers on the island following the Seven Years' War were Irish, although upon settlement they merged with local French communities to form a culture rich in music and tradition. From 1763 to 1784, the island was administratively part of the colony of Nova Scotia and was governed from Halifax.\n\nThe first permanently settled Scottish community on Cape Breton Island was Judique, settled in 1775 by Michael Mor MacDonald. He spent his first winter using his upside-down boat for shelter, which is reflected in the architecture of the village's Community Centre. He composed a song about the area called \"O 's àlainn an t-àite\", or \"O, Fair is the Place.\"\n\nDuring the American Revolution, on 1 November 1776, John Paul Jones – the father of the American Navy – set sail in command of \"Alfred\" to free hundreds of American prisoners working in the area's coal mines. Although winter conditions prevented the freeing of the prisoners, the mission did result in the capture of \"Mellish\", a vessel carrying a vital supply of winter clothing intended for John Burgoyne's troops in Canada.\n\nMajor Timothy Hierlihy and his regiment on board HMS \"Hope\" worked in and protected from privateer attacks on the coal mines at Sydney Cape Breton. Sydney Cape Breton provided a vital supply of coal for Halifax throughout the war. The British began developing the mining site at Sydney Mines in 1777. On 14 May 1778, Major Hierlihy arrived at Cape Breton. While there, Hierlihy reported that he \"beat off many piratical attacks, killed some and took other prisoners.\"\n\nA few years into the war there was also a naval engagement between French ships and a British convoy off Sydney, Nova Scotia, near Spanish River (1781), Cape Breton. French ships (fighting with the Americans) were re-coaling and defeated a British convoy. Six French sailors were killed and 17 British, with many more wounded.\n\nIn 1784, Britain split the colony of Nova Scotia into three separate colonies: New Brunswick, Cape Breton Island, and present-day peninsular Nova Scotia, in addition to the adjacent colonies of St. John's Island (renamed Prince Edward Island in 1798) and Newfoundland. The colony of Cape Breton Island had its capital at Sydney on its namesake harbour fronting on Spanish Bay and the Cabot Strait. Its first Lieutenant-Governor was Joseph Frederick Wallet DesBarres (1784–1787) and his successor was William Macarmick (1787).\n\nA number of United Empire Loyalists emigrated to the Canadian colonies, including Cape Breton. David Mathews, the former Mayor of New York City during the American Revolution, emigrated with his family to Cape Breton in 1783. He succeeded Macarmick as head of the colony and served from 1795 to 1798.\n\nFrom 1799 to 1807, the military commandant was John Despard, brother of Edward.\n\nAn order forbidding the granting of land in Cape Breton, issued in 1763, was removed in 1784. The mineral rights to the island were given over to the Duke of York by an order-in-council. The British government had intended that the Crown take over the operation of the mines when Cape Breton was made a colony, but this was never done, probably because of the rehabilitation cost of the mines. The mines were in a neglected state, caused by careless operations dating back at least to the time of the final fall of Louisbourg in 1758.\n\nLarge-scale shipbuilding began in the 1790s, beginning with schooners for local trade moving in the 1820s to larger brigs and brigantines, mostly built for British shipowners. Shipbuilding peaked in the 1850s, marked in 1851 by the full-rigged ship \"Lord Clarendon\", the largest wooden ship ever built in Cape Breton.\n\nIn 1820, the colony of Cape Breton Island was merged for the second time with Nova Scotia. This development is one of the factors which led to large-scale industrial development in the Sydney Coal Field of eastern Cape Breton County. By the late 19th century, as a result of the faster shipping, expanding fishery and industrialization of the island, exchanges of people between the island of Newfoundland and Cape Breton increased, beginning a cultural exchange that continues to this day.\n\nThe 1920s were some of the most violent times in Cape Breton. They were marked by several severe labour disputes. The famous murder of William Davis by strike breakers, and the seizing of the New Waterford power plant by striking miners led to a major union sentiment that persists to this day in some circles. William Davis Miners' Memorial Day is celebrated in coal mining towns to commemorate the deaths of miners at the hands of the coal companies.\n\nThe turn of the 20th century saw Cape Breton Island at the forefront of scientific achievement with the now-famous activities launched by inventors Alexander Graham Bell and Guglielmo Marconi.\n\nFollowing his successful invention of the telephone and being relatively wealthy, Bell acquired land near Baddeck in 1885, largely due to surroundings reminiscent of his early years in Scotland. He established a summer estate complete with research laboratories, working with deaf people—including Helen Keller—and continued to invent. Baddeck would be the site of his experiments with hydrofoil technologies as well as the Aerial Experiment Association, financed by his wife, which saw the first powered flight in Canada when the AEA \"Silver Dart\" took off from the ice-covered waters of Bras d'Or Lake. Bell also built the forerunner to the iron lung and experimented with breeding sheep.\n\nMarconi's contributions to Cape Breton Island were also quite significant, as he used the island's geography to his advantage in transmitting the first North American trans-Atlantic radio message from a station constructed at Table Head in Glace Bay to a receiving station at Poldhu in Cornwall, England. Marconi's pioneering work in Cape Breton marked the beginning of modern radio technology. Marconi's station at Marconi Towers, on the outskirts of Glace Bay, became the chief communication centre for the Royal Canadian Navy in World War I through to the early years of World War II.\n\nPromotions for tourism beginning in the 1950s recognized the importance of the Scottish culture to the province, and the provincial government started encouraging the use of Gaelic once again. The establishment of funding for the Gaelic College of Celtic Arts and Crafts and formal Gaelic language courses in public schools are intended to address the near-loss of this culture to English assimilation.\nIn the 1960s, the Fortress of Louisbourg was partially reconstructed by Parks Canada. Since 2009, this National Historic Site of Canada has attracted an average of 90 000 visitors per year.\n\nGaelic speakers in Cape Breton, as elsewhere in Nova Scotia, furnished a large proportion of the local population from the eighteenth century on. They brought with them a common culture of poetry, traditional songs and tales, music and dance, and used this to develop distinctive local traditions.\n\nMost Gaelic settlement in Nova Scotia happened between 1770 and 1840, with probably over 50,000 Gaelic speakers emigrating from the Scottish Highlands and the Hebrides to Nova Scotia and Prince Edward Island. Such emigration was facilitated by changes in Gaelic society and the economy, with sharp increases in rents, confiscation of land and disruption of local customs and rights. Gaelic settlement in Cape Breton began in earnest in the early nineteenth century.\nThe Gaelic language became dominant from Colchester County in the west of Nova Scotia into Cape Breton County in the east. It was reinforced in Cape Breton in the first half of the 19th century with an influx of Highland Scots numbering approximately 50,000 as a result of the Highland Clearances.\n\nGaelic speakers, however, tended to be poor; they were largely illiterate and had little access to education. This situation still obtained in the early twentieth century. In 1921 Gaelic was approved as an optional subject in the curriculum of Nova Scotia, but few teachers could be found and children were discouraged from using the language in schools. By 1931 the number of Gaelic speakers in Nova Scotia had fallen to approximately 25,000, mostly in discrete pockets. In Cape Breton it was still a majority language, but the proportion was falling. Children were no longer being raised with Gaelic.\n\nFrom 1939 on attempts were made to strengthen its position in the public school system in Nova Scotia, but funding, official commitment and the availability of teachers continued to be a problem. By the 1950s the number of speakers was less than 7,000. The advent of multiculturalism in Canada in the 1960s meant that new educational opportunities became available, with a gradual strengthening of the language at secondary and tertiary level. At present several schools in Cape Breton offer Gaelic Studies and Gaelic language programs, and the language is taught at University College of Cape Breton.\n\nThe 2016 Canadian Census shows that there are only 40 reported speakers of Gaelic as a mother tongue in Cape Breton. On the other hand, there are families and individuals who have recommenced intergenerational transmission. They include fluent speakers from Gaelic-speaking areas of Scotland and speakers who became fluent in Nova Scotia and who in some cases studied in Scotland. Other revitalization activities include adult education, community cultural events and publishing.\n\nThe island measures in area, making it the 77th largest island in the world and Canada's 18th largest island. Cape Breton Island is composed mainly of rocky shores, rolling farmland, glacial valleys, barren headlands, mountains, woods and plateaus. Geological evidence suggests at least part of the island was joined with present-day Scotland and Norway, now separated by millions of years of plate tectonics.\n\nCape Breton Island's northern portion is dominated by the Cape Breton Highlands, commonly shortened to simply the \"Highlands\", which are an extension of the Appalachian mountain chain. The Highlands comprise the northern portions of Inverness and Victoria counties. In 1936, the federal government established the Cape Breton Highlands National Park covering across the northern third of the Highlands. The Cabot Trail scenic highway also encircles the plateau's coastal perimeter.\n\nCape Breton Island's hydrological features include the Bras d'Or Lake system, a salt-water fjord at the heart of the island, and freshwater features including Lake Ainslie, the Margaree River system, and the Mira River. Innumerable smaller rivers and streams drain into the Bras d'Or Lake estuary and on to the Gulf of St. Lawrence and Atlantic coasts.\n\nCape Breton Island is joined to the mainland by the Canso Causeway, which was completed in 1955, enabling direct road and rail traffic to and from the island, but requiring marine traffic to pass through the Canso Canal at the eastern end of the causeway.\n\nCape Breton Island is divided into four counties: Cape Breton, Inverness, Richmond, and Victoria.\n\nThe climate is one of mild, often pleasantly warm summers and cold winters, although the proximity to the Atlantic Ocean and Gulf Stream moderates the extreme winter cold found on the mainland, especially on the east side that faces the Atlantic. Precipitation is abundant year round, with annual totals up to 60 inches on the eastern side facing the Atlantic storms. Considerable snowfall occurs in winter, especially in the highlands.\n\nThe island's residents can be grouped into five main cultures: Scottish, Mi'kmaq, Acadian, Irish, English, with respective languages Scottish Gaelic, Mi'kmaq, French, and English. English is now the primary language, including a locally distinctive Cape Breton accent, while Mi'kmaq, Scottish Gaelic and Acadian French are still spoken in some communities.\n\nLater migrations of Black Loyalists, Italians, and Eastern Europeans mostly settled in the island's eastern part around the industrial Cape Breton region. Cape Breton Island's population has been in decline two decades with an increasing exodus in recent years due to economic conditions.\n\nAccording to the Census of Canada, the population of Cape Breton [Economic region] in 2016 / 2011 / 2006 / 1996 was 132,010 / 135,974 / 142,298 / 158,260.\n\nReligious groups\n\nStatistics Canada in 2001 reported a \"religion\" total of 145,525 for Cape Breton, including 5,245 with \"no religious affiliation.\" Major categories included:\n\nMuch of the recent economic history of Cape Breton Island can be tied to the coal industry.\n\nThe island has two major coal deposits:\n\n\nSydney has traditionally been the main port, with facilities in a large, sheltered, natural harbour. It is the island's largest commercial centre and home to the \"Cape Breton Post\" daily newspaper, as well as one television station, CJCB-TV (CTV), and several radio stations. The Marine Atlantic terminal at North Sydney is the terminal for large ferries traveling to Channel-Port aux Basques and seasonally to Argentia, both on the island of Newfoundland.\n\nPoint Edward on the west side of Sydney Harbour is the location of Sydport, a former navy base () now converted to commercial use. The Canadian Coast Guard College is nearby at Westmount. Petroleum, bulk coal, and cruise ship facilities are also in Sydney Harbour.\n\nGlace Bay, the second largest urban community in population, was the island's main coal mining centre until its last mine closed in the 1980s. Glace Bay was the hub of the Sydney & Louisburg Railway and a major fishing port. At one time, Glace Bay was known as the largest town in Nova Scotia, based on population.\n\nPort Hawkesbury has risen to prominence since the completion of the Canso Causeway and Canso Canal created an artificial deep-water port, allowing extensive petrochemical, pulp and paper, and gypsum handling facilities to be established. The Strait of Canso is completely navigable to Seawaymax vessels, and Port Hawkesbury is open to the deepest-draught vessels on the world's oceans. Large marine vessels may also enter Bras d'Or Lake through the Great Bras d'Or channel, and small craft can use the Little Bras d'Or channel or St. Peters Canal. While commercial shipping no longer uses the St. Peters Canal, it remains an important waterway for recreational vessels.\n\nThe industrial Cape Breton area faced several challenges with the closure of the Cape Breton Development Corporation's (DEVCO) coal mines and the Sydney Steel Corporation's (SYSCO) steel mill. In recent years, the Island's residents have tried to diversify the area economy by investing in tourism developments, call centres, and small businesses, as well as manufacturing ventures in fields such as auto parts, pharmaceuticals, and window glazings.\n\nWhile the Cape Breton Regional Municipality is in transition from an industrial to a service-based economy, the rest of Cape Breton Island outside the industrial area surrounding Sydney-Glace Bay has been more stable, with a mixture of fishing, forestry, small-scale agriculture, and tourism.\n\nTourism in particular has grown throughout the post-Second World War era, especially the growth in vehicle-based touring, which was furthered by the creation of the Cabot Trail scenic drive. The scenery of the island is rivalled in northeastern North America by only Newfoundland; and Cape Breton Island tourism marketing places a heavy emphasis on its Scottish Gaelic heritage through events such as the Celtic Colours Festival, held each October, as well as promotions through the Gaelic College of Celtic Arts and Crafts.\n\nWhale-watching is a popular attraction for tourists. Whale-watching cruises are operated by vendors from Baddeck to Cheticamp. The most popular species of whale found in Cape Breton's waters is the Pilot whale.\n\nThe island's primary east-west road is Highway 105, the Trans-Canada Highway, although Trunk 4 is also heavily used. Highway 125 is an important arterial route around Sydney Harbour in the Cape Breton Regional Municipality. The Cabot Trail, circling the Cape Breton Highlands, and Trunk 19, along the island's western coast, are important secondary roads. The Cape Breton and Central Nova Scotia Railway maintains railway connections between the port of Sydney to the Canadian National Railway in Truro\n\nThe Cabot Trail is a scenic road circuit around and over the Cape Breton Highlands with spectacular coastal vistas; over 400,000 visitors drive the Cabot Trail each summer and fall. Coupled with the Fortress of Louisbourg, it has driven the growth of the tourism industry on the island in recent decades. The \"Condé Nast\" travel guide has rated Cape Breton Island as one of the world's best island destinations.\n\nCape Breton is well known for its traditional fiddle music, which was brought to North America by Scottish immigrants during the Highland Clearances. The traditional style has been well preserved in Cape Breton, and céilidhs have become a popular attraction for tourists. Inverness County in particular has a heavy concentration of musical activity, with regular performances in communities such as Mabou and Judique. Judique is recognized as 'Baile nam Fonn', (literally: Village of Tunes) or the 'Home of Celtic Music', featuring the Celtic Music Interpretive Centre. Performers who have received significant recognition outside of Cape Breton include Bruce Guthro, Buddy MacMaster, Natalie MacMaster, Ashley MacIsaac, The Rankin Family, Aselin Debison, Gordie Sampson, Lee Cremo, and the Barra MacNeils. The Margaree's of Cape Breton also serve as a large contributor of fiddle music celebrated throughout the island. This traditional fiddle music of Cape Breton is studied by musicians around the world, where its global recognition continues to rise.\n\nThe Men of the Deeps are a male choral group of current and former miners from the industrial Cape Breton area.\n\n\nCape Breton artists who have been recognized with major national or international awards include actor Harold Russell of North Sydney, who won an Academy Award in 1946 for his portrayal of Homer Parrish in \"The Best Years of Our Lives\", and Lynn Coady and Linden MacIntyre of Inverness County, who are both past winners of the Giller Prize for Canadian literature. The Rankin Family and Rita MacNeil have recorded multiple albums certified as Double Platinum by Music Canada.\n\nPeople from Cape Breton have also achieved a number of firsts in Canadian politics and governance. These include Mayann Francis of Whitney Pier, the first Black Lieutenant Governor of Nova Scotia, Isaac Phills of Sydney, Nova Scotia, the first person of African descent to be awarded the Order of Canada, and Elizabeth May of Margaree Harbour, the first member of the Green Party of Canada elected to the House of Commons of Canada.\n\nCape Breton Island is also home to YouTube weather sensation Frankie MacDonald. Frankie MacDonald on his YouTube channel \"dogsandwolves\" has over 100,000 subscribers. He accurately predicted an above Magnitude 7 earthquake in New Zealand in November 2016.\n\nAmerican artists like sculptor Richard Serra, composer Philip Glass and abstract painter John Beardman spent part of the year on Cape Breton Island.\n\nSteve Arbuckle is a Canadian born actor born in Cape Breton Island, Nova Scotia. He started his career as a theatre actor at the Boardmore Playhouse and Savoy Theatre, along with other independent theatre companies, then made his first move into film in 2003 with the lead role in the short film Todd and the Book of Pure Evil, later becoming a TV show on Space as well as an animated film. Since then, he has and continues to appear on many feature films and television series, including:\"Blue Bloods\", Saving Hope, Murdoch Mysteries, Falling Water (TV series), and many more. \n\nDirector Ashley McKenzie's 2016 film \"Werewolf\" is set on the island and features local actors; McKenzie grew up on the island.\n\n\n\n\n\n\n"}
{"id": "5725", "url": "https://en.wikipedia.org/wiki?curid=5725", "title": "Cthulhu Mythos", "text": "Cthulhu Mythos\n\nThe Cthulhu Mythos is a shared fictional universe, originating in the works of American horror writer H. P. Lovecraft. The term was coined by August Derleth, a contemporary correspondent and protégé of Lovecraft, to identify the settings, tropes, and lore that were employed by Lovecraft and his literary successors. The name \"Cthulhu\" derives from the central creature in Lovecraft's seminal short story, \"The Call of Cthulhu\", first published in the pulp magazine \"Weird Tales\" in 1928. \n\nRichard L. Tierney, a writer who also wrote Mythos tales, later applied the term \"Derleth Mythos\" to distinguish Lovecraft's works from Derleth's later stories, which modify key tenets of the Mythos. Authors of Lovecraftian horror in particular frequently use elements of the Cthulhu Mythos.\n\nIn his essay \"H. P. Lovecraft and the Cthulhu Mythos\", Robert M. Price described two stages in the development of the Cthulhu Mythos. Price called the first stage the \"Cthulhu Mythos proper.\" This stage was formulated during Lovecraft's lifetime and was subject to his guidance. The second stage was guided by August Derleth who, in addition to publishing Lovecraft's stories after his death, attempted to categorize and expand the Mythos.\n\nAn ongoing theme in Lovecraft's work is the complete irrelevance of mankind in the face of the cosmic horrors that apparently exist in the universe. Lovecraft made frequent references to the \"Great Old Ones\", a loose pantheon of ancient, powerful deities from space who once ruled the Earth and have since fallen into a deathlike sleep. While these monstrous deities have been present in almost all of Lovecraft's published work (his second short story \"Dagon,\" published in 1919, is considered the start of the mythos), the first story to really expand the pantheon of Great Old Ones and its themes is \"The Call of Cthulhu,\" which was published in 1928. \n\nLovecraft broke with other pulp writers of the time by having his main characters' minds deteriorate when afforded a glimpse of what exists outside their perceived reality. He emphasized the point by stating in the opening sentence of the story that \"The most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents.\"\n\nWriter Dirk W. Mosig notes that Lovecraft was a \"mechanistic materialist\" who embraced the philosophy of \"cosmic indifference\". Lovecraft believed in a purposeless, mechanical, and uncaring universe. Human beings, with their limited faculties, can never fully understand this universe, and the cognitive dissonance caused by this revelation leads to insanity, in his view. This perspective made no allowance for religious belief which could not be supported scientifically, with the incomprehensible, cosmic forces of his tales having as little regard for humanity as humans have for insects.\n\nThere have been attempts at categorizing this fictional group of beings. Phillip A. Schreffler argues that by carefully scrutinizing Lovecraft's writings, a workable framework emerges that outlines the entire \"pantheon\"from the unreachable \"Outer Ones\" (e.g. Azathoth, who occupies the centre of the universe) and \"Great Old Ones\" (e.g. Cthulhu, imprisoned on Earth in the sunken city of R'lyeh) to the lesser castes (the lowly slave shoggoths and the Mi-go).\n\nDavid E. Schultz, however, believes that Lovecraft never meant to create a canonical Mythos but rather intended his imaginary pantheon to merely serve as a background element. Lovecraft himself humorously referred to his Mythos as \"Yog Sothothery\" (Dirk W. Mosig coincidentally suggested the term \"Yog-Sothoth Cycle of Myth\" be substituted for \"Cthulhu Mythos\"). At times, Lovecraft even had to remind his readers that his Mythos creations were entirely fictional.\n\nThe view that there was no rigid structure is expounded upon by S. T. Joshi, who said \"Lovecraft's imaginary cosmogony was never a static system but rather a sort of aesthetic construct that remained ever adaptable to its creator's developing personality and altering interests. . . . There was never a rigid system that might be posthumously appropriated. . . . The essence of the mythos lies not in a pantheon of imaginary deities nor in a cobwebby collection of forgotten tomes, but rather in a certain convincing cosmic attitude.\"\n\nPrice, however, believed that Lovecraft's writings could at least be divided into categories and identified three distinct themes: the \"Dunsanian\" (written a similar style as Lord Dunsany), \"Arkham\" (occurring in Lovecraft's fictionalized New England setting), and \"Cthulhu\" (the cosmic tales) cycles. Writer Will Murray noted that while Lovecraft often used his fictional pantheon in the stories he ghostwrote for other authors, he reserved Arkham and its environs exclusively for those tales he wrote under his own name.\n\nAlthough the Mythos was not formalized or acknowledged between them, Lovecraft did correspond and share story elements with other contemporary writers including Clark Ashton Smith, Robert E. Howard, Robert Bloch, Frank Belknap Long, Henry Kuttner, Henry S. Whitehead, and Fritz Leibera group referred to as the \"Lovecraft Circle.\" \n\nFor example, Robert E. Howard's character Friedrich Von Junzt reads Lovecraft's \"Necronomicon\" in the short story \"The Children of the Night\" (1931), and in turn Lovecraft mentions Howard's \"Unaussprechlichen Kulten\" in the stories \"Out of the Aeons\" (1935) and \"The Shadow Out of Time\" (1936). Many of Howard's original unedited \"Conan\" stories also involve parts of the Cthulhu Mythos.\n\nPrice denotes the second stage's commencement with August Derleth. The principal difference between Lovecraft and Derleth being Derleth's use of hope and development of the idea that the Cthulhu mythos essentially represented a struggle between good and evil. Derleth is credited with creating the \"Elder Gods.\" He stated:\n\nPrice believes that the basis for Derleth's system is found in Lovecraft: \"Was Derleth's use of the rubric 'Elder Gods' so alien to Lovecraft's in \"At the Mountains of Madness\"? Perhaps not. In fact, this very story, along with some hints from \"The Shadow over Innsmouth\", provides the key to the origin of the 'Derleth Mythos'. For in \"At the Mountains of Madness\" is shown the history of a conflict between interstellar races, first among them the Elder Ones and the Cthulhu-spawn. \n\nDerleth himself believed that Lovecraft wished for other authors to actively write about the Mythos as opposed to it being a discrete plot device within Lovecraft's own stories. Derleth expanded the boundaries of the Mythos by including any passing reference to another author's story elements by Lovecraft as part of the genre. Just as Lovecraft made passing reference to Clark Ashton Smith's \"Book of Eibon\", Derleth in turn added Smith's Ubbo-Sathla to the Mythos.\n\nDerleth also attempted to connect the deities of the Mythos to the four elements (\"air\", \"earth\", \"fire\", and \"water\"), creating new beings representative of certain elements in order to legitimize his system of classification. Derleth created \"Cthugha\" as a sort of fire elemental when a fan, Francis Towner Laney, complained that he had neglected to include the element in his schema. Laney, the editor of \"The Acolyte\", had categorized the Mythos in an essay that first appeared in the Winter 1942 issue of the magazine. \n\nImpressed by the glossary, Derleth asked Laney to rewrite it for publication in the Arkham House collection \"Beyond the Wall of Sleep\" (1943). Laney's essay (\"The Cthulhu Mythos\") was later republished in \"Crypt of Cthulhu #32\" (1985). In applying the elemental theory to beings that function on a cosmic scale (e.g. Yog-Sothoth) some authors created a fifth element that they termed \"aethyr\".\n\n\n\n"}
{"id": "5726", "url": "https://en.wikipedia.org/wiki?curid=5726", "title": "Crane shot", "text": "Crane shot\n\nIn filmmaking and video production, a crane shot is a shot taken by a camera on a moving crane or jib. Most cranes accommodate both the camera and an operator, but some can be moved by remote control. Camera cranes go back to the dawn of movie-making, and were frequently used in silent films to enhance the epic nature of large sets and massive crowds. Another use is to move up and away from the actors, a common way of ending a movie. Crane shots are often found in what are supposed to be emotional or suspenseful scenes. One example of this technique is the shots taken by remote cranes in the car-chase sequence of the 1985 film \"To Live and Die in L.A.\". Some filmmakers place the camera on a boom arm simply to make it easier to move around between ordinary set-ups.\n\nThe major supplier of cranes in the cinema of the United States throughout the 1940s, 1950s, and 1960s was the Chapman Company (later Chapman-Leonard of North Hollywood), supplanted by dozens of similar manufacturers around the world. The traditional design provided seats for both the director and the camera operator, and sometimes a third seat for the cinematographer as well. Large weights on the back of the crane compensate for the weight of the people riding the crane and must be adjusted carefully to avoid the possibility of accidents. During the 1960s, the tallest crane was the Chapman Titan crane, a massive design over 20 feet high that won an Academy Scientific & Engineering award. Most such cranes were manually operated, requiring an experienced boom operator who knew how to vertically raise, lower, and \"crab\" the camera alongside actors while the crane platform rolled on separate tracks. The crane operator and camera operator had to precisely coordinate their moves so that focus, pan, and camera position all started and stopped at the same time, requiring great skill and rehearsal.\n\nCamera cranes may be small, medium, or large, depending on the load capacity and length of the loading arm. Historically, the first camera crane provided for lifting the chamber together with the operator, and sometimes an assistant. The range of motion of the boom was restricted because of the high load capacity and the need to ensure operator safety. In recent years a camera crane boom tripod with a remote control has become popular. It carries on the boom only a movie or television camera without an operator and allows shooting from difficult positions as a small load capacity makes it possible to achieve a long reach of the crane boom and relative freedom of movement. The operator controls the camera from the ground through a motorized panoramic head, using remote control and video surveillance by watching the image on the monitor. A separate category consists of telescopic camera cranes. These devices allow setting an arbitrary trajectory of the camera, eliminating the characteristic jib crane radial displacement that comes with traditional spanning shots. \n\nLarge camera cranes are almost indistinguishable from the usual boom-type cranes, with the exception of special equipment for smoothly moving the boom and controlling noise. Small camera cranes and crane-trucks have a lightweight construction, often without a mechanical drive. The valves are controlled manually by balancing the load-specific counterweight, facilitating manipulation. To improve usability and repeatability of movement of the crane in different takes, the axis of rotation arrows are provided with limbs and a pointer. In some cases, the camera crane is mounted on a dolly for even greater camera mobility. Such devices are called crane trolleys. In modern films robotic cranes allow use of multiple actuators for high-accuracy repeated movement of the camera in trick photography. These devices are called tap-robots; some sources use the term motion control.\n\nDuring the last few years, camera cranes have been miniaturized and costs have dropped so dramatically that most aspiring film makers have access to these tools. What was once a \"Hollywood\" effect is now available for under $400. Main producers of cranes companies include ABC-Products, Cambo, Filmotechnic, Polecam, Panther and Matthews Studio Equipment.\n\n\nCinema Technologies Group http://cinetechno.com/\n"}
{"id": "5729", "url": "https://en.wikipedia.org/wiki?curid=5729", "title": "Chariots of Fire", "text": "Chariots of Fire\n\nChariots of Fire is a 1981 British historical drama film. It tells the fact-based story of two athletes in the 1924 Olympics: Eric Liddell, a devout Scottish Christian who runs for the glory of God, and Harold Abrahams, an English Jew who runs to overcome prejudice.\n\nThe film was conceived and produced by David Puttnam, written by Colin Welland, and directed by Hugh Hudson. Ben Cross and Ian Charleson starred as Abrahams and Liddell, alongside Nigel Havers, Ian Holm, Lindsay Anderson, John Gielgud, Cheryl Campbell, and Alice Krige in supporting roles.\n\nIt was nominated for seven Academy Awards and won four, including Best Picture and Best Original Screenplay. It is ranked 19th in the British Film Institute's list of Top 100 British films. The film is also notable for its memorable electronic theme tune by Vangelis, who won the Academy Award for Best Original Score.\n\nThe film's title was inspired by the line, \"Bring me my Chariot of fire!\", from the William Blake poem adapted into the popular British hymn \"Jerusalem\"; the hymn is heard at the end of the film. The original phrase \"chariot(s) of fire\" is from 2 Kings and in the Bible.\n\nIn 1919, Harold Abrahams (Ben Cross) enters the University of Cambridge, where he experiences anti-Semitism from the staff, but enjoys participating in the Gilbert and Sullivan club. He becomes the first person to ever complete the Trinity Great Court Run, running around the college courtyard in the time it takes for the clock to strike 12, and achieves an undefeated string of victories in various national running competitions. Although focused on his running, he falls in love with a leading Gilbert and Sullivan soprano, Sybil (Alice Krige).\n\nEric Liddell (Ian Charleson), born in China of Scottish missionary parents, is in Scotland. His devout sister Jennie (Cheryl Campbell) disapproves of Liddell's plans to pursue competitive running, but Liddell sees running as a way of glorifying God before returning to China to work as a missionary.\n\nWhen they first race against each other, Liddell beats Abrahams. Abrahams takes it poorly, but Sam Mussabini (Ian Holm), a professional trainer whom he had approached earlier, offers to take him on to improve his technique. This attracts criticism from the Cambridge college masters (John Gielgud and Lindsay Anderson), who allege it is not gentlemanly for an amateur to \"play the tradesman\" by employing a professional coach. Abrahams dismisses this concern, interpreting it as cover for anti-Semitic and class-based prejudice.\n\nWhen Eric Liddell accidentally misses a church prayer meeting because of his running, his sister Jennie upbraids him and accuses him of no longer caring about God. Eric tells her that though he intends to eventually return to the China mission, he feels divinely inspired when running, and that not to run would be to dishonour God, saying, \"I believe that God made me for a purpose. But He also made me fast, and when I run, I feel His pleasure.\"\n\nThe two athletes, after years of training and racing, are accepted to represent Great Britain in the 1924 Olympics in Paris. Also accepted are Abrahams' Cambridge friends, Lord Andrew Lindsay (Nigel Havers), Aubrey Montague (Nicholas Farrell), and Henry Stallard (Daniel Gerroll).\n\nWhile boarding the boat to Paris for the Olympics, Liddell discovers the heats for his 100-metre race will be on a Sunday. He refuses to run the race, despite strong pressure from the Prince of Wales and the British Olympic committee, because his Christian convictions prevent him from running on the Sabbath.\n\nHope appears when Liddell's teammate Lindsay, having already won a silver medal in the 400 metres hurdles, proposes to concede his place in the 400-metre race on the following Thursday to Liddell, who gratefully agrees. His religious convictions in the face of national athletic pride make headlines around the world.\n\nLiddell delivers a sermon at the Paris Church of Scotland that Sunday, and quotes from , ending with, \"But they that wait upon the Lord shall renew their strength; they shall mount up with wings as eagles; they shall run, and not be weary; and they shall walk, and not faint.\"\n\nAbrahams is badly beaten by the heavily favoured United States runners in the 200 metre race. He knows his last chance for a medal will be the 100 metres. He competes in the race, and wins. His coach Sam Mussabini is overcome that the years of dedication and training have paid off with an Olympic gold medal. Now Abrahams can get on with his life and reunite with his girlfriend Sybil, whom he had neglected for the sake of running.\n\nBefore Liddell's race, the American coach remarks dismissively to his runners that Liddell has little chance of doing well in his now far longer 400 metre race. But one of the American runners, Jackson Scholz, hands Liddell a note of support for his convictions. Liddell defeats the American favourites and wins the gold medal.\n\nThe British team returns home triumphant. As the film ends, onscreen text explains that Abrahams married Sybil, and became the elder statesman of British athletics. Liddell went on to missionary work in China. All of Scotland mourned his death in 1945 in Japanese-occupied China.\n\n\nThe film depicts Abrahams as attending Gonville and Caius College, Cambridge with three other Olympic athletes: Henry Stallard, Aubrey Montague, and Lord Andrew Lindsay. Abrahams and Stallard were in fact students there and competed in the 1924 Olympics. Montague also competed in the Olympics as depicted, but he attended Oxford, not Cambridge. Aubrey Montague sent daily letters to his mother about his time at Oxford and the Olympics; these letters were the basis of Montague's narration in the film.\n\nThe character of Lindsay was based partially on Lord Burghley, a significant figure in the history of British athletics. Although Burghley did attend Cambridge, he was not a contemporary of Harold Abrahams, as Abrahams was an undergraduate from 1919 to 1923 and Burghley was at Cambridge from 1923 to 1927. One scene in the film depicts the Burghley-based \"Lindsay\" as practising hurdles on his estate with full champagne glasses placed on each hurdle – this was something the wealthy Burghley did, although he used matchboxes instead of champagne glasses. The fictional character of Lindsay was created when Douglas Lowe, who was Britain's third athletics gold medallist in the 1924 Olympics, was not willing to be involved with the film.\nAnother scene in the film recreates the Great Court Run, in which the runners attempt to run around the perimeter of the Great Court at Trinity College, Cambridge in the time it takes the clock to strike 12 at midday. The film shows Abrahams performing the feat for the first time in history. In fact, Abrahams never attempted this race, and at the time of filming the only person on record known to have succeeded was Lord Burghley, in 1927. In \"Chariots of Fire\", Lindsay, who is based on Lord Burghley, runs the Great Court Run with Abrahams in order to spur him on, and crosses the finish line just a moment too late. Since the film's release, the Great Court Run has also been successfully run by Trinity undergraduate Sam Dobin, in October 2007.\n\nIn the film, Eric Liddell is tripped up by a Frenchman in the 400-metre event of a Scotland–France international athletic meeting. He recovers, makes up a 20-metre deficit, and wins. This was based on fact; the actual race was the 440 yards at a Triangular Contest meet between Scotland, England, and Ireland at Stoke-on-Trent in England in July 1923. His achievement was remarkable as he had already won the 100- and 220-yard events that day. Also unmentioned with regard to Liddell is that it was he who introduced Abrahams to Sam Mussabini. This is alluded to: In the film Abrahams first encounters Mussabini while he is watching Liddell race. The film, however, suggests that Abrahams himself sought Mussabini's assistance.\n\nAbrahams and Liddell did race against each other once, but not quite as depicted in the film, which shows Liddell winning the final of the 100 yards against a shattered Abrahams at the 1923 AAA Championship at Stamford Bridge. In fact, they raced only in a heat of the 220 yards, which Liddell won, five yards ahead of Abrahams, who did not progress to the final. In the 100 yards, Abrahams was eliminated in the heats and never raced against Liddell, who won the finals of both races the next day.\n\nAbrahams' fiancée is misidentified as Sybil Gordon, a soprano at the D'Oyly Carte Opera Company. In fact, in 1936, Abrahams married Sybil Evers, who sang at the D'Oyly Carte, but they did not meet until 1934. Also, in the film, Sybil is depicted as singing the role of Yum-Yum in \"The Mikado\", but neither Sybil Gordon nor Sybil Evers ever sang that role with D'Oyly Carte, although Evers was known for her charm in singing Peep-Bo, one of the two other \"little maids from school\". Harold Abrahams' love of and heavy involvement with Gilbert and Sullivan, as depicted in the film, is factual.\n\nLiddell's sister was several years younger than she was portrayed in the film. Her disapproval of Liddell's track career was creative licence; she actually fully supported his sporting work. Jenny Liddell Somerville cooperated fully with the making of the film and has a brief cameo in the Paris Church of Scotland during Liddell's sermon.\n\nAt the memorial service for Harold Abrahams, which opens the film, Lord Lindsay mentions that he and Aubrey Montague are the only members of the 1924 Olympic team still alive. However, Montague died in 1948, 30 years before Abrahams' death.\n\nThe film takes some liberties with the events at the 1924 Olympics, including the events surrounding Liddell's refusal to race on a Sunday. In the film, he doesn't learn that the 100-metre heat is to be held on the Christian Sabbath until he is boarding the boat to Paris. In fact, the schedule was made public several months in advance; Liddell did however face immense pressure to run on that Sunday and to compete in the 100 metres, getting called before a grilling by the British Olympic Committee, the Prince of Wales, and other grandees, and his refusal to run made headlines around the world.\n\nThe decision to change races was, even so, made well before embarking to Paris, and Liddell spent the intervening months training for the 400 metres, an event in which he had previously excelled. It is true, nonetheless, that Liddell's success in the Olympic 400m was largely unexpected.\n\nThe film depicts Lindsay, having already won a medal in the 400-metre hurdles, giving up his place in the 400-metre race for Liddell. In fact Burghley, on whom Lindsay is loosely based, was eliminated in the heats of the 110 hurdles (he would go on to win a gold medal in the 400 hurdles at the 1928 Olympics), and was not entered for the 400 metres.\n\nThe film reverses the order of Abrahams' 100m and 200m races at the Olympics. In reality, after winning the 100 metres race, Abrahams ran the 200 metres but finished last, Jackson Scholz taking the gold medal. In the film, before his triumph in the 100m, Abrahams is shown losing the 200m and being scolded by Mussabini. And during the following scene in which Abrahams speaks with his friend Montague while receiving a massage from Mussabini, there is a French newspaper clipping showing Scholz and Charlie Paddock with a headline which states that the 200 metres was a triumph for the United States. In the same conversation, Abrahams laments getting \"beaten out of sight\" in the 200. The film thus has Abrahams overcoming the disappointment of losing the 200 by going on to win the 100, a reversal of the real order.\n\nEric Liddell actually also ran in the 200m race, and finished third, behind Paddock and Scholz. This was the only time in reality that Liddell and Abrahams competed in the same race. While their meeting in the 1923 AAA Championship in the film was fictitious, Liddell's record win in that race did spur Abrahams to train even harder.\n\nAbrahams also won a silver medal as an opening runner for the 4 x 100 metres relay team, not shown in the film, and Aubrey Montague placed sixth in the steeplechase, as depicted.\n\nIn the film, the 100m bronze medallist is a character called \"Tom Watson\"; the real medallist was Arthur Porritt of New Zealand, who refused permission for his name to be used in the film, allegedly out of modesty, and his wish was accepted by the film's producers, even though his permission was not necessary. However, the brief back-story given for Watson, who is called up to the New Zealand team from the University of Oxford, substantially matches Porritt's history. With the exception of Porritt, all the runners in the 100m final are identified correctly when they line up for inspection by the Prince of Wales.\n\nJackson Scholz is depicted as handing Liddell an inspirational Bible-quotation message before the 400 metres final: \"It says in the Old Book, 'He that honors me, I will honor.' Good luck.\" In reality, the note was from members of the British team, and was handed to Liddell before the race by his attending masseur at the team's Paris hotel. For dramatic purposes, screenwriter Welland asked Scholz if he could be depicted handing the note, and Scholz readily agreed, saying \"Yes, great, as long as it makes me look good.\"\n\nProducer David Puttnam was looking for a story in the mold of \"A Man for All Seasons\" (1966), regarding someone who follows his conscience, and felt sports provided clear situations in this sense. He discovered Eric Liddell's story by accident in 1977, when he happened upon a reference book on the Olympics while housebound from the flu in a rented house in Los Angeles.\n\nScreenwriter Colin Welland, commissioned by Puttnam, did an enormous amount of research for his Academy Award-winning script. Among other things, he took out advertisements in London newspapers seeking memories of the 1924 Olympics, went to the National Film Archives for pictures and footage of the 1924 Olympics, and interviewed everyone involved who was still alive. Welland just missed Abrahams, who died 14 January 1978, but he did attend Abrahams' February 1978 memorial service, which inspired the present-day framing device of the film. Aubrey Montague's son saw Welland's newspaper ad and sent him copies of the letters his father had sent home – which gave Welland something to use as a narrative bridge in the film. Except for changes in the greetings of the letters from \"Darling Mummy\" to \"Dear Mum\" and the change from Oxford to Cambridge, all of the readings from Montague's letters are from the originals.\n\nWelland's original script also featured, in addition to Eric Liddell and Harold Abrahams, a third protagonist, 1924 Olympic gold medallist Douglas Lowe, who was presented as a privileged aristocratic athlete. However, Lowe refused to have anything to do with the film, and his character was written out and replaced by the fictional character of Lord Andrew Lindsay.\n\nIan Charleson himself wrote Eric Liddell's speech to the post-race workingmen's crowd at the Scotland v. Ireland races. Charleson, who had studied the Bible intensively in preparation for the role, told director Hugh Hudson that he didn't feel the portentous and sanctimonious scripted speech was either authentic or inspiring. Hudson and Welland allowed him to write words he personally found inspirational instead.\n\nThe film was slightly altered for the U.S. audience. A brief scene depicting a pre-Olympics cricket game between Abrahams, Liddell, Montague, and the rest of the British track team appears shortly after the beginning of the original film. For the American audience, this brief scene was deleted. In the U.S., to avoid the initial G rating, which had been strongly associated with children's films and might have hindered box office sales, a different scene was used – one depicting Abrahams and Montague arriving at a Cambridge railway station and encountering two World War I veterans who use an obscenity – in order to be given a PG rating.\n\nPuttnam chose Hugh Hudson, a multiple award-winning advertising and documentary filmmaker who had never helmed a feature film, to direct \"Chariots of Fire\". Hudson and Puttnam had known each other since the 1960s, when Puttnam was an advertising executive and Hudson was making films for ad agencies. In 1977, Hudson had also been second-unit director on the Puttnam-produced film \"Midnight Express\".\n\nDirector Hugh Hudson was determined to cast young, unknown actors in all the major roles of the film, and to back them up by using veterans like John Gielgud, Lindsay Anderson, and Ian Holm as their supporting cast. Hudson and producer David Puttnam did months of fruitless searching for the perfect actor to play Eric Liddell. They then saw Scottish stage actor Ian Charleson performing the role of Pierre in the Royal Shakespeare Company's production of \"Piaf\", and knew immediately they had found their man. Unbeknownst to them, Charleson had heard about the film from his father, and desperately wanted to play the part, feeling it would \"fit like a kid glove\".\n\nBen Cross, who plays Harold Abrahams, was discovered while playing Billy Flynn in \"Chicago\". In addition to having a natural pugnaciousness, he had the desired ability to sing and play the piano. Cross was thrilled to be cast, and said he was moved to tears by the film's script.\n\n20th Century Fox, which put up half of the production budget in exchange for distribution rights outside of North America, insisted on having a couple of notable American names in the cast. Thus the small parts of the two American champion runners, Jackson Scholz and Charlie Paddock, were cast with recent headliners: Brad Davis had recently starred in \"Midnight Express\" (also produced by Puttnam), and Dennis Christopher had recently starred, as a young bicycle racer, in the popular indie film \"Breaking Away\".\n\nAll of the actors portraying runners underwent a gruelling three-month training intensive with renowned running coach Tom McNab. This training and isolation of the actors also created a strong bond and sense of camaraderie among them.\n\nAlthough the film is a period piece, set in the 1920s, the Academy Award-winning original soundtrack composed by Vangelis uses a modern 1980s electronic sound, with a strong use of synthesizer and piano among other instruments. This was a bold and significant departure from earlier period films, which employed sweeping orchestral instrumentals. The title theme of the film has become iconic, and has been used in subsequent films and television shows during slow-motion segments.\n\nVangelis, a Greek-born electronic composer who moved to Paris in the late 1960s, had been living in London since 1974. Director Hugh Hudson had collaborated with him on documentaries and commercials, and was also particularly impressed with his 1979 albums \"Opera Sauvage\" and \"China\". David Puttnam also greatly admired Vangelis's body of work, having originally selected his compositions for his previous film \"Midnight Express\". Hudson made the choice for Vangelis and for a modern score: \"I knew we needed a piece which was anachronistic to the period to give it a feel of modernity. It was a risky idea but we went with it rather than have a period symphonic score.\" The soundtrack had a personal significance to Vangelis: After composing the iconic theme tune he told Puttnam, \"My father is a runner, and this is an anthem to him.\"\n\nHudson originally wanted Vangelis's 1977 tune \"L'Enfant\", from his \"Opera Sauvage\" album, to be the title theme of the film, and the beach running sequence was actually filmed with \"L'Enfant\" playing on loudspeakers for the runners to pace to. Vangelis finally convinced Hudson he could create a new and better piece for the film's main theme – and when he played the now-iconic \"Chariots of Fire\" theme for Hudson, it was agreed the new tune was unquestionably better. The \"L'Enfant\" melody still made it into the film: When the athletes reach Paris and enter the stadium, a brass band marches through the field, and first plays a modified, acoustic performance of the piece. Vangelis's electronic \"L'Enfant\" track eventually was used prominently in the 1982 film \"The Year of Living Dangerously\".\n\nSome pieces of Vangelis's music in the film did not end up on the film's soundtrack album. One of them is the background music to the race Eric Liddell runs in the Scottish highlands. This piece is a version of \"Hymne\", the original version of which appears on Vangelis's 1979 album, \"Opéra sauvage\". Various versions are also included on Vangelis's compilation albums \"Themes\", \"Portraits\", and \"\", though none of these include the version used in the film.\n\nFive lively Gilbert and Sullivan tunes also appear in the soundtrack, and serve as jaunty period music which nicely counterpoints Vangelis's modern electronic score. These are: \"He is an Englishman\" from \"H.M.S. Pinafore\", \"Three Little Maids from School Are We\" from \"The Mikado\", \"With Catlike Tread\" from \"The Pirates of Penzance\", \"The Soldiers of Our Queen\" from \"Patience\", and \"There Lived a King\" from \"The Gondoliers\".\n\nThe film also incorporates a major traditional work: \"Jerusalem\", sung by a British choir at the 1978 funeral of Harold Abrahams. The words, written by William Blake in 1804-8, were set to music by Parry in 1916 as a celebration of England. This hymn has been described as \"England's unofficial national anthem\", concludes the film and inspired its title. A handful of other traditional anthems and hymns and period-appropriate instrumental ballroom-dance music round out the film's soundtrack.\n\nThe beach scenes associated with the theme tune were filmed at West Sands, St Andrews. A plaque commemorating the filming can be found there today. The very last scene of the opening titles crosses the 1st and 18th holes of the Old Course at St Andrews Links.\n\nAll of the Cambridge scenes were actually filmed at Hugh Hudson's alma mater Eton College, because Cambridge refused filming rights, fearing depictions of anti-Semitism. The Cambridge administration greatly regretted the decision after the film's enormous success.\n\nLiverpool Town Hall was the setting for the scenes depicting the British Embassy in Paris. The Colombes Olympic Stadium in Paris was represented by the Oval Sports Centre, Bebington, Merseyside. The nearby Woodside ferry terminal was used to represent the embarkation scenes set in Dover. The railway station scenes were filmed in York, using locomotives from the National Railway Museum. The scene depicting a performance of \"The Mikado\" was filmed in the Royal Court Theatre, Liverpool with members of the D'Oyly Carte Opera Company who were on tour.\n\n\"Chariots of Fire\" became a recurring theme in promotions for the 2012 Summer Olympics in London. The film's theme tune was featured at the opening of the 2012 London New Years fireworks celebrating the Olympics, and the film's iconic beach-running scene and theme tune were used in \"The Sun\"'s \"Let's Make It Great, Britain\" Olympic ads. The runners who first tested the new Olympic Park were spurred on by the \"Chariots of Fire\" theme tune, and the iconic music was also used to fanfare the carriers of the Olympic flame on parts of its route through the UK. The beach-running sequence was also recreated at St. Andrews and filmed as part of the Olympic torch relay.\n\nThe film's theme was also performed by the London Symphony Orchestra, conducted by Simon Rattle, during the Opening Ceremony of the games; the performance was accompanied by a comedy skit by Rowan Atkinson (as Mr Bean) which included the opening beach-running footage from the film. The film's theme tune was also played during each medal ceremony of the 2012 Olympics.\n\nA stage adaptation of \"Chariots of Fire\" was mounted in honour of the 2012 Olympics. The play, \"Chariots of Fire\", which was adapted by playwright Mike Bartlett and included the iconic Vangelis score, ran from 9 May to 16 June 2012 at London's Hampstead Theatre, and transferred to the Gielgud Theatre in the West End on 23 June, where it ran until 5 January 2013. It starred Jack Lowden as Eric Liddell and James McArdle as Harold Abrahams, and Edward Hall directed. Stage designer Miriam Buether transformed each theatre into an Olympic stadium, and composer Jason Carr wrote additional music. Vangelis also created several new pieces of music for the production. The stage version for the London Olympic year was the idea of the film's director, Hugh Hudson, who co-produced the play; he stated, \"Issues of faith, of refusal to compromise, standing up for one's beliefs, achieving something for the sake of it, with passion, and not just for fame or financial gain, are even more vital today.\"\n\nAnother play, \"Running for Glory\", written by Philip Dart, based on the 1924 Olympics, and focusing on Abrahams and Liddell, toured parts of Britain from 25 February to 1 April 2012. It starred Nicholas Jacobs as Harold Abrahams, and Tom Micklem as Eric Liddell.\n\nAs an official part of the London 2012 Festival celebrations, a new digitally re-mastered version of the film screened in 150 cinemas throughout the UK. The re-release began 13 July 2012, two weeks before the opening ceremony of the London Olympics.\n\nA Blu-ray of the film was released on 10 July 2012 in North America, and was released 16 July 2012 in the UK. The release includes nearly an hour of special features, a CD sampler, and a 32-page \"digibook\".\n\nSince its release \"Chariots of Fire\" has received generally positive reviews from critics. the film holds an 83% \"Certified Fresh\" rating on review aggregator website Rotten Tomatoes based on 64 reviews, accompanied by the summary \"Decidedly slower and less limber than the Olympic runners at the center of its story, the film nevertheless manages to make effectively stirring use of its spiritual and patriotic themes.\"\n\nFor its 2012 re-release, Kate Muir of \"The Times\" gave the film five stars, writing: \"In a time when drug tests and synthetic fibres have replaced gumption and moral fibre, the tale of two runners competing against each other in the 1924 Olympics has a simple, undiminished power. From the opening scene of pale young men racing barefoot along the beach, full of hope and elation, backed by Vangelis's now famous anthem, the film is utterly compelling.\"\n\n\"Chariots of Fire\" was very successful at the 54th Academy Awards, winning four of seven nominations. When accepting his Oscar for Best Original Screenplay, Colin Welland famously announced \"The British are coming\". At the 1981 Cannes Film Festival the film won two awards and competed for the Palme d'Or.\n\n\nAmerican Film Institute recognition\n\n\n\n"}
{"id": "5734", "url": "https://en.wikipedia.org/wiki?curid=5734", "title": "Consequentialism", "text": "Consequentialism\n\nConsequentialism is the class of normative ethical theories holding that the consequences of one's conduct are the ultimate basis for any judgment about the rightness or wrongness of that conduct. Thus, from a consequentialist standpoint, a morally right act (or omission from acting) is one that will produce a good outcome, or consequence. \n\nConsequentialism is primarily non-prescriptive, meaning the moral worth of an action is determined by its potential consequence, not by whether it follows a set of written edicts or laws. One example would entail lying under the threat of government punishment to save an innocent person's life, even though it is illegal to lie under oath.\n\nConsequentialism is usually contrasted with deontological ethics (or \"deontology\"), in that deontology, in which rules and moral duty are central, derives the rightness or wrongness of one's conduct from the character of the behaviour itself rather than the outcomes of the conduct. It is also contrasted with virtue ethics, which focuses on the character of the agent rather than on the nature or consequences of the act (or omission) itself, and pragmatic ethics which treats morality like science: advancing socially over the course of many lifetimes, such that any moral criterion is subject to revision. Consequentialist theories differ in how they define moral goods.\n\nSome argue that consequentialist and deontological theories are not necessarily mutually exclusive. For example, T. M. Scanlon advances the idea that human rights, which are commonly considered a \"deontological\" concept, can only be justified with reference to the consequences of having those rights. Similarly, Robert Nozick argues for a theory that is mostly consequentialist, but incorporates inviolable \"side-constraints\" which restrict the sort of actions agents are permitted to do.\n\nMohist consequentialism, also known as state consequentialism, is an ethical theory which evaluates the moral worth of an action based on how much it contributes to the welfare of a state. According to the \"Stanford Encyclopedia of Philosophy\", Mohist consequentialism, dating back to the 5th century BCE, is the \"world's earliest form of consequentialism, a remarkably sophisticated version based on a plurality of intrinsic goods taken as constitutive of human welfare\".\n\nUnlike utilitarianism, which views utility as the sole moral good, \"the basic goods in Mohist consequentialist thinking are... order, material wealth, and increase in population\". During Mozi's era, war and famines were common, and population growth was seen as a moral necessity for a harmonious society. The \"material wealth\" of Mohist consequentialism refers to basic needs like shelter and clothing, and the \"order\" of Mohist consequentialism refers to Mozi's stance against warfare and violence, which he viewed as pointless and a threat to social stability. Stanford sinologist David Shepherd Nivison, in \"The Cambridge History of Ancient China\", writes that the moral goods of Mohism \"are interrelated: more basic wealth, then more reproduction; more people, then more production and wealth... if people have plenty, they would be good, filial, kind, and so on unproblematically\".\n\nThe Mohists believed that morality is based on \"promoting the benefit of all under heaven and eliminating harm to all under heaven\". In contrast to Jeremy Bentham's views, state consequentialism is not utilitarian because it is not hedonistic or individualistic. The importance of outcomes that are good for the community outweigh the importance of individual pleasure and pain. The term state consequentialism has also been applied to the political philosophy of the Confucian philosopher Xunzi.\n\nOn the other hand, the \"Legalist\" Han Fei \"is motivated almost totally from the ruler's point of view\".\n\nIn summary, Jeremy Bentham states that people are driven by their interests and their fears, but their interests take precedence over their fears, and their interests are carried out in accordance with how people view the consequences that might be involved with their interests. \"Happiness\" on this account is defined as the maximization of pleasure and the minimization of pain.\nHistorically, hedonistic utilitarianism is the paradigmatic example of a consequentialist moral theory. This form of utilitarianism holds that what matters is the aggregate happiness; the happiness of everyone and not the happiness of any particular person. John Stuart Mill, in his exposition of hedonistic utilitarianism, proposed a hierarchy of pleasures, meaning that the pursuit of certain kinds of pleasure is more highly valued than the pursuit of other pleasures. However, some contemporary utilitarians, such as Peter Singer, are concerned with maximizing the satisfaction of preferences, hence \"preference utilitarianism\". Other contemporary forms of utilitarianism mirror the forms of consequentialism outlined below.\n\nEthical egoism can be understood as a consequentialist theory according to which the consequences for the individual agent are taken to matter more than any other result. Thus, egoism will prescribe actions that may be beneficial, detrimental, or neutral to the welfare of others. Some, like Henry Sidgwick, argue that a certain degree of egoism \"promotes\" the general welfare of society for two reasons: because individuals know how to please themselves best, and because if everyone were an austere altruist then general welfare would inevitably decrease.\n\nEthical altruism can be seen as a consequentialist ethic which prescribes that an individual take actions that have the best consequences for everyone except for himself. This was advocated by Auguste Comte, who coined the term \"altruism,\" and whose ethics can be summed up in the phrase \"Live for others\".\n\nIn general, consequentialist theories focus on actions. However, this need not be the case. Rule consequentialism is a theory that is sometimes seen as an attempt to reconcile deontology and consequentialism—and in some cases, this is stated as a criticism of rule consequentialism. Like deontology, rule consequentialism holds that moral behavior involves following certain rules. However, rule consequentialism chooses rules based on the consequences that the selection of those rules has. Rule consequentialism exists in the forms of rule utilitarianism and rule egoism.\n\nVarious theorists are split as to whether the rules are the only determinant of moral behavior or not. For example, Robert Nozick holds that a certain set of minimal rules, which he calls \"side-constraints\", are necessary to ensure appropriate actions. There are also differences as to how absolute these moral rules are. Thus, while Nozick's side-constraints are absolute restrictions on behavior, Amartya Sen proposes a theory that recognizes the importance of certain rules, but these rules are not absolute. That is, they may be violated if strict adherence to the rule would lead to much more undesirable consequences.\n\nOne of the most common objections to rule-consequentialism is that it is incoherent, because it is based on the consequentialist principle that what we should be concerned with is maximizing the good, but then it tells us not to act to maximize the good, but to follow rules (even in cases where we know that breaking the rule could produce better results).\n\nBrad Hooker avoided this objection by not basing his form of rule-consequentialism on the ideal of maximizing the good. He writes:\n\n…the best argument for rule-consequentialism is not that it derives from an overarching commitment to maximise the good. The best argument for rule-consequentialism is that it does a better job than its rivals of matching and tying together our moral convictions, as well as offering us help with our moral disagreements and uncertainties.\n\nDerek Parfit described Brad Hooker's book on rule-consequentialism \"Ideal Code, Real World\" as the \"best statement and defence, so far, of one of the most important moral theories\".\n\nRule-consequentialism may offer a means to reconcile pure consequentialism with deontological, or rules-based ethics.\n\nThe two-level approach involves engaging in critical reasoning and considering all the possible ramifications of one's actions before making an ethical decision, but reverting to generally reliable moral rules when one is not in a position to stand back and examine the dilemma as a whole. In practice, this equates to adhering to rule consequentialism when one can only reason on an intuitive level, and to act consequentialism when in a position to stand back and reason on a more critical level.\n\nThis position can be described as a reconciliation between act consequentialism – in which the morality of an action is determined by that action's effects – and rule consequentialism – in which moral behavior is derived from following rules that lead to positive outcomes.\n\nThe two-level approach to consequentialism is most often associated with R. M. Hare and Peter Singer.\n\nAnother consequentialist version is motive consequentialism which looks at whether the state of affairs that results from the motive to choose an action is better or at least as good as each of the alternative state of affairs that would have resulted from alternative actions. This version gives relevance to the motive of an act and links it to its consequences. An act can therefore not be wrong if the decision to act was based on a right motive. A possible inference is, that one can not be blamed for mistaken judgments if the motivation was to do good.\n\nMost consequentialist theories focus on \"promoting\" some sort of good consequences. However, negative utilitarianism lays out a consequentialist theory that focuses solely on minimizing bad consequences.\n\nOne major difference between these two approaches is the agent's responsibility. Positive consequentialism demands that we bring about good states of affairs, whereas negative consequentialism requires that we avoid bad ones. Stronger versions of negative consequentialism will require active intervention to prevent bad and ameliorate existing harm. In weaker versions, simple forbearance from acts tending to harm others is sufficient. An example of this is the Slippery Slope Argument, which encourages others to avoid a specified act on the grounds that it may ultimately lead to undesirable consequences.\n\nOften \"negative\" consequentialist theories assert that reducing suffering is more important than increasing pleasure. Karl Popper, for example, claimed \"…from the moral point of view, pain cannot be outweighed by pleasure...\". (While Popper is not a consequentialist per se, this is taken as a classic statement of negative utilitarianism.) When considering a theory of justice, negative consequentialists may use a statewide or global-reaching principle: the reduction of suffering (for the disadvantaged) is more valuable than increased pleasure (for the affluent or luxurious).\n\nTeleological ethics (Greek telos, \"end\"; logos, \"science\") is an ethical theory that holds that the ends or consequences of an act determine whether an act is good or evil. Teleological theories are often discussed in opposition to deontological ethical theories, which hold that acts themselves are \"inherently\" good or evil, regardless of the consequences of acts. The saying, \"the end justifies the means\", meaning that if a goal is morally important enough, any method of achieving it is acceptable.\n\nTeleological theories differ on the nature of the end that actions ought to promote. Eudaemonist theories (Greek eudaimonia, \"happiness\") hold that the goal of ethics consists in some function or activity appropriate to man as a human being, and thus tend to emphasize the cultivation of virtue or excellence in the agent as the end of all action. These could be the classical virtues—courage, temperance, justice, and wisdom—that promoted the Greek ideal of man as the \"rational animal\", or the theological virtues—faith, hope, and love—that distinguished the Christian ideal of man as a being created in the image of God.\nUtilitarian-type theories hold that the end consists in an experience or feeling produced by the action. Hedonism, for example, teaches that this feeling is pleasure—either one's own, as in egoism (the 17th-century English philosopher Thomas Hobbes), or everyone's, as in universalistic hedonism, or utilitarianism (the 19th-century English philosophers Jeremy Bentham, John Stuart Mill, and Henry Sidgwick), with its formula of the \"greatest pleasure of the greatest number\".\n\nOther utilitarian-type views include the claims that the end of action is survival and growth, as in evolutionary ethics (the 19th-century English philosopher Herbert Spencer); the experience of power, as in despotism; satisfaction and adjustment, as in pragmatism (20th-century American philosophers Ralph Barton Perry and John Dewey); and freedom, as in existentialism (the 20th-century French philosopher Jean-Paul Sartre).\n\nThe chief problem for eudaemonist theories is to show that leading a life of virtue will also be attended by happiness—by the winning of the goods regarded as the chief end of action. That Job should suffer and Socrates and Jesus die while the wicked prosper, then seems unjust. Eudaemonists generally reply that the universe is moral and that, in Socrates' words, \"No evil can happen to a good man, either in life or after death,\" or, in Jesus' words, \"But he who endures to the end will be saved.\" (Matt 10:22).\n\nUtilitarian theories, on the other hand, must answer the charge that ends do not justify the means. The problem arises in these theories because they tend to separate the achieved ends from the action by which these ends were produced. One implication of utilitarianism is that one's intention in performing an act may include all of its foreseen consequences. The goodness of the intention then reflects the balance of the good and evil of these consequences, with no limits imposed upon it by the nature of the act itself—even if it be, say, the breaking of a promise or the execution of an innocent man. Utilitarianism, in answering this charge, must show either that what is apparently immoral is not really so or that, if it really is so, then closer examination of the consequences will bring this fact to light. Ideal utilitarianism (G.E. Moore and Hastings Rashdall) tries to meet the difficulty by advocating a plurality of ends and including among them the attainment of virtue itself, which, as John Stuart Mill affirmed, \"may be felt a good in itself, and desired as such with as great intensity as any other good\".\n\nSince pure consequentialism holds that an action is to be judged solely by its result, most consequentialist theories hold that a deliberate action is no different from a deliberate decision not to act. This contrasts with the \"acts and omissions doctrine\", which is upheld by some medical ethicists and some religions: it asserts there is a significant moral distinction between acts and deliberate non-actions which lead to the same outcome. This contrast is brought out in issues such as voluntary euthanasia.\n\nOne important characteristic of many normative moral theories such as consequentialism is the ability to produce practical moral judgements. At the very least, any moral theory needs to define the standpoint from which the goodness of the consequences are to be determined. What is primarily at stake here is the \"responsibility\" of the agent.\n\nOne common tactic among consequentialists, particularly those committed to an altruistic (selfless) account of consequentialism, is to employ an ideal, neutral observer from which moral judgements can be made. John Rawls, a critic of utilitarianism, argues that utilitarianism, in common with other forms of consequentialism, relies on the perspective of such an ideal observer. The particular characteristics of this ideal observer can vary from an omniscient observer, who would grasp all the consequences of any action, to an ideally informed observer, who knows as much as could reasonably be expected, but not necessarily all the circumstances or all the possible consequences. Consequentialist theories that adopt this paradigm hold that right action is the action that will bring about the best consequences from this ideal observer's perspective.\n\nIn practice, it is very difficult, and at times arguably impossible, to adopt the point of view of an ideal observer. Individual moral agents do not know everything about their particular situations, and thus do not know all the possible consequences of their potential actions. For this reason, some theorists have argued that consequentialist theories can only require agents to choose the best action in line with what they know about the situation. However, if this approach is naïvely adopted, then moral agents who, for example, recklessly fail to reflect on their situation, and act in a way that brings about terrible results, could be said to be acting in a morally justifiable way. Acting in a situation without first informing oneself of the circumstances of the situation can lead to even the most well-intended actions yielding miserable consequences. As a result, it could be argued that there is a moral imperative for an agent to inform himself as much as possible about a situation before judging the appropriate course of action. This imperative, of course, is derived from consequential thinking: a better-informed agent is able to bring about better consequences.\n\nMoral action always has consequences for certain people or things. Varieties of consequentialism can be differentiated by the beneficiary of the good consequences. That is, one might ask \"Consequences for whom?\"\n\nA fundamental distinction can be drawn between theories which require that agents act for ends perhaps disconnected from their own interests and drives, and theories which permit that agents act for ends in which they have some personal interest or motivation. These are called \"agent-neutral\" and \"agent-focused\" theories respectively.\n\nAgent-neutral consequentialism ignores the specific value a state of affairs has for any particular agent. Thus, in an agent-neutral theory, an actor's personal goals do not count any more than anyone else's goals in evaluating what action the actor should take. Agent-focused consequentialism, on the other hand, focuses on the particular needs of the moral agent. Thus, in an agent-focused account, such as one that Peter Railton outlines, the agent might be concerned with the general welfare, but the agent is \"more\" concerned with the immediate welfare of herself and her friends and family.\n\nThese two approaches could be reconciled by acknowledging the tension between an agent's interests as an individual and as a member of various groups, and seeking to somehow optimize among all of these interests. For example, it may be meaningful to speak of an action as being good for someone as an individual, but bad for them as a citizen of their town.\n\nMany consequentialist theories may seem primarily concerned with human beings and their relationships with other human beings. However, some philosophers argue that we should not limit our ethical consideration to the interests of human beings alone. Jeremy Bentham, who is regarded as the founder of utilitarianism, argues that animals can experience pleasure and pain, thus demanding that 'non-human animals' should be a serious object of moral concern. More recently, Peter Singer has argued that it is unreasonable that we do not give equal consideration to the interests of animals as to those of human beings when we choose the way we are to treat them. Such equal consideration does not necessarily imply identical treatment of humans and non-humans, any more than it necessarily implies identical treatment of all humans.\n\nOne way to divide various consequentialisms is by the types of consequences that are taken to matter most, that is, which consequences count as good states of affairs. According to utilitarianism, a good action is one that results in an increase in pleasure, and the best action is one that results in the most pleasure for the greatest number. Closely related is eudaimonic consequentialism, according to which a full, flourishing life, which may or may not be the same as enjoying a great deal of pleasure, is the ultimate aim. Similarly, one might adopt an aesthetic consequentialism, in which the ultimate aim is to produce beauty. However, one might fix on non-psychological goods as the relevant effect. Thus, one might pursue an increase in material equality or political liberty instead of something like the more ephemeral \"pleasure\". Other theories adopt a package of several goods, all to be promoted equally.\n\nConsequentialism can also be contrasted with aretaic moral theories such as virtue ethics. Whereas consequentialist theories posit that consequences of action should be the primary focus of our thinking about ethics, virtue ethics insists that it is the character rather than the consequences of actions that should be the focal point. Some virtue ethicists hold that consequentialist theories totally disregard the development and importance of moral character. For example, Philippa Foot argues that consequences in themselves have no ethical content, unless it has been provided by a virtue such as benevolence.\n\nHowever, consequentialism and virtue ethics need not be entirely antagonistic. Iain King has developed an approach that reconciles the two schools. Other consequentialists consider effects on the character of people involved in an action when assessing consequence. Similarly, a consequentialist theory may aim at the maximization of a particular virtue or set of virtues. Finally, following Foot's lead, one might adopt a sort of consequentialism that argues that virtuous activity ultimately produces the best consequences.\nThe \"ultimate end\" is a concept in the moral philosophy of Max Weber, in which individuals act in a faithful, rather than rational, manner.\n\nThe term \"consequentialism\" was G. E. M. Anscombe in her essay \"Modern Moral Philosophy\" in 1958, to describe what she saw as the central error of certain moral theories, such as those propounded by Mill and Sidgwick.\n\nThe phrase and concept of \"The end justifies the means\" are at least as old as the first century BC. Ovid wrote in his Heroides that \"Exitus acta probat\" \"The result justifies the deed\".\n\nG. E. M. Anscombe objects to consequentialism on the grounds that it does not provide ethical guidance in what one ought to do because there is no distinction between consequences that are foreseen and those that are intended.\nBernard Williams has argued that consequentialism is alienating because it requires moral agents to put too much distance between themselves and their own projects and commitments. Williams argues that consequentialism requires moral agents to take a strictly impersonal view of all actions, since it is only the consequences, and not who produces them, that are said to matter. Williams argues that this demands too much of moral agents—since (he claims) consequentialism demands that they be willing to sacrifice any and all personal projects and commitments in any given circumstance in order to pursue the most beneficent course of action possible. He argues further that consequentialism fails to make sense of intuitions that it can matter whether or not someone is personally the author of a particular consequence. For example, that participating in a crime can matter, even if the crime would have been committed anyway, or would even have been worse, without the agent's participation.\n\nSome consequentialists—most notably Peter Railton—have attempted to develop a form of consequentialism that acknowledges and avoids the objections raised by Williams. Railton argues that Williams's criticisms can be avoided by adopting a form of consequentialism in which moral decisions are to be determined by the sort of life that they express. On his account, the agent should choose the sort of life that will, on the whole, produce the best overall effects.\n\n\n"}
{"id": "5735", "url": "https://en.wikipedia.org/wiki?curid=5735", "title": "Conscription", "text": "Conscription\n\nConscription, sometimes called the draft, is the compulsory enlistment of people in a national service, most often a military service. Conscription dates back to antiquity and continues in some countries to the present day under various names. The modern system of near-universal national conscription for young men dates to the French Revolution in the 1790s, where it became the basis of a very large and powerful military. Most European nations later copied the system in peacetime, so that men at a certain age would serve 1–8 years on active duty and then transfer to the reserve force.\n\nConscription is controversial for a range of reasons, including conscientious objection to military engagements on religious or philosophical grounds; political objection, for example to service for a disliked government or unpopular war; and ideological objection, for example, to a perceived violation of individual rights. Those conscripted may evade service, sometimes by leaving the country, and seeking asylum in another country. Some selection systems accommodate these attitudes by providing alternative service outside combat-operations roles or even outside the military, such as 'Siviilipalvelus' (alternative civil service) in Finland, Zivildienst (compulsory community service) in Austria and Switzerland. Many post-Soviet countries conscript male soldiers not only for armed forces but also for paramilitary organizations which are dedicated to police-like \"domestic only\" service (Internal Troops) or \"non-combat\" rescue duties (Civil defence troops) – none of which is considered alternative to the military conscription.\n\nAs of the early 21st century, many states no longer conscript soldiers, relying instead upon professional militaries with volunteers enlisted to meet the demand for troops. The ability to rely on such an arrangement, however, presupposes some degree of predictability with regard to both war-fighting requirements and the scope of hostilities. Many states that have abolished conscription therefore still reserve the power to resume it during wartime or times of crisis. States involved in wars or interstate rivalries are most likely to implement conscription, whereas democracies are less likely than autocracies to implement conscription. Former British colonies are less likely to have conscription, as they are influenced by British anticonscription norms that can be traced back to the English Civil War.\n\nAround the reign of Hammurabi (1791–1750 BC), the Babylonian Empire used a system of conscription called \"Ilkum\". Under that system those eligible were required to serve in the royal army in time of war. During times of peace they were instead required to provide labour for other activities of the state. In return for this service, people subject to it gained the right to hold land. It is possible that this right was not to hold land \"per se\" but specific land supplied by the state.\n\nVarious forms of avoiding military service are recorded. While it was outlawed by the Code of Hammurabi, the hiring of substitutes appears to have been practiced both before and after the creation of the code. Later records show that Ilkum commitments could become regularly traded. In other places, people simply left their towns to avoid their Ilkum service. Another option was to sell Ilkum lands and the commitments along with them. With the exception of a few exempted classes, this was forbidden by the Code of Hammurabi.\n\nIn medieval Scandinavia the \"leiðangr\" (Old Norse), \"leidang\" (Norwegian), \"leding\", (Danish), \"ledung\" (Swedish), \"lichting\" (Dutch), \"expeditio\" (Latin) or sometimes \"leþing\" (Old English), was a levy of free farmers conscripted into coastal fleets for seasonal excursions and in defence of the realm.\n\nThe bulk of the Anglo-Saxon English army, called the \"fyrd\", was composed of part-time English soldiers drawn from the freemen of each county. In the 690s Laws of Ine, three levels of fines are imposed on different social classes for neglecting military service. Some modern writers claim military service was restricted to the landowning minor nobility. These thegns were the land-holding aristocracy of the time and were required to serve with their own armour and weapons for a certain number of days each year. The historian David Sturdy has cautioned about regarding the \"fyrd\" as a precursor to a modern national army composed of all ranks of society, describing it as a \"ridiculous fantasy\":The persistent old belief that peasants and small farmers gathered to form a national army or \"fyrd\" is a strange delusion dreamt up by antiquarians in the late eighteenth or early nineteenth centuries to justify universal military conscription.\n\nMedieval levy in Poland was known as the \"pospolite ruszenie\".\n\nThe system of military slaves was widely used in the Middle East, beginning with the creation of the corps of Turkish slave-soldiers (\"ghulams\" or \"mamluks\") by the Abbasid caliph al-Mu'tasim in the 820s and 830s. The Turkish troops soon came to dominate the government, establishing a pattern throughout the Islamic world of a ruling military class, often separated by ethnicity, culture and even religion by the mass of the population, a paradigm that found its apogee in the Mamluks of Egypt and the Janissary corps of the Ottoman Empire, institutions that survived until the early 19th century.\n\nIn the middle of the 14th century, Ottoman Sultan Murad I developed personal troops to be loyal to him, with a slave army called the \"Kapıkulu\". The new force was built by taking Christian children from newly conquered lands, especially from the far areas of his empire, in a system known as the \"devşirme\" (translated \"gathering\" or \"converting\"). The captive children were forced to convert to Islam. The Sultans had the young boys trained over several years. Those who showed special promise in fighting skills were trained in advanced warrior skills, put into the sultan's personal service, and turned into the Janissaries, the elite branch of the \"Kapıkulu\". A number of distinguished military commanders of the Ottomans, and most of the imperial administrators and upper-level officials of the Empire, such as Pargalı İbrahim Pasha and Sokollu Mehmet Paşa, were recruited in this way. By 1609, the Sultan's \"Kapıkulu\" forces increased to about 100,000.\n\nIn later years, Sultans turned to the Barbary Pirates to supply their Jannissaries corps. Their attacks on ships off the coast of Africa or in the Mediterranean, and subsequent capture of able-bodied men for ransom or sale provided some captives for the Sultan's system. Starting in the 17th century, Christian families living under the Ottoman rule began to submit their sons into the Kapikulu system willingly, as they saw this as a potentially invaluable career opportunity for their children. Eventually the Sultan turned to foreign volunteers from the warrior clans of Circassians in southern Russia to fill his Janissary armies. As a whole the system began to break down, the loyalty of the Jannissaries became increasingly suspect. Mahmud II forcibly disbanded the Janissary corps in 1826.\n\nSimilar to the Janissaries in origin and means of development were the Mamluks of Egypt in the Middle Ages. The Mamluks were usually captive non-Muslim Iranian and Turkish children who had been kidnapped or bought as slaves from the Barbary coasts. The Egyptians assimilated and trained the boys and young men to become Islamic soldiers who served the Muslim caliphs and the Ayyubid sultans during the Middle Ages. The first mamluks served the Abbasid caliphs in 9th-century Baghdad. Over time they became a powerful military caste. On more than one occasion, they seized power, for example, ruling Egypt from 1250–1517.\n\nFrom 1250 Egypt had been ruled by the Bahri dynasty of Kipchak origin. Slaves from the Caucasus served in the army and formed an elite corp of troops. They eventually revolted in Egypt to form the Burgi dynasty. The Mamluks' excellent fighting abilities, massed Islamic armies, and overwhelming numbers succeeded in overcoming the Christian Crusader fortresses in the Holy Land. The Mamluks were the most successful defense against the Mongol Ilkhanate of Persia and Iraq from entering Egypt.\n\nOn the western coast of Africa, Berber Muslims captured non-Muslims to put to work as laborers. They generally converted the younger people to Islam and many became quite assimilated. In Morocco, the Berber looked south rather than north. The Moroccan Sultan Moulay Ismail, called \"the Bloodthirsty\" (1672–1727), employed a corps of 150,000 black slaves, called his Black Guard. He used them to coerce the country into submission.\n\nModern conscription, the massed military enlistment of national citizens, was devised during the French Revolution, to enable the Republic to defend itself from the attacks of European monarchies. Deputy Jean-Baptiste Jourdan gave its name to the 5 September 1798 Act, whose first article stated: \"Any Frenchman is a soldier and owes himself to the defense of the nation.\" It enabled the creation of the \"Grande Armée\", what Napoleon Bonaparte called \"the nation in arms\", which overwhelmed European professional armies that often numbered only into the low tens of thousands. More than 2.6 million men were inducted into the French military in this way between the years 1800 and 1813.\n\nThe defeat of the Prussian Army in particular shocked the Prussian establishment, which had believed it was invincible after the victories of Frederick the Great. The Prussians were used to relying on superior organization and tactical factors such as order of battle to focus superior troops against inferior ones. Given approximately equivalent forces, as was generally the case with professional armies, these factors showed considerable importance. However, they became considerably less important when the Prussian armies faced forces that outnumbered their own in some cases by more than ten to one. Scharnhorst advocated adopting the \"levée en masse\", the military conscription used by France. The \"Krümpersystem\" was the beginning of short-term compulsory service in Prussia, as opposed to the long-term conscription previously used.\n\nIn the Russian Empire, the military service time \"owed\" by serfs was 25 years at the beginning of the 19th century. In 1834 it was decreased to 20 years. The recruits were to be not younger than 17 and not older than 35. In 1874 Russia introduced universal conscription in the modern pattern, an innovation only made possible by the abolition of serfdom in 1861. New military law decreed that all male Russian subjects, when they reached the age of 20, were eligible to serve in the military for six years.\n\nIn the decades prior to World War I universal conscription along broadly Prussian lines became the norm for European armies, and those modeled on them. By 1914 the only substantial armies still completely dependent on voluntary enlistment were those of Britain and the United States. Some colonial powers such as France reserved their conscript armies for home service while maintaining professional units for overseas duties.\n\nThe range of eligible ages for conscripting was expanded to meet national demand during the World Wars.\nIn the United States, the Selective Service System drafted men for World War I initially in an age range from 21 to 30 but expanded its eligibility in 1918 to an age range of 18 to 45. In the case of a widespread mobilization of forces where service includes homefront defense, ages of conscripts may range much higher, with the oldest conscripts serving in roles requiring lesser mobility. Expanded-age conscription was common during the Second World War: in Britain, it was commonly known as \"call-up\" and extended to age 51. Nazi Germany termed it \"Volkssturm\" (\"People's Storm\") and included children as young as 16 and men as old as 60. During the Second World War, both Britain and the Soviet Union conscripted women. The United States was on the verge of drafting women into the Nurse Corps because it anticipated it would need the extra personnel for its planned invasion of Japan. However, the Japanese surrendered and the idea was abandoned.\n\nFeminists and other opponents of discrimination against men have criticized military conscription, or compulsory military service, as sexist.\n\nFeminists have argued that military conscription is sexist because wars serve the interests of what they view as the patriarchy, the military is a sexist institution, conscripts are therefore indoctrinated in sexism, and conscription of men normalizes violence by men as socially acceptable. Feminists have been organizers and participants in resistance to conscription in several countries.\n\nHistorically, only men have been subjected to conscription. Men who opt out of military service must often perform alternative service, such as Zivildienst in Austria and Switzerland, whereas women do not have these obligations.\n\nAmerican libertarians oppose conscription and call for the abolition of the Selective Service System, believing that impressment of individuals into the armed forces is \"involuntary servitude.\" Ron Paul, a former presidential nominee of the U.S. Libertarian Party has said that conscription \"is wrongly associated with patriotism, when it really represents slavery and involuntary servitude.\" The philosopher Ayn Rand opposed conscription, suggesting that \"of all the statist violations of individual rights in a mixed economy, the military draft is the worst. It is an abrogation of rights. It negates man's fundamental right—the right to life—and establishes the fundamental principle of statism: that a man's life belongs to the state, and the state may claim it by compelling him to sacrifice it in battle.\"\n\nIn 1917, a number of radicals and anarchists, including Emma Goldman, challenged the new draft law in federal court arguing that it was a direct violation of the Thirteenth Amendment's prohibition against slavery and involuntary servitude. However, the Supreme Court unanimously upheld the constitutionality of the draft act in the case of \"Arver v. United States\" on 7 January 1918. The decision said the Constitution gave Congress the power to declare war and to raise and support armies. The Court emphasized the principle of the reciprocal rights and duties of citizens:\n\nIt can be argued that in a cost-to-benefit ratio, conscription during peacetime is not worthwhile. Months or years of service performed by the most fit and capable subtract from the productivity of the economy; add to this the cost of training them, and in some countries paying them. Compared to these extensive costs, some would argue there is very little benefit; if there ever was a war then conscription and basic training could be completed quickly, and in any case there is little threat of a war in most countries with conscription. In the United States, every male resident is required by law to register with the Selective Service System within 30 days following his 18th birthday and be available for a draft; this is often accomplished automatically by a motor vehicle department during licensing or by voter registration).\n\nThe cost of conscription can be related to the parable of the broken window in anti-draft arguments. The cost of the work, military service, does not disappear even if no salary is paid. The work effort of the conscripts is effectively wasted, as an unwilling workforce is extremely inefficient. The impact is especially severe in wartime, when civilian professionals are forced to fight as amateur soldiers. Not only is the work effort of the conscripts wasted and productivity lost, but professionally skilled conscripts are also difficult to replace in the civilian workforce. Every soldier conscripted in the army is taken away from his civilian work, and away from contributing to the economy which funds the military. This may be less a problem in an agrarian or pre-industrialized state where the level of education is generally low, and where a worker is easily replaced by another. However, this is potentially more costly in a post-industrial society where educational levels are high and where the workforce is sophisticated and a replacement for a conscripted specialist is difficult to find. Even direr economic consequences result if the professional conscripted as an amateur soldier is killed or maimed for life; his work effort and productivity are lost.\n\nJean Jacques Rousseau argued vehemently against professional armies, feeling it was the right and privilege of every citizen to participate to the defense of the whole society and a mark of moral decline to leave this business to professionals. He based this view on the development of the Roman republic, which came to an end at the same time as the Roman army changed from a conscript to professional force. Similarly, Aristotle linked the division of armed service among the populace intimately with the political order of the state. Niccolò Machiavelli argued strongly for conscription, seeing the professional armies as the cause of the failure of societal unity in Italy.\n\nOther proponents, such as William James, consider both mandatory military and national service as ways of instilling maturity in young adults. Some proponents, such as Jonathan Alter and Mickey Kaus, support a draft in order to reinforce social equality, create social consciousness, break down class divisions and for young adults to immerse themselves in public enterprise. Charles Rangel called for the reinstatement of the draft during the Iraq conflict, not because he seriously expected it to be adopted, but to stress how the socioeconomic restratification meant that very few children of upper-class Americans served in the all-volunteer American armed forces. \n\nIt is estimated by the British military that in a professional military, a company deployed for active duty in peacekeeping corresponds to three inactive companies at home. Salaries for each are paid from the military budget. In contrast, volunteers from a trained reserve are in their civilian jobs when they are not deployed.\n\nIt was more financially beneficial for less-educated young Portuguese men born in 1967 to participate in conscription, as opposed to participating in the highly competitive job market with men of the same age who continued through to higher education.\n\nTraditionally conscription has been limited to the male population of a given body. Women and disabled men have been exempt from conscription. Many societies have considered, and continue to consider, military service as a test of manhood and a rite of passage from boyhood into manhood.\n\n, countries that were actively drafting women into military service included\nBolivia, Chad, Eritrea, Israel, Mozambique and North Korea. Israel has universal female conscription, although in practice women can avoid service by claiming a religious exemption and over a third of Israeli women do so.\nSudanese law allows for conscription of women, but this is not implemented in practice.\nIn the United Kingdom during World War II, beginning in 1941, women were brought into the scope of conscription but, as all women with dependent children were exempt and many women were informally left in occupations such as nursing or teaching, the number conscripted was relatively few.\n\nAfter a law passed in March 2003, Tunisia became the first Arab country to universally conscript women.\n\nIn 2015 Norway introduced female conscription, making it the first NATO member and first European country to have a legally compulsory national service for both men and women. In practice only motivated volunteers are selected to join the army in Norway.\n\nIn the USSR, there was no systematic conscription of women for the armed forces, but the severe disruption of normal life and the high proportion of civilians affected by World War II after the German invasion attracted many volunteers for what was termed \"The Great Patriotic War\". Medical doctors of both sexes could and would be conscripted (as officers). Also, the free Soviet university education system required Department of Chemistry students of both sexes to complete an ROTC course in NBC defense, and such female reservist officers could be conscripted in times of war. The United States came close to drafting women into the Nurse Corps in preparation for a planned invasion of Japan.\n\nIn 1981 in the United States, several men filed lawsuit in the case \"Rostker v. Goldberg\", alleging that the Selective Service Act of 1948 violates the Due Process Clause of the Fifth Amendment by requiring that only men register with the Selective Service System (SSS). The Supreme Court eventually upheld the Act, stating that \"the argument for registering women was based on considerations of equity, but Congress was entitled, in the exercise of its constitutional powers, to focus on the question of military need, rather than 'equity.'\"\n\nOn October 1, 1999 in Taiwan, the Judicial Yuan of the Republic of China in its Interpretation 490 considered that the physical differences between males and females and the derived role differentiation in their respective social functions and lives would not make drafting only males a violation of the Constitution of the Republic of China. Though women are not conscripted in Taiwan, transsexual persons are exempt.\n\nA conscientious objector is an individual whose personal beliefs are incompatible with military service, or, more often, with any role in the armed forces. In some countries, conscientious objectors have special legal status, which augments their conscription duties. For example, Sweden used to allow (and once again, with the re-introduction of conscription, allows) conscientious objectors to choose a service in the \"weapons-free\" branch, such as an airport fireman, nurse or telecommunications technician.\n\nThe reasons for refusing to serve are varied. Some conscientious objectors are so for religious reasons—notably, the members of the historic peace churches, pacifist by doctrine; Jehovah's Witnesses, while not strictly pacifists, refuse to participate in the armed forces on the ground that they believe Christians should be neutral in worldly conflicts.\n\nUniversal conscription in China dates back to the State of Qin, which eventually became the Qin Empire of 221 BC. Following unification, historical records show that a total of 300,000 conscript soldiers and 500,000 conscript labourers constructed the Great Wall of China.\n\nIn the following dynasties, universal conscription was abolished and reintroduced on numerous occasions.\n\n, universal military conscription is theoretically mandatory in the People's Republic of China, and reinforced by law. However, due to the large population of China and large pool of candidates available for recruitment, the People's Liberation Army has always had sufficient volunteers, so conscription has not been required in practice at all.\n\nEvery male citizen of the Republic of Austria up to the age of 35 can be drafted for a six month long basic military training in the Bundesheer. For men refusing to undergo this training, a nine-month lasting community service is mandatory.\n\nBelgium abolished the conscription in 1994. The last conscripts left active service in February 1995. To this day (2019), a strong minority of the Belgian citizens (especially in Flanders) supports the idea of reintroducing military conscription, for both men and women. \n\nBulgaria had mandatory military service for males above 18 until conscription was ended in 2008. Due to a shortfall in the army of some 5500 soldiers, parts of the current ruling coalition have expressed their support for the return of mandatory military service, most notably Krasimir Karakachanov. Opposition towards this idea from the main coalition partner, GERB, has seen a compromise, where instead of mandatory military service, Bulgaria could possibly introduce a voluntary military service by 2019 where young citizens can volunteer for a period of 6 to 9 months, and will receive a basic wage.\n\nMilitary service in the Cypriot National Guard is mandatory for all male citizens of the Republic of Cyprus, as well as any male non-citizens born of a parent of Greek Cypriot descent, lasting from the January 1 of the year in which they turn 18 years of age to December 31, of the year in which they turn 50. All male residents of Cyprus who are of military age (16 and over) are required to obtain an exit visa from the Ministry of Defense. Currently, military conscription in Cyprus lasts 14 months.\n\nConscription is known in Denmark since the Viking Age, where one man out of every 10 had to serve the king. Frederick IV of Denmark changed the law in 1710 to every 4th man. The men were chosen by the landowner and it was seen as a penalty.\n\nSince 12 February 1849, every physically fit man must do military service. According to §81 in the Constitution of Denmark, which was promulgated in 1849: Every male person able to carry arms shall be liable with his person to contribute to the defence of his country under such rules as are laid down by Statute. — Constitution of DenmarkThe legislation about compulsory military service is articulated in the Danish Law of Conscription. National service takes 4–12 months. It is possible to postpone the duty when one is still in full-time education. Every male turning 18 will be drafted to the 'Day of Defence', where they will be introduced to the Danish military and their health will be tested. Physically unfit persons are not required to do military service. It is only compulsory for men, while women are free to choose to join the Danish army. Almost all of the men have been volunteers in recent years, 96.9% of the total number of recruits having been volunteers in the 2015 draft.\n\nAfter lottery, one can become a conscientious objector. Total objection (refusal from alternative civilian service) results in up to 4 months jailtime according to the law. However, in 2014 a Danish man, who signed up for the service and objected later, got only 14 days of home arrest. In many countries the act of desertion (objection after signing up) is punished harder than objecting the compulsory service.\n\nConscription in Finland is part of a general compulsion for national military service for all adult males (; ) defined in the 127§ of the Constitution of Finland.\n\nConscription can take the form of military or of civilian service. According to Finnish Defence Forces 2011 data slightly under 80% of Finnish males turned 30 had entered and finished the military service. The number of female volunteers to annually enter armed service had stabilised at approximately 300. The service period is 165, 255 or 347 days for the rank and file conscripts and 347 days for conscripts trained as NCOs or reserve officers. The length of civilian service is always twelve months. Those electing to serve unarmed in duties where unarmed service is possible serve either nine or twelve months, depending on their training.\n\nAny Finnish male citizen who refuses to perform both military and civilian service faces a penalty of 173 days in prison, minus any served days. Such sentences are usually served fully in prison, with no parole. Jehovah's Witnesses are exempted in that they may be granted a deferment of service for 3 years upon presentation of a certificate from their congregation's minister showing they are an active member of that religious community. Providing they are still an active member 3 years later, there is nothing to stop them getting a further certificate and deferment. The inhabitants of the demilitarized Åland Islands are exempt from military service. By the Conscription Act of 1951, they are, however, required to serve a time at a local institution, like the coast guard. However, until such service has been arranged, they are freed from service obligation. The non-military service of Åland islands has not been arranged since the introduction of the act, and there are no plans to institute it. The inhabitants of Åland islands can also volunteer for military service on the mainland. As of 1995, women are permitted to serve on a voluntary basis and pursue careers in the military after their initial voluntary military service.\n\nThe military service takes place in Finnish Defence Forces or in the Finnish Border Guard. All services of the Finnish Defence Forces train conscripts. However, the Border Guard trains conscripts only in land-based units, not in coast guard detachments or in the Border Guard Air Wing. Civilian service may take place in the Civilian Service Center in Lapinjärvi or in an accepted non-profit organization of educational, social or medical nature.\n\nIn both East and West Germany, military service was mandatory for all male citizens. With the end of the Cold War, then reunified Germany drastically reduced the size of its armed forces. The low demand for conscripts led to the suspension of compulsory conscription in 2011. Since then only volunteer professionals serve in the Bundeswehr.\n\nSince 1914, Greece has had a period of mandatory military service lasting 9 months for men between the ages of 16 and 45. Citizens discharged from active service are normally placed in the reserve and are subject to periodic recalls of 1–10 days at irregular intervals.\n\nUniversal conscription was introduced in Greece during the military reforms of 1909, although various forms of selective conscription had been in place earlier. In more recent years, conscription was associated with the state of general mobilisation declared on July 20, 1974 due to the crisis in Cyprus (the mobilisation was formally ended on December 18, 2002).\n\nThe period of time that a conscript is required to serve has varied historically, between 12–36 months depending on various factors particular to the conscript, and the political situation. Although women are accepted into the Greek army on a voluntary basis, they are not required to enlist, as men are. Soldiers receive no health insurance, but they are provided medical support during their army service, including hospitalization costs.\n\nSince 2009, Greece has mandatory military service of 9 months for male citizens between the ages of 19 and 45. However, as the Armed forces had been gearing towards a completely professional army, the government had announced that the mandatory military service period would be cut to 6 months by 2008 or even abolished completely. However, this timetable was under reconsideration as of April 2006, due to severe manpower shortages. These had been caused by a combination of financial difficulties, meaning that professional soldiers could not be hired at the projected rate, and widespread abuse of the deferment process, resulting in two thirds of the conscripts deferred service in 2005. In August 2009, the mandatory service period was reduced to 9 months for the army, but has remained at 12 months for the navy and the air force. The number of conscripts affected to the latter two has been greatly reduced, with an aim towards full professionalisation.\n\nLithuania abolished its conscription in 2008. In May 2015 the Lithuanian parliament voted to return the conscription and the conscripts started their training in August 2015. In practice there is no conscription in Lithuania, since all recruits have been volunteers.\n\nLuxembourg practiced military conscription from 1948 until 1967. \n\nMoldova, which currently has male conscription, has announced plans to abolish the practice. Moldova's Defense Ministry announced that a plan which stipulates the gradual elimination of military conscription will be implemented starting from the autumn of 2018.\n\nConscription, which was called \"Service Duty\" () in the Netherlands, was first employed in 1810 by French occupying forces. Napoleon's brother Louis Bonaparte, who was King of Holland from 1806 to 1810, had tried to introduce conscription a few years earlier, unsuccessfully. Every man aged 20 years or older had to enlist. By means of drawing lots it was decided who had to undertake service in the French army. It was possible to arrange a substitute against payment.\n\nLater on, conscription was used for all men over the age of 18. Postponement was possible, due to study, for example. Conscientious objectors could perform an alternative civilian service instead of military service. For various reasons, this forced military service was criticized at the end of the twentieth century. Since the Cold War was over, so was the direct threat of a war. Instead, the Dutch army was employed in more and more peacekeeping operations. The complexity and danger of these missions made the use of conscripts controversial. Furthermore, the conscription system was thought to be unfair as only men were drafted.\n\nIn the European part of Netherlands, compulsory attendance has been officially suspended since 1 May 1997. Between 1991 and 1996, the Dutch armed forces phased out their conscript personnel and converted to an all-professional force. The last conscript troops were inducted in 1995, and demobilized in 1996. The suspension means that citizens are no longer forced to serve in the armed forces, as long as it is not required for the safety of the country. Until then, the Dutch army has become an all-professional force. However, to this day, every male and female citizen aged 17 gets a letter in which they are told that they have been registered but do not have to present themselves for service.\n\n, Norway currently employs a weak form of mandatory military service for men and women. In practice recruits are not forced to serve, instead only those who are motivated are selected. About 60,000 Norwegians are available for conscription every year, but only 8,000 to 10,000 are conscripted. Since 1985, women have been able to enlist for voluntary service as regular recruits. On 14 June 2013 the Norwegian Parliament voted to extend conscription to women, making Norway the first NATO member and first European country to make national service compulsory for both sexes. In earlier times, up until at least the early 2000s, all men aged 19–44 were subject to mandatory service, with good reasons required to avoid becoming drafted. There is a right of conscientious objection.\n\nIn addition to the military service, the Norwegian government draft a total of 8,000 men and women between 18 and 55 to non-military Civil defence duty. (Not to be confused with Alternative civilian service.) Former service in the military does not exclude anyone from later being drafted to the Civil defence, but an upper limit of total 19 months of service applies. Neglecting mobilisation orders to training exercises and actual incidents, may impose fines.\n\n, Serbia no longer practises mandatory military service. Prior to this, mandatory military service lasted 6 months for men. Conscientious objectors could however opt for 9 months of civil service instead.\n\nOn 15 December 2010, the Parliament of Serbia voted to suspend mandatory military service. The decision fully came into force on January 1, 2011.\n\nSweden had conscription () for men between 1901 and 2010. Peacetime conscription was made dormant in 2010, and the law on conscription was simultaneously made gender-neutral.\n\nThe Swedish government reintroduced a selective military conscription for men and women beginning January 1, 2018.\n\nThe United Kingdom introduced conscription to full-time military service for the first time in January 1916 (the eighteenth month of World War I) and abolished it in 1920. Ireland, then part of the United Kingdom, was exempted from the original 1916 military service legislation, and although further legislation in 1918 gave power for an extension of conscription to Ireland, the power was never put into effect.\n\nConscription was reintroduced in 1939, in the lead up to World War II, and continued in force until 1963. Northern Ireland was exempted from conscription legislation throughout the whole period.\n\nIn all, eight million men were conscripted during both World Wars, as well as several hundred thousand younger single women. The introduction of conscription in May 1939, before the war began, was partly due to pressure from the French, who emphasized the need for a large British army to oppose the Germans. From early 1942 unmarried women age 19–30 were conscripted. Most were sent to the factories, but they could volunteer for the Auxiliary Territorial Service (ATS) and other women's services. None was assigned to combat roles unless she volunteered. By 1943 women were liable to some form of directed labour up to age 51. During the Second World War, 1.4 million British men volunteered for service and 3.2 million were conscripted. Conscripts comprised 50% of the Royal Air Force, 60% of the Royal Navy and 80% of the British Army.\n\nBritain and its colonies did not develop such pervasive administrative states, and therefore did not opt out for regulatory solutions, such as conscription, as a reliability. The abolition of conscription in Britain was announced on 4 April 1957, by new prime minister Harold Macmillan, with the last conscripts being recruited three years later.\n\nThere is a mandatory service for all men and women who are fit and 18 years old. Men must serve 32 months while women serve 24 months with some exempt from mandatory service:\n\nAll of the above can choose to volunteer to the IDF. Relatively large numbers of Bedouin choose to volunteer.\n\nMale Druze and Circassian Israeli citizens are liable, by agreement with their community leaders (Female Druze and Circassian are exempt from service).\n\nIn the United States, conscription, also called \"the draft\", ended in 1973, but males aged between 18 and 25 are required to register with the Selective Service System to enable a reintroduction of conscription if necessary. President Gerald Ford suspended mandatory draft registration in 1975, but President Jimmy Carter reinstated that requirement when the Soviet Union intervened in Afghanistan five years later. Selective Service registration is still required of almost all young men, although the draft has not been used since 1973 and there have been no prosecutions for violations of the draft registration law since 1986. Males between the ages of 17 and 45, and female members of the US National Guard may be conscripted for federal militia service pursuant to 10 U.S. Code § 246 and the Militia Clauses of the United States Constitution.\n\n\n\n"}
{"id": "5736", "url": "https://en.wikipedia.org/wiki?curid=5736", "title": "Catherine Coleman", "text": "Catherine Coleman\n\nCatherine Grace \"Cady\" Coleman (born December 14, 1960) is an American chemist, a former United States Air Force officer, and a former NASA astronaut. She is a veteran of two Space Shuttle missions, and departed the International Space Station on May 23, 2011, as a crew member of Expedition 27 after logging 159 days in space.\n\nColeman graduated from Wilbert Tucker Woodson High School, Fairfax, Virginia, in 1978; in 1978–1979, she was an exchange student at Røyken upper secondary school in Norway with the AFS Intercultural Programs. She received a B.S. degree in chemistry from the Massachusetts Institute of Technology in 1983, and a Ph.D. degree in polymer science and engineering from the University of Massachusetts Amherst in 1991 as a member of the Air Force ROTC. She was advised by Professor Thomas J. McCarthy on her doctorate. She was a member of the intercollegiate crew and was a resident of Baker House.\n\nAfter completing her regular education, Coleman joined the U.S. Air Force as a Second Lieutenant while continuing her graduate work for a PhD at the University of Massachusetts Amherst. In 1988 she entered active duty at Wright-Patterson Air Force Base as a research chemist. During her work she participated as a surface analysis consultant on the NASA Long Duration Exposure Facility experiment. In 1991, she received her doctorate in polymer science and engineering. She retired from the Air Force in November 2009.\n\nColeman was selected by NASA in 1992 to join the NASA Astronaut Corps. In 1995, she was a member of the STS-73 crew on the scientific mission USML-2 with experiments including biotechnology, combustion science, and the physics of fluids. During the flight, she reported to Houston Mission Control that she had spotted an unidentified flying object. She also trained for the mission STS-83 to be the backup for Donald A. Thomas; however, as he recovered on time, she did not fly that mission. STS-93 was Coleman's second space flight in 1999. She was mission specialist in charge of deploying the Chandra X-ray Observatory and its Inertial Upper Stage out of the shuttle's cargo bay.\n\nColeman served as Chief of Robotics for the Astronaut Office, to include robotic arm operations and training for all Space Shuttle and International Space Station missions. In October 2004, Coleman served as an aquanaut during the mission aboard the Aquarius underwater laboratory, living and working underwater for eleven days.\n\nColeman was assigned as a backup U.S. crew member for Expeditions 19, 20 and 21 and served as a backup crew member for Expeditions 24 and 25 as part of her training for Expedition 26.\n\nColeman launched on December 15, 2010 (December 16 Baikonur time), aboard Soyuz TMA-20 to join the Expedition 26 mission aboard the International Space Station. She retired from NASA on December 1, 2016.\n\nSTS-73 on Space Shuttle \"Columbia\" (October 20 to November 5, 1995) was the second United States Microgravity Laboratory mission. The mission focused on materials science, biotechnology, combustion science, the physics of fluids, and numerous scientific experiments housed in the pressurized Spacelab module. In completing her first space flight, Coleman orbited the Earth 256 times, traveled over 6 million miles, and logged a total of 15 days, 21 hours, 52 minutes and 21 seconds in space.\n\nSTS-93 on \"Columbia\" (July 22 to 27, 1999) was a five-day mission during which Coleman was the lead mission specialist for the deployment of the Chandra X-ray Observatory. Designed to conduct comprehensive studies of the universe, the telescope will enable scientists to study exotic phenomena such as exploding stars, quasars, and black holes. Mission duration was 118 hours and 50 minutes.\n\nSoyuz TMA-20 / Expedition 26/27 (December 15, 2010, to May 23, 2011) was an extended duration mission to the International Space Station.\n\nColeman is married to glass artist Josh Simpson who lives in Massachusetts. They have one son. She is part of the band Bandella, which also includes fellow NASA astronaut Steven Robinson, Canadian astronaut Chris Hadfield, and Micki Pettit (astronaut Don Pettit's wife). Coleman is a flute player and has taken several flutes with her to the ISS, including a pennywhistle from Paddy Moloney of the Chieftains, an old Irish flute from Matt Molloy of the Chieftains, and a flute from Ian Anderson of Jethro Tull. On February 15, 2011, she played one of the instruments live from orbit on National Public Radio. On April 12, 2011, she played live through video link for the audience of Jethro Tull's show in Russia in honour of the 50th anniversary of Yuri Gagarin's flight. She played the duet from orbit while Anderson played on the ground in Russia. On May 13 of that year, Coleman delivered a taped commencement address to the class of 2011 at the University of Massachusetts Amherst.\n\nAs do many other astronauts, Coleman holds an amateur radio license (callsign: KC5ZTH).\n\nAs of 2015, she is also a member of the board of directors for the Hollywood Science Fiction Museum.\n\nAs of 2015 she is also known to be working as a guest speaker at the Baylor College of Medicine, for the children's program 'Saturday Morning Science'.\n\n"}
{"id": "5738", "url": "https://en.wikipedia.org/wiki?curid=5738", "title": "Cervix", "text": "Cervix\n\nThe cervix or cervix uteri (\"\") is the lower part of the uterus in the human female reproductive system. The cervix is usually 2 to 3 cm long (~1 inch) and roughly cylindrical in shape, which changes during pregnancy. The narrow, central cervical canal runs along its entire length, connecting the uterine cavity and the lumen of the vagina. The opening into the uterus is called the internal os, and the opening into the vagina is called the external os. The lower part of the cervix, known as the vaginal portion of the cervix (or ectocervix), bulges into the top of the vagina. The cervix has been documented anatomically since at least the time of Hippocrates, over 2,000 years ago.\n\nThe cervical canal is a passage through which sperm must travel to fertilize an egg cell after sexual intercourse. Several methods of contraception, including cervical caps and cervical diaphragms, aim to block or prevent the passage of sperm through the cervical canal. Cervical mucus is used in several methods of fertility awareness, such as the Creighton model and Billings method, due to its changes in consistency throughout the menstrual period. During vaginal childbirth, the cervix must flatten and dilate to allow the fetus to progress along the birth canal. Midwives and doctors use the extent of the dilation of the cervix to assist decision-making during childbirth.\n\nThe cervical canal is lined with a single layer of column-shaped cells, while the ectocervix is covered with multiple layers of cells topped with flat cells. The two types of epithelia meet the squamocolumnar junction. Infection with the human papillomavirus (HPV) can cause changes in the epithelium, which can lead to cancer of the cervix. Cervical cytology tests can often detect cervical cancer and its precursors, and enable early successful treatment. Ways to avoid HPV include avoiding sex, using condoms, and HPV vaccination. HPV vaccines, developed in the early 21st century, reduce the risk of cervical cancer by preventing infections from the main cancer-causing strains of HPV.\n\nThe cervix is part of the female reproductive system. Around in length, it is the lower narrower part of the uterus continuous above with the broader upper part—or body—of the uterus. The lower end of the cervix bulges through the anterior wall of the vagina, and is referred to as the vaginal portion of cervix (or ectocervix) while the rest of the cervix above the vagina is called the supravaginal portion of cervix. A central canal, known as the cervical canal, runs along its length and connects the cavity of the body of the uterus with the lumen of the vagina. The openings are known as the internal os and external orifice of the uterus (or external os) respectively. The mucosa lining the cervical canal is known as the endocervix, and the mucosa covering the ectocervix is known as the exocervix. The cervix has an inner mucosal layer, a thick layer of smooth muscle, and posteriorly the supravaginal portion has a serosal covering consisting of connective tissue and overlying peritoneum.\n\nIn front of the upper part of the cervix lies the bladder, separated from it by cellular connective tissue known as parametrium, which also extends over the sides of the cervix. To the rear, the supravaginal cervix is covered by peritoneum, which runs onto the back of the vaginal wall and then turns upwards and onto the rectum, forming the recto-uterine pouch. The cervix is more tightly connected to surrounding structures than the rest of the uterus.\n\nThe cervical canal varies greatly in length and width between women or over the course of a woman's life, and it can measure 8 mm (0.3 inch) at its widest diameter in premenopausal adults. It is wider in the middle and narrower at each end. The anterior and posterior walls of the canal each have a vertical fold, from which ridges run diagonally upwards and laterally. These are known as \"palmate folds\", due to their resemblance to a palm leaf. The anterior and posterior ridges are arranged in such a way that they interlock with each other and close the canal. They are often effaced after pregnancy.\n\nThe ectocervix (also known as the vaginal portion of the cervix) has a convex, elliptical shape and projects into the cervix between the anterior and posterior vaginal fornices. On the rounded part of the ectocervix is a small, depressed external opening, connecting the cervix with the vagina. The size and shape of the ectocervix and the external opening (external os) can vary according to age, hormonal state, and whether natural or normal childbirth has taken place. In women who have not had a vaginal delivery, the external opening is small and circular, and in women who have had a vaginal delivery, it is slit-like. On average, the ectocervix is long and wide.\n\nBlood is supplied to the cervix by the descending branch of the uterine artery and drains into the uterine vein. The pelvic splanchnic nerves, emerging as S2–S3, transmit the sensation of pain from the cervix to the brain. These nerves travel along the uterosacral ligaments, which pass from the uterus to the anterior sacrum.\n\nThree channels facilitate lymphatic drainage from the cervix. The anterior and lateral cervix drains to nodes along the uterine arteries, travelling along the cardinal ligaments at the base of the broad ligament to the external iliac lymph nodes and ultimately the paraaortic lymph nodes. The posterior and lateral cervix drains along the uterine arteries to the internal iliac lymph nodes and ultimately the paraaortic lymph nodes, and the posterior section of the cervix drains to the obturator and presacral lymph nodes. However, there are variations as lymphatic drainage from the cervix travels to different sets of pelvic nodes in some people. This has implications in scanning nodes for involvement in cervical cancer.\n\nAfter menstruation and directly under the influence of estrogen, the cervix undergoes a series of changes in position and texture. During most of the menstrual cycle, the cervix remains firm, and is positioned low and closed. However, as ovulation approaches, the cervix becomes softer and rises to open in response to the higher levels of estrogen present. These changes are also accompanied by changes in cervical mucus, described below.\n\nAs a component of the female reproductive system, the cervix is derived from the two paramesonephric ducts (also called Müllerian ducts), which develop around the sixth week of embryogenesis. During development, the outer parts of the two ducts fuse, forming a single urogenital canal that will become the vagina, cervix and uterus. The cervix grows in size at a smaller rate than the body of the uterus, so the relative size of the cervix over time decreases, decreasing from being much larger than the body of the uterus in fetal life, twice as large during childhood, and decreasing to its adult size, smaller than the uterus, after puberty. Previously it was thought that during fetal development, the original squamous epithelium of the cervix is derived from the urogenital sinus and the original columnar epithelium is derived from the paramesonephric duct. The point at which these two original epithelia meet is called the original squamocolumnar junction. New studies show, however, that all the cervical as well as large part of the vaginal epithelium are derived from Müllerian duct tissue and that phenotypic differences might be due to other causes.\n\nThe endocervical mucosa is about thick, lined with a single layer of columnar mucous cells, and contains numerous tubular mucous glands which empty viscous alkaline mucus into the lumen. In contrast, the ectocervix is covered with nonkeratinized stratified squamous epithelium, which resembles the squamous epithelium lining the vaginal. The junction between these two types of epithelia is called the squamocolumnar junction. Underlying both types of epithelium is a tough layer of collagen. The mucosa of the endocervix is not shed during menstruation. The cervix has more fibrous tissue, including collagen and elastin, than the rest of the uterus.\nIn prepubertal girls, the functional squamocolumnar junction is present just within the cervical canal. Upon entering puberty, due to hormonal influence, and during pregnancy, the columnar epithelium extends outwards over the ectocervix as the cervix everts. Hence, this also causes the squamocolumnar junction to move outwards onto the vaginal portion of the cervix, where it is exposed to the acidic vaginal environment. The exposed columnar epithelium can undergo physiological metaplasia and change to tougher metaplastic squamous epithelium in days or weeks, which is very similar to the original squamous epithelium when mature. The new squamocolumnar junction is therefore internal to the original squamocolumnar junction, and the zone of unstable epithelium between the two junctions is called the transformation zone of the cervix. After menopause, the uterine structures involute and the functional squamocolumnar junction moves into the cervical canal.\n\nNabothian cysts (or Nabothian follicles) form in the transformation zone where the lining of metaplastic epithelium has replaced mucous epithelium and caused a strangulation of the outlet of some of the mucous glands. A build up of mucus in the glands forms Nabothian cysts, usually less than about in diameter, which are considered physiological rather than pathological. Both gland openings and Nabothian cysts are helpful to identify the transformation zone.\n\nThe cervical canal is a pathway through which sperm enter the uterus after sexual intercourse, and some forms of artificial insemination. Some sperm remains in cervical crypts, infoldings of the endocervix, which act as a reservoir, releasing sperm over several hours and maximising the chances of fertilisation. A theory states the cervical and uterine contractions during orgasm draw semen into the uterus. Although the \"upsuck theory\" has been generally accepted for some years, it has been disputed due to lack of evidence, small sample size, and methodological errors.\n\nSome methods of fertility awareness, such as the Creighton model and the Billings method involve estimating a woman's periods of fertility and infertility by observing physiological changes in her body. Among these changes are several involving the quality of her cervical mucus: the sensation it causes at the vulva, its elasticity (\"Spinnbarkeit\"), its transparency, and the presence of ferning.\n\nSeveral hundred glands in the endocervix produce 20–60 mg of cervical mucus a day, increasing to 600 mg around the time of ovulation. It is viscous as it contains large proteins known as mucins. The viscosity and water content varies during the menstrual cycle; mucus is composed of around 93% water, reaching 98% at midcycle. These changes allow it to function either as a barrier or a transport medium to spermatozoa. It contains electrolytes such as calcium, sodium, and potassium; organic components such as glucose, amino acids, and soluble proteins; trace elements including zinc, copper, iron, manganese, and selenium; free fatty acids; enzymes such as amylase; and prostaglandins. Its consistency is determined by the influence of the hormones estrogen and progesterone. At midcycle around the time of ovulation—a period of high estrogen levels— the mucus is thin and serous to allow sperm to enter the uterus, and is more alkaline and hence more hospitable to sperm. It is also higher in electrolytes, which results in the \"ferning\" pattern that can be observed in drying mucus under low magnification; as the mucus dries, the salts crystallize, resembling the leaves of a fern. The mucus has stretchy character described as \"Spinnbarkeit\" most prominent around the time of ovulation.\n\nAt other times in the cycle, the mucus is thick and more acidic due to the effects of progesterone. This \"infertile\" mucus acts as a barrier to sperm from entering the uterus. Women taking an oral contraceptive pill also have thick mucus from the effects of progesterone. Thick mucus also prevents pathogens from interfering with a nascent pregnancy.\n\nA cervical mucus plug, called the operculum, forms inside the cervical canal during pregnancy. This provides a protective seal for the uterus against the entry of pathogens and against leakage of uterine fluids. The mucus plug is also known to have antibacterial properties. This plug is released as the cervix dilates, either during the first stage of childbirth or shortly before. It is visible as a blood-tinged mucous discharge.\n\nThe cervix plays a major role in childbirth. As the fetus descends within the uterus in preparation for birth, the presenting part, usually the head, rests on and is supported by the cervix. As labour progresses, the cervix becomes softer and shorter, begins to dilate, and rotates to face anteriorly. The support the cervix provides to the fetal head starts to give way when the uterus begins its contractions. During childbirth, the cervix must dilate to a diameter of more than to accommodate the head of the fetus as it descends from the uterus to the vagina. In becoming wider, the cervix also becomes shorter, a phenomenon known as effacement.\n\nAlong with other factors, midwives and doctors use the extent of cervical dilation to assist decision making during childbirth. Generally, the active first stage of labour, when the uterine contractions become strong and regular, begins when the cervical dilation is more than . The second phase of labor begins when the cervix has dilated to , which is regarded as its fullest dilation, and is when active pushing and contractions push the baby along the birth canal leading to the birth of the baby. The number of past vaginal deliveries is a strong factor in influencing how rapidly the cervix is able to dilate in labour. The time taken for the cervix to dilate and efface is one factor used in reporting systems such as the Bishop score, used to recommend whether interventions such as a forceps delivery, induction, or Caesarean section should be used in childbirth.\n\nCervical incompetence is a condition in which shortening of the cervix due to dilation and thinning occurs, before term pregnancy. Short cervical length is the strongest predictor of preterm birth.\n\nSeveral methods of contraception involve the cervix. Cervical diaphragms are reusable, firm-rimmed plastic devices inserted by a woman prior to intercourse that cover the cervix. Pressure against the walls of the vagina maintain the position of the diaphragm, and it acts as a physical barrier to prevent the entry of sperm into the uterus, preventing fertilisation. Cervical caps are a similar method, although they are smaller and adhere to the cervix by suction. Diaphragms and caps are often used in conjunction with spermicides. In one year, 12% of women using the diaphragm will undergo an unintended pregnancy, and with optimal use this falls to 6%. Efficacy rates are lower for the cap, with 18% of women undergoing an unintended pregnancy, and 10–13% with optimal use. Most types of progestogen-only pills are effective as a contraceptive because they thicken cervical mucus making it difficult for sperm to pass along the cervical canal. In addition, they may also sometimes prevent ovulation. In contrast, contraceptive pills that contain both oestrogen and progesterone, the combined oral contraceptive pills, work mainly by preventing ovulation. They also thicken cervical mucus and thin the lining of the uterus enhancing their effectiveness.\n\nIn 2008, cervical cancer was the third-most common cancer in women worldwide, with rates varying geographically from less than one to more than 50 cases per 100,000 women. It is a leading cause of cancer-related death in poor countries, where delayed diagnosis leading to poor outcomes is common. The introduction of routine screening has resulted in fewer cases of (and deaths from) cervical cancer, however this has mainly taken place in developed countries. Most developing countries have limited or no screening, and 85% of the global burden occurring there.\n\nCervical cancer nearly always involves human papillomavirus (HPV) infection. HPV is a virus with numerous strains, several of which predispose to precancerous changes in the cervical epithelium, particularly in the transformation zone, which is the most common area for cervical cancer to start. HPV vaccines, such as Gardasil and Cervarix, reduce the incidence of cervical cancer, by inoculating against the viral strains involved in cancer development.\n\nPotentially precancerous changes in the cervix can be detected by cervical screening, using methods including a Pap smear (also called a cervical smear), in which epithelial cells are scraped from the surface of the cervix and examined under a microscope. The colposcope, an instrument used to see a magnified view of the cervix, was invented in 1925. The Pap smear was developed by Georgios Papanikolaou in 1928. A LEEP procedure using a heated loop of platinum to excise a patch of cervical tissue was developed by Aurel Babes in 1927. In some parts of the developed world including the UK, the Pap test has been superseded with liquid-based cytology.\n\nA cheap, cost-effective and practical alternative in poorer countries is visual inspection with acetic acid (VIA). Instituting and sustaining cytology-based programs in these regions can be difficult, due to the need for trained personnel, equipment and facilities and difficulties in follow-up. With VIA, results and treatment can be available on the same day. As a screening test, VIA is comparable to cervical cytology in accurately identifying precancerous lesions.\n\nA result of dysplasia is usually further investigated, such as by taking a cone biopsy, which may also remove the cancerous lesion. Cervical intraepithelial neoplasia is a possible result of the biopsy, and represents dysplastic changes that may eventually progress to invasive cancer. Most cases of cervical cancer are detected in this way, without having caused any symptoms. When symptoms occur, they may include vaginal bleeding, discharge, or discomfort.\n\nInflammation of the cervix is referred to as cervicitis. This inflammation may be of the endocervix or ectocervix. When associated with the endocervix, it is associated with a mucous vaginal discharge and the sexually transmitted infections such as chlamydia and gonorrhoea. As many as half of pregnant women having a gonorrheal infection of the cervix are asymptomatic. Other causes include overgrowth of the commensal flora of the vagina. When associated with the ectocervix, inflammation may be caused by the herpes simplex virus. Inflammation is often investigated through directly visualising the cervix using a speculum, which may appear whiteish due to exudate, and by taking a Pap smear and examining for causal bacteria. Special tests may be used to identify particular bacteria. If the inflammation is due to a bacterium, then antibiotics may be given as treatment.\n\nCervical stenosis refers to an abnormally narrow cervical canal, typically associated with trauma caused by removal of tissue for investigation or treatment of cancer, or cervical cancer itself. Diethylstilbestrol, used from 1938 to 1971 to prevent preterm labour and miscarriage, is also strongly associated with the development of cervical stenosis and other abnormalities in the daughters of the exposed women. Other abnormalities include: vaginal adenosis, in which the squamous epithelium of the ectocervix becomes columnar; cancers such as clear cell adenocarcinomas; cervical ridges and hoods; and development of a cockscomb cervix appearance, which is the condition wherein, as the name suggests, the cervix of the uterus is shaped like a cockscomb. About one third of women born to diethylstilbestrol-treated mothers (i.e. in-utero exposure) develop a cockscomb cervix.\n\nEnlarged folds or ridges of cervical stroma (fibrous tissues) and epithelium constitute a cockscomb cervix. Similarly, cockscomb polyps lining the cervix are usually considered or grouped into the same overarching description. It is in and of itself considered a benign abnormality; its presence, however is usually indicative of DES exposure, and as such women who experience these abnormalities should be aware of their increased risk of associated pathologies.\n\nCervical agenesis is a rare congenital condition in which the cervix completely fails to develop, often associated with the concurrent failure of the vagina to develop. Other congenital cervical abnormalities exist, often associated with abnormalities of the vagina and uterus. The cervix may be duplicated in situations such as bicornuate uterus and uterine didelphys.\n\nCervical polyps, which are benign overgrowths of endocervical tissue, if present, may cause bleeding, or a benign overgrowth may be present in the cervical canal. Cervical ectropion refers to the horizontal overgrowth of the endocervical columnar lining in a one-cell-thick layer over the ectocervix.\n\nFemale marsupials have paired uteri and cervices. Most eutherian (placental) mammal species have a single cervix and single, bipartite or bicornuate uterus. Lagomorphs, rodents, aardvarks and hyraxes have a duplex uterus and two cervices. Lagomorphs and rodents share many morphological characteristics and are grouped together in the clade Glires. Anteaters of the family myrmecophagidae are unusual in that they lack a defined cervix; they are thought to have lost the characteristic rather than other mammals developing a cervix on more than one lineage. In domestic pigs, the cervix contains a series of five interdigitating pads that hold the boar's corkscrew-shaped penis during copulation.\n\nThe word \"cervix\" () came to English from Latin, where it means \"neck\", and like its Germanic counterpart, it can refer not only to the neck [of the body] but also to an analogous narrowed part of an object. The cervix uteri (neck of the uterus) is thus the uterine cervix, but in English the word \"cervix\" used alone usually refers to it. Thus the adjective \"cervical\" may refer either to the neck (as in \"cervical vertebrae\" or \"cervical lymph nodes\") or to the uterine cervix (as in \"cervical cap\" or \"cervical cancer\").\n\nLatin \"cervix\" came from the Proto-Indo-European root \"ker-\", referring to a \"structure that projects\". Thus, the word cervix is linguistically related to the English word \"horn\", the Persian word for \"head\" ( \"sar\"), the Greek word for \"head\" ( \"koruphe\"), and the Welsh word for \"deer\" ().\n\nThe cervix was documented in anatomical literature in at least the time of Hippocrates; cervical cancer was first described more than 2,000 years ago, with descriptions provided by both Hippocrates and Aretaeus. However, there was some variation in word sense among early writers, who used the term to refer to both the cervix and the internal uterine orifice. The first attested use of the word to refer to the cervix of the uterus was in 1702.\n\n"}
{"id": "5739", "url": "https://en.wikipedia.org/wiki?curid=5739", "title": "Compiler", "text": "Compiler\n\nA compiler is a computer program that transforms computer code written in one programming language (the source language) into another programming language (the target language). Compilers are a type of translator that support digital devices, primarily computers. The name \"compiler\" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program.\n\nHowever, there are many different types of compilers. If the compiled program can run on a computer whose CPU or operating system is different from the one on which the compiler runs, the compiler is a cross-compiler. A bootstrap compiler is written in the language that it intends to compile. A program that translates from a low-level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. The term compiler-compiler refers to tools used to create parsers that perform syntax analysis.\n\nA compiler is likely to perform many or all of the following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.\n\nCompilers are not the only translators used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations. The translation process influences the design of computer languages which leads to a preference of compilation or interpretation. In practice, an interpreter can be implemented for compiled languages and compilers can be implemented for interpreted languages.\n\nTheoretical computing concepts developed by scientists, mathematicians, and engineers formed the basis of digital modern computing development during World War II. Primitive binary languages evolved because digital devices only understand ones and zeros and the circuit patterns in the underlying machine architecture. In the late 1940s, assembly languages were created to offer a more workable abstraction of the computer architectures. Limited memory capacity of early computers led to substantial technical challenges when the first compilers were designed. Therefore, the compilation process needed to be divided into several small programs. The front end programs produce the analysis products used by the back end programs to generate target code. As computer technology provided more resources, compiler designs could align better with the compilation process.\n\nIt is usually more productive for a programmer to use a high-level language, so the development of high-level languages followed naturally from the capabilities offered by the digital computers. High-level languages are formal languages that are strictly defined by their syntax and semantics which form the high-level language architecture. Elements of these formal languages include:\n\nThe sentences in a language may be defined by a set of rules called a grammar.\n\nBackus–Naur form (BNF) describes the syntax of \"sentences\" of a language and was used for the syntax of Algol 60 by John Backus. The ideas derive from the context-free grammar concepts by Noam Chomsky, a linguist. \"BNF and its extensions have become standard tools for describing the syntax of programming notations, and in many cases parts of compilers are generated automatically from a BNF description.\"\n\nIn the 1940s, Konrad Zuse designed an algorithmic programming language called Plankalkül (\"Plan Calculus\"). While no actual implementation occurred until the 1970s, it presented concepts later seen in APL designed by Ken Iverson in the late 1950s. APL is a language for mathematical computations.\n\nHigh-level language design during the formative years of digital computing provided useful programming tools for a variety of applications:\n\nCompiler technology evolved from the need for a strictly defined transformation of the high-level source program into a low-level target program for the digital computer. The compiler could be viewed as a front end to deal with analysis of the source code and a back end to synthesize the analysis into the target code. Optimization between the front end and back end could produce more efficient target code.\n\nSome early milestones in the development of compiler technology:\n\nEarly operating systems and software were written in assembly language. In the 60s and early 70s, the use of high-level languages for system programming was still controversial due to resource limitations. However, several research and industry efforts began the shift toward high-level systems programming languages, for example, BCPL, BLISS, B, and C.\n\nBCPL (Basic Combined Programming Language) designed in 1966 by Martin Richards at the University of Cambridge was originally developed as a compiler writing tool. Several compilers have been implemented, Richards' book provides insights to the language and its compiler. BCPL was not only an influential systems programming language that is still used in research but also provided a basis for the design of B and C languages.\n\nBLISS (Basic Language for Implementation of System Software) was developed for a Digital Equipment Corporation (DEC) PDP-10 computer by W.A. Wulf's Carnegie Mellon University (CMU) research team. The CMU team went on to develop BLISS-11 compiler one year later in 1970.\n\nMultics (Multiplexed Information and Computing Service), a time-sharing operating system project, involved MIT, Bell Labs, General Electric (later Honeywell) and was led by Fernando Corbató from MIT. Multics was written in the PL/I language developed by IBM and IBM User Group. IBM's goal was to satisfy business, scientific, and systems programming requirements. There were other languages that could have been considered but PL/I offered the most complete solution even though it had not been implemented. For the first few years of the Mulitics project, a subset of the language could be compiled to assembly language with the Early PL/I (EPL) compiler by Doug McIlory and Bob Morris from Bell Labs. EPL supported the project until a boot-strapping compiler for the full PL/I could be developed.\n\nBell Labs left the Multics project in 1969: \"Over time, hope was replaced by frustration as the group effort initially failed to produce an economically useful system.\" Continued participation would drive up project support costs. So researchers turned to other development efforts. A system programming language B based on BCPL concepts was written by Dennis Ritchie and Ken Thompson. Ritchie created a boot-strapping compiler for B and wrote Unics (Uniplexed Information and Computing Service) operating system for a PDP-7 in B. Unics eventually became spelled Unix.\n\nBell Labs started development and expansion of C based on B and BCPL. The BCPL compiler had been transported to Multics by Bell Labs and BCPL was a preferred language at Bell Labs. Initially, a front-end program to Bell Labs' B compiler was used while a C compiler was developed. In 1971, a new PDP-11 provided the resource to define extensions to B and rewrite the compiler. By 1973 the design of C language was essentially complete and the Unix kernel for a PDP-11 was rewritten in C. Steve Johnson started development of Portable C Compiler (PCC) to support retargeting of C compilers to new machines.\n\nObject-oriented programming (OOP) offered some interesting possibilities for application development and maintenance. OOP concepts go further back but were part of LISP and Simula language science. At Bell Labs, the development of C++ became interested in OOP. C++ was first used in 1980 for systems programming. The initial design leveraged C language systems programming capabilities with Simula concepts. Object-oriented facilities were added in 1983. The Cfront program implemented a C++ front-end for C84 language compiler. In subsequent years several C++ compilers were developed as C++ popularity grew.\n\nIn many application domains, the idea of using a higher-level language quickly caught on. Because of the expanding functionality supported by newer programming languages and the increasing complexity of computer architectures, compilers became more complex.\n\nDARPA (Defense Advanced Research Projects Agency) sponsored a compiler project with Wulf's CMU research team in 1970. The Production Quality Compiler-Compiler PQCC design would produce a Production Quality Compiler (PQC) from formal definitions of source language and the target. PQCC tried to extend the term compiler-compiler beyond the traditional meaning as a parser generator (e.g., Yacc) without much success. PQCC might more properly be referred to as a compiler generator.\n\nPQCC research into code generation process sought to build a truly automatic compiler-writing system. The effort discovered and designed the phase structure of the PQC. The BLISS-11 compiler provided the initial structure. The phases included analyses (front end), intermediate translation to virtual machine (middle end), and translation to the target (back end). TCOL was developed for the PQCC research to handle language specific constructs in the intermediate representation. Variations of TCOL supported various languages. The PQCC project investigated techniques of automated compiler construction. The design concepts proved useful in optimizing compilers and compilers for the object-oriented programming language Ada.\n\nThe Ada Stoneman Document formalized the program support environment (APSE) along with the kernel (KAPSE) and minimal (MAPSE). An Ada interpreter NYU/ED supported development and standardization efforts with American National Standards Institute (ANSI) and the International Standards Organization (ISO). Initial Ada compiler development by the U.S. Military Services included the compilers in a complete integrated design environment along the lines of the Stoneman Document. Army and Navy worked on the Ada Language System (ALS) project targeted to DEC/VAX architecture while the Air Force started on the Ada Integrated Environment (AIE) targeted to IBM 370 series. While the projects did not provide the desired results, they did contribute to the overal effort on Ada development.\n\nOther Ada compiler efforts got under way in Britain at University of York and in Germany at University of Karlsruhe. In the U. S., Verdix (later acquired by Rational) delivered the Verdix Ada Development System (VADS) to the Army. VADS provided a set of development tools including a compiler. Unix/VADS could be hosted on a variety of Unix platforms such as DEC Ultrix and the Sun 3/60 Solaris targeted to Motorola 68020 in an Army CECOM evaluation. There were soon many Ada compilers available that passed the Ada Validation tests. The Free Software Foundation GNU project developed the GNU Compiler Collection (GCC) which provides a core capability to support multiple languages and targets. The Ada version GNAT is one of the most widely used Ada compilers. GNAT is free but there is also commercial support, for example, AdaCore, was founded in 1994 to provide commercial software solutions for Ada. GNAT Pro includes the GNU GCC based GNAT with a tool suite to provide an integrated development environment.\n\nHigh-level languages continued to drive compiler research and development. Focus areas included optimization and automatic code generation. Trends in programming languages and development environments influenced compiler technology. More compilers became included in language distributions (PERL, Java Development Kit) and as a component of an IDE (VADS, Eclipse, Ada Pro). The interrelationship and interdependence of technologies grew. The advent of web services promoted growth of web languages and scripting languages. Scripts trace back to the early days of Command Line Interfaces (CLI) where the user could enter commands to be executed by the system. User Shell concepts developed with languages to write shell programs. Early Windows designs offered a simple batch programming capability. The conventional transformation of these language used an interpreter. While not widely used, Bash and Batch compilers have been written. More recently sophisticated interpreted languages became part of the developers tool kit. Modern scripting languages include PHP, Python, Ruby and Lua. (Lua is widely used in game development.) All of these have interpreter and compiler support.\n\n\"When the field of compiling began in the late 50s, its focus was limited to the translation of high-level language programs into machine code ... The compiler field is increasingly intertwined with other disciplines including computer architecture, programming languages, formal methods, software engineering, and computer security.\" The \"Compiler Research: The Next 50 Years\" article noted the importance of object-oriented languages and Java. Security and parallel computing were cited among the future research targets.\n\nA compiler implements a formal transformation from a high-level source program to a low-level target program. Compiler design can define an end to end solution or tackle a defined subset that interfaces with other compilation tools e.g. preprocessors, assemblers, linkers. Design requirements include rigorously defined interfaces both internally between compiler components and externally between supporting toolsets.\n\nIn the early days, the approach taken to compiler design was directly affected by the complexity of the computer language to be processed, the experience of the person(s) designing it, and the resources available. Resource limitations led to the need to pass through the source code more than once.\n\nA compiler for a relatively simple language written by one person might be a single, monolithic piece of software. However, as the source language grows in complexity the design may be split into a number of interdependent phases. Separate phases provide design improvements that focus development on the functions in the compilation process.\n\nClassifying compilers by number of passes has its background in the hardware resource limitations of computers. Compiling involves performing lots of work and early computers did not have enough memory to contain one program that did all of this work. So compilers were split up into smaller programs which each made a pass over the source (or some representation of it) performing some of the required analysis and translations.\n\nThe ability to compile in a single pass has classically been seen as a benefit because it simplifies the job of writing a compiler and one-pass compilers generally perform compilations faster than multi-pass compilers. Thus, partly driven by the resource limitations of early systems, many early languages were specifically designed so that they could be compiled in a single pass (e.g., Pascal).\n\nIn some cases the design of a language feature may require a compiler to perform more than one pass over the source. For instance, consider a declaration appearing on line 20 of the source which affects the translation of a statement appearing on line 10. In this case, the first pass needs to gather information about declarations appearing after statements that they affect, with the actual translation happening during a subsequent pass.\n\nThe disadvantage of compiling in a single pass is that it is not possible to perform many of the sophisticated optimizations needed to generate high quality code. It can be difficult to count exactly how many passes an optimizing compiler makes. For instance, different phases of optimization may analyse one expression many times but only analyse another expression once.\n\nSplitting a compiler up into small programs is a technique used by researchers interested in producing provably correct compilers. Proving the correctness of a set of small programs often requires less effort than proving the correctness of a larger, single, equivalent program.\n\nRegardless of the exact number of phases in the compiler design, the phases can be assigned to one of three stages. The stages include a front end, a middle end, and a back end.\n\nThis front/middle/back-end approach makes it possible to combine front ends for different languages with back ends for different CPUs while sharing the optimizations of the middle end. Practical examples of this approach are the GNU Compiler Collection, LLVM, and the Amsterdam Compiler Kit, which have multiple front-ends, shared optimizations and multiple back-ends.\n\nThe front end analyzes the source code to build an internal representation of the program, called the intermediate representation (IR). It also manages the symbol table, a data structure mapping each symbol in the source code to associated information such as location, type and scope.\n\nWhile the frontend can be a single monolithic function or program, as in a scannerless parser, it is more commonly implemented and analyzed as several phases, which may execute sequentially or concurrently. This method is favored due to its modularity and separation of concerns. Most commonly today, the frontend is broken into three phases: lexical analysis (also known as lexing), syntax analysis (also known as scanning or parsing), and semantic analysis. Lexing and parsing comprise the syntactic analysis (word syntax and phrase syntax, respectively), and in simple cases these modules (the lexer and parser) can be automatically generated from a grammar for the language, though in more complex cases these require manual modification. The lexical grammar and phrase grammar are usually context-free grammars, which simplifies analysis significantly, with context-sensitivity handled at the semantic analysis phase. The semantic analysis phase is generally more complex and written by hand, but can be partially or fully automated using attribute grammars. These phases themselves can be further broken down: lexing as scanning and evaluating, and parsing as building a concrete syntax tree (CST, parse tree) and then transforming it into an abstract syntax tree (AST, syntax tree). In some cases additional phases are used, notably \"line reconstruction\" and \"preprocessing,\" but these are rare.\n\nThe main phases of the front end include the following:\n\nThe middle end performs optimizations on the intermediate representation in order to improve the performance and the quality of the produced machine code. The middle end contains those optimizations that are independent of the CPU architecture being targeted.\n\nThe main phases of the middle end include the following:\nCompiler analysis is the prerequisite for any compiler optimization, and they tightly work together. For example, dependence analysis is crucial for loop transformation.\n\nThe scope of compiler analysis and optimizations vary greatly; their scope may range from operating within a basic block, to whole procedures, or even the whole program. There is a trade-off between the granularity of the optimizations and the cost of compilation. For example, peephole optimizations are fast to perform during compilation but only affect a small local fragment of the code, and can be performed independently of the context in which the code fragment appears. In contrast, interprocedural optimization requires more compilation time and memory space, but enable optimizations which are only possible by considering the behavior of multiple functions simultaneously.\n\nInterprocedural analysis and optimizations are common in modern commercial compilers from HP, IBM, SGI, Intel, Microsoft, and Sun Microsystems. The free software GCC was criticized for a long time for lacking powerful interprocedural optimizations, but it is changing in this respect. Another open source compiler with full analysis and optimization infrastructure is Open64, which is used by many organizations for research and commercial purposes.\n\nDue to the extra time and space needed for compiler analysis and optimizations, some compilers skip them by default. Users have to use compilation options to explicitly tell the compiler which optimizations should be enabled.\n\nThe back end is responsible for the CPU architecture specific optimizations and for code generation\".\"\n\nThe main phases of the back end include the following:\n\nCompiler correctness is the branch of software engineering that deals with trying to show that a compiler behaves according to its language specification. Techniques include developing the compiler using formal methods and using rigorous testing (often called compiler validation) on an existing compiler.\n\nHigher-level programming languages usually appear with a type of translation in mind: either designed as compiled language or interpreted language. However, in practice there is rarely anything about a language that \"requires\" it to be exclusively compiled or exclusively interpreted, although it is possible to design languages that rely on re-interpretation at run time. The categorization usually reflects the most popular or widespread implementations of a language — for instance, BASIC is sometimes called an interpreted language, and C a compiled one, despite the existence of BASIC compilers and C interpreters.\n\nInterpretation does not replace compilation completely. It only hides it from the user and makes it gradual. Even though an interpreter can itself be interpreted, a directly executed program is needed somewhere at the bottom of the stack (see machine language).\n\nFurther, compilers can contain interpreters for optimization reasons. For example, where an expression can be executed during compilation and the results inserted into the output program, then it prevents it having to be recalculated each time the program runs, which can greatly speed up the final program. Modern trends toward just-in-time compilation and bytecode interpretation at times blur the traditional categorizations of compilers and interpreters even further.\n\nSome language specifications spell out that implementations \"must\" include a compilation facility; for example, Common Lisp. However, there is nothing inherent in the definition of Common Lisp that stops it from being interpreted. Other languages have features that are very easy to implement in an interpreter, but make writing a compiler much harder; for example, APL, SNOBOL4, and many scripting languages allow programs to construct arbitrary source code at runtime with regular string operations, and then execute that code by passing it to a special evaluation function. To implement these features in a compiled language, programs must usually be shipped with a runtime library that includes a version of the compiler itself.\n\nOne classification of compilers is by the platform on which their generated code executes. This is known as the \"target platform.\"\n\nA \"native\" or \"hosted\" compiler is one whose output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment.\n\nThe output of a compiler that produces code for a virtual machine (VM) may or may not be executed on the same platform as the compiler that produced it. For this reason such compilers are not usually classified as native or cross compilers.\n\nThe lower level language that is the target of a compiler may itself be a high-level programming language. C, often viewed as some sort of portable assembler, can also be the target language of a compiler. E.g.: Cfront, the original compiler for C++ used C as target language. The C created by such a compiler is usually not intended to be read and maintained by humans. So indent style and pretty C intermediate code are irrelevant. Some features of C turn it into a good target language. E.g.: C code with codice_1 directives can be generated to support debugging of the original source.\n\nWhile a common compiler type outputs machine code, there are many other types:\n\n\n"}
{"id": "5742", "url": "https://en.wikipedia.org/wiki?curid=5742", "title": "Castrato", "text": "Castrato\n\nA castrato (Italian, plural: \"castrati\") is a type of classical male singing voice equivalent to that of a soprano, mezzo-soprano, or contralto. The voice is produced by castration of the singer before puberty, or it occurs in one who, due to an endocrinological condition, never reaches sexual maturity.\n\nCastration before puberty (or in its early stages) prevents a boy's larynx from being transformed by the normal physiological events of puberty. As a result, the vocal range of prepubescence (shared by both sexes) is largely retained, and the voice develops into adulthood in a unique way. Prepubescent castration for this purpose diminished greatly in the late 18th century and was made illegal in the Papal states, the last to prohibit them, in 1870.\n\nAs the castrato's body grew, his lack of testosterone meant that his epiphyses (bone-joints) did not harden in the normal manner. Thus the limbs of the castrati often grew unusually long, as did the bones of their ribs. This, combined with intensive training, gave them unrivalled lung-power and breath capacity. Operating through small, child-sized vocal cords, their voices were also extraordinarily flexible, and quite different from the equivalent adult female voice. Their vocal range was higher than that of the uncastrated adult male. Listening to the only surviving recordings of a castrato (see below), one can hear that the lower part of the voice sounds like a \"super-high\" tenor, with a more falsetto-like upper register above that.\n\nCastrati were rarely referred to as such: in the 18th century, the euphemism \"musico\" (pl \"musici\") was much more generally used, although it usually carried derogatory implications; another synonym was \"evirato,\" literally meaning \"emasculated\". Eunuch is a more general term since, historically, many eunuchs were castrated after puberty and thus the castration had no impact on their voices.\n\nCastration as a means of subjugation, enslavement or other punishment has a very long history, dating back to ancient Sumer. In a Western context, eunuch singers are known to have existed from the early Byzantine Empire. In Constantinople around 400 AD, the empress Aelia Eudoxia had a eunuch choir-master, Brison, who may have established the use of castrati in Byzantine choirs, though whether Brison himself was a singer and whether he had colleagues who were eunuch singers is not certain. By the 9th century, eunuch singers were well-known (not least in the choir of Hagia Sophia) and remained so until the sack of Constantinople by the Western forces of the Fourth Crusade in 1204. Their fate from then until their reappearance in Italy more than three hundred years later is not clear. It seems likely that the Spanish tradition of soprano falsettists may have hidden castrati. Much of Spain was under Muslim rulers during the Middle Ages, and castration had a history going back to the ancient Near East. Stereotypically, eunuchs served as harem guards, but they were also valued as high-level political appointees since they could not start a dynasty which would threaten the ruler.\n\nCastrati first appeared in Italy in the mid-16th century, though at first the terms describing them were not always clear. The phrase \"soprano maschio\" (male soprano), which could also mean falsettist, occurs in the \"Due Dialoghi della Musica\" of Luigi Dentice, an Oratorian priest, published in Rome in 1553. On 9 November 1555 Cardinal Ippolito II d'Este (famed as the builder of the Villa d'Este at Tivoli), wrote to Guglielmo Gonzaga, Duke of Mantua (1538–1587), that he has heard that His Grace is interested in his \"cantoretti\" and offered to send him two, so that he could choose one for his own service. This is a rare term but probably does equate to \"castrato\". The Cardinal's nephew, Alfonso II d'Este, Duke of Ferrara, was another early enthusiast, enquiring about castrati in 1556. There were certainly castrati in the Sistine Chapel choir in 1558, although not described as such: on 27 April of that year, Hernando Bustamante, a Spaniard from Palencia, was admitted (the first castrati so termed who joined the Sistine choir were Pietro Paolo Folignato and Girolamo Rossini, admitted in 1599). Surprisingly, considering the later French distaste for castrati they certainly existed in France at this time also, being known of in Paris, Orléans, Picardy and Normandy, though they were not abundant: the King of France himself had difficulty in obtaining them. By 1574, there were castrati in the Ducal court chapel at Munich, where the Kapellmeister (music director) was the famous Orlando di Lasso. In 1589, by the bull \"Cum pro nostro pastorali munere\", Pope Sixtus V re-organised the choir of St Peter's, Rome specifically to include castrati. Thus the castrati came to supplant both boys (whose voices broke after only a few years) and falsettists (whose voices were weaker and less reliable) from the top line in such choirs. Women were banned by the Pauline dictum \"mulieres in ecclesiis taceant\" (\"let women keep silent in the churches\"; see I Corinthians, ch. 14, v. 34).\n\nAlthough the castrato (or musico) predates opera, there is some evidence that castrati had parts in the earliest operas. In the first performance of Monteverdi's \"Orfeo\" (1607), for example, they played subsidiary roles, including Speranza and (possibly) that of Euridice. Although female roles were performed by castrati in some of the papal states, this was increasingly rare; by 1680, they had supplanted \"normal\" male voices in lead roles, and retained their position as \"primo uomo\" for about a hundred years; an Italian opera not featuring at least one renowned castrato in a lead part would be doomed to fail. Because of the popularity of Italian opera throughout 18th-century Europe (except France), singers such as Ferri, Farinelli, Senesino and Pacchierotti became the first operatic superstars, earning enormous fees and hysterical public adulation. The strictly hierarchical organisation of \"opera seria\" favoured their high voices as symbols of heroic virtue, though they were frequently mocked for their strange appearance and bad acting. In his 1755 \"Reflections upon theatrical expression in tragedy\", Roger Pickering wrote:\nFarinelli drew every Body to the Haymarket. What a Pipe! What Modulation! What Extasy to the Ear! But, Heavens! What Clumsiness! What Stupidity! What Offence to the Eye! Reader, if of the City, thou mayest probably have seen in the Fields of Islington or Mile-End or, If thou art in the environs of St James', thou must have observed in the Park with what Ease and Agility a cow, heavy with calf, has rose up at the command of the Milk-woman's foot: thus from the mossy bank sprang the DIVINE FARINELLI.\nThe means by which future singers were prepared could lead to premature death. To prevent the child from experiencing the intense pain of castration, many were inadvertently administered lethal doses of opium or some other narcotic, or were killed by overlong compression of the carotid artery in the neck (intended to render them unconscious during the castration procedure).\n\nDuring the 18th century itself, the music historian Charles Burney was sent from pillar to post in search of places where \"the operation\" was carried out:\nI enquired throughout Italy at what place boys were chiefly qualified for singing by castration, but could get no certain intelligence. I was told at Milan that it was at Venice; at Venice that it was at Bologna; but at Bologna the fact was denied, and I was referred to Florence; from Florence to Rome, and from Rome I was sent to Naples ... it is said that there are shops in Naples with this inscription: 'QUI SI CASTRANO RAGAZZI' (\"Here boys are castrated\"); but I was utterly unable to see or hear of any such shops during my residence in that city.\n\nThe training of the boys was rigorous. The regimen of one singing school in Rome (c. 1700) consisted of one hour of singing difficult and awkward pieces, one hour practising trills, one hour practising ornamented passaggi, one hour of singing exercises in their teacher's presence and in front of a mirror so as to avoid unnecessary movement of the body or facial grimaces, and one hour of literary study; all this, moreover, before lunch. After, half an hour would be devoted to musical theory, another to writing counterpoint, an hour copying down the same from dictation, and another hour of literary study. During the remainder of the day, the young castrati had to find time to practice their harpsichord playing, and to compose vocal music, either sacred or secular depending on their inclination. This demanding schedule meant that, if sufficiently talented, they were able to make a debut in their mid-teens with a perfect technique and a voice of a flexibility and power no woman or ordinary male singer could match.\nIn the 1720s and 1730s, at the height of the craze for these voices, it has been estimated that upwards of 4,000 boys were castrated annually in the service of art. Many came from poor homes and were castrated by their parents in the hope that their child might be successful and lift them from poverty (this was the case with Senesino). There are, though, records of some young boys asking to be operated on to preserve their voices (e.g. Caffarelli, who was from a wealthy family: his grandmother gave him the income from two vineyards to pay for his studies). Caffarelli was also typical of many castrati in being famous for tantrums on and off-stage, and for amorous adventures with noble ladies. Some, as described by Casanova, preferred gentlemen (noble or otherwise). Only a small percentage of boys castrated to preserve their voices had successful careers on the operatic stage; the better \"also-rans\" sang in cathedral or church choirs, but because of their marked appearance and the ban on their marrying, there was little room for them in society outside a musical context.\n\nThe castrati came in for a great amount of scurrilous and unkind abuse, and as their fame increased, so did the hatred of them. They were often castigated as malign creatures who lured men into homosexuality. There were homosexual castrati, as Casanova's accounts of 18th-century Italy bear witness. He mentions meeting an abbé whom he took for a girl in disguise, only later discovering that \"she\" was a famous castrato. In Rome in 1762 he attended a performance at which the prima donna was a castrato, \"the favourite pathic\" of Cardinal Borghese, who dined every evening with his protector. From his behaviour on stage \"it was obvious that he hoped to inspire the love of those who liked him as a man, and probably would not have done so as a woman\".\n\nBy the late 18th century, changes in operatic taste and social attitudes spelled the end for castrati. They lingered on past the end of the \"ancien régime\" (which their style of opera parallels), and two of their number, Pacchierotti and Crescentini, even entranced the iconoclastic Napoleon. The last great operatic castrato was Giovanni Battista Velluti (1781–1861), who performed the last operatic castrato role ever written: Armando in \"Il crociato in Egitto\" by Meyerbeer (Venice, 1824). Soon after this they were replaced definitively as the first men of the operatic stage by a new breed of heroic tenor, as first incarnated by the Frenchman Gilbert-Louis Duprez, the earliest so-called \"king of the high Cs\". His successors have included such singers as Enrico Tamberlik, Jean de Reszke, Francesco Tamagno, Enrico Caruso, Giovanni Martinelli, Beniamino Gigli, Jussi Björling, Franco Corelli and Luciano Pavarotti, among others.\n\nAfter the unification of Italy in 1861, castration for musical purposes was officially made illegal (the new Italian state had adopted a French legal code which expressly forbade the practice). In 1878, Pope Leo XIII prohibited the hiring of new castrati by the church: only in the Sistine Chapel and in other papal basilicas in Rome did a few castrati linger. A group photo of the Sistine Choir taken in 1898 shows that by then only six remained (plus the \"Direttore Perpetuo\", the fine soprano castrato Domenico Mustafà), and in 1902 a ruling was extracted from Pope Leo that no further castrati should be admitted. The official end to the castrati came on St. Cecilia's Day, 22 November 1903, when the new pope, Pius X, issued his \"motu proprio\", \"Tra le Sollecitudini\" ('Amongst the Cares'), which contained this instruction: \"Whenever ... it is desirable to employ the high voices of sopranos and contraltos, these parts must be taken by boys, according to the most ancient usage of the Church.\"\n\nThe last Sistine castrato to survive was Alessandro Moreschi, the only castrato to have made solo recordings. While an interesting historical record, these discs of his give us only a glimpse of the castrato voice – although he had been renowned as \"The Angel of Rome\" at the beginning of his career, some would say he was past his prime when the recordings were made in 1902 and 1904 and he never attempted to sing opera. The recording technology of the day was not of modern high quality. He retired officially in March 1913, and died in 1922.\n\nThe Catholic Church's involvement in the castrato phenomenon has long been controversial, and there have recently been calls for it to issue an official apology for its role. As early as 1748, Pope Benedict XIV tried to ban castrati from churches, but such was their popularity at the time that he realised that doing so might result in a drastic decline in church attendance.\n\nThe rumours of another castrato sequestered in the Vatican for the personal delectation of the Pontiff until as recently as 1959 have been proven false. The singer in question was a pupil of Moreschi's, Domenico Mancini, such a successful imitator of his teacher's voice that even Lorenzo Perosi, Direttore Perpetuo of the Sistine Choir from 1898 to 1956 and a strenuous opponent of the practice of castrato singers, thought he was a castrato. Mancini was in fact a moderately skilful falsettist and professional double bass player.\n\nSo-called \"natural\" or \"endocrinological castrati\" are born with hormonal anomalies, such as Klinefelter's syndrome and Kallmann's syndrome, or have undergone unusual physical or medical events during their early lives that reproduce the vocal effects of castration without being castrated. Basically, a male can retain his child voice if it never changes during puberty. The retained voice can be the treble voice shared by both sexes in childhood and is the same as boy soprano voice. But as evidence shows, many castratos, such as Senesino and Caffarelli, were actually altos (mezzosoprano) – not sopranos.\n\nJimmy Scott, Robert Crowe and Radu Marian are examples of this type of high male voice. Michael Maniaci is somewhat different, in that he has no hormonal or other anomalies, but for some unknown reason, his voice did not \"break\" in the usual manner, leaving him still able to sing in the soprano register. Other uncastrated male adults sing soprano, generally using some form of falsetto but in a much higher range than most countertenors. Examples are Aris Christofellis, Jörg Waschinski, and Ghio Nannini.\n\nHowever, it is believed the castrati possessed more of a tenorial chest register (the aria \"Navigante che non spera\" in Leonardo Vinci's opera \"Il Medo\", written for Farinelli, requires notes down to C, 131 Hz). Similar low-voiced singing can be heard from the jazz vocalist Jimmy Scott, whose range matches approximately that used by female blues singers. High-pitched singer Jordan Smith has demonstrated having more of a tenorial chest register.\n\nActor Chris Colfer has soprano voice. Colfer has stated in interviews that when his voice began to change at puberty he sang in a high voice \"constantly\" in an effort to retain his range. Actor and singer Alex Newell has soprano range. Voice actor Walter Tetley may or may not have been a \"castrato\"; Bill Scott, a co-worker of Tetley's during their later work in television, once half-jokingly quipped that Tetley's mother \"had him fixed\" to protect the child star's voice-acting career. Tetley never personally divulged the exact reason for his condition, which left him with the voice of a preteen boy for his entire adult life.\n\nTurkish popular singer Cem Adrian has the ability to sing from bass to soprano, his vocal folds having been reported to be three times the average length.\n\n\n\n\n"}
{"id": "5743", "url": "https://en.wikipedia.org/wiki?curid=5743", "title": "Counting-out game", "text": "Counting-out game\n\nA counting-out game is a simple game intended to select a person to be \"it\", often for the purpose of playing another game. These games usually require no materials, and are played with spoken words or hand gestures. The historian Henry Carrington Bolton suggested in his 1888 book \"Counting Out Rhymes of Children\" that the custom of counting out originated in the \"superstitious practice of divination by lot.\" \n\nMany such games involve one person pointing at each participant in a circle of players while reciting a rhyme. A new person is pointed at as each word is said. The player who is selected at the conclusion of the rhyme is \"it\" or \"out\". In an alternate version, the circle of players may each put two feet in and at the conclusion of the rhyme, that player removes one foot and the rhyme starts over with the next person. In this case, the first player that has both feet removed is \"it\" or \"out\". These are often accepted as random selections because the number of words has not been calculated beforehand, so the result is unknown until someone is selected.\n\nA variant of counting-out game, known as the Josephus problem, represents a famous theoretical problem in mathematics and computer science.\n\nSeveral simple games can be played to select one person from a group, either as a straightforward winner, or as someone who is eliminated. Rock, Paper, Scissors, Odd or Even and Blue Shoe require no materials and are played using hand gestures, although with the former it is possible for a player to win or lose through skill rather than luck. Coin flipping and drawing straws are fair methods of randomly determining a player. Bizz Buzz is a spoken word game where if a player slips up and speaks a word out of sequence, they are eliminated.\n\n\nA scene in the Marx Brothers movie \"Duck Soup\" plays on the fact that counting-out games are not really random. Faced with selecting someone to go on a dangerous mission, the character Chicolini (Chico Marx) chants:\n\nonly to stop as he realizes he is about to select himself. He then says, \"I did it wrong. Wait, wait, I start here\", and repeats the chant—with the same result. After that, he says, \"That's no good too. I got it!\" and reduces the chant to\n\nAnd with this version he finally manages to \"randomly\" select someone else.\n\n\n"}
{"id": "5749", "url": "https://en.wikipedia.org/wiki?curid=5749", "title": "Key size", "text": "Key size\n\nIn cryptography, key size or key length is the number of bits in a key used by a cryptographic algorithm (such as a cipher).\n\nKey length defines the upper-bound on an algorithm's security (i.e., a logarithmic measure of the fastest known attack against an algorithm, relative to the key length), since the security of all algorithms can be violated by brute force attacks. Ideally, key length would coincide with the lower-bound on an algorithm's security. Indeed, most symmetric-key algorithms are designed to have security equal to their key length. However, after design, a new attack might be discovered. For instance, Triple DES was designed to have a 168 bit key, but an attack of complexity 2 is now known (i.e., Triple DES has 112 bits of security). Nevertheless, as long as the relation between key length and security is sufficient for a particular application, then it doesn't matter if key length and security coincide. This is important for asymmetric-key algorithms, because no such algorithm is known to satisfy this property; elliptic curve cryptography comes the closest with an effective security of roughly half its key length.\n\nKeys are used to control the operation of a cipher so that only the correct key can convert encrypted text (ciphertext) to plaintext. Many ciphers are actually based on publicly known algorithms or are open source and so it is only the difficulty of obtaining the key that determines security of the system, provided that there is no analytic attack (i.e., a 'structural weakness' in the algorithms or protocols used), and assuming that the key is not otherwise available (such as via theft, extortion, or compromise of computer systems). The widely accepted notion that the security of the system should depend on the key alone has been explicitly formulated by Auguste Kerckhoffs (in the 1880s) and Claude Shannon (in the 1940s); the statements are known as Kerckhoffs' principle and Shannon's Maxim respectively.\n\nA key should therefore be large enough that a brute force attack (possible against any encryption algorithm) is infeasible – i.e., would take too long to execute. Shannon's work on information theory showed that to achieve so called \"perfect secrecy\", the key length must be at least as large as the message and only used once (this algorithm is called the One-time pad). In light of this, and the practical difficulty of managing such long keys, modern cryptographic practice has discarded the notion of perfect secrecy as a requirement for encryption, and instead focuses on \"computational security\", under which the computational requirements of breaking an encrypted text must be infeasible for an attacker.\n\nEncryption systems are often grouped into families. Common families include symmetric systems (e.g. AES) and asymmetric systems (e.g. RSA); they may alternatively be grouped according to the central algorithm used (e.g. elliptic curve cryptography).\n\nAs each of these is of a different level of cryptographic complexity, it is usual to have different key sizes for the same level of security, depending upon the algorithm used. For example, the security available with a 1024-bit key using asymmetric RSA is considered approximately equal in security to an 80-bit key in a symmetric algorithm.\n\nThe actual degree of security achieved over time varies, as more computational power and more powerful mathematical analytic methods become available. For this reason cryptologists tend to look at indicators that an algorithm or key length shows signs of potential vulnerability, to move to longer key sizes or more difficult algorithms. For example, , a 1039 bit integer was factored with the special number field sieve using 400 computers over 11 months. The factored number was of a special form; the special number field sieve cannot be used on RSA keys. The computation is roughly equivalent to breaking a 700 bit RSA key. However, this might be an advance warning that 1024 bit RSA used in secure online commerce should be deprecated, since they may become breakable in the near future. Cryptography professor Arjen Lenstra observed that \"Last time, it took nine years for us to generalize from a special to a nonspecial, hard-to-factor number\" and when asked whether 1024-bit RSA keys are dead, said: \"The answer to that question is an unqualified yes.\"\n\nThe 2015 Logjam attack revealed additional dangers in using Diffie-Helman key exchange when only one or a few common 1024-bit or smaller prime moduli are in use. This common practice allows large amounts of communications to be compromised at the expense of attacking a small number of primes.\n\nEven if a symmetric cipher is currently unbreakable by exploiting structural weaknesses in its algorithm, it is possible to run through the entire space of keys in what is known as a \"brute force attack\". Since longer symmetric keys require exponentially more work to brute force search, a sufficiently long symmetric key makes this line of attack impractical.\n\nWith a key of length \"n\" bits, there are 2 possible keys. This number grows very rapidly as \"n\" increases. The large number of operations (2) required to try all possible 128-bit keys is widely considered out of reach for conventional digital computing techniques for the foreseeable future. However, experts anticipate alternative computing technologies that may have processing power superior to current computer technology. If a suitably sized quantum computer capable of running Grover's algorithm reliably becomes available, it would reduce a 128-bit key down to 64-bit security, roughly a DES equivalent. This is one of the reasons why AES supports a 256-bit key length. See the discussion on the relationship between key lengths and quantum computing attacks at the bottom of this page for more information.\n\nUS Government export policy has long restricted the 'strength' of cryptography that can be sent out of the country. For many years the limit was 40 bits. Today, a key length of 40 bits offers little protection against even a casual attacker with a single PC, a predictable and inevitable consequence of governmental restrictions limiting key length. In response, by the year 2000, most of the major US restrictions on the use of strong encryption were relaxed. However, not all regulations have been removed, and encryption registration with the U.S. Bureau of Industry and Security is still required to export \"mass market encryption commodities, software and components with encryption exceeding 64 bits\" ().\n\nIBM's Lucifer cipher was selected in 1974 as the base for what would become the Data Encryption Standard. Lucifer's key length was reduced from 128 bits to 56 bits, which the NSA and NIST argued was sufficient. The NSA has major computing resources and a large budget; some cryptographers including Whitfield Diffie and Martin Hellman complained that this made the cipher so weak that NSA computers would be able to break a DES key in a day through brute force parallel computing. The NSA disputed this, claiming brute forcing DES would take them something like 91 years. However, by the late 90s, it became clear that DES could be cracked in a few days' time-frame with custom-built hardware such as could be purchased by a large corporation or government. The book \"Cracking DES\" (O'Reilly and Associates) tells of the successful attempt in 1998 to break 56-bit DES by a brute force attack mounted by a cyber civil rights group with limited resources; see EFF DES cracker. Even before that demonstration, 56 bits was considered insufficient length for symmetric algorithm keys; DES has been replaced in many applications by Triple DES, which has 112 bits of security when used 168-bit keys (triple key). In 2002, Distributed.net and its volunteers broke a 64-bit RC5 key after several years effort, using about seventy thousand (mostly home) computers.\n\nThe Advanced Encryption Standard published in 2001 uses key sizes of 128 bits, 192 or 256 bits. Many observers consider 128 bits sufficient for the foreseeable future for symmetric algorithms of AES's quality until quantum computers become available. However, as of 2015, the U.S. National Security Agency has issued guidance that it plans to switch to quantum computing resistant algorithms and now requires 256-bit AES keys for data classified up to Top Secret.\n\nIn 2003, the U.S. National Institute for Standards and Technology, NIST proposed phasing out 80-bit keys by 2015. At 2005, 80-bit keys were allowed only until 2010.\n\nSince 2015, NIST guidance says that \"the use of keys that provide less than 112 bits of security strength for key agreement is now disallowed.\" NIST approved symmetric encryption algorithms include three-key Triple DES, and AES. Approvals for two-key Triple DES and Skipjack were withdrawn in 2015; the NSA's Skipjack algorithm used in its Fortezza program employs 80-bit keys.\n\nThe effectiveness of public key cryptosystems depends on the intractability (computational and theoretical) of certain mathematical problems such as integer factorization. These problems are time consuming to solve, but usually faster than trying all possible keys by brute force. Thus, asymmetric algorithm keys must be longer for equivalent resistance to attack than symmetric algorithm keys. As of 2002, an asymmetric key length of 1024 bits was generally considered by cryptology experts to be the minimum necessary for the RSA encryption algorithm.\n\nThe Finite Field Diffie-Hellman algorithm has roughly the same key strength as RSA for the same key sizes. The work factor for breaking Diffie-Hellman is based on the discrete logarithm problem, which is related to the integer factorization problem on which RSA's strength is based. Thus, a 3072-bit Diffie-Hellman key has about the same strength as a 3072-bit RSA key.\n\nOne of the asymmetric algorithm types, elliptic curve cryptography, or ECC, appears to be secure with shorter keys than other asymmetric key algorithms require. NIST guidelines state that ECC keys should be twice the length of equivalent strength symmetric key algorithms. So, for example, a 224-bit ECC key would have roughly the same strength as a 112-bit symmetric key. These estimates assume no major breakthroughs in solving the underlying mathematical problems that ECC is based on. A message encrypted with an elliptic key algorithm using a 109-bit long key has been broken by brute force.\n\nThe NSA previously specified that \"Elliptic Curve Public Key Cryptography using the 256-bit prime modulus elliptic curve as specified in FIPS-186-2 and SHA-256 are appropriate for protecting classified information up to the SECRET level. Use of the 384-bit prime modulus elliptic curve and SHA-384 are necessary for the protection of TOP SECRET information.\" In 2015 the NSA announced that it plans to transition from Elliptic Curve Cryptography to new algorithms that are resistant to attack by future quantum computers. In the interim it recommends the larger 384-bit curve for all classified information.\n\nThe two best known quantum computing attacks are based on Shor's algorithm and Grover's algorithm. Of the two, Shor's offers the greater risk to current security systems.\n\nDerivatives of Shor's algorithm are widely conjectured to be effective against all mainstream public-key algorithms including RSA, Diffie-Hellman and elliptic curve cryptography. According to Professor Gilles Brassard, an expert in quantum computing: \"The time needed to factor an RSA integer is the same order as the time needed to use that same integer as modulus for a single RSA encryption. In other words, it takes no more time to break RSA on a quantum computer (up to a multiplicative constant) than to use it legitimately on a classical computer.\" The general consensus is that these public key algorithms are insecure at any key size if sufficiently large quantum computers capable of running Shor's algorithm become available. The implication of this attack is that all data encrypted using current standards based security systems such as the ubiquitous SSL used to protect e-commerce and Internet banking and SSH used to protect access to sensitive computing systems is at risk. Encrypted data protected using public-key algorithms can be archived and may be broken at a later time.\n\nMainstream symmetric ciphers (such as AES or Twofish) and collision resistant hash functions (such as SHA) are widely conjectured to offer greater security against known quantum computing attacks. They are widely thought most vulnerable to Grover's algorithm. Bennett, Bernstein, Brassard, and Vazirani proved in 1996 that a brute-force key search on a quantum computer cannot be faster than roughly 2 invocations of the underlying cryptographic algorithm, compared with roughly 2 in the classical case. Thus in the presence of large quantum computers an \"n\"-bit key can provide at least \"n\"/2 bits of security. Quantum brute force is easily defeated by doubling the key length, which has little extra computational cost in ordinary use. This implies that at least a 256-bit symmetric key is required to achieve 128-bit security rating against a quantum computer. As mentioned above, the NSA announced in 2015 that it plans to transition to quantum-resistant algorithms.\n\nAccording to NSA \"A sufficiently large quantum computer, if built, would be capable of undermining all widely-deployed public key algorithms used for key establishment and digital signatures. ... It is generally accepted that quantum computing techniques are much less effective against symmetric algorithms than against current widely used public key algorithms. While public key cryptography requires changes in the fundamental design to protect against a potential future quantum computer, symmetric key algorithms are believed to be secure provided a sufficiently large key size is used. ... In the longer term, NSA looks to NIST to identify a broadly accepted, standardized suite of commercial public key algorithms that are not vulnerable to quantum attacks.\n\n, the NSA's Commercial National Security Algorithm Suite includes:\n\n\n\n"}
{"id": "5750", "url": "https://en.wikipedia.org/wiki?curid=5750", "title": "Cognitive behavioral therapy", "text": "Cognitive behavioral therapy\n\nCognitive behavioral therapy (CBT) is a psycho-social intervention that aims to improve mental health. CBT focuses on challenging and changing unhelpful cognitive distortions (e.g. thoughts, beliefs, and attitudes) and behaviors, improving emotional regulation, and the development of personal coping strategies that target solving current problems. Originally, it was designed to treat depression, but its use has been expanded to include treatment of a number of mental health conditions, including anxiety.\n\nThe CBT model is based on the combination of the basic principles from behavioral and cognitive psychology. It is different from historical approaches to psychotherapy, such as the psychoanalytic approach where the therapist looks for the unconscious meaning behind the behaviors and then formulates a diagnosis. Instead, CBT is a \"problem-focused\" and \"action-oriented\" form of therapy, meaning it is used to treat specific problems related to a diagnosed mental disorder. The therapist's role is to assist the client in finding and practicing effective strategies to address the identified goals and decrease symptoms of the disorder. CBT is based on the belief that thought distortions and maladaptive behaviors play a role in the development and maintenance of psychological disorders, and that symptoms and associated distress can be reduced by teaching new information-processing skills and coping mechanisms.\n\nWhen compared to psychoactive medications, review studies have found CBT alone to be as effective for treating less severe forms of depression and anxiety, posttraumatic stress disorder (PTSD), tics, substance abuse, eating disorders and borderline personality disorder. It is often recommended in combination with medications for treating other conditions, such as severe obsessive compulsive disorder (OCD) and major depressive disorder, opioid use disorder, bipolar disorder and psychotic disorders. In addition, CBT is recommended as the first line of treatment for majority of psychological disorders in children and adolescents, including aggression and conduct disorder. Researchers have found that other \"bona fide\" therapeutic interventions were equally effective for treating certain conditions in adults. Along with interpersonal psychotherapy (IPT), CBT is recommended in treatment guidelines as a psychosocial treatment of choice, and CBT and IPT are the only psychosocial interventions that psychiatry residents are mandated to be trained in.\n\nMainstream cognitive behavioral therapy assumes that changing maladaptive thinking leads to change in behavior and affect, but recent variants emphasize changes in one's relationship to maladaptive thinking rather than changes in thinking itself. The goal of cognitive behavioral therapy is not to diagnose a person with a particular disease, but to look at the person as a whole and decide what can be altered.\n\nTherapists or computer-based programs use CBT techniques to help individuals challenge their patterns and beliefs and replace errors in thinking, known as cognitive distortions, such as \"\"overgeneralizing, magnifying negatives, minimizing positives and catastrophizing\"\" with \"\"more realistic and effective thoughts, thus decreasing emotional distress and self-defeating behavior\"\". Cognitive distortions can be either a pseudo-discrimination belief or an over-generalization of something. CBT techniques may also be used to help individuals take a more open, mindful, and aware posture toward cognitive distortions so as to diminish their impact.\n\nMainstream CBT helps individuals replace \"\"maladaptive... coping skills, cognitions, emotions and behaviors with more adaptive ones\"\", by challenging an individual's way of thinking and the way that they react to certain habits or behaviors, but there is still controversy about the degree to which these traditional cognitive elements account for the effects seen with CBT over and above the earlier behavioral elements such as exposure and skills training.\n\nCBT can be seen as having six phases:\n\nThese steps are based on a system created by Kanfer and Saslow. After identifying the behaviors that need changing, whether they be in excess or deficit, and treatment has occurred, the psychologist must identify whether or not the intervention succeeded. For example, \"If the goal was to decrease the behavior, then there should be a decrease relative to the baseline. If the critical behavior remains at or above the baseline, then the intervention has failed.\"\n\nThe steps in the assessment phase include:\n\nThe re-conceptualization phase makes up much of the \"cognitive\" portion of CBT. A summary of modern CBT approaches is given by Hofmann.\n\nThere are different protocols for delivering cognitive behavioral therapy, with important similarities among them. Use of the term \"CBT\" may refer to different interventions, including \"self-instructions (e.g. distraction, imagery, motivational self-talk), relaxation and/or biofeedback, development of adaptive coping strategies (e.g. minimizing negative or self-defeating thoughts), changing maladaptive beliefs about pain, and goal setting\". Treatment is sometimes manualized, with brief, direct, and time-limited treatments for individual psychological disorders that are specific technique-driven. CBT is used in both individual and group settings, and the techniques are often adapted for self-help applications. Some clinicians and researchers are cognitively oriented (e.g. cognitive restructuring), while others are more behaviorally oriented (e.g. \"in vivo\" exposure therapy). Interventions such as imaginal exposure therapy combine both approaches.\n\nCBT may be delivered in conjunction with a variety of diverse but related techniques such as exposure therapy, stress inoculation, cognitive processing therapy, cognitive therapy, relaxation training, dialectical behavior therapy, and acceptance and commitment therapy. Some practitioners promote a form of mindful cognitive therapy which includes a greater emphasis on self-awareness as part of the therapeutic process.\n\nIn adults, CBT has been shown to have effectiveness and a role in the treatment plans for anxiety disorders, body dysmorphic disorder, depression, eating disorders, chronic low back pain, personality disorders, psychosis, schizophrenia, substance use disorders, in the adjustment, depression, and anxiety associated with fibromyalgia, and with post-spinal cord injuries.\n\nIn children or adolescents, CBT is an effective part of treatment plans for anxiety disorders, body dysmorphic disorder, depression and suicidality, eating disorders and obesity, obsessive–compulsive disorder (OCD), and posttraumatic stress disorder, as well as tic disorders, trichotillomania, and other repetitive behavior disorders. CBT-SP, an adaptation of CBT for suicide prevention (SP), was specifically designed for treating youths who are severely depressed and who have recently attempted suicide within the past 90 days, and was found to be effective, feasible, and acceptable. CBT has also been shown to be effective for posttraumatic stress disorder in very young children (3 to 6 years of age). CBT has also been applied to a variety of childhood disorders, including depressive disorders and various anxiety disorders.\n\nCBT combined with hypnosis and distraction reduces self-reported pain in children.\n\nCochrane reviews have found no evidence that CBT is effective for tinnitus, although there appears to be an effect on management of associated depression and quality of life in this condition. Other recent Cochrane Reviews found no convincing evidence that CBT training helps foster care providers manage difficult behaviors in the youths under their care, nor was it helpful in treating people who abuse their intimate partners.\n\nAccording to a 2004 review by INSERM of three methods, cognitive behavioral therapy was either \"proven\" or \"presumed\" to be an effective therapy on several specific mental disorders. According to the study, CBT was effective at treating schizophrenia, depression, bipolar disorder, panic disorder, post-traumatic stress, anxiety disorders, bulimia, anorexia, personality disorders and alcohol dependency.\n\nSome meta-analyses find CBT more effective than psychodynamic therapy and equal to other therapies in treating anxiety and depression.\n\nComputerized CBT (CCBT) has been proven to be effective by randomized controlled and other trials in treating depression and anxiety disorders, including children, as well as insomnia. Some research has found similar effectiveness to an intervention of informational websites and weekly telephone calls. CCBT was found to be equally effective as face-to-face CBT in adolescent anxiety and insomnia. \"Sparx\" is a video game to help young persons, using the CBT method to teach them how to resolve their own issues.\n\nCriticism of CBT sometimes focuses on implementations (such as the UK IAPT) which may result initially in low quality therapy being offered by poorly trained practitioners. However, evidence supports the effectiveness of CBT for anxiety and depression. Acceptance and commitment therapy (ACT) is a specialist branch of CBT (sometimes referred to as contextual CBT). ACT uses mindfulness and acceptance interventions and has been found to have a greater longevity in therapeutic outcomes. In a study with anxiety, CBT and ACT improved similarly across all outcomes from pre-to post-treatment. However, during a 12-month follow-up, ACT proved to be more effective, showing that it is a highly viable lasting treatment model for anxiety disorders.\n\nEvidence suggests that the addition of hypnotherapy as an adjunct to CBT improves treatment efficacy for a variety of clinical issues.\n\nCBT has been applied in both clinical and non-clinical environments to treat disorders such as personality conditions and behavioral problems. A systematic review of CBT in depression and anxiety disorders concluded that \"CBT delivered in primary care, especially including computer- or Internet-based self-help programs, is potentially more effective than usual care and could be delivered effectively by primary care therapists.\"\n\nEmerging evidence suggests a possible role for CBT in the treatment of attention deficit hyperactivity disorder (ADHD); hypochondriasis; coping with the impact of multiple sclerosis; sleep disturbances related to aging; dysmenorrhea; and bipolar disorder, but more study is needed and results should be interpreted with caution. CBT can have a therapeutic effects on easing symptoms of anxiety and depression in people with Alzheimer's disease. CBT has been studied as an aid in the treatment of anxiety associated with stuttering. Initial studies have shown CBT to be effective in reducing social anxiety in adults who stutter, but not in reducing stuttering frequency.\n\nIn the case of people with metastatic breast cancer, data is limited but CBT and other psychosocial interventions might help with psychological outcomes and pain management.\n\nThere is some evidence that CBT is superior in the long-term to benzodiazepines and the nonbenzodiazepines in the treatment and management of insomnia. CBT has been shown to be moderately effective for treating chronic fatigue syndrome.\n\nIn the United Kingdom, the National Institute for Health and Care Excellence (NICE) recommends CBT in the treatment plans for a number of mental health difficulties, including posttraumatic stress disorder, obsessive–compulsive disorder (OCD), bulimia nervosa, and clinical depression.\n\nCBT has been shown to be effective in the treatment of adults with anxiety disorders.\n\nA basic concept in some CBT treatments used in anxiety disorders is \"in vivo\" exposure. The term refers to the direct confrontation of feared objects, activities, or situations by a patient. For example, a woman with PTSD who fears the location where she was assaulted may be assisted by her therapist in going to that location and directly confronting those fears. Likewise, a person with social anxiety disorder who fears public speaking may be instructed to directly confront those fears by giving a speech. This \"two-factor\" model is often credited to O. Hobart Mowrer. Through exposure to the stimulus, this harmful conditioning can be \"unlearned\" (referred to as extinction and habituation). Studies have provided evidence that when examining animals and humans that glucocorticoids may possibly lead to a more successful extinction learning during exposure therapy. For instance, glucocorticoids can prevent aversive learning episodes from being retrieved and heighten reinforcement of memory traces creating a non-fearful reaction in feared situations. A combination of glucocorticoids and exposure therapy may be a better improved treatment for treating patients with anxiety disorders.\n\nA 2015 Cochrane review also found that CBT might be helpful for patients with non-cardiac chest pain, and may reduce frequency of chest pain episodes.\n\nCognitive behavioral therapy has been shown as an effective treatment for clinical depression. The American Psychiatric Association Practice Guidelines (April 2000) indicated that, among psychotherapeutic approaches, cognitive behavioral therapy and interpersonal psychotherapy had the best-documented efficacy for treatment of major depressive disorder. One etiological theory of depression is Aaron T. Beck's cognitive theory of depression. His theory states that depressed people think the way they do because their thinking is biased towards negative interpretations. According to this theory, depressed people acquire a negative schema of the world in childhood and adolescence as an effect of stressful life events, and the negative schema is activated later in life when the person encounters similar situations.\n\nBeck also described a negative cognitive triad. The cognitive triad is made up of the depressed individual's negative evaluations of themselves, the world, and the future. Beck suggested that these negative evaluations derive from the negative schemata and cognitive biases of the person. According to this theory, depressed people have views such as \"I never do a good job\", \"It is impossible to have a good day\", and \"things will never get better\". A negative schema helps give rise to the cognitive bias, and the cognitive bias helps fuel the negative schema. Beck further proposed that depressed people often have the following cognitive biases: arbitrary inference, selective abstraction, over-generalization, magnification, and minimization. These cognitive biases are quick to make negative, generalized, and personal inferences of the self, thus fueling the negative schema.\n\nIn long-term psychoses, CBT is used to complement medication and is adapted to meet individual needs. Interventions particularly related to these conditions include exploring reality testing, changing delusions and hallucinations, examining factors which precipitate relapse, and managing relapses. Several meta-analyses suggested that CBT is effective in schizophrenia, and the American Psychiatric Association includes CBT in its schizophrenia guideline as an evidence-based treatment. There is also limited evidence of effectiveness for CBT in bipolar disorder and severe depression.\n\nA 2010 meta-analysis found that no trial employing both blinding and psychological placebos has shown CBT to be effective in either schizophrenia or bipolar disorder, and that the effect size of CBT was small in major depressive disorder. They also found a lack of evidence to conclude that CBT was effective in preventing relapses in bipolar disorder. Evidence that severe depression is mitigated by CBT is also lacking, with anti-depressant medications still viewed as significantly more effective than CBT, although success with CBT for depression was observed beginning in the 1990s.\n\nAccording to Cox, Lyn Yvonne Abramson, Patricia Devine, and Hollon (2012), cognitive behavioral therapy can also be used to reduce prejudice towards others. This other-directed prejudice can cause depression in the \"others\", or in the self when a person becomes part of a group he or she previously had prejudice towards (i.e. deprejudice). \"Devine and colleagues (2012) developed a successful Prejudice Perpetrator intervention with many conceptual parallels to CBT. Like CBT, their intervention taught Sources to be aware of their automative thoughts and to intentionally deploy a variety of cognitive techniques against automatic stereotyping.\" A 2012 systematic review investigated the effects of CBT compared with other psychosocial therapies for people with schizophrenia:\nCBT is used to help people of all ages, but the therapy should be adjusted based on the age of the patient with whom the therapist is dealing. Older individuals in particular have certain characteristics that need to be acknowledged and the therapy altered to account for these differences thanks to age.\n\nFor anxiety disorders, use of CBT with people at risk has significantly reduced the number of episodes of generalized anxiety disorder and other anxiety symptoms, and also given significant improvements in explanatory style, hopelessness, and dysfunctional attitudes. In another study, 3% of the group receiving the CBT intervention developed generalized anxiety disorder by 12 months postintervention compared with 14% in the control group. Subthreshold panic disorder sufferers were found to significantly benefit from use of CBT. Use of CBT was found to significantly reduce social anxiety prevalence.\n\nFor depressive disorders, a stepped-care intervention (watchful waiting, CBT and medication if appropriate) achieved a 50% lower incidence rate in a patient group aged 75 or older. Another depression study found a neutral effect compared to personal, social, and health education, and usual school provision, and included a comment on potential for increased depression scores from people who have received CBT due to greater self recognition and acknowledgement of existing symptoms of depression and negative thinking styles. A further study also saw a neutral result. A meta-study of the Coping with Depression course, a cognitive behavioral intervention delivered by a psychoeducational method, saw a 38% reduction in risk of major depression.\n\nFor people at risk of psychosis, in 2014 the UK National Institute for Health and Care Excellence (NICE) recommended preventive CBT.\n\nCBT is also used for gambling addiction. The percentage of people who problem gamble is 1–3% around the world. Cognitive behavioral therapy develops skills for relapse prevention and someone can learn to control their mind and manage high-risk cases.\n\nCBT looks at the habit of smoking cigarettes as a learned behavior, which later evolves into a coping strategy to handle daily stressors. Because smoking is often easily accessible, and quickly allows the user to feel good, it can take precedence over other coping strategies, and eventually work its way into everyday life during non-stressful events as well. CBT aims to target the function of the behavior, as it can vary between individuals, and works to inject other coping mechanisms in place of smoking. CBT also aims to support individuals suffering from strong cravings, which are a major reported reason for relapse during treatment.\n\nIn a 2008 controlled study out of Stanford University School of Medicine, CBT was proven as an effective tool for most participants. The results of 304 random adult participants were tracked over the course of one year. During this program, some participants were provided medication, CBT, 24 hour phone support, or some combination of the three methods. At 20 weeks, the participants who received CBT had a 45% abstinence rate, versus non-CBT participants, who had a 29% abstinence rate. Overall, the study concluded that emphasizing cognitive and behavioral strategies to support smoking cessation can help individuals build tools for long term smoking abstinence.\n\nMental health history can affect the outcomes of treatment. It should be noted that individuals with a history of depressive disorders had a lower rate of success when using CBT alone to combat smoking addiction.\n\nThough many forms of treatment can support individuals with eating disorders, CBT is proven to be a more effective treatment than medications and interpersonal psychotherapy alone. CBT aims to combat major causes of distress such as negative cognitions surrounding body weight, shape and size. CBT therapists also work with individuals to regulate strong emotions and thoughts that lead to dangerous compensatory behaviors. CBT is the first line of treatment for Bulimia Nervosa, and Eating Disorder Non-Specific.\n\nResearch has identified Internet addiction as a new clinical disorder that causes relational, occupational, and social problems. Cognitive behavioral therapy (CBT) has been suggested as the treatment of choice for Internet addiction, and addiction recovery in general has used CBT as part of treatment planning.\n\nPrecursors of certain fundamental aspects of CBT have been identified in various ancient philosophical traditions, particularly Stoicism. Stoic philosophers, particularly Epictetus, believed logic could be used to identify and discard false beliefs that lead to destructive emotions, which has influenced the way modern cognitive-behavioral therapists identify cognitive distortions that contribute to depression and anxiety. For example, Aaron T. Beck's original treatment manual for depression states, \"The philosophical origins of cognitive therapy can be traced back to the Stoic philosophers\". Another example of Stoic influence on cognitive theorists is Epictetus on Albert Ellis. A key philosophical figure who also influenced the development of CBT was John Stuart Mill.\n\nThe modern roots of CBT can be traced to the development of behavior therapy in the early 20th century, the development of cognitive therapy in the 1960s, and the subsequent merging of the two. Groundbreaking work of behaviorism began with John B. Watson and Rosalie Rayner's studies of conditioning in 1920. Behaviorally-centered therapeutic approaches appeared as early as 1924 with Mary Cover Jones' work dedicated to the unlearning of fears in children. These were the antecedents of the development of Joseph Wolpe's behavioral therapy in the 1950s. It was the work of Wolpe and Watson, which was based on Ivan Pavlov's work on learning and conditioning, that influenced Hans Eysenck and Arnold Lazarus to develop new behavioral therapy techniques based on classical conditioning. One of Eysenck's colleagues, Glenn Wilson showed that classical fear conditioning in humans could be controlled by verbally induced cognitive expectations, thus opening a field of research that supports the rationale of cognitive behaviorial therapy.\n\nDuring the 1950s and 1960s, behavioral therapy became widely utilized by researchers in the United States, the United Kingdom, and South Africa, who were inspired by the behaviorist learning theory of Ivan Pavlov, John B. Watson, and Clark L. Hull. In Britain, Joseph Wolpe, who applied the findings of animal experiments to his method of systematic desensitization, applied behavioral research to the treatment of neurotic disorders. Wolpe's therapeutic efforts were precursors to today's fear reduction techniques. British psychologist Hans Eysenck presented behavior therapy as a constructive alternative.\n\nAt the same time of Eysenck's work, B. F. Skinner and his associates were beginning to have an impact with their work on operant conditioning. Skinner's work was referred to as radical behaviorism and avoided anything related to cognition. However, Julian Rotter, in 1954, and Albert Bandura, in 1969, contributed behavior therapy with their respective work on social learning theory, by demonstrating the effects of cognition on learning and behavior modification.\n\nThe emphasis on behavioral factors constituted the \"first wave\" of CBT.\n\nOne of the first therapists to address cognition in psychotherapy was Alfred Adler with his notion of basic mistakes and how they contributed to creation of unhealthy or useless behavioral and life goals. Adler's work influenced the work of Albert Ellis, who developed the earliest cognitive-based psychotherapy, known today as rational emotive behavior therapy, or REBT.\n\nAround the same time that rational emotive therapy, as it was known then, was being developed, Aaron T. Beck was conducting free association sessions in his psychoanalytic practice. During these sessions, Beck noticed that thoughts were not as unconscious as Freud had previously theorized, and that certain types of thinking may be the culprits of emotional distress. It was from this hypothesis that Beck developed cognitive therapy, and called these thoughts \"automatic thoughts\".\n\nIt was these two therapies, rational emotive therapy and cognitive therapy, that started the \"second wave\" of CBT, which was the emphasis on cognitive factors.\n\nAlthough the early behavioral approaches were successful in many of the neurotic disorders, they had little success in treating depression. Behaviorism was also losing in popularity due to the so-called \"cognitive revolution\". The therapeutic approaches of Albert Ellis and Aaron T. Beck gained popularity among behavior therapists, despite the earlier behaviorist rejection of \"mentalistic\" concepts like thoughts and cognitions. Both of these systems included behavioral elements and interventions and primarily concentrated on problems in the present.\n\nIn initial studies, cognitive therapy was often contrasted with behavioral treatments to see which was most effective. During the 1980s and 1990s, cognitive and behavioral techniques were merged into cognitive behavioral therapy. Pivotal to this merging was the successful development of treatments for panic disorder by David M. Clark in the UK and David H. Barlow in the US.\n\nOver time, cognitive behavior therapy became to be known not only as a therapy, but as an umbrella term for all cognitive-based psychotherapies. These therapies include, but are not limited to, rational emotive therapy (REBT), cognitive therapy, acceptance and commitment therapy, dialectical behavior therapy, reality therapy/choice theory, cognitive processing therapy, EMDR, and multimodal therapy. All of these therapies are a blending of cognitive- and behavior-based elements.\n\nThis blending of theoretical and technical foundations from both behavior and cognitive therapies constituted the \"third wave\" of CBT. The most prominent therapies of this third wave are dialectical behavior therapy and acceptance and commitment therapy.\n\nA typical CBT programme would consist of face-to-face sessions between patient and therapist, made up of 6-18 sessions of around an hour each with a gap of a 1–3 weeks between sessions. This initial programme might be followed by some booster sessions, for instance after one month and three months. CBT has also been found to be effective if patient and therapist type in real time to each other over computer links.\n\nCognitive behavioral therapy is most closely allied with the scientist–practitioner model in which clinical practice and research is informed by a scientific perspective, clear operationalization of the problem, and an emphasis on measurement, including measuring changes in cognition and behavior and in the attainment of goals. These are often met through \"homework\" assignments in which the patient and the therapist work together to craft an assignment to complete before the next session. The completion of these assignments – which can be as simple as a person suffering from depression attending some kind of social event – indicates a dedication to treatment compliance and a desire to change. The therapists can then logically gauge the next step of treatment based on how thoroughly the patient completes the assignment. Effective cognitive behavioral therapy is dependent on a therapeutic alliance between the healthcare practitioner and the person seeking assistance. Unlike many other forms of psychotherapy, the patient is very involved in CBT. For example, an anxious patient may be asked to talk to a stranger as a homework assignment, but if that is too difficult, he or she can work out an easier assignment first. The therapist needs to be flexible and willing to listen to the patient rather than acting as an authority figure.\n\nComputerized cognitive behavioral therapy (CCBT) has been described by NICE as a \"\"generic term for delivering CBT via an interactive computer interface delivered by a personal computer, internet, or interactive voice response system\"\", instead of face-to-face with a human therapist. It is also known as internet-delivered cognitive behavioral therapy or ICBT. CCBT has potential to improve access to evidence-based therapies, and to overcome the prohibitive costs and lack of availability sometimes associated with retaining a human therapist. In this context, it is important not to confuse CBT with 'computer-based training', which nowadays is more commonly referred to as e-Learning.\n\nCCBT has been found in meta-studies to be cost-effective and often cheaper than usual care, including for anxiety. Studies have shown that individuals with social anxiety and depression experienced improvement with online CBT-based methods. A review of current CCBT research in the treatment of OCD in children found this interface to hold great potential for future treatment of OCD in youths and adolescent populations. Additionally, most internet interventions for posttraumatic stress disorder use CCBT. CCBT is also predisposed to treating mood disorders amongst non-heterosexual populations, who may avoid face-to-face therapy from fear of stigma. However presently CCBT programs seldom cater to these populations.\n\nA key issue in CCBT use is low uptake and completion rates, even when it has been clearly made available and explained. CCBT completion rates and treatment efficacy have been found in some studies to be higher when use of CCBT is supported personally, with supporters not limited only to therapists, than when use is in a self-help form alone. Another approach to improving uptake and completion rate, as well as treatment outcome, is to design software that supports the formation of a strong therapeutic alliance between the user and the technology.\n\nIn February 2006 NICE recommended that CCBT be made available for use within the NHS across England and Wales for patients presenting with mild-to-moderate depression, rather than immediately opting for antidepressant medication, and CCBT is made available by some health systems. The 2009 NICE guideline recognized that there are likely to be a number of computerized CBT products that are useful to patients, but removed endorsement of any specific product.\n\nA relatively new avenue of research is the combination of artificial intelligence and CCBT. It has been proposed to use modern technology to create CCBT that simulates face-to-face therapy. This might be achieved in cognitive behavior therapy for a specific disorder using the comprehensive domain knowledge of CBT. One area where this has been attempted is the specific domain area of social anxiety in those who stutter.\n\nAnother new method of access is the use of mobile app or smartphone applications to deliver self-help or guided CBT. Technology companies are developing mobile-based artificial intelligence chatbot applications in delivering CBT as an early intervention to support mental health, to build Psychological resilience and to promote emotional well-being. Artificial intelligence (AI) text-based conversational application delivered securely and privately over smartphone devices have the ability to scale globally and offer contextual and always-available support. Active research is underway including real world data studies that measure effectiveness and engagement of text-based smartphone chatbot apps for delivery of CBT using a text-based conversational interface.\n\nEnabling patients to read self-help CBT guides has been shown to be effective by some studies. However one study found a negative effect in patients who tended to ruminate, and another meta-analysis found that the benefit was only significant when the self-help was guided (e.g. by a medical professional).\n\nPatient participation in group courses has been shown to be effective. In a meta-analysis reviewing evidence-based treatment of OCD in children, individual CBT was found to be more efficacious than group CBT.\n\nBrief cognitive behavioral therapy (BCBT) is a form of CBT which has been developed for situations in which there are time constraints on the therapy sessions. BCBT takes place over a couple of sessions that can last up to 12 accumulated hours by design. This technique was first implemented and developed on soldiers overseas in active duty by David M. Rudd to prevent suicide.\n\nBreakdown of treatment\n\nCognitive emotional behavioral therapy (CEBT) is a form of CBT developed initially for individuals with eating disorders but now used with a range of problems including anxiety, depression, obsessive compulsive disorder (OCD), post-traumatic stress disorder (PTSD) and anger problems. It combines aspects of CBT and dialectical behavioral therapy and aims to improve understanding and tolerance of emotions in order to facilitate the therapeutic process. It is frequently used as a \"pretreatment\" to prepare and better equip individuals for longer-term therapy.\n\nStructured cognitive behavioral training (SCBT) is a cognitive-based process with core philosophies that draw heavily from CBT. Like CBT, SCBT asserts that behavior is inextricably related to beliefs, thoughts and emotions. SCBT also builds on core CBT philosophy by incorporating other well-known modalities in the fields of behavioral health and psychology: most notably, Albert Ellis's rational emotive behavior therapy. SCBT differs from CBT in two distinct ways. First, SCBT is delivered in a highly regimented format. Second, SCBT is a predetermined and finite training process that becomes personalized by the input of the participant. SCBT is designed with the intention to bring a participant to a specific result in a specific period of time. SCBT has been used to challenge addictive behavior, particularly with substances such as tobacco, alcohol and food, and to manage diabetes and subdue stress and anxiety. SCBT has also been used in the field of criminal psychology in the effort to reduce recidivism.\n\nMoral reconation therapy, a type of CBT used to help felons overcome antisocial personality disorder (ASPD), slightly decreases the risk of further offending. It is generally implemented in a group format because of the risk of offenders with ASPD being given one-on-one therapy reinforces narcissistic behavioral characteristics, and can be used in correctional or outpatient settings. Groups usually meet weekly for two to six months.\n\nThis type of therapy uses a blend of cognitive, behavioral and some humanistic training techniques to target the stressors of the client. This usually is used to help clients better cope with their stress or anxiety after stressful events. This is a three-phase process that trains the client to use skills that they already have to better adapt to their current stressors. The first phase is an interview phase that includes psychological testing, client self-monitoring, and a variety of reading materials. This allows the therapist to individually tailor the training process to the client. Clients learn how to categorize problems into emotion-focused or problem-focused, so that they can better treat their negative situations. This phase ultimately prepares the client to eventually confront and reflect upon their current reactions to stressors, before looking at ways to change their reactions and emotions in relation to their stressors. The focus is conceptualization.\n\nThe second phase emphasizes the aspect of skills acquisition and rehearsal that continues from the earlier phase of conceptualization. The client is taught skills that help them cope with their stressors. These skills are then practised in the space of therapy. These skills involve self-regulation, problem-solving, interpersonal communication skills, etc.\n\nThe third and final phase is the application and following through of the skills learned in the training process. This gives the client opportunities to apply their learned skills to a wide range of stressors. Activities include role-playing, imagery, modeling, etc. In the end, the client will have been trained on a preventative basis to inoculate personal, chronic, and future stressors by breaking down their stressors into problems they will address in long-term, short-term, and intermediate coping goals.\n\nMindfulness-based cognitive behavioral hypnotherapy (MCBH) is a form of CBT focusing on awareness in reflective approach with addressing of subconscious tendencies. It is more the process that contains basically three phases that are used for achieving wanted goals.\n\nThe Unified Protocol for Transdiagnostic Treatment of Emotional Disorders (UP) is a form of CBT, developed by David H. Barlow and researchers at Boston University, that can be applied to a range of depression and anxiety disorders. The rationale is that anxiety and depression disorders often occur together due to common underlying causes and can efficiently be treated together.\n\nThe UP includes a common set of components:\n\n\nThe UP has been shown to produce equivalent results to single-diagnosis protocols for specific disorders, such as OCD and social anxiety disorder.\nThe UP is disseminated by the Unified Protocol Institute.\n\nThe research conducted for CBT has been a topic of sustained controversy. While some researchers write that CBT is more effective than other treatments, many other researchers and practitioners have questioned the validity of such claims. For example, one study determined CBT to be superior to other treatments in treating anxiety and depression. However, researchers responding directly to that study conducted a re-analysis and found no evidence of CBT being superior to other bona fide treatments, and conducted an analysis of thirteen other CBT clinical trials and determined that they failed to provide evidence of CBT superiority.\n\nA major criticism has been that clinical studies of CBT efficacy (or any psychotherapy) are not double-blind (i.e., either the subjects or the therapists in psychotherapy studies are not blind to the type of treatment). They may be single-blinded, i.e. the rater may not know the treatment the patient received, but neither the patients nor the therapists are blinded to the type of therapy given (two out of three of the persons involved in the trial, i.e., all of the persons involved in the treatment, are unblinded). The patient is an active participant in correcting negative distorted thoughts, thus quite aware of the treatment group they are in.\n\nThe importance of double-blinding was shown in a meta-analysis that examined the effectiveness of CBT when placebo control and blindedness were factored in. Pooled data from published trials of CBT in schizophrenia, major depressive disorder (MDD), and bipolar disorder that used controls for non-specific effects of intervention were analyzed. This study concluded that CBT is no better than non-specific control interventions in the treatment of schizophrenia and does not reduce relapse rates; treatment effects are small in treatment studies of MDD, and it is not an effective treatment strategy for prevention of relapse in bipolar disorder. For MDD, the authors note that the pooled effect size was very low. Nevertheless, the methodological processes used to select the studies in the previously mentioned meta-analysis and the worth of its findings have been called into question.\n\nAdditionally, a 2015 meta-analysis revealed that the positive effects of CBT on depression have been declining since 1977. The overall results showed two different declines in effect sizes: 1) an overall decline between 1977 and 2014, and 2) a steeper decline between 1995 and 2014. Additional sub-analysis revealed that CBT studies where therapists in the test group were instructed to adhere to the Beck CBT manual had a steeper decline in effect sizes since 1977 than studies where therapists in the test group were instructed to use CBT without a manual. The authors reported that they were unsure why the effects were declining but did list inadequate therapist training, failure to adhere to a manual, lack of therapist experience, and patients' hope and faith in its efficacy waning as potential reasons. The authors did mention that the current study was limited to depressive disorders only.\n\nFurthermore, other researchers write that CBT studies have high drop-out rates compared to other treatments. At times, the CBT drop-out rates can be more than five times higher than other treatments groups. For example, the researchers provided statistics of 28 participants in a group receiving CBT therapy dropping out, compared to 5 participants in a group receiving problem-solving therapy dropping out, or 11 participants in a group receiving psychodynamic therapy dropping out. This high drop-out rate is also evident in the treatment of several disorders, particularly the eating disorder anorexia nervosa, which is commonly treated with CBT. Those treated with CBT have a high chance of dropping out of therapy before completion and reverting to their anorexia behaviors.\n\nOther researchers conducting an analysis of treatments for youths who self-injure found similar drop-out rates in CBT and DBT groups. In this study, the researchers analyzed several clinical trials that measured the efficacy of CBT administered to youths who self-injure. The researchers concluded that none of them were found to be efficacious. These conclusions were made using the APA Division 12 Task Force on the Promotion and Dissemination of Psychological Procedures to determine intervention potency.\n\nThe methods employed in CBT research have not been the only criticisms; some individuals have called its theory and therapy into question. For example, Fancher argues that CBT has failed to provide a framework for clear and correct thinking. He states that it is strange for CBT theorists to develop a framework for determining distorted thinking without ever developing a framework for \"cognitive clarity\" or what would count as \"healthy, normal thinking\". Additionally, he writes that irrational thinking cannot be a source of mental and emotional distress when there is no evidence of rational thinking causing psychological well-being. Or, that social psychology has proven the normal cognitive processes of the average person to be irrational, even those who are psychologically well. Fancher also says that the theory of CBT is inconsistent with basic principles and research of rationality, and even ignores many rules of logic. He argues that CBT makes something of thinking that is far less exciting and true than thinking probably is. Among his other arguments are the maintaining of the status quo promoted in CBT, the self-deception encouraged within clients and patients engaged in CBT, how poorly the research is conducted, and some of its basic tenets and norms: \"The basic norm of cognitive therapy is this: except for how the patient thinks, everything is ok\".\n\nMeanwhile, Slife and Williams write that one of the hidden assumptions in CBT is that of determinism, or the absence of free will. They argue that CBT invokes a type of cause-and-effect relationship with cognition. They state that CBT holds that external stimuli from the environment enter the mind, causing different thoughts that cause emotional states: nowhere in CBT theory is agency, or free will, accounted for. According to Slife and Williams, at its most basic foundational assumptions, CBT holds that human beings have no free will and are just determined by the cognitive processes invoked by external stimuli.\n\nAnother criticism of CBT theory, especially as applied to major depressive disorder (MDD), is that it confounds the symptoms of the disorder with its causes.\n\nCBT is generally seen as having very low if any side effects. Calls have been made for more appraisal of CBT side effects.\n\nThe UK's National Health Service announced in 2008 that more therapists would be trained to provide CBT at government expense as part of an initiative called Improving Access to Psychological Therapies (IAPT). the NICE said that CBT would become the mainstay of treatment for non-severe depression, with medication used only in cases where CBT had failed. Therapists complained that the data does not fully support the attention and funding CBT receives. Psychotherapist and professor Andrew Samuels stated that this constitutes \"a coup, a power play by a community that has suddenly found itself on the brink of corralling an enormous amount of money ... Everyone has been seduced by CBT's apparent cheapness.\" The UK Council for Psychotherapy issued a press release in 2012 saying that the IAPT's policies were undermining traditional psychotherapy and criticized proposals that would limit some approved therapies to CBT, claiming that they restricted patients to \"a watered down version of cognitive behavioural therapy (CBT), often delivered by very lightly trained staff\".\n\nThe NICE also recommends offering CBT to people suffering from schizophrenia, as well as those at risk of suffering from a psychotic episode.\n\n\n"}
{"id": "5751", "url": "https://en.wikipedia.org/wiki?curid=5751", "title": "Chinese language", "text": "Chinese language\n\nChinese (; or ) is a group of related, but in many cases not mutually intelligible, language varieties, forming the Sinitic branch of the Sino-Tibetan language family. Chinese is spoken by the Han majority and many minority ethnic groups in China. About 1.2 billion people (around 16% of the world's population) speak some form of Chinese as their first language.\n\nThe varieties of Chinese are usually described by native speakers as dialects of a single Chinese language, but linguists note that they are as diverse as a language family. The internal diversity of Chinese has been likened to that of the Romance languages, but may be even more varied. There are between 7 and 13 main regional groups of Chinese (depending on classification scheme), of which the most spoken by far is Mandarin (about 960 million, e.g. Southwestern Mandarin), followed by Wu (80 million, e.g. Shanghainese), Min (70 million, e.g. Southern Min), Yue (60 million, e.g. Cantonese), etc. Most of these groups are mutually unintelligible, and even dialect groups within Min Chinese may not be mutually intelligible. Some, however, like Xiang and certain Southwest Mandarin dialects, may share common terms and a certain degree of intelligibility. All varieties of Chinese are tonal and analytic.\n\nStandard Chinese \"(Pǔtōnghuà/Guóyǔ/Huáyǔ)\" is a standardized form of spoken Chinese based on the Beijing dialect of Mandarin. It is the official language of China and Taiwan, as well as one of the four official languages of Singapore. It is one of the six official languages of the United Nations. The written form of the standard language (; \"Zhōngwén\"), based on the logograms known as Chinese characters (/; \"Hànzì\"), is shared by literate speakers of otherwise unintelligible dialects.\n\nThe earliest Chinese written records are Shang dynasty-era oracle inscriptions, which can be traced back to 1250 BCE. The phonetic categories of Archaic Chinese can be reconstructed from the rhymes of ancient poetry. During the Northern and Southern dynasties period, Middle Chinese went through several sound changes and split into several varieties following prolonged geographic and political separation. \"Qieyun\", a rime dictionary, recorded a compromise between the pronunciations of different regions. The royal courts of the Ming and early Qing dynasties operated using a koiné language (Guanhua) based on Nanjing dialect of Lower Yangtze Mandarin. Standard Chinese was adopted in the 1930s, and is now the official language of both the People's Republic of China and Taiwan.\n\nMost linguists classify all varieties of Chinese as part of the Sino-Tibetan language family, together with Burmese, Tibetan and many other languages spoken in the Himalayas and the Southeast Asian Massif. Although the relationship was first proposed in the early 19th century and is now broadly accepted, reconstruction of Sino-Tibetan is much less developed than that of families such as Indo-European or Austroasiatic. Difficulties have included the great diversity of the languages, the lack of inflection in many of them, and the effects of language contact. In addition, many of the smaller languages are spoken in mountainous areas that are difficult to reach, and are often also sensitive border zones. Without a secure reconstruction of proto-Sino-Tibetan, the higher-level structure of the family remains unclear. A top-level branching into Chinese and Tibeto-Burman languages is often assumed, but has not been convincingly demonstrated.\n\nThe first written records appeared over 3,000 years ago during the Shang dynasty. As the language evolved over this period, the various local varieties became mutually unintelligible. In reaction, central governments have repeatedly sought to promulgate a unified standard.\n\nThe earliest examples of Chinese are divinatory inscriptions on oracle bones from around 1250 BCE in the late Shang dynasty. Old Chinese was the language of the Western Zhou period (1046–771 BCE), recorded in inscriptions on bronze artifacts, the \"Classic of Poetry\" and portions of the \"Book of Documents\" and \"I Ching\". Scholars have attempted to reconstruct the phonology of Old Chinese by comparing later varieties of Chinese with the rhyming practice of the \"Classic of Poetry\" and the phonetic elements found in the majority of Chinese characters. Although many of the finer details remain unclear, most scholars agree that Old Chinese differs from Middle Chinese in lacking retroflex and palatal obstruents but having initial consonant clusters of some sort, and in having voiceless nasals and liquids. Most recent reconstructions also describe an atonal language with consonant clusters at the end of the syllable, developing into tone distinctions in Middle Chinese. Several derivational affixes have also been identified, but the language lacks inflection, and indicated grammatical relationships using word order and grammatical particles.\n\nMiddle Chinese was the language used during Northern and Southern dynasties and the Sui, Tang, and Song dynasties (6th through 10th centuries CE). It can be divided into an early period, reflected by the \"Qieyun\" rime book (601 CE), and a late period in the 10th century, reflected by rhyme tables such as the \"Yunjing\" constructed by ancient Chinese philologists as a guide to the \"Qieyun\" system. These works define phonological categories, but with little hint of what sounds they represent. Linguists have identified these sounds by comparing the categories with pronunciations in modern varieties of Chinese, borrowed Chinese words in Japanese, Vietnamese, and Korean, and transcription evidence. The resulting system is very complex, with a large number of consonants and vowels, but they are probably not all distinguished in any single dialect. Most linguists now believe it represents a diasystem encompassing 6th-century northern and southern standards for reading the classics.\n\nThe relationship between spoken and written Chinese is rather complex. Its spoken varieties have evolved at different rates, while written Chinese itself has changed much less. Classical Chinese literature began in the Spring and Autumn period.\n\nAfter the fall of the Northern Song dynasty, and during the reign of the Jin (Jurchen) and Yuan (Mongol) dynasties in northern China, a common speech (now called Old Mandarin) developed based on the dialects of the North China Plain around the capital.\nThe \"Zhongyuan Yinyun\" (1324) was a dictionary that codified the rhyming conventions of new \"sanqu\" verse form in this language.\nTogether with the slightly later \"Menggu Ziyun\", this dictionary describes a language with many of the features characteristic of modern Mandarin dialects.\n\nUp to the early 20th century, most of the people in China spoke only their local variety.\nAs a practical measure, officials of the Ming and Qing dynasties carried out the administration of the empire using a common language based on Mandarin varieties, known as \"Guānhuà\" (/, literally \"language of officials\").\nFor most of this period, this language was a koiné based on dialects spoken in the Nanjing area, though not identical to any single dialect.\nBy the middle of the 19th century, the Beijing dialect had become dominant and was essential for any business with the imperial court.\n\nIn the 1930s a standard national language \"Guóyǔ\" (/ \"national language\") was adopted.\nAfter much dispute between proponents of northern and southern dialects and an abortive attempt at an artificial pronunciation, the National Language Unification Commission finally settled on the Beijing dialect in 1932. The People's Republic founded in 1949 retained this standard, calling it \"pǔtōnghuà\" (/ \"common speech\"). The national language is now used in education, the media, and formal situations in both Mainland China and Taiwan. In Hong Kong and Macau, because of their colonial and linguistic history, the language used in education, the media, formal speech, and everyday life remains the local Cantonese, although the standard language has become very influential and is being taught in schools.\n\nThe Chinese language has spread to neighbouring countries through a variety of means. Northern Vietnam was incorporated into the Han empire in 111 BCE, marking the beginning of a period of Chinese control that ran almost continuously for a millennium. The Four Commanderies were established in northern Korea in the first century BCE, but disintegrated in the following centuries. Chinese Buddhism spread over East Asia between the 2nd and 5th centuries CE, and with it the study of scriptures and literature in Literary Chinese. Later Korea, Japan, and Vietnam developed strong central governments modeled on Chinese institutions, with Literary Chinese as the language of administration and scholarship, a position it would retain until the late 19th century in Korea and (to a lesser extent) Japan, and the early 20th century in Vietnam. Scholars from different lands could communicate, albeit only in writing, using Literary Chinese.\n\nAlthough they used Chinese solely for written communication, each country had its own tradition of reading texts aloud, the so-called Sino-Xenic pronunciations. Chinese words with these pronunciations were also extensively imported into the Korean, Japanese and Vietnamese languages, and today comprise over half of their vocabularies. This massive influx led to changes in the phonological structure of the languages, contributing to the development of moraic structure in Japanese and the disruption of vowel harmony in Korean.\n\nBorrowed Chinese morphemes have been used extensively in all these languages to coin compound words for new concepts, in a similar way to the use of Latin and Ancient Greek roots in European languages. Many new compounds, or new meanings for old phrases, were created in the late 19th and early 20th centuries to name Western concepts and artifacts. These coinages, written in shared Chinese characters, have then been borrowed freely between languages. They have even been accepted into Chinese, a language usually resistant to loanwords, because their foreign origin was hidden by their written form. Often different compounds for the same concept were in circulation for some time before a winner emerged, and sometimes the final choice differed between countries. The proportion of vocabulary of Chinese origin thus tends to be greater in technical, abstract, or formal language. For example, in Japan, Sino-Japanese words account for about 35% of the words in entertainment magazines, over half the words in newspapers, and 60% of the words in science magazines.\n\nVietnam, Korea, and Japan each developed writing systems for their own languages, initially based on Chinese characters, but later replaced with the \"Hangul\" alphabet for Korean and supplemented with \"kana\" syllabaries for Japanese, while Vietnamese continued to be written with the complex \"Chữ nôm\" script. However, these were limited to popular literature until the late 19th century. Today Japanese is written with a composite script using both Chinese characters (\"Kanji\") and kana. Korean is written exclusively with Hangul in North Korea, and supplementary Chinese characters (\"Hanja\") are increasingly rarely used in South Korea. Vietnamese is written with a Latin-based alphabet.\n\nExamples of loan words in English include \"tea\", from Hokkien (Min Nan) (), \"dim sum\", from Cantonese \"dim sam\" and \"kumquat\", from Cantonese \"gamgwat\" ().\n\nJerry Norman estimated that there are hundreds of mutually unintelligible varieties of Chinese. These varieties form a dialect continuum, in which differences in speech generally become more pronounced as distances increase, though the rate of change varies immensely. Generally, mountainous South China exhibits more linguistic diversity than the North China Plain. In parts of South China, a major city's dialect may only be marginally intelligible to close neighbors. For instance, Wuzhou is about upstream from Guangzhou, but the Yue variety spoken there is more like that of Guangzhou than is that of Taishan, southwest of Guangzhou and separated from it by several rivers. In parts of Fujian the speech of neighboring counties or even villages may be mutually unintelligible.\n\nUntil the late 20th century, Chinese emigrants to Southeast Asia and North America came from southeast coastal areas, where Min, Hakka, and Yue dialects are spoken.\nThe vast majority of Chinese immigrants to North America spoke the Taishan dialect, from a small coastal area southwest of Guangzhou.\n\nLocal varieties of Chinese are conventionally classified into seven dialect groups, largely on the basis of the different evolution of Middle Chinese voiced initials:\n\nThe classification of Li Rong, which is used in the \"Language Atlas of China\" (1987), distinguishes three further groups:\n\nSome varieties remain unclassified, including Danzhou dialect (spoken in Danzhou, on Hainan Island), Waxianghua (spoken in western Hunan) and Shaozhou Tuhua (spoken in northern Guangdong).\n\nStandard Chinese, often called Mandarin, is the official standard language of China and Taiwan, and one of the four official languages of Singapore (where it is called \"Huáyŭ\" or simply Chinese). Standard Chinese is based on the Beijing dialect, the dialect of Mandarin as spoken in Beijing. The governments of both China and Taiwan intend for speakers of all Chinese speech varieties to use it as a common language of communication. Therefore, it is used in government agencies, in the media, and as a language of instruction in schools.\n\nIn mainland China and Taiwan, diglossia has been a common feature. For example, in addition to Standard Chinese, a resident of Shanghai might speak Shanghainese; and, if he or she grew up elsewhere, then he or she is also likely to be fluent in the particular dialect of that local area. A native of Guangzhou may speak both Cantonese and Standard Chinese. In addition to Mandarin, most Taiwanese also speak Minnan, Hakka, or an Austronesian language. A Taiwanese may commonly mix pronunciations, phrases, and words from Mandarin and other Taiwanese languages, and this mixture is considered normal in daily or informal speech.\n\nThe official Chinese designation for the major branches of Chinese is \"fāngyán\" (, literally \"regional speech\"), whereas the more closely related varieties within these are called \"dìdiǎn fāngyán\" (/ \"local speech\"). Conventional English-language usage in Chinese linguistics is to use \"dialect\" for the speech of a particular place (regardless of status) and \"dialect group\" for a regional grouping such as Mandarin or Wu. Because varieties from different groups are not mutually intelligible, some scholars prefer to describe Wu and others as separate languages. Jerry Norman called this practice misleading, pointing out that Wu, which itself contains many mutually unintelligible varieties, could not be properly called a single language under the same criterion, and that the same is true for each of the other groups.\n\nMutual intelligibility is considered by some linguists to be the main criterion for determining whether varieties are separate languages or dialects of a single language, although others do not regard it as decisive, particularly when cultural factors interfere as they do with Chinese. As explains, linguists often ignore mutual intelligibility when varieties share intelligibility with a central variety (i.e. prestige variety, such as Standard Mandarin), as the issue requires some careful handling when mutual intelligibility is inconsistent with language identity. John DeFrancis argues that it is inappropriate to refer to Mandarin, Wu and so on as \"dialects\" because the mutual unintelligibility between them is too great. On the other hand, he also objects to considering them as separate languages, as it incorrectly implies a set of disruptive \"religious, economic, political, and other differences\" between speakers that exist, for example, between French Catholics and English Protestants in Canada, but not between speakers of Cantonese and Mandarin in China, owing to China's near-uninterrupted history of centralized government.\n\nBecause of the difficulties involved in determining the difference between language and dialect, other terms have been proposed: ISO 639-3 follows \"Ethnologue\" in assigning individual language codes to the 13 main subdivisions, while Chinese as a whole is classified as a 'macrolanguage'. Other options include \"vernacular\", \"lect\" \"regionalect\", \"topolect\", and \"variety\".\n\nMost Chinese people consider the spoken varieties as one single language because speakers share a common culture and history, as well as a shared national identity and a common written form. To Chinese nationalists, the idea of Chinese as a language family may suggest that the Chinese identity is much more fragmented and disunified than it actually is and as such is often looked upon as culturally and politically provocative. Additionally, some of whose supporters promote the local Taiwanese Hokkien variety.\n\nThe phonological structure of each syllable consists of a nucleus that has a vowel (which can be a monophthong, diphthong, or even a triphthong in certain varieties), preceded by an onset (a single consonant, or consonant+glide; zero onset is also possible), and followed (optionally) by a coda consonant; a syllable also carries a tone. There are some instances where a vowel is not used as a nucleus. An example of this is in Cantonese, where the nasal sonorant consonants and can stand alone as their own syllable.\n\nIn Mandarin much more than in other spoken varieties, most syllables tend to be open syllables, meaning they have no coda (assuming that a final glide is not analyzed as a coda), but syllables that do have codas are restricted to nasals , , , the retroflex approximant , and voiceless stops , , , or . Some varieties allow most of these codas, whereas others, such as Standard Chinese, are limited to only , and .\n\nThe number of sounds in the different spoken dialects varies, but in general there has been a tendency to a reduction in sounds from Middle Chinese. The Mandarin dialects in particular have experienced a dramatic decrease in sounds and so have far more multisyllabic words than most other spoken varieties. The total number of syllables in some varieties is therefore only about a thousand, including tonal variation, which is only about an eighth as many as English.\n\nAll varieties of spoken Chinese use tones to distinguish words. A few dialects of north China may have as few as three tones, while some dialects in south China have up to 6 or 12 tones, depending on how one counts. One exception from this is Shanghainese which has reduced the set of tones to a two-toned pitch accent system much like modern Japanese.\n\nA very common example used to illustrate the use of tones in Chinese is the application of the four tones of Standard Chinese (along with the neutral tone) to the syllable \"ma\". The tones are exemplified by the following five Chinese words:\n\nStandard Cantonese, in contrast, has six tones in open syllables and three tones in syllables ending with stops:\nChinese is often described as a \"monosyllabic\" language. However, this is only partially correct. It is largely accurate when describing Classical Chinese and Middle Chinese; in Classical Chinese, for example, perhaps 90% of words correspond to a single syllable and a single character. In the modern varieties, it is usually the case that a morpheme (unit of meaning) is a single syllable; In contrast, English has plenty of multi-syllable morphemes, both bound and free, such as \"seven\", \"elephant\", \"para-\" and \"-able\".\n\nSome of the conservative southern varieties of modern Chinese have largely monosyllabic words, especially among the more basic vocabulary. In modern Mandarin, however, most nouns, adjectives and verbs are largely disyllabic. A significant cause of this is phonological attrition. Sound change over time has steadily reduced the number of possible syllables. In modern Mandarin, there are now only about 1,200 possible syllables, including tonal distinctions, compared with about 5,000 in Vietnamese (still largely monosyllabic) and over 8,000 in English.\n\nThis phonological collapse has led to a corresponding increase in the number of homophones. As an example, the small Langenscheidt Pocket Chinese Dictionary lists six words that are commonly pronounced as \"shí\" (tone 2): \"ten\"; / \"real, actual\"; / \"know (a person), recognize\"; \"stone\"; / \"time\"; \"food, eat\". These were all pronounced differently in Early Middle Chinese; in William H. Baxter's transcription they were \"dzyip\", \"zyit\", \"syik\", \"dzyek\", \"dzyi\" and \"zyik\" respectively. They are still pronounced differently in today's Cantonese; in Jyutping they are \"sap9\", \"sat9\", \"sik7\", \"sek9\", \"si4\", \"sik9\". In modern spoken Mandarin, however, tremendous ambiguity would result if all of these words could be used as-is; Yuen Ren Chao's modern poem Lion-Eating Poet in the Stone Den exploits this, consisting of 92 characters all pronounced \"shi\". As such, most of these words have been replaced (in speech, if not in writing) with a longer, less-ambiguous compound. Only the first one, \"ten\", normally appears as such when spoken; the rest are normally replaced with, respectively, \"shíjì\" / (lit. \"actual-connection\"); \"rènshi\" / (lit. \"recognize-know\"); \"shítou\" / (lit. \"stone-head\"); \"shíjiān\" / (lit. \"time-interval\"); \"shíwù\" (lit. \"food-thing\"). In each case, the homophone was disambiguated by adding another morpheme, typically either a synonym or a generic word of some sort (for example, \"head\", \"thing\"), the purpose of which is simply to indicate which of the possible meanings of the other, homophonic syllable should be selected.\n\nHowever, when one of the above words forms part of a compound, the disambiguating syllable is generally dropped and the resulting word is still disyllabic. For example, \"shí\" alone, not \"shítou\" /, appears in compounds meaning \"stone-\", for example, \"shígāo\" \"plaster\" (lit. \"stone cream\"), \"shíhuī\" \"lime\" (lit. \"stone dust\"), \"shíkū\" \"grotto\" (lit. \"stone cave\"), \"shíyīng\" \"quartz\" (lit. \"stone flower\"), \"shíyóu\" \"petroleum\" (lit. \"stone oil\").\n\nMost modern varieties of Chinese have the tendency to form new words through disyllabic, trisyllabic and tetra-character compounds. In some cases, monosyllabic words have become disyllabic without compounding, as in \"kūlong\" from \"kǒng\" 孔; this is especially common in Jin.\n\nChinese morphology is strictly bound to a set number of syllables with a fairly rigid construction. Although many of these single-syllable morphemes (\"zì\", ) can stand alone as individual words, they more often than not form multi-syllabic compounds, known as \"cí\" (/), which more closely resembles the traditional Western notion of a word. A Chinese \"cí\" (\"word\") can consist of more than one character-morpheme, usually two, but there can be three or more.\n\nFor example:\n\nAll varieties of modern Chinese are analytic languages, in that they depend on syntax (word order and sentence structure) rather than morphology—i.e., changes in form of a word—to indicate the word's function in a sentence. In other words, Chinese has very few grammatical inflections—it possesses no tenses, no voices, no numbers (singular, plural; though there are plural markers, for example for personal pronouns), and only a few articles (i.e., equivalents to \"the, a, an\" in English).\n\nThey make heavy use of grammatical particles to indicate aspect and mood. In Mandarin Chinese, this involves the use of particles like \"le\" (perfective), \"hái\" / (\"still\"), \"yǐjīng\" / (\"already\"), and so on.\n\nChinese has a subject–verb–object word order, and like many other languages of East Asia, makes frequent use of the topic–comment construction to form sentences. Chinese also has an extensive system of classifiers and measure words, another trait shared with neighboring languages like Japanese and Korean. Other notable grammatical features common to all the spoken varieties of Chinese include the use of serial verb construction, pronoun dropping and the related subject dropping.\n\nAlthough the grammars of the spoken varieties share many traits, they do possess differences.\n\nThe entire Chinese character corpus since antiquity comprises well over 20,000 characters, of which only roughly 10,000 are now commonly in use. However Chinese characters should not be confused with Chinese words. Because most Chinese words are made up of two or more characters, there are many more Chinese words than characters. A more accurate equivalent for a Chinese character is the morpheme, as characters represent the smallest grammatical units with individual meanings in the Chinese language.\n\nEstimates of the total number of Chinese words and lexicalized phrases vary greatly. The \"Hanyu Da Zidian\", a compendium of Chinese characters, includes 54,678 head entries for characters, including bone oracle versions. The \"Zhonghua Zihai\" (1994) contains 85,568 head entries for character definitions, and is the largest reference work based purely on character and its literary variants. The CC-CEDICT project (2010) contains 97,404 contemporary entries including idioms, technology terms and names of political figures, businesses and products. The 2009 version of the Webster's Digital Chinese Dictionary (WDCD), based on CC-CEDICT, contains over 84,000 entries.\n\nThe most comprehensive pure linguistic Chinese-language dictionary, the 12-volume \"Hanyu Da Cidian\", records more than 23,000 head Chinese characters and gives over 370,000 definitions. The 1999 revised \"Cihai\", a multi-volume encyclopedic dictionary reference work, gives 122,836 vocabulary entry definitions under 19,485 Chinese characters, including proper names, phrases and common zoological, geographical, sociological, scientific and technical terms.\n\nThe 7th (2016) edition of \"Xiandai Hanyu Cidian\", an authoritative one-volume dictionary on modern standard Chinese language as used in mainland China, has 13,000 head characters and defines 70,000 words.\n\nLike any other language, Chinese has absorbed a sizable number of loanwords from other cultures. Most Chinese words are formed out of native Chinese morphemes, including words describing imported objects and ideas. However, direct phonetic borrowing of foreign words has gone on since ancient times.\n\nSome early Indo-European loanwords in Chinese have been proposed, notably \"mì\" \"honey\", / \"shī\" \"lion,\" and perhaps also / \"mǎ\" \"horse\", / \"zhū\" \"pig\", \"quǎn\" \"dog\", and / \"é\" \"goose\".\nAncient words borrowed from along the Silk Road since Old Chinese include \"pútáo\" \"grape\", \"shíliu\"/\"shíliú\" \"pomegranate\" and / \"shīzi\" \"lion\". Some words were borrowed from Buddhist scriptures, including \"Fó\" \"Buddha\" and / \"Púsà\" \"bodhisattva.\" Other words came from nomadic peoples to the north, such as \"hútòng\" \"hutong\". Words borrowed from the peoples along the Silk Road, such as \"grape,\" generally have Persian etymologies. Buddhist terminology is generally derived from Sanskrit or Pāli, the liturgical languages of North India. Words borrowed from the nomadic tribes of the Gobi, Mongolian or northeast regions generally have Altaic etymologies, such as \"pípá\", the Chinese lute, or \"lào\"/\"luò\" \"cheese\" or \"yoghurt\", but from exactly which source is not always clear.\n\nModern neologisms are primarily translated into Chinese in one of three ways: free translation (\"calque\", or by meaning), phonetic translation (by sound), or a combination of the two. Today, it is much more common to use existing Chinese morphemes to coin new words in order to represent imported concepts, such as technical expressions and international scientific vocabulary. Any Latin or Greek etymologies are dropped and converted into the corresponding Chinese characters (for example, \"anti-\" typically becomes \"\", literally \"opposite\"), making them more comprehensible for Chinese but introducing more difficulties in understanding foreign texts. For example, the word \"telephone\" was loaned phonetically as / (Shanghainese: \"télífon\" , Mandarin: \"délǜfēng\") during the 1920s and widely used in Shanghai, but later / \"diànhuà\" (lit. \"electric speech\"), built out of native Chinese morphemes, became prevalent ( is in fact from the Japanese \"denwa\"; see below for more Japanese loans). Other examples include / \"diànshì\" (lit. \"electric vision\") for television, / \"diànnǎo\" (lit. \"electric brain\") for computer; / \"shǒujī\" (lit. \"hand machine\") for mobile phone, / \"lányá\" (lit. \"blue tooth\") for Bluetooth, and / \"wǎngzhì\" (lit. \"internet logbook\") for blog in Hong Kong and Macau Cantonese. Occasionally half-transliteration, half-translation compromises are accepted, such as / \"hànbǎobāo\" ( \"hànbǎo\" \"Hamburg\" + \"bāo\" \"bun\") for \"hamburger\". Sometimes translations are designed so that they sound like the original while incorporating Chinese morphemes (phono-semantic matching), such as / \"tuōlājī\" \"tractor\" (lit. \"dragging-pulling machine\"), or / Mǎlì'ào for the video game character Mario. This is often done for commercial purposes, for example / \"bēnténg\" (lit. \"dashing-leaping\") for Pentium and / \"Sàibǎiwèi\" (lit. \"better-than hundred tastes\") for Subway restaurants.\n\nForeign words, mainly proper nouns, continue to enter the Chinese language by transcription according to their pronunciations. This is done by employing Chinese characters with similar pronunciations. For example, \"Israel\" becomes \"Yǐsèliè\", \"Paris\" becomes \"Bālí\". A rather small number of direct transliterations have survived as common words, including / \"shāfā\" \"sofa\", / \"mǎdá\" \"motor\", \"yōumò\" \"humor\", / \"luóji\"/\"luójí\" \"logic\", / \"shímáo\" \"smart, fashionable\", and \"xiēsīdǐlǐ\" \"hysterics\". The bulk of these words were originally coined in the Shanghai dialect during the early 20th century and were later loaned into Mandarin, hence their pronunciations in Mandarin may be quite off from the English. For example, / \"sofa\" and / \"motor\" in Shanghainese sound more like their English counterparts. Cantonese differs from Mandarin with some transliterations, such as \"so faa\" \"sofa\" and \"mo daa\" \"motor\".\n\nWestern foreign words representing Western concepts have influenced Chinese since the 20th century through transcription. From French came \"bāléi\" \"ballet\" and \"xiāngbīn\", \"champagne\"; from Italian, \"kāfēi\" \"caffè\". English influence is particularly pronounced. From early 20th century Shanghainese, many English words are borrowed, such as / \"gāoěrfū\" \"golf\" and the above-mentioned / \"shāfā\" \"sofa\". Later, the United States soft influences gave rise to \"dísikē\"/\"dísīkē\" \"disco\", / \"kělè\" \"cola\", and \"mínǐ\" \"mini [skirt]\". Contemporary colloquial Cantonese has distinct loanwords from English, such as \"kaa tung\" \"cartoon\", \"gei lou\" \"gay people\", \"dik si\" \"taxi\", and \"baa si\" \"bus\". With the rising popularity of the Internet, there is a current vogue in China for coining English transliterations, for example, / \"fěnsī\" \"fans\", \"hēikè\" \"hacker\" (lit. \"black guest\"), and \"bókè\" \"blog\". In Taiwan, some of these transliterations are different, such as \"hàikè\" for \"hacker\" and \"bùluògé\" for \"blog\" (lit. \"interconnected tribes\").\n\nAnother result of the English influence on Chinese is the appearance in Modern Chinese texts of so-called / \"zìmǔcí\" (lit. \"lettered words\") spelled with letters from the English alphabet. This has appeared in magazines, newspapers, on web sites, and on TV: / \"3rd generation cell phones\" ( \"sān\" \"three\" + G \"generation\" + / \"shǒujī\" \"mobile phones\"), \"IT circles\" (IT \"information technology\" + \"jiè\" \"industry\"), HSK (\"Hànyǔ Shuǐpíng Kǎoshì\", /), GB (\"Guóbiāo\", /), / (CIF \"Cost, Insurance, Freight\" + / \"jià\" \"price\"), \"e-home\" (e \"electronic\" + \"jiātíng\" \"home\"), / \"wireless era\" (W \"wireless\" + / \"shídài\" \"era\"), \"TV watchers\" (TV \"television\" + \"zú\" \"social group; clan\"), / \"post-PC era\" (/ \"hòu\" \"after/post-\" + PC \"personal computer\" + /), and so on.\n\nSince the 20th century, another source of words has been Japanese using existing kanji (Chinese characters used in Japanese). Japanese re-molded European concepts and inventions into , and many of these words have been re-loaned into modern Chinese. Other terms were coined by the Japanese by giving new senses to existing Chinese terms or by referring to expressions used in classical Chinese literature. For example, \"jīngjì\" (/; \"keizai\" in Japanese), which in the original Chinese meant \"the workings of the state\", was narrowed to \"economy\" in Japanese; this narrowed definition was then re-imported into Chinese. As a result, these terms are virtually indistinguishable from native Chinese words: indeed, there is some dispute over some of these terms as to whether the Japanese or Chinese coined them first. As a result of this loaning, Chinese, Korean, Japanese, and Vietnamese share a corpus of linguistic terms describing modern terminology, paralleling the similar corpus of terms built from Greco-Latin and shared among European languages.\n\nThe Chinese orthography centers on Chinese characters, which are written within imaginary square blocks, traditionally arranged in vertical columns, read from top to bottom down a column, and right to left across columns. Chinese characters denote morphemes independent of phonetic change. Thus the character (\"one\") is uttered \"yī\" in Standard Chinese, \"yat\" in Cantonese and \"it\" in Hokkien (form of Min). Vocabularies from different major Chinese variants have diverged, and colloquial nonstandard written Chinese often makes use of unique \"dialectal characters\", such as and for Cantonese and Hakka, which are considered archaic or unused in standard written Chinese.\n\nWritten colloquial Cantonese has become quite popular in online chat rooms and instant messaging amongst Hong-Kongers and Cantonese-speakers elsewhere. It is considered highly informal, and does not extend to many formal occasions.\n\nThe Chinese had no uniform phonetic transcription system until the mid-20th century, although enunciation patterns were recorded in early rime books and dictionaries. Early Indian translators, working in Sanskrit and Pali, were the first to attempt to describe the sounds and enunciation patterns of Chinese in a foreign language. After the 15th century, the efforts of Jesuits and Western court missionaries resulted in some rudimentary Latin transcription systems, based on the Nanjing Mandarin dialect.\n\nIn Hunan, women in certain areas write their local language in Nü Shu, a syllabary derived from Chinese characters. The Dungan language, considered by many a dialect of Mandarin, is nowadays written in Cyrillic, and was previously written in the Arabic script. The Dungan people are primarily Muslim and live mainly in Kazakhstan, Kyrgyzstan, and Russia; some of the related Hui people also speak the language and live mainly in China.\n\nEach Chinese character represents a monosyllabic Chinese word or morpheme. In 100 CE, the famed Han dynasty scholar Xu Shen classified characters into six categories, namely pictographs, simple ideographs, compound ideographs, phonetic loans, phonetic compounds and derivative characters. Of these, only 4% were categorized as pictographs, including many of the simplest characters, such as \"rén\" (human), \"rì\" (sun), \"shān\" (mountain; hill), \"shuǐ\" (water). Between 80% and 90% were classified as phonetic compounds such as \"chōng\" (pour), combining a phonetic component \"zhōng\" (middle) with a semantic radical (water). Almost all characters created since have been made using this format. The 18th-century Kangxi Dictionary recognized 214 radicals.\n\nModern characters are styled after the regular script. Various other written styles are also used in Chinese calligraphy, including seal script, cursive script and clerical script. Calligraphy artists can write in traditional and simplified characters, but they tend to use traditional characters for traditional art.\n\nThere are currently two systems for Chinese characters. The traditional system, used in Hong Kong, Taiwan, Macau and Chinese speaking communities (except Singapore and Malaysia) outside mainland China, takes its form from standardized character forms dating back to the late Han dynasty. The Simplified Chinese character system, introduced by the People's Republic of China in 1954 to promote mass literacy, simplifies most complex traditional glyphs to fewer strokes, many to common cursive shorthand variants.\n\nSingapore, which has a large Chinese community, was the second nation to officially adopt simplified characters, although it has also become the \"de facto\" standard for younger ethnic Chinese in Malaysia. The Internet provides the platform to practice reading these alternative systems, be it traditional or simplified.\n\nA well-educated Chinese reader today recognizes approximately 4,000 to 6,000 characters; approximately 3,000 characters are required to read a Mainland newspaper. The PRC government defines literacy amongst workers as a knowledge of 2,000 characters, though this would be only functional literacy. School-children typically learn around 2,000 characters whereas scholars may memorize up to 10,000. A large unabridged dictionary, like the Kangxi Dictionary, contains over 40,000 characters, including obscure, variant, rare, and archaic characters; fewer than a quarter of these characters are now commonly used.\n\nRomanization is the process of transcribing a language into the Latin script. There are many systems of romanization for the Chinese varieties, due to the lack of a native phonetic transcription until modern times. Chinese is first known to have been written in Latin characters by Western Christian missionaries in the 16th century.\n\nToday the most common romanization standard for Standard Chinese is \"Hanyu Pinyin\", often known simply as pinyin, introduced in 1956 by the People's Republic of China, and later adopted by Singapore and Taiwan. Pinyin is almost universally employed now for teaching standard spoken Chinese in schools and universities across America, Australia and Europe. Chinese parents also use Pinyin to teach their children the sounds and tones of new words. In school books that teach Chinese, the Pinyin romanization is often shown below a picture of the thing the word represents, with the Chinese character alongside.\n\nThe second-most common romanization system, the Wade–Giles, was invented by Thomas Wade in 1859 and modified by Herbert Giles in 1892. As this system approximates the phonology of Mandarin Chinese into English consonants and vowels, i.e. it is an Anglicization, it may be particularly helpful for beginner Chinese speakers of an English-speaking background. Wade–Giles was found in academic use in the United States, particularly before the 1980s, and until recently was widely used in Taiwan.\n\nWhen used within European texts, the tone transcriptions in both pinyin and Wade–Giles are often left out for simplicity; Wade–Giles' extensive use of apostrophes is also usually omitted. Thus, most Western readers will be much more familiar with \"Beijing\" than they will be with \"Běijīng\" (pinyin), and with \"Taipei\" than \"T'ai²-pei³\" (Wade–Giles). This simplification presents syllables as homophones which really are none, and therefore exaggerates the number of homophones almost by a factor of four.\n\nHere are a few examples of \"Hanyu Pinyin\" and Wade–Giles, for comparison:\nOther systems of romanization for Chinese include Gwoyeu Romatzyh, the French EFEO, the Yale system (invented during WWII for U.S. troops), as well as separate systems for Cantonese, Min Nan, Hakka, and other Chinese varieties.\n\nChinese varieties have been phonetically transcribed into many other writing systems over the centuries. The 'Phags-pa script, for example, has been very helpful in reconstructing the pronunciations of premodern forms of Chinese.\n\nZhuyin (colloquially \"bopomofo\"), a semi-syllabary is still widely used in Taiwan's elementary schools to aid standard pronunciation. Although zhuyin characters are reminiscent of katakana script, there is no source to substantiate the claim that Katakana was the basis for the zhuyin system. A comparison table of zhuyin to pinyin exists in the zhuyin article. Syllables based on pinyin and zhuyin can also be compared by looking at the following articles:\n\nThere are also at least two systems of cyrillization for Chinese. The most widespread is the Palladius system.\n\nWith the growing importance and influence of China's economy globally, Mandarin instruction is gaining popularity in schools in the United States, and has become an increasingly popular subject of study amongst the young in the Western world, as in the UK.\n\nIn 1991 there were 2,000 foreign learners taking China's official Chinese Proficiency Test (also known as HSK, comparable to the English Cambridge Certificate), while in 2005, the number of candidates had risen sharply to 117,660. By 2010, 750,000 people had taken the Chinese Proficiency Test. By 2017, 6.5 million candidates had taken the Chinese Proficiency Test of various kinds.\n\nAccording to the Modern Language Association, there were 550 elementary, junior high and senior high schools providing Chinese programs in the United States in 2015, which represented a 100% increase in two years. At the same time, enrollment in Chinese language classes at college level had an increase of 51% from 2002 to 2015. On the other hand, the American Council on the Teaching of Foreign Languages also had figures suggesting that 30,000 – 50,000 students were studying Chinese in 2015.\n\nIn 2016, more than half a million Chinese students pursued post-secondary education overseas, whereas 400,000 international students came to China for higher education. Tsinghua University hosted 35,000 students from 116 countries in the same year.\n\nWith the increase in demand for Chinese as a second language, there are 330 institutions teaching Chinese language globally according to the Chinese Ministry of Education. The establishment of Confucius Institutes, which are the public institutions affiliated with the Ministry of Education of China, aims at promoting Chinese language and culture as well as supporting Chinese teaching overseas. There were more than 480 Confucius Institutes worldwide as of 2014.\n\n\n\n\n"}
{"id": "5759", "url": "https://en.wikipedia.org/wiki?curid=5759", "title": "Complex analysis", "text": "Complex analysis\n\nComplex analysis, traditionally known as the theory of functions of a complex variable, is the branch of mathematical analysis that investigates functions of complex numbers. It is useful in many branches of mathematics, including algebraic geometry, number theory, analytic combinatorics, applied mathematics; as well as in physics, including the branches of hydrodynamics, thermodynamics, and particularly quantum mechanics. By extension, use of complex analysis also has applications in engineering fields such as nuclear, aerospace, mechanical and electrical engineering.\n\nAs a differentiable function of a complex variable is equal to the sum of its Taylor series (that is, it is analytic), complex analysis is particularly concerned with analytic functions of a complex variable (that is, holomorphic functions).\n\nComplex analysis is one of the classical branches in mathematics, with roots in the 18th century and just prior. Important mathematicians associated with complex numbers include Euler, Gauss, Riemann, Cauchy, Weierstrass, and many more in the 20th century. Complex analysis, in particular the theory of conformal mappings, has many physical applications and is also used throughout analytic number theory. In modern times, it has become very popular through a new boost from complex dynamics and the pictures of fractals produced by iterating holomorphic functions. Another important application of complex analysis is in string theory which studies conformal invariants in quantum field theory.\n\nA complex function is a function from complex numbers to complex numbers. In other words, it is a function that has a subset of the complex numbers as a domain and the complex numbers as a codomain. Complex function are generally supposed to have a domain that contains a nonempty open subset of the complex plane.\n\nFor any complex function, the values formula_1 from the domain and their images formula_2 in the range may be separated into real and imaginary parts:\n\nwhere formula_4 are all real-valued. \n\nIn other words, a complex function formula_5 may be decomposed into \n\ni.e., into two real-valued functions (formula_8, formula_9) of two real variables (formula_10, formula_11).\n\nSimilarly, any complex-valued function on an arbitrary set can be considered as an ordered pair of two real-valued functions: or, alternatively, as a vector-valued function from into formula_12\n\nSome properties of complex-valued functions (such as continuity) are nothing more than the corresponding properties of vector valued functions of two real variables. Other concepts of complex analysis, such as differentiability are direct generalizations of the similar concepts for real functions, but may have very different properties. In particular, every differentiable complex function is analytic (see next section), and two differentiable functions that are equal in a neighborhood of a point are equal on the intersection of their domain (if the domains are connected). The latter property is the basis of the principle of analytic continuation which allows extending every real analytic function in a unique way for getting a complex analytic function whose domain is the whole complex plane with a finite number of curve arcs removed. Many basic and special complex functions are defined in this way, including exponential functions, logarithmic functions, and trigonometric functions.\n\nComplex functions that are differentiable at every point of an open subset formula_13 of the complex plane are said to be \"holomorphic\" \"on\" formula_13. In the context of complex analysis, the derivative of formula_15 at formula_16 is defined to be\n\nSuperficially, this definition is formally analogous to that of the derivative of a real function. However, complex derivatives and differentiable functions behave in significantly different ways compared to their real counterparts. In particular, for this limit to exist, the value of the difference quotient must approach the same complex number, regardless of the manner in which we approach formula_16 in the complex plane. Consequently, complex differentiability has much stronger implications than real differentiability. For instance, holomorphic functions are infinitely differentiable, whereas the existence of the \"n\"th derivative need not imply the existence of the (\"n\" + 1)th derivative for real functions. Furthermore, all holomorphic functions satisfy the stronger condition of analyticity, meaning that the function is, at every point in its domain, locally given by a convergent power series. In essence, this means that functions holomorphic on formula_13 can be approximated arbitrarily well by polynomials in some neighborhood of every point in formula_13. This stands in sharp contrast to differentiable real functions; even infinitely differentiable real functions can be \"nowhere\" analytic.\n\nMost elementary functions, including the exponential function, the trigonometric functions, and all polynomial functions, extended appropriately to complex arguments as functions formula_21, are holomorphic over the entire complex plane, making them \"entire\" \"functions\", while rational functions formula_22, where \"p\" and \"q\" are polynomials, are holomorphic on domains that exclude points where \"q\" is zero. Such functions that are holomorphic everywhere except a set of isolated points are known as \"meromorphic functions\". On the other hand, the functions formula_23, formula_24, and formula_25 are not holomorphic anywhere on the complex plane, as can be shown by their failure to satisfy the Cauchy–Riemann conditions (see below).\n\nAn important property of holomorphic functions is the relationship between the partial derivatives of their real and imaginary components, known as the Cauchy–Riemann conditions. If formula_5, defined by formula_27, where formula_28, is holomorphic on a region formula_13, then \"formula_30\" must hold for all formula_31. Here, the differential operator formula_32 is defined as formula_33\".\" In terms of the real and imaginary parts of the function, \"u\" and \"v\", this is equivalent to the pair of equations formula_34 and formula_35, where the subscripts indicate partial differentiation. However, the Cauchy–Riemann conditions do not characterize holomorphic functions, without additional continuity conditions (see Looman–Menchoff theorem).\n\nHolomorphic functions exhibit some remarkable features. For instance, Picard's theorem asserts that the range of an entire function can only take three possible forms: formula_36, formula_37, or formula_38 for some formula_39. In other words, if two distinct complex numbers formula_1 and formula_41 are not in the range of entire function formula_15, then formula_15 is a constant function. Moreover, given a holomorphic function formula_15 defined on an open set formula_45, the analytic continuation of formula_15 to a larger open set formula_47 is unique. As a result, the value of a holomorphic function over an arbitrarily small region in fact determines the value of the function everywhere to which it can be extended as a holomorphic function.\n\n\"See also\": analytic function, coherent sheaf and vector bundles.\n\nOne of the central tools in complex analysis is the line integral. The line integral around a closed path of a function that is holomorphic everywhere inside the area bounded by the closed path is always zero, which is what the Cauchy integral theorem states. The values of such a holomorphic function inside a disk can be computed by a path integral on the disk's boundary (as shown in Cauchy's integral formula). Path integrals in the complex plane are often used to determine complicated real integrals, and here the theory of residues among others is applicable (see methods of contour integration). A \"pole\" (or isolated singularity) of a function is a point where the function's value becomes unbounded, or \"blows up\". If a function has such a pole, then one can compute the function's residue there, which can be used to compute path integrals involving the function; this is the content of the powerful residue theorem. The remarkable behavior of holomorphic functions near essential singularities is described by Picard's Theorem. Functions that have only poles but no essential singularities are called meromorphic. Laurent series are the complex-valued equivalent to Taylor series, but can be used to study the behavior of functions near singularities through infinite sums of more well understood functions, such as polynomials.\n\nA bounded function that is holomorphic in the entire complex plane must be constant; this is Liouville's theorem. It can be used to provide a natural and short proof for the fundamental theorem of algebra which states that the field of complex numbers is algebraically closed.\n\nIf a function is holomorphic throughout a connected domain then its values are fully determined by its values on any smaller subdomain. The function on the larger domain is said to be analytically continued from its values on the smaller domain. This allows the extension of the definition of functions, such as the Riemann zeta function, which are initially defined in terms of infinite sums that converge only on limited domains to almost the entire complex plane. Sometimes, as in the case of the natural logarithm, it is impossible to analytically continue a holomorphic function to a non-simply connected domain in the complex plane but it is possible to extend it to a holomorphic function on a closely related surface known as a Riemann surface.\n\nAll this refers to complex analysis in one variable. There is also a very rich theory of complex analysis in more than one complex dimension in which the analytic properties such as power series expansion carry over whereas most of the geometric properties of holomorphic functions in one complex dimension (such as conformality) do not carry over. The Riemann mapping theorem about the conformal relationship of certain domains in the complex plane, which may be the most important result in the one-dimensional theory, fails dramatically in higher dimensions.\n\nA major user of certain complex spaces is in quantum mechanics as wave functions.\n\n\n\n"}
{"id": "5760", "url": "https://en.wikipedia.org/wiki?curid=5760", "title": "History of China", "text": "History of China\n\nThe earliest known written records of the history of China date from as early as 1250 BC, from the Shang dynasty (c. 1600–1046 BC), during the king Wu Ding's reign, who was recorded as the twenty-first Shang king by the written records of Shang dynasty unearthed. Ancient historical texts such as the \"Records of the Grand Historian\" (c. 100 BC) and the \"Bamboo Annals\" (296 BC) describe a Xia dynasty (c. 2070–1600 BC) before the Shang, but no writing is known from the period, and Shang writings do not indicate the existence of the Xia. The Shang ruled in the Yellow River valley, which is commonly held to be the cradle of Chinese civilization. However, Neolithic civilizations originated at various cultural centers along both the Yellow River and Yangtze River. These Yellow River and Yangtze civilizations arose millennia before the Shang. With thousands of years of continuous history, China is one of the world's oldest civilizations, and is regarded as one of the cradles of civilization.\n\nThe Zhou dynasty (1046–256 BC) supplanted the Shang, and introduced the concept of the Mandate of Heaven to justify their rule. The central Zhou government began to weaken due to external and internal pressures in the 8th century BC, and the country eventually splintered into smaller states during the Spring and Autumn period. These states became independent and warred with one another in the following Warring States period. Much of traditional Chinese culture, literature and philosophy first developed during those troubled times.\n\nIn 221 BC Qin Shi Huang conquered the various warring states and created for himself the title of \"Huangdi\" or \"emperor\" of the Qin, marking the beginning of imperial China. However, the oppressive government fell soon after his death, and was supplanted by the longer-lived Han dynasty (206 BC – 220 AD). Successive dynasties developed bureaucratic systems that enabled the emperor to control vast territories directly. In the 21 centuries from 206 BC until AD 1912, routine administrative tasks were handled by a special elite of \"scholar-officials\". Young men, well-versed in calligraphy, history, literature, and philosophy, were carefully selected through difficult government examinations. China's last dynasty was the Qing (1644–1912), which was replaced by the Republic of China in 1912, and in the mainland by the People's Republic of China in 1949, resulting in two \"de facto\" states claiming to be the legitimate government of all China.\n\nChinese history has alternated between periods of political unity and peace, and periods of war and failed statehood – the most recent being the Chinese Civil War (1927–1949). China was occasionally dominated by steppe peoples, most of whom were eventually assimilated into the Han Chinese culture and population. Between eras of multiple kingdoms and warlordism, Chinese dynasties have ruled parts or all of China; in some eras control stretched as far as Xinjiang and Tibet, as at present. Traditional culture, and influences from other parts of Asia and the Western world (carried by waves of immigration, cultural assimilation, expansion, and foreign contact), form the basis of the modern culture of China.\n\nWhat is now China was inhabited by \"Homo erectus\" more than a million years ago. Recent study shows that the stone tools found at Xiaochangliang site are magnetostratigraphically dated to 1.36 million years ago. The archaeological site of Xihoudu in Shanxi Province has evidence of use of fire by \"Homo erectus\", which is dated 1.27 million years ago, and \"Homo erectus\" fossils in China include the Yuanmou Man, the Lantian Man and the Peking Man. Fossilised teeth of \"Homo sapiens\" dating to 125,000–80,000 BC have been discovered in Fuyan Cave in Dao County in Hunan. Evidence of Middle Palaeolithic Levallois technology has been found in the lithic assemblage of Guanyindong Cave site in southwest China, dated to approximately 170,000–80,000 years ago.\n\nThe Neolithic age in China can be traced back to about 10,000 BC.\n\nThe earliest evidence of cultivated rice, found by the Yangtze River, is carbon-dated to 8,000 years ago. Early evidence for proto-Chinese millet agriculture is radiocarbon-dated to about 7000 BC. Farming gave rise to the Jiahu culture (7000 to 5800 BC). At Damaidi in Ningxia, 3,172 cliff carvings dating to 6000–5000 BC have been discovered, \"featuring 8,453 individual characters such as the sun, moon, stars, gods and scenes of hunting or grazing\". These pictographs are reputed to be similar to the earliest characters confirmed to be written Chinese. Chinese proto-writing existed in Jiahu around 7000 BC, Dadiwan from 5800 BC to 5400 BC, Damaidi around 6000 BC and Banpo dating from the 5th millennium BC. Some scholars have suggested that Jiahu symbols (7th millennium BC) were the earliest Chinese writing system. Excavation of a Peiligang culture site in Xinzheng county, Henan, found a community that flourished in 5,500 to 4,900 BC, with evidence of agriculture, constructed buildings, pottery, and burial of the dead. With agriculture came increased population, the ability to store and redistribute crops, and the potential to support specialist craftsmen and administrators. In late Neolithic times, the Yellow River valley began to establish itself as a center of Yangshao culture (5000 BC to 3000 BC), and the first villages were founded; the most archaeologically significant of these was found at Banpo, Xi'an. Later, Yangshao culture was superseded by the Longshan culture, which was also centered on the Yellow River from about 3000 BC to 2000 BC.\n\nBronze artifacts have been found at the Majiayao culture site (between 3100 and 2700 BC), The Bronze Age is also represented at the Lower Xiajiadian culture (2200–1600 BC) site in northeast China. Sanxingdui located in what is now Sichuan province is believed to be the site of a major ancient city, of a previously unknown Bronze Age culture (between 2000 and 1200 BC). The site was first discovered in 1929 and then re-discovered in 1986. Chinese archaeologists have identified the Sanxingdui culture to be part of the ancient kingdom of Shu, linking the artifacts found at the site to its early legendary kings.\nFerrous metallurgy begins to appear in the late 6th century in the Yangzi Valley. \nA bronze tomahawk with a blade of meteoric iron excavated near the city of Gaocheng in Shijiazhuang (now Hebei province) has been dated to the 14th century BC.\nFor this reason, authors such as Liana Chua and Mark Elliott have used the term \"Iron Age\" by convention for the transitional period of c. 500 BC to 100 BC, roughly corresponding to the Warring States period of Chinese historiography.\nAn Iron Age culture of the Tibetan Plateau has tentatively been associated with the Zhang Zhung culture described in early Tibetan writings.\n\nThe Xia dynasty of China (from c. 2070 to c. 1600 BC) is the first dynasty to be described in ancient historical records such as Sima Qian's \"Records of the Grand Historian\" and \"Bamboo Annals\".\n\nThe dynasty was considered mythical by historians until scientific excavations found early Bronze Age sites at Erlitou, Henan in 1959. With few clear records matching the Shang oracle bones, it remains unclear whether these sites are the remains of the Xia dynasty or of another culture from the same period. Excavations that overlap the alleged time period of the Xia indicate a type of culturally similar groupings of chiefdoms. Early markings from this period found on pottery and shells are thought to be ancestral to modern Chinese characters.\n\nAccording to ancient records, the dynasty ended around 1600 BC as a consequence of the Battle of Mingtiao.\n\nArchaeological findings providing evidence for the existence of the Shang dynasty, c. 1600–1046 BC, are divided into two sets. The first set, from the earlier Shang period, comes from sources at Erligang, Zhengzhou, and Shangcheng. The second set, from the later Shang or Yin (殷) period, is at Anyang, in modern-day Henan, which has been confirmed as the last of the Shang's nine capitals (c. 1300–1046 BC). The findings at Anyang include the earliest written record of the Chinese so far discovered: inscriptions of divination records in ancient Chinese writing on the bones or shells of animals — the \"oracle bones\", dating from around 1250 BC.\n\nA series of thirty-one kings reigned over the Shang dynasty. During their reign, according to the \"Records of the Grand Historian\", the capital city was moved six times. The final (and most important) move was to Yin in 1350 BC which led to the dynasty's golden age. The term Yin dynasty has been synonymous with the Shang dynasty in history, although it has lately been used to refer specifically to the latter half of the Shang dynasty.\n\nChinese historians in later periods were accustomed to the notion of one dynasty succeeding another, but the political situation in early China was much more complicated. Hence, as some scholars of China suggest, the Xia and the Shang can refer to political entities that existed concurrently, just as the early Zhou existed at the same time as the Shang.\n\nAlthough written records found at Anyang confirm the existence of the Shang dynasty, Western scholars are often hesitant to associate settlements that are contemporaneous with the Anyang settlement with the Shang dynasty. For example, archaeological findings at Sanxingdui suggest a technologically advanced civilization culturally unlike Anyang. The evidence is inconclusive in proving how far the Shang realm extended from Anyang. The leading hypothesis is that Anyang, ruled by the same Shang in the official history, coexisted and traded with numerous other culturally diverse settlements in the area that is now referred to as China proper.\n\nThe Zhou dynasty (1046 BC to approximately 256 BC) is the longest-lasting dynasty in Chinese history. By the end of the 2nd millennium BC, the Zhou dynasty began to emerge in the Yellow River valley, overrunning the territory of the Shang. The Zhou appeared to have begun their rule under a semi-feudal system. The Zhou lived west of the Shang, and the Zhou leader was appointed Western Protector by the Shang. The ruler of the Zhou, King Wu, with the assistance of his brother, the Duke of Zhou, as regent, managed to defeat the Shang at the Battle of Muye.\n\nThe king of Zhou at this time invoked the concept of the Mandate of Heaven to legitimize his rule, a concept that was influential for almost every succeeding dynasty. Like Shangdi, Heaven (\"tian\") ruled over all the other gods, and it decided who would rule China. It was believed that a ruler lost the Mandate of Heaven when natural disasters occurred in great number, and when, more realistically, the sovereign had apparently lost his concern for the people. In response, the royal house would be overthrown, and a new house would rule, having been granted the Mandate of Heaven.\n\nThe Zhou initially moved their capital west to an area near modern Xi'an, on the Wei River, a tributary of the Yellow River, but they would preside over a series of expansions into the Yangtze River valley. This would be the first of many population migrations from north to south in Chinese history.\n\nIn the 8th century BC, power became decentralized during the Spring and Autumn period, named after the influential \"Spring and Autumn Annals\". In this period, local military leaders used by the Zhou began to assert their power and vie for hegemony. The situation was aggravated by the invasion of other peoples from the northwest, such as the Qin, forcing the Zhou to move their capital east to Luoyang. This marks the second major phase of the Zhou dynasty: the Eastern Zhou. The Spring and Autumn period is marked by a falling apart of the central Zhou power. In each of the hundreds of states that eventually arose, local strongmen held most of the political power and continued their subservience to the Zhou kings in name only. Some local leaders even started using royal titles for themselves. China now consisted of hundreds of states, some of them only as large as a village with a fort.\n\nAs the era continued, larger and more powerful states annexed or claimed suzerainty over smaller ones. By the 6th century BC most small states had disappeared by being annexed and just a few large and powerful principalities dominated China. Some southern states, such as Chu and Wu, claimed independence from the Zhou, who undertook wars against some of them (Wu and Yue). Many new cities were established in this period and Chinese culture was slowly shaped.\n\nOnce all these powerful rulers had firmly established themselves within their respective dominions, the bloodshed focused more fully on interstate conflict in the Warring States period, which began when the three remaining élite families in the Jin state – Zhao, Wei and Han – partitioned the state. Many famous individuals such as Laozi, Confucius and Sun Tzu lived during this chaotic period.\n\nThe Hundred Schools of Thought of Chinese philosophy blossomed during this period, and such influential intellectual movements as Confucianism, Taoism, Legalism and Mohism were founded, partly in response to the changing political world. The first two philosophical thoughts would have an enormous influence on Chinese culture.\n\nAfter further political consolidation, seven prominent states remained by the end of the 5th century BC, and the years in which these few states battled each other are known as the Warring States period. Though there remained a nominal Zhou king until 256 BC, he was largely a figurehead and held little real power.\n\nNumerous developments were made during this period in culture and mathematics. Examples include an important literary achievement, the \"Zuo Commentary\" on the \"Spring and Autumn Annals\", which summarizes the preceding Spring and Autumn period, and the bundle of 21 bamboo slips from the Tsinghua collection, which was invented during this period dated to 305 BC, are the worlds' earliest example of a two digit decimal multiplication table, indicating that sophisticated commercial arithmetic was already established during this period.\n\nAs neighboring territories of these warring states, including areas of modern Sichuan and Liaoning, were annexed, they were governed under the new local administrative system of commandery and prefecture. This system had been in use since the Spring and Autumn period, and parts can still be seen in the modern system of Sheng and Xian (province and county).\n\nThe final expansion in this period began during the reign of Ying Zheng, the king of Qin. His unification of the other six powers, and further annexations in the modern regions of Zhejiang, Fujian, Guangdong and Guangxi in 214 BC, enabled him to proclaim himself the First Emperor (Qin Shi Huang).\n\nThe Imperial China Period can be divided into three subperiods: Early, Middle, and Late.\n\nMajor events in the Early subperiod include the Qin unification of China and their replacement by the Han, the First Split followed by the Jin unification, and the loss of north China. The Middle subperiod was marked by the Sui unification and their supplementation by the Tang, the Second Split, and the Song unification. The Late subperiod included the Yuan, Ming, and Qing dynasties.\n\nHistorians often refer to the period from the Qin dynasty to the end of the Qing dynasty as Imperial China. Though the unified reign of the First Qin Emperor lasted only 12 years, he managed to subdue great parts of what constitutes the core of the Han Chinese homeland and to unite them under a tightly centralized Legalist government seated at Xianyang (close to modern Xi'an). The doctrine of Legalism that guided the Qin emphasized strict adherence to a legal code and the absolute power of the emperor. This philosophy, while effective for expanding the empire in a military fashion, proved unworkable for governing it in peacetime. The Qin Emperor presided over the brutal silencing of political opposition, including the event known as the burning of books and burying of scholars. This would be the impetus behind the later Han synthesis incorporating the more moderate schools of political governance.\n\nMajor contributions of the Qin include the concept of a centralized government, and the unification and development of the legal code, the written language, measurement, and currency of China after the tribulations of the Spring and Autumn and Warring States periods. Even something as basic as the length of axles for carts—which need to match ruts in the roads—had to be made uniform to ensure a viable trading system throughout the empire. Also as part of its centralization, the Qin connected the northern border walls of the states it defeated, making the first Great Wall of China.\n\nThe tribes of the north, collectively called the Wu Hu by the Qin, were free from Chinese rule during the majority of the dynasty. Prohibited from trading with Qin dynasty peasants, the Xiongnu tribe living in the Ordos region in northwest China often raided them instead, prompting the Qin to retaliate. After a military campaign led by General Meng Tian, the region was conquered in 215 BC and agriculture was established; the peasants, however, were discontented and later revolted. The succeeding Han dynasty also expanded into the Ordos due to overpopulation, but depleted their resources in the process. Indeed, this was true of the dynasty's borders in multiple directions; modern Inner Mongolia, Xinjiang, Tibet, Manchuria, and regions to the southeast were foreign to the Qin, and even areas over which they had military control were culturally distinct.\n\nA major Qin innovation that lasted until 1912 was reliance upon a trained intellectual elite, the \"scholar-official\" (\"scholar-gentlemen\"). They were civil servants appointed by the Emperor to handle daily governance. Talented young men were selected through an elaborate process of imperial examination. They had to demonstrate skill at calligraphy, and had to know Confucian philosophy. Historian Wing-Tsit Chan concludes that:\n\nAfter Emperor Qin Shi Huang's unnatural death due to the consumption of mercury pills, the Qin government drastically deteriorated and eventually capitulated in 207 BC after the Qin capital was captured and sacked by rebels, which would ultimately lead to the establishment of a new dynasty of a unified China. Despite the short 15-year duration of the Qin dynasty, it was immensely influential on China and the structure of future Chinese dynasties.\n\nThe Han dynasty was founded by Liu Bang, who emerged victorious in the Chu–Han Contention that followed the fall of the Qin dynasty. A golden age in Chinese history, the Han dynasty's long period of stability and prosperity consolidated the foundation of China as a unified state under a central imperial bureaucracy, which was to last intermittently for most of the next two millennia. During the Han dynasty, territory of China was extended to most of the China proper and to areas far west. Confucianism was officially elevated to orthodox status and was to shape the subsequent Chinese civilization. Art, culture and science all advanced to unprecedented heights. With the profound and lasting impacts of this period of Chinese history, the dynasty name \"Han\" had been taken as the name of the Chinese people, now the dominant ethnic group in modern China, and had been commonly used to refer to Chinese language and written characters. The Han dynasty also saw many mathematical innovations being invented such as the method of Gaussian elimination which appeared in the Chinese mathematical text Chapter Eight \"Rectangular Arrays\" of \"The Nine Chapters on the Mathematical Art\". Its use is illustrated in eighteen problems, with two to five equations. The first reference to the book by this title is dated to 179 AD, but parts of it were written as early as approximately 150 BC, more than 1500 years before the Europeans came up with the method in the 18th century.\n\nAfter the initial laissez-faire policies of Emperors Wen and Jing, the ambitious Emperor Wu brought the empire to its zenith. To consolidate his power, Confucianism, which emphasizes stability and order in a well-structured society, was given exclusive patronage to be the guiding philosophical thoughts and moral principles of the empire. Imperial Universities were established to support its study and further development, while other schools of thought were discouraged.\n\nMajor military campaigns were launched to weaken the nomadic Xiongnu Empire, limiting their influence north of the Great Wall. Along with the diplomatic efforts led by Zhang Qian, the sphere of influence of the Han Empire extended to the states in the Tarim Basin, opened up the Silk Road that connected China to the west, stimulating bilateral trade and cultural exchange. To the south, various small kingdoms far beyond the Yangtze River Valley were formally incorporated into the empire.\n\nEmperor Wu also dispatched a series of military campaigns against the Baiyue tribes. The Han annexed Minyue in 135 BC and 111 BC, Nanyue in 111 BC, and Dian in 109 BC. Migration and military expeditions led to the cultural assimilation of the south. It also brought the Han into contact with kingdoms in Southeast Asia, introducing diplomacy and trade.\n\nAfter Emperor Wu, the empire slipped into gradual stagnation and decline. Economically, the state treasury was strained by excessive campaigns and projects, while land acquisitions by elite families gradually drained the tax base. Various consort clans exerted increasing control over strings of incompetent emperors and eventually the dynasty was briefly interrupted by the usurpation of Wang Mang.\n\nIn AD 9, the usurper Wang Mang claimed that the Mandate of Heaven called for the end of the Han dynasty and the rise of his own, and he founded the short-lived Xin (\"New\") dynasty. Wang Mang started an extensive program of land and other economic reforms, including the outlawing of slavery and land nationalization and redistribution. These programs, however, were never supported by the landholding families, because they favored the peasants. The instability of power brought about chaos, uprisings, and loss of territories. This was compounded by mass flooding of the Yellow River; silt buildup caused it to split into two channels and displaced large numbers of farmers. Wang Mang was eventually killed in Weiyang Palace by an enraged peasant mob in AD 23.\n\nEmperor Guangwu reinstated the Han dynasty with the support of landholding and merchant families at Luoyang, \"east\" of the former capital Xi'an. Thus, this new era is termed the Eastern Han dynasty. With the capable administrations of Emperors Ming and Zhang, former glories of the dynasty was reclaimed, with brilliant military and cultural achievements. The Xiongnu Empire was decisively defeated. The diplomat and general Ban Chao further expanded the conquests across the Pamirs to the shores of the Caspian Sea, thus reopening the Silk Road, and bringing trade, foreign cultures, along with the arrival of Buddhism. With extensive connections with the west, the first of several Roman embassies to China were recorded in Chinese sources, coming from the sea route in AD 166, and a second one in AD 284.\n\nThe Eastern Han dynasty was one of the most prolific era of science and technology in ancient China, notably the historic invention of papermaking by Cai Lun, and the numerous scientific and mathematical contributions by the famous polymath Zhang Heng.\n\nBy the 2nd century, the empire declined amidst land acquisitions, invasions, and feuding between consort clans and eunuchs. The Yellow Turban Rebellion broke out in AD 184, ushering in an era of warlords. In the ensuing turmoil, three states tried to gain predominance in the period of the Three Kingdoms. This time period has been greatly romanticized in works such as \"Romance of the Three Kingdoms\".\n\nAfter Cao Cao reunified the north in 208, his son proclaimed the Wei dynasty in 220. Soon, Wei's rivals Shu and Wu proclaimed their independence, leading China into the Three Kingdoms period. This period was characterized by a gradual decentralization of the state that had existed during the Qin and Han dynasties, and an increase in the power of great families.\n\nIn 266, the Jin dynasty overthrew the Wei and later unified the country in 280, but this union was short-lived.\n\nThe Jin dynasty was severely weakened by internecine fighting among imperial princes and lost control of northern China after non-Han Chinese settlers rebelled and captured Luoyang and Chang'an. In 317, a Jin prince in modern-day Nanjing became emperor and continued the dynasty, now known as the Eastern Jin, which held southern China for another century. Prior to this move, historians refer to the Jin dynasty as the Western Jin.\n\nNorthern China fragmented into a series of independent kingdoms, most of which were founded by Xiongnu, Xianbei, Jie, Di and Qiang rulers. These non-Han peoples were ancestors of the Turks, Mongols, and Tibetans. Many had, to some extent, been \"sinicized\" long before their ascent to power. In fact, some of them, notably the Qiang and the Xiongnu, had already been allowed to live in the frontier regions within the Great Wall since late Han times. During the period of the Sixteen Kingdoms, warfare ravaged the north and prompted large-scale Han Chinese migration south to the Yangtze Basin and Delta.\n\nIn the early 5th century, China entered a period known as the Northern and Southern dynasties, in which parallel regimes ruled the northern and southern halves of the country. In the south, the Eastern Jin gave way to the Liu Song, Southern Qi, Liang and finally Chen. Each of these Southern dynasties were led by Han Chinese ruling families and used Jiankang (modern Nanjing) as the capital. They held off attacks from the north and preserved many aspects of Chinese civilization, while northern barbarian regimes began to sinify.\n\nIn the north, the last of the Sixteen Kingdoms was extinguished in 439 by the Northern Wei, a kingdom founded by the Xianbei, a nomadic people who unified northern China. The Northern Wei eventually split into the Eastern and Western Wei, which then became the Northern Qi and Northern Zhou. These regimes were dominated by Xianbei or Han Chinese who had married into Xianbei families. During this period most Xianbei people adopted Han surnames, eventually leading to complete assimilation into the Han.\n\nDespite the division of the country, Buddhism spread throughout the land. In southern China, fierce debates about whether Buddhism should be allowed were held frequently by the royal court and nobles. By the end of the era, Buddhists and Taoists had become much more tolerant of each other.\n\nThe short-lived Sui dynasty was a pivotal period in Chinese history. Founded by Emperor Wen in 581 in succession of the Northern Zhou, the Sui went on to conquer the Southern Chen in 589 to reunify China, ending three centuries of political division. The Sui pioneered many new institutions, including the government system of Three Departments and Six Ministries, imperial examinations for selecting officials from commoners, while improved on the systems of fubing system of the army conscription and the Equal-field system of land distributions. These policies, which were adopted by later dynasties, brought enormous population growth, and amassed excessive wealth to the state. Standardized coinage were enforced throughout the unified empire. Buddhism took root as a prominent religion and was supported officially. Sui China was known for its numerous mega-construction projects. Intended for grains shipment and transporting troops, the Grand Canal was constructed, linking the capitals Daxing (Chang'an) and Luoyang to the wealthy southeast region, and in another route, to the northeast border. The Great Wall was also expanded, while series of military conquests and diplomatic maneuvers further pacified its borders. However, the massive invasions of the Korean Peninsula during the Goguryeo–Sui War failed disastrously, triggering widespread revolts that led to the fall of the dynasty.\n\nThe Tang dynasty was founded by Emperor Gaozu on 18 June 618. It was a golden age of Chinese civilization and considered to be the most prosperous period of China with significant developments in culture, art, literature, particularly poetry, and technology. Buddhism became the predominant religion for the common people. Chang'an (modern Xi'an), the national capital, was the largest city in the world during its time.\n\nThe second emperor, Taizong, is widely regarded as one of the greatest emperors in Chinese history, who had laid the foundation for the dynasty to flourish for centuries beyond his reign. Combined military conquests and diplomatic maneuvers were implemented to eliminate threats from nomadic tribes, extend the border, and submit neighboring states into a tributary system. Military victories in the Tarim Basin kept the Silk Road open, connecting Chang'an to Central Asia and areas far to the west. In the south, lucrative maritime trade routes began from port cities such as Guangzhou. There was extensive trade with distant foreign countries, and many foreign merchants settled in China, encouraging a cosmopolitan culture. The Tang culture and social systems were observed and imitated by neighboring countries, most notably, Japan. Internally the Grand Canal linked the political heartland in Chang'an to the agricultural and economic centers in the eastern and southern parts of the empire. Xuanzang, a Chinese Buddhist monk, scholar, traveller, and translator who travelled to India on his own, and returned with, \"over six hundred Mahayana and Hinayana texts, seven statues of the Buddha and more than a hundred sarira relics.\"\n\nUnderlying the prosperity of the early Tang dynasty was a strong centralized bureaucracy with efficient policies. The government was organized as \"Three Departments and Six Ministries\" to separately draft, review, and implement policies. These departments were run by royal family members as well as scholar officials who were selected by imperial examinations. These practices, which matured in the Tang dynasty, were continued by the later dynasties, with some modifications.\n\nUnder the Tang \"equal-field system\" all land was owned by the Emperor and granted to people according to household size. Men granted land were conscripted for military service for a fixed period each year, a military policy known as the \"Fubing system\". These policies stimulated a rapid growth in productivity and a significant army without much burden on the state treasury. By the dynasty's midpoint, however, standing armies had replaced conscription, and land was continuously falling into the hands of private owners.\n\nThe dynasty continued to flourish under the rule of Empress Wu Zetian, the only empress regnant in Chinese history, and reached its zenith during the long reign of Emperor Xuanzong, who oversaw an empire that stretched from the Pacific to the Aral Sea with at least 50 million people. There were vibrant artistic and cultural creations, including works of the greatest Chinese poets, Li Bai, and Du Fu.\n\nAt the zenith of prosperity of the empire, the An Lushan Rebellion from 755 to 763 was a watershed event that devastated the population and drastically weakened the central imperial government. Upon suppression of the rebellion, regional military governors, known as Jiedushi, gained increasingly autonomous status. With loss of revenue from land tax, the central imperial government relied heavily on salt monopoly. Externally, former submissive states raided the empire and the vast border territories were irreversibly lost for subsequent centuries. Nevertheless, civil society recovered and thrived amidst the weakened imperial bureaucracy.\n\nIn late Tang period, the empire was worn out by recurring revolts of regional warlords, while internally, as scholar-officials engaged in fierce factional strife, corrupted eunuchs amassed immense power. Catastrophically, the Huang Chao Rebellion, from 874 to 884, devastated the entire empire for a decade. The sack of the southern port Guangzhou in 879 was followed by the massacre of most of its inhabitants, along with the large foreign merchant enclaves. By 881, both capitals, Luoyang and Chang'an, fell successively. The reliance on ethnic Han and Turkic warlords in suppressing the rebellion increased their power and influence. Consequently, the fall of the dynasty following Zhu Wen's usurpation led to an era of division.\n\nAccording to historian Mark Edward Lewis:\n\nThe period of political disunity between the Tang and the Song, known as the Five Dynasties and Ten Kingdoms period, lasted from 907 to 960. During this half-century, China was in all respects a multi-state system. Five regimes, namely, (Later) Liang, Tang, Jin, Han and Zhou, rapidly succeeded one another in control of the traditional Imperial heartland in northern China. Among the regimes, rulers of (Later) Tang, Jin and Han were sinicized Shatuo Turks, which ruled over the ethnic majority of Han Chinese. More stable and smaller regimes of mostly ethnic Han rulers coexisted in south and western China over the period, cumulatively constituted the \"Ten Kingdoms\".\n\nAmidst political chaos in the north, the strategic Sixteen Prefectures (region along today's Great Wall) were ceded to the emerging Khitan Liao dynasty, which drastically weakened the defense of the China proper against northern nomadic empires. To the south, Vietnam gained lasting independence after being a Chinese prefecture for many centuries. With wars dominated in Northern China, there were mass southward migrations of population, which further enhanced the southward shift of cultural and economic centers in China. The era ended with the coup of Later Zhou general Zhao Kuangyin, and the establishment the Song dynasty in 960, which eventually annihilated the remains of the \"Ten Kingdoms\" and reunified China.\n\nIn 960, the Song dynasty was founded by Emperor Taizu, with its capital established in Kaifeng (also known as Bianjing). In 979, the Song dynasty reunified most of the China proper, while large swaths of the outer territories were occupied by sinicized nomadic empires. The Khitan Liao dynasty, which lasted from 907 to 1125, ruled over Manchuria, Mongolia, and parts of Northern China. Meanwhile, in what are now the north-western Chinese provinces of Gansu, Shaanxi, and Ningxia, the Tangut tribes founded the Western Xia dynasty from 1032 to 1227.\n\nAiming to recover the strategic Sixteen Prefectures lost in the previous dynasty, campaigns were launched against the Liao dynasty in the early Song period, which all ended in failure. Then in 1004, the Liao cavalry swept over the exposed North China Plain and reached the outskirts of Kaifeng, forcing the Song's submission and then agreement to the Chanyuan Treaty, which imposed heavy annual tributes from the Song treasury. The treaty was a significant reversal of Chinese dominance of the traditional tributary system. Yet the annual outflow of Song's silver to the Liao was paid back through the purchase of Chinese goods and products, which expanded the Song economy, and replenished its treasury. This dampened the incentive for the Song to further campaign against the Liao. Meanwhile, this cross-border trade and contact induced further sinicization within the Liao Empire, at the expense of its military might which was derived from its primitive nomadic lifestyle. Similar treaties and social-economical consequences occurred in Song's relations with the Jin dynasty.\n\nWithin the Liao Empire, the Jurchen tribes revolted against their overlords to establish the Jin dynasty in 1115. In 1125, the devastating Jin cataphract annihilated the Liao dynasty, while remnants of Liao court members fled to Central Asia to found the Qara Khitai Empire (Western Liao dynasty). Jin's invasion of the Song dynasty followed swiftly. In 1127, Kaifeng was sacked, a massive catastrophe known as the Jingkang Incident, ending the Northern Song dynasty. Later the entire north of China was conquered. The survived members of Song court regrouped in the new capital city of Hangzhou, and initiated the Southern Song dynasty, which ruled territories south of the Huai River. In the ensuing years, the territory and population of China were divided between the Song dynasty, the Jin dynasty and the Western Xia dynasty. The era ended with the Mongol conquest, as Western Xia fell in 1227, the Jin dynasty in 1234, and finally the Southern Song dynasty in 1279.\n\nDespite its military weakness, the Song dynasty is widely considered to be the high point of classical Chinese civilization. The Song economy, facilitated by technology advancement, had reached a level of sophistication probably unseen in world history before its time. The population soared to over 100 million and the living standards of common people improved tremendously due to improvements in rice cultivation and the wide availability of coal for production. The capital cities of Kaifeng and subsequently Hangzhou were both the most populous cities in the world for their time, and encouraged vibrant civil societies unmatched by previous Chinese dynasties. Although land trading routes to the far west were blocked by nomadic empires, there were extensive maritime trade with neighbouring states, which facilitated the use of Song coinage as the de facto currency of exchange. Giant wooden vessels equipped with compasses travelled throughout the China Seas and northern Indian Ocean. The concept of insurance was practised by merchants to hedge the risks of such long-haul maritime shipments. With prosperous economic activities, the historically first use of paper currency emerged in the western city of Chengdu, as a supplement to the existing copper coins.\n\nThe Song dynasty was considered to be the golden age of great advancements in science and technology of China, thanks to innovative scholar-officials such as Su Song (1020–1101) and Shen Kuo (1031–1095). Inventions such as the hydro-mechanical astronomical clock, the first continuous and endless power-transmitting chain, woodblock printing and paper money were all invented during the Song dynasty.\n\nThere was court intrigue between the political reformers and conservatives, led by the chancellors Wang Anshi and Sima Guang, respectively. By the mid-to-late 13th century, the Chinese had adopted the dogma of Neo-Confucian philosophy formulated by Zhu Xi. Enormous literary works were compiled during the Song dynasty, such as the historical work, the \"Zizhi Tongjian\" (\"Comprehensive Mirror to Aid in Government\"). The invention of movable-type printing further facilitated the spread of knowledge. Culture and the arts flourished, with grandiose artworks such as \"Along the River During the Qingming Festival\" and \"Eighteen Songs of a Nomad Flute\", along with great Buddhist painters such as the prolific Lin Tinggui.\n\nThe Song dynasty was also a period of major innovation in the history of warfare. Gunpowder, while invented in the Tang dynasty, was first put into use in battlefields by the Song army, inspiring a succession of new firearms and siege engines designs. During the Southern Song dynasty, as its survival hinged decisively on guarding the Yangtze and Huai River against the cavalry forces from the north, the first standing navy in China was assembled in 1132, with its admiral's headquarters established at Dinghai. Paddle-wheel warships equipped with trebuchets could launch incendiary bombs made of gunpowder and lime, as recorded in Song's victory over the invading Jin forces at the Battle of Tangdao in the East China Sea, and the Battle of Caishi on the Yangtze River in 1161.\n\nThe advances in civilization during the Song dynasty came to an abrupt end following the devastating Mongol conquest, during which the population sharply dwindled, with a marked contraction in economy. Despite viciously halting Mongol advance for more than three decades, the Southern Song capital Hangzhou fell in 1276, followed by the final annihilation of the Song standing navy at the Battle of Yamen in 1279.\n\nThe Yuan dynasty was formally proclaimed in 1271, when the Great Khan of Mongol, Kublai Khan, one of the grandsons of Genghis Khan, assumed the additional title of the Emperor of China, and considered his inherited part of the Mongol Empire as a Chinese dynasty. In the preceding decades, the Mongols had conquered the Jin dynasty in Northern China, and the Southern Song dynasty fell in 1279 after a protracted and bloody war. The Mongol Yuan dynasty became the first conquest dynasty in Chinese history to rule the entire China proper and its population as an ethnic minority. The dynasty also directly controlled the Mongolian heartland and other regions, inheriting the largest share of territory of the divided Mongol Empire, which roughly coincided with the modern area of China and nearby regions in East Asia. Further expansion of the empire was halted after defeats in the invasions of Japan and Vietnam. Following the previous Jin dynasty, the capital of Yuan dynasty was established at Khanbaliq (also known as Dadu, modern-day Beijing). The Grand Canal was reconstructed to connect the remote capital city to economic hubs in southern part of China, setting the precedence and foundation where Beijing would largely remain as the capital of the successive regimes that unified China mainland.\n\nAfter the peace treaty in 1304 that ended a series of Mongols civil wars, the emperors of the Yuan dynasty were upheld as the nominal Great Khan (Khagan) of the greater Mongol Empire over other Mongol Khanates, which nonetheless remained de facto autonomous. The era was known as \"Pax Mongolica\", when much of the Asian continent was ruled by the Mongols. For the first and only time in history, the silk road was controlled entirely by a single state, facilitating the flow of people, trade, and cultural exchange. Network of roads and a postal system were established to connect the vast empire. Lucrative maritime trade, developed from the previous Song dynasty, continued to flourish, with Quanzhou and Hangzhou emerging as the largest ports in the world. Adventurous travelers from the far west, most notably the Venetian, Marco Polo, would have settled in China for decades. Upon his return, his detail travel record inspired generations of medieval Europeans with the splendors of the far East. The Yuan dynasty was the first ancient economy, where paper currency, known at the time as Chao, was used as the predominant medium of exchange. Its unrestricted issuance in the late Yuan dynasty inflicted hyperinflation, which eventually brought the downfall of the dynasty.\n\nWhile the Mongol rulers of the Yuan dynasty adopted substantially to Chinese culture, their sinicization was of lesser extent compared to earlier conquest dynasties in Chinese history. For preserving racial superiority as the conqueror and ruling class, traditional nomadic customs and heritage from the Mongolian steppe were held in high regard. On the other hand, the Mongol rulers also adopted flexibly to a variety of cultures from many advanced civilizations within the vast empire. Traditional social structure and culture in China underwent immense transform during the Mongol dominance. Large group of foreign migrants settled in China, who enjoyed elevated social status over the majority Han Chinese, while enriching Chinese culture with foreign elements. The class of scholar officials and intellectuals, traditional bearers of elite Chinese culture, lost substantial social status. This stimulated the development of culture of the common folks. There were prolific works in zaju variety shows and literary songs (sanqu), which were written in a distinctive poetry style known as qu. Novels of vernacular style gained unprecedented status and popularity.\n\nBefore the Mongol invasion, Chinese dynasties reported approximately 120 million inhabitants; after the conquest had been completed in 1279, the 1300 census reported roughly 60 million people. This major decline is not necessarily due only to Mongol killings. Scholars such as Frederick W. Mote argue that the wide drop in numbers reflects an administrative failure to record rather than an actual decrease; others such as Timothy Brook argue that the Mongols created a system of enserfment among a huge portion of the Chinese populace, causing many to disappear from the census altogether; other historians including William McNeill and David Morgan consider that plague was the main factor behind the demographic decline during this period. In the 14th century China suffered additional depredations from epidemics of plague, estimated to have killed 25 million people, 30% of the population of China.\n\nThroughout the Yuan dynasty, there was some general sentiment among the populace against the Mongol dominance. Yet rather than the nationalist cause, it was mainly strings of natural disasters and incompetent governance that triggered widespread peasant uprisings since the 1340s. After the massive naval engagement at Lake Poyang, Zhu Yuanzhang prevailed over other rebel forces in the south. He proclaimed himself emperor and founded the Ming dynasty in 1368. The same year his northern expedition army captured the capital Khanbaliq. The Yuan remnants fled back to Mongolia and sustained the regime. Other Mongol Khanates in Central Asia continued to exist after the fall of Yuan dynasty in China.\n\nThe Ming dynasty was founded by Zhu Yuanzhang in 1368, who proclaimed himself as the Hongwu Emperor. The capital was initially set at Nanjing, and was later moved to Beijing from Yongle Emperor's reign onward.\n\nUrbanization increased as the population grew and as the division of labor grew more complex. Large urban centers, such as Nanjing and Beijing, also contributed to the growth of private industry. In particular, small-scale industries grew up, often specializing in paper, silk, cotton, and porcelain goods. For the most part, however, relatively small urban centers with markets proliferated around the country. Town markets mainly traded food, with some necessary manufactures such as pins or oil.\n\nDespite the xenophobia and intellectual introspection characteristic of the increasingly popular new school of neo-Confucianism, China under the early Ming dynasty was not isolated. Foreign trade and other contacts with the outside world, particularly Japan, increased considerably. Chinese merchants explored all of the Indian Ocean, reaching East Africa with the voyages of Zheng He.\n\nThe Hongwu Emperor, being the only founder of a Chinese dynasty who was also of peasant origin, had laid the foundation of a state that relied fundamentally in agriculture. Commerce and trade, which flourished in the previous Song and Yuan dynasties, were less emphasized. Neo-feudal landholdings of the Song and Mongol periods were expropriated by the Ming rulers. Land estates were confiscated by the government, fragmented, and rented out. Private slavery was forbidden. Consequently, after the death of the Yongle Emperor, independent peasant landholders predominated in Chinese agriculture. These laws might have paved the way to removing the worst of the poverty during the previous regimes. Towards later era of the Ming dynasty, with declining government control, commerce, trade and private industries revived.\n\nThe dynasty had a strong and complex central government that unified and controlled the empire. The emperor's role became more autocratic, although Hongwu Emperor necessarily continued to use what he called the \"Grand Secretariat\" to assist with the immense paperwork of the bureaucracy, including memorials (petitions and recommendations to the throne), imperial edicts in reply, reports of various kinds, and tax records. It was this same bureaucracy that later prevented the Ming government from being able to adapt to changes in society, and eventually led to its decline.\n\nThe Yongle Emperor strenuously tried to extend China's influence beyond its borders by demanding other rulers send ambassadors to China to present tribute. A large navy was built, including four-masted ships displacing 1,500 tons. A standing army of 1 million troops was created. The Chinese armies conquered and occupied Vietnam for around 20 years, while the Chinese fleet sailed the China seas and the Indian Ocean, cruising as far as the east coast of Africa. The Chinese gained influence in eastern Moghulistan. Several maritime Asian nations sent envoys with tribute for the Chinese emperor. Domestically, the Grand Canal was expanded and became a stimulus to domestic trade. Over 100,000 tons of iron per year were produced. Many books were printed using movable type. The imperial palace in Beijing's Forbidden City reached its current splendor. It was also during these centuries that the potential of south China came to be fully exploited. New crops were widely cultivated and industries such as those producing porcelain and textiles flourished.\n\nIn 1449 Esen Tayisi led an Oirat Mongol invasion of northern China which culminated in the capture of the Zhengtong Emperor at Tumu. Since then, the Ming became on the defensive on the northern frontier, which led to the Ming Great Wall being built. Most of what remains of the Great Wall of China today was either built or repaired by the Ming. The brick and granite work was enlarged, the watchtowers were redesigned, and cannons were placed along its length.\n\nAt sea, the Ming became increasingly isolationist after the death of the Yongle Emperor. The treasure voyages which sailed Indian Ocean were discontinued, and the maritime prohibition laws were set in place banning the Chinese from sailing abroad. European traders who reached China in the midst of the Age of Discovery were repeatedly rebuked in their requests for trade, with the Portuguese being repulsed by the Ming navy at Tuen Mun in 1521 and again in 1522. Domestic and foreign demands for overseas trade, deemed illegal by the state, led to widespread \"wokou\" piracy attacking the southeastern coastline during the rule of the Jiajing Emperor (1507–1567), which only subsided after the opening of ports in Guangdong and Fujian and much military suppression. The Portuguese were allowed to settle in Macau in 1557 for trade, which remained in Portuguese hands until 1999. The Dutch entry into the Chinese seas was also met with fierce resistance, with the Dutch being chased off the Penghu islands in the Sino-Dutch conflicts of 1622–1624 and were forced to settle in Taiwan instead. The Dutch in Taiwan fought with the Ming in the Battle of Liaoluo Bay in 1633 and lost, and eventually surrendered to the Ming loyalist Koxinga in 1662, after the fall of the Ming dynasty.\n\nIn 1556, during the rule of the Jiajing Emperor, the Shaanxi earthquake killed about 830,000 people, the deadliest earthquake of all time.\n\nThe Ming dynasty intervened deeply in the Japanese invasions of Korea (1592–98), which ended with the withdrawal of all invading Japanese forces in Korea, and the restoration of the Joseon dynasty, its traditional ally and tributary state. The regional hegemony of the Ming dynasty was preserved at a toll on its resources. Coincidentally, with Ming's control in Manchuria in decline, the Manchu (Jurchen) tribes, under their chieftain Nurhaci, broke away from Ming's rule, and emerged as a powerful, unified state, which was later proclaimed as the Qing dynasty. It went on to subdue the much weakened Korea as its tributary, conquered Mongolia, and expanded its territory to the outskirt of the Great Wall. The most elite army of the Ming dynasty was to station at the Shanhai Pass to guard the last stronghold against the Manchus, which weakened its suppression of internal peasants uprisings.\n\nThe Qing dynasty (1644–1911) was the last imperial dynasty in China. Founded by the Manchus, it was the second conquest dynasty to rule the entire territory of China and its people. The Manchus were formerly known as Jurchens, residing in the northeastern part of the Ming territory outside the Great Wall. They emerged as the major threat to the late Ming dynasty after Nurhaci united all Jurchen tribes and established an independent state. However, the Ming dynasty would be overthrown by Li Zicheng's peasants rebellion, with Beijing captured in 1644 and the Chongzhen Emperor, the last Ming emperor, committing suicide. The Manchus allied with the former Ming general Wu Sangui to seize Beijing, which was made the capital of the Qing dynasty, and then proceeded to subdue the Ming remnants in the south. The decades of Manchu conquest caused enormous loss of lives and the economic scale of China shrank drastically. In total, the Qing conquest of the Ming (1618–1683) cost as many as 25 million lives. Nevertheless, the Manchus adopted the Confucian norms of traditional Chinese government in their rule and were considered a Chinese dynasty.\n\nThe Manchus enforced a 'queue order,' forcing the Han Chinese to adopt the Manchu queue hairstyle. Officials were required to wear Manchu-style clothing \"Changshan\" (bannermen dress and \"Tangzhuang\"), but ordinary Han civilians were allowed to wear traditional Han clothing, or \"Hanfu\". Most Han then voluntarily shifted to wearing Qipao anyway. The Kangxi Emperor ordered the creation of the \"Kangxi Dictionary\", the most complete dictionary of Chinese characters that had been compiled. The Qing dynasty set up the Eight Banners system that provided the basic framework for the Qing military organization. Bannermen could not undertake trade or manual labor; they had to petition to be removed from banner status. They were considered a form of nobility and were given preferential treatment in terms of annual pensions, land, and allotments of cloth.\n\nOver the next half-century, all areas previously under the Ming dynasty were consolidated under the Qing. Xinjiang, Tibet, and Mongolia were also formally incorporated into Chinese territory. Between 1673 and 1681, the Kangxi Emperor suppressed the Revolt of the Three Feudatories, an uprising of three generals in Southern China who had been denied hereditary rule of large fiefdoms granted by the previous emperor. In 1683, the Qing staged an amphibious assault on southern Taiwan, bringing down the rebel Kingdom of Tungning, which was founded by the Ming loyalist Koxinga (Zheng Chenggong) in 1662 after the fall of the Southern Ming, and had served as a base for continued Ming resistance in Southern China. The Qing defeated the Russians at Albazin, resulting in the Treaty of Nerchinsk.\n\nBy the end of Qianlong Emperor's long reign, the Qing Empire was at its zenith. China ruled more than one-third of the world's population, and had the largest economy in the world. By area it was one of the largest empires ever.\n\nIn the 19th century the empire was internally stagnant and externally threatened by western powers. The defeat by the British Empire in the First Opium War (1840) led to the Treaty of Nanking (1842), under which Hong Kong was ceded to Britain and importation of opium (produced by British Empire territories) was allowed. Subsequent military defeats and unequal treaties with other western powers continued even after the fall of the Qing dynasty.\n\nInternally the Taiping Rebellion (1851–1864), a quasi-Christian religious movement led by the \"Heavenly King\" Hong Xiuquan, raided roughly a third of Chinese territory for over a decade until they were finally crushed in the Third Battle of Nanking in 1864. This was one of the largest wars in the 19th century in terms of troop involvement; there was massive loss of life, with a death toll of about 20 million. A string of civil disturbances followed, including the Punti–Hakka Clan Wars, Nian Rebellion, Dungan Revolt, and Panthay Rebellion. All rebellions were ultimately put down, but at enormous cost and with millions dead, seriously weakening the central imperial authority. The Banner system that the Manchus had relied upon for so long failed: Banner forces were unable to suppress the rebels, and the government called upon local officials in the provinces, who raised \"New Armies\", which successfully crushed the challenges to Qing authority. China never rebuilt a strong central army, and many local officials became warlords who used military power to effectively rule independently in their provinces.\n\nIn response to calamities within the empire and threats from imperialism, the Self-Strengthening Movement was an institutional reform in the second half of the 1800s. The aim was to modernize the empire, with prime emphasis on strengthening the military. However, the reform was undermined by corrupt officials, cynicism, and quarrels within the imperial family. As a result, the \"Beiyang Fleet\" were soundly defeated in the First Sino-Japanese War (1894–1895). The Guangxu Emperor and the reformists then launched a more comprehensive reform effort, the Hundred Days' Reform (1898), but it was soon overturned by the conservatives under Empress Dowager Cixi in a military coup.\n\nAt the turn of the 20th century, the violent Boxer Rebellion opposed foreign influence in Northern China, and attacked Chinese Christians and missionaries. When Boxers entered Beijing, the Qing government ordered all foreigners to leave. But instead the foreigners and many Chinese were besieged in the foreign legations quarter. The Eight-Nation Alliance sent the Seymour Expedition of Japanese, Russian, Italian, German, French, American, and Austrian troops to relieve the siege. The Expedition was stopped by the Boxers at the Battle of Langfang and forced to retreat. Due to the Alliance's attack on the Dagu Forts, the Qing government in response sided with the Boxers and declared war on the Alliance. There was fierce fighting at Tientsin. The Alliance formed the second, much larger Gaselee Expedition and finally reached Beijing; the Qing government evacuated to Xi'an. The Boxer Protocol ended the war.\n\nFrustrated by the Qing court's resistance to reform and by China's weakness, young officials, military officers, and students began to advocate the overthrow of the Qing dynasty and the creation of a republic. They were inspired by the revolutionary ideas of Sun Yat-sen. A revolutionary military uprising, the Wuchang Uprising, began on 10 October 1911, in Wuhan. The provisional government of the Republic of China was formed in Nanjing on 12 March 1912. The Xinhai Revolution ended 2,000 years of dynastic rule in China.\n\nAfter the success of the overthrow of the Qing dynasty, Sun Yat-sen was declared President, but Sun was forced to turn power over to Yuan Shikai, who commanded the New Army and was Prime Minister under the Qing government, as part of the agreement to let the last Qing monarch abdicate (a decision Sun would later regret). Over the next few years, Yuan proceeded to abolish the national and provincial assemblies, and declared himself emperor in late 1915. Yuan's imperial ambitions were fiercely opposed by his subordinates; faced with the prospect of rebellion, he abdicated in March 1916, and died in June of that year.\n\nYuan's death in 1916 left a power vacuum in China; the republican government was all but shattered. This ushered in the Warlord Era, during which much of the country was ruled by shifting coalitions of competing provincial military leaders.\n\nIn 1919, the May Fourth Movement began as a response to the terms imposed on China by the Treaty of Versailles ending World War I, but quickly became a nationwide protest movement about the domestic situation in China. The protests were a moral success as the cabinet fell and China refused to sign the Treaty of Versailles, which had awarded German holdings to Japan. The New Culture Movement stimulated by the May Fourth Movement waxed strong throughout the 1920s and 1930s. According to Ebrey:\n\nThe discrediting of liberal Western philosophy amongst leftist Chinese intellectuals led to more radical lines of thought inspired by the Russian Revolution, and supported by agents of the Comintern sent to China by Moscow. This created the seeds for the irreconcilable conflict between the left and right in China that would dominate Chinese history for the rest of the century.\n\nIn the 1920s, Sun Yat-sen established a revolutionary base in south China, and set out to unite the fragmented nation. With assistance from the Soviet Union (itself fresh from Lenin's takeover), he entered into an alliance with the fledgling Communist Party of China. After Sun's death from cancer in 1925, one of his protégés, Chiang Kai-shek, seized control of the \"Kuomintang\" (Nationalist Party or KMT) and succeeded in bringing most of south and central China under its rule in a military campaign known as the Northern Expedition (1926–1927). Having defeated the warlords in south and central China by military force, Chiang was able to secure the nominal allegiance of the warlords in the North. In 1927, Chiang turned on the CPC and relentlessly chased the CPC armies and its leaders from their bases in southern and eastern China. In 1934, driven from their mountain bases such as the Chinese Soviet Republic, the CPC forces embarked on the Long March across China's most desolate terrain to the northwest, where they established a guerrilla base at Yan'an in Shaanxi Province. During the Long March, the communists reorganized under a new leader, Mao Zedong (Mao Tse-tung).\n\nThe bitter struggle between the KMT and the CPC continued, openly or clandestinely, through the 14-year-long Japanese occupation of various parts of the country (1931–1945). The two Chinese parties nominally formed a united front to oppose the Japanese in 1937, during the Second Sino-Japanese War (1937–1945), which became a part of World War II. Japanese forces committed numerous war atrocities against the civilian population, including biological warfare (see Unit 731) and the Three Alls Policy (\"Sankō Sakusen\"), the three alls being: \"\"Kill All, Burn All and Loot All\"\".\n\nFollowing the defeat of Japan in 1945, the war between the Nationalist government forces and the CPC resumed, after failed attempts at reconciliation and a negotiated settlement. By 1949, the CPC had established control over most of the country \"(see Chinese Civil War)\". Westad says the Communists won the Civil War because they made fewer military mistakes than Chiang, and because in his search for a powerful centralized government, Chiang antagonized too many interest groups in China. Furthermore, his party was weakened in the war against the Japanese. Meanwhile, the Communists told different groups, such as peasants, exactly what they wanted to hear, and cloaked themselves in the cover of Chinese Nationalism. During the civil war both the Nationalists and Communists carried out mass atrocities, with millions of non-combatants killed by both sides. These included deaths from forced conscription and massacres. When the Nationalist government forces were defeated by CPC forces in mainland China in 1949, the Nationalist government retreated to Taiwan with its forces, along with Chiang and most of the KMT leadership and a large number of their supporters; the Nationalist government had taken effective control of Taiwan at the end of WWII as part of the overall Japanese surrender, when Japanese troops in Taiwan surrendered to Republic of China troops.\n\nMajor combat in the Chinese Civil War ended in 1949 with Kuomintang (KMT) pulling out of the mainland, with the government relocating to Taipei and maintaining control only over a few islands. The Communist Party of China was left in control of mainland China. On 1 October 1949, Mao Zedong proclaimed the People's Republic of China. \"Communist China\" and \"Red China\" were two common names for the PRC.\n\nThe PRC was shaped by a series of campaigns and five-year plans. The economic and social plan known as the Great Leap Forward caused an estimated 45 million deaths. Mao's government carried out mass executions of landowners, instituted collectivisation and implemented the Laogai camp system. Execution, deaths from forced labor and other atrocities resulted in millions of deaths under Mao. In 1966 Mao and his allies launched the Cultural Revolution, which continued until Mao's death a decade later. The Cultural Revolution, motivated by power struggles within the Party and a fear of the Soviet Union, led to a major upheaval in Chinese society.\n\nIn 1972, at the peak of the Sino-Soviet split, Mao and Zhou Enlai met US president Richard Nixon in Beijing to establish relations with the United States. In the same year, the PRC was admitted to the United Nations in place of the Republic of China, with permanent membership of the Security Council.\nA power struggle followed Mao's death in 1976. The Gang of Four were arrested and blamed for the excesses of the Cultural Revolution, marking the end of a turbulent political era in China. Deng Xiaoping outmaneuvered Mao's anointed successor chairman Hua Guofeng, and gradually emerged as the \"de facto\" leader over the next few years.\n\nDeng Xiaoping was the Paramount Leader of China from 1978 to 1992, although he never became the head of the party or state, and his influence within the Party led the country to significant economic reforms. The Communist Party subsequently loosened governmental control over citizens' personal lives and the communes were disbanded with many peasants receiving multiple land leases, which greatly increased incentives and agricultural production. In addition, there were many free market areas opened. The most successful free market areas was shenzhen. It is located in guangdong and the property tax free area still exists today. This turn of events marked China's transition from a planned economy to a mixed economy with an increasingly open market environment, a system termed by some as \"market socialism\", and officially by the Communist Party of China as \"Socialism with Chinese characteristics\". The PRC adopted its current constitution on 4 December 1982.\nIn 1989 the death of former general secretary Hu Yaobang helped to spark the Tiananmen Square protests of that year, during which students and others campaigned for several months, speaking out against corruption and in favour of greater political reform, including democratic rights and freedom of speech. However, they were eventually put down on 4 June when PLA troops and vehicles entered and forcibly cleared the square, with many fatalities. This event was widely reported, and brought worldwide condemnation and sanctions against the government. A filmed incident involving the \"tank man\" was seen worldwide.\n\nCPC general secretary and PRC President Jiang Zemin and PRC Premier Zhu Rongji, both former mayors of Shanghai, led post-Tiananmen PRC in the 1990s. Under Jiang and Zhu's ten years of administration, the PRC's economic performance pulled an estimated 150 million peasants out of poverty and sustained an average annual gross domestic product growth rate of 11.2%. The country formally joined the World Trade Organization in 2001.\n\nAlthough the PRC needs economic growth to spur its development, the government began to worry that rapid economic growth was degrading the country's resources and environment. Another concern is that certain sectors of society are not sufficiently benefiting from the PRC's economic development; one example of this is the wide gap between urban and rural areas. As a result, under former CPC general secretary and President Hu Jintao and Premier Wen Jiabao, the PRC initiated policies to address issues of equitable distribution of resources, but the outcome was not known . More than 40 million farmers were displaced from their land, usually for economic development, contributing to 87,000 demonstrations and riots across China in 2005. For much of the PRC's population, living standards improved very substantially and freedom increased, but political controls remained tight and rural areas poor.\n\n\n"}
{"id": "5762", "url": "https://en.wikipedia.org/wiki?curid=5762", "title": "Civil engineering", "text": "Civil engineering\n\nCivil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including works such as residences, institutional buildings, roads, bridges, canals, dams, airports, sewerage systems, pipelines, and railways. Civil engineering is traditionally broken into a number of sub-disciplines. It is considered the second-oldest engineering discipline after military engineering, and it is defined to distinguish non-military engineering from military engineering. Civil engineering takes place in the public sector from municipal through to national governments, and in the private sector from individual homeowners through to international companies.\n\nCivil engineering is the application of physical and scientific principles for solving the problems of society, and its history is intricately linked to advances in the understanding of physics and mathematics throughout history. Because civil engineering is a wide-ranging profession, including several specialized sub-disciplines, its history is linked to knowledge of structures, materials science, geography, geology, soils, hydrology, environment, mechanics and other fields.\n\nThroughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. Knowledge was retained in guilds and seldom supplanted by advances. Structures, roads, and infrastructure that existed were repetitive, and increases in scale were incremental.\n\nOne of the earliest examples of a scientific approach to physical and mathematical problems applicable to civil engineering is the work of Archimedes in the 3rd century BC, including Archimedes Principle, which underpins our understanding of buoyancy, and practical solutions such as Archimedes' screw. Brahmagupta, an Indian mathematician, used arithmetic in the 7th century AD, based on Hindu-Arabic numerals, for excavation (volume) computations.\n\nEngineering has been an aspect of life since the beginnings of human existence. The earliest practice of civil engineering may have commenced between 4000 and 2000 BC in ancient Egypt, the Indus Valley Civilization, and Mesopotamia (ancient Iraq) when humans started to abandon a nomadic existence, creating a need for the construction of shelter. During this time, transportation became increasingly important leading to the development of the wheel and sailing.\n\nUntil modern times there was no clear distinction between civil engineering and architecture, and the term engineer and architect were mainly geographical variations referring to the same occupation, and often used interchangeably. The construction of pyramids in Egypt (circa 2700–2500 BC) were some of the first instances of large structure constructions. Other ancient historic civil engineering constructions include the Qanat water management system (the oldest is older than 3000 years and longer than 71 km,) the Parthenon by Iktinos in Ancient Greece (447–438 BC), the Appian Way by Roman engineers (c. 312 BC), the Great Wall of China by General Meng T'ien under orders from Ch'in Emperor Shih Huang Ti (c. 220 BC) and the stupas constructed in ancient Sri Lanka like the Jetavanaramaya and the extensive irrigation works in Anuradhapura. The Romans developed civil structures throughout their empire, including especially aqueducts, insulae, harbors, bridges, dams and roads.\n\nIn the 18th century, the term civil engineering was coined to incorporate all things civilian as opposed to military engineering. The first self-proclaimed civil engineer was John Smeaton, who constructed the Eddystone Lighthouse. In 1771 Smeaton and some of his colleagues formed the Smeatonian Society of Civil Engineers, a group of leaders of the profession who met informally over dinner. Though there was evidence of some technical meetings, it was little more than a social society.In 1818 the Institution of Civil Engineers was founded in London, and in 1820 the eminent engineer Thomas Telford became its first president. The institution received a Royal Charter in 1828, formally recognising civil engineering as a profession. Its charter defined civil engineering as:\n\nThe first private college to teach civil engineering in the United States was Norwich University, founded in 1819 by Captain Alden Partridge. The first degree in civil engineering in the United States was awarded by Rensselaer Polytechnic Institute in 1835. The first such degree to be awarded to a woman was granted by Cornell University to Nora Stanton Blatch in 1905.\n\nIn the UK during the early 19th century, the division between civil engineering and military engineering (served by the Royal Military Academy, Woolwich), coupled with the demands of the Industrial Revolution, spawned new engineering education initiatives: the Class of Civil Engineering and Mining was founded at King's College London in 1838, mainly as a response to the growth of the railway system and the need for more qualified engineers, the private College for Civil Engineers in Putney was established in 1839, and the UK's first Chair of Engineering was established at the University of Glasgow in 1840.\n\n\"Civil engineers\" typically possess an academic degree in civil engineering. The length of study is three to five years, and the completed degree is designated as a bachelor of technology, or a bachelor of engineering. The curriculum generally includes classes in physics, mathematics, project management, design and specific topics in civil engineering. After taking basic courses in most sub-disciplines of civil engineering, they move onto specialize in one or more sub-disciplines at advanced levels. While an undergraduate degree (BEng/BSc) normally provides successful students with industry-accredited qualification, some academic institutions offer post-graduate degrees (MEng/MSc), which allow students to further specialize in their particular area of interest.\n\nIn most countries, a bachelor's degree in engineering represents the first step towards professional certification, and a professional body certifies the degree program. After completing a certified degree program, the engineer must satisfy a range of requirements (including work experience and exam requirements) before being certified. Once certified, the engineer is designated as a professional engineer (in the United States, Canada and South Africa), a chartered engineer (in most Commonwealth countries), a chartered professional engineer (in Australia and New Zealand), or a European engineer (in most countries of the European Union). There are international agreements between relevant professional bodies to allow engineers to practice across national borders.\n\nThe benefits of certification vary depending upon location. For example, in the United States and Canada, \"only a licensed professional engineer may prepare, sign and seal, and submit engineering plans and drawings to a public authority for approval, or seal engineering work for public and private clients.\" This requirement is enforced under provincial law such as the Engineers Act in Quebec.\n\nNo such legislation has been enacted in other countries including the United Kingdom. In Australia, state licensing of engineers is limited to the state of Queensland. Almost all certifying bodies maintain a code of ethics which all members must abide by.\n\nEngineers must obey contract law in their contractual relationships with other parties. In cases where an engineer's work fails, they may be subject to the law of tort of negligence, and in extreme cases, criminal charges. An engineer's work must also comply with numerous other rules and regulations such as building codes and environmental law.\n\nThere are a number of sub-disciplines within the broad field of civil engineering. General civil engineers work closely with surveyors and specialized civil engineers to design grading, drainage, pavement, water supply, sewer service, dams, electric and communications supply. General civil engineering is also referred to as site engineering, a branch of civil engineering that primarily focuses on converting a tract of land from one usage to another. Site engineers spend time visiting project sites, meeting with stakeholders, and preparing construction plans. Civil engineers apply the principles of geotechnical engineering, structural engineering, environmental engineering, transportation engineering and construction engineering to residential, commercial, industrial and public works projects of all sizes and levels of construction.\n\n\"Coastal engineering\" is concerned with managing coastal areas. In some jurisdictions, the terms sea defense and coastal protection mean defense against flooding and erosion, respectively. The term coastal defense is the more traditional term, but coastal management has become more popular as the field has expanded to techniques that allow erosion to claim land.\n\n\"Construction engineering\" involves planning and execution, transportation of materials, site development based on hydraulic, environmental, structural and geotechnical engineering. As construction firms tend to have higher business risk than other types of civil engineering firms do, construction engineers often engage in more business-like transactions, for example, drafting and reviewing contracts, evaluating logistical operations, and monitoring prices of supplies.\n\n\"Earthquake engineering\" involves designing structures to withstand hazardous earthquake exposures. Earthquake engineering is a sub-discipline of structural engineering. The main objectives of earthquake engineering are to understand interaction of structures on the shaky ground; foresee the consequences of possible earthquakes; and design, construct and maintain structures to perform at earthquake in compliance with building codes.\n\n\"Environmental engineering\" is the contemporary term for sanitary engineering, though sanitary engineering traditionally had not included much of the hazardous waste management and environmental remediation work covered by environmental engineering. Public health engineering and environmental health engineering are other terms being used.\n\nEnvironmental engineering deals with treatment of chemical, biological, or thermal wastes, purification of water and air, and remediation of contaminated sites after waste disposal or accidental contamination. Among the topics covered by environmental engineering are pollutant transport, water purification, waste water treatment, air pollution, solid waste treatment, and hazardous waste management. Environmental engineers administer pollution reduction, green engineering, and industrial ecology. Environmental engineers also compile information on environmental consequences of proposed actions.\n\n\"Forensic engineering\" is the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property. The consequences of failure are dealt with by the law of product liability. The field also deals with retracing processes and procedures leading to accidents in operation of vehicles or machinery. The subject is applied most commonly in civil law cases, although it may be of use in criminal law cases. Generally the purpose of a Forensic engineering investigation is to locate cause or causes of failure with a view to improve performance or life of a component, or to assist a court in determining the facts of an accident. It can also involve investigation of intellectual property claims, especially patents.\n\n\"Geotechnical engineering\" studies rock and soil supporting civil engineering systems. Knowledge from the field of soil science, materials science, mechanics, and hydraulics is applied to safely and economically design foundations, retaining walls, and other structures. Environmental efforts to protect groundwater and safely maintain landfills have spawned a new area of research called geoenvironmental engineering.\n\nIdentification of soil properties presents challenges to geotechnical engineers. Boundary conditions are often well defined in other branches of civil engineering, but unlike steel or concrete, the material properties and behavior of soil are difficult to predict due to its variability and limitation on investigation. Furthermore, soil exhibits nonlinear (stress-dependent) strength, stiffness, and dilatancy (volume change associated with application of shear stress), making studying soil mechanics all the more difficult. Geotechnical engineers frequently work with professional geologists and soil scientists.\n\n\"Materials science\" is closely related to civil engineering. It studies fundamental characteristics of materials, and deals with ceramics such as concrete and mix asphalt concrete, strong metals such as aluminum and steel, and thermosetting polymers including polymethylmethacrylate (PMMA) and carbon fibers.\n\n\"Materials engineering\" involves protection and prevention (paints and finishes). Alloying combines two types of metals to produce another metal with desired properties. It incorporates elements of applied physics and chemistry. With recent media attention on nanoscience and nanotechnology, materials engineering has been at the forefront of academic research. It is also an important part of forensic engineering and failure analysis.\n\n\"Structural engineering\" is concerned with the structural design and structural analysis of buildings, bridges, towers, flyovers (overpasses), tunnels, off shore structures like oil and gas fields in the sea, aerostructure and other structures. This involves identifying the loads which act upon a structure and the forces and stresses which arise within that structure due to those loads, and then designing the structure to successfully support and resist those loads. The loads can be self weight of the structures, other dead load, live loads, moving (wheel) load, wind load, earthquake load, load from temperature change etc. The structural engineer must design structures to be safe for their users and to successfully fulfill the function they are designed for (to be \"serviceable\"). Due to the nature of some loading conditions, sub-disciplines within structural engineering have emerged, including wind engineering and earthquake engineering.\n\nDesign considerations will include strength, stiffness, and stability of the structure when subjected to loads which may be static, such as furniture or self-weight, or dynamic, such as wind, seismic, crowd or vehicle loads, or transitory, such as temporary construction loads or impact. Other considerations include cost, constructability, safety, aesthetics and sustainability.\n\n\"Surveying\" is the process by which a surveyor measures certain dimensions that occur on or near the surface of the Earth. Surveying equipment, such as levels and theodolites, are used for accurate measurement of angular deviation, horizontal, vertical and slope distances. With computerisation, electronic distance measurement (EDM), total stations, GPS surveying and laser scanning have to a large extent supplanted traditional instruments. Data collected by survey measurement is converted into a graphical representation of the Earth's surface in the form of a map. This information is then used by civil engineers, contractors and realtors to design from, build on, and trade, respectively. Elements of a structure must be sized and positioned in relation to each other and to site boundaries and adjacent structures. Although surveying is a distinct profession with separate qualifications and licensing arrangements, civil engineers are trained in the basics of surveying and mapping, as well as geographic information systems. Surveyors also lay out the routes of railways, tramway tracks, highways, roads, pipelines and streets as well as position other infrastructure, such as harbors, before construction.\n\n\nIn the United States, Canada, the United Kingdom and most Commonwealth countries land surveying is considered to be a separate and distinct profession. Land surveyors are not considered to be engineers, and have their own professional associations and licensing requirements. The services of a licensed land surveyor are generally required for boundary surveys (to establish the boundaries of a parcel using its legal description) and subdivision plans (a plot or map based on a survey of a parcel of land, with boundary lines drawn inside the larger parcel to indicate the creation of new boundary lines and roads), both of which are generally referred to as Cadastral surveying.\n\nConstruction surveying is generally performed by specialised technicians. Unlike land surveyors, the resulting plan does not have legal status. Construction surveyors perform the following tasks:\n\n\"Transportation engineering\" is concerned with moving people and goods efficiently, safely, and in a manner conducive to a vibrant community. This involves specifying, designing, constructing, and maintaining transportation infrastructure which includes streets, canals, highways, rail systems, airports, ports, and mass transit. It includes areas such as transportation design, transportation planning, traffic engineering, some aspects of urban engineering, queueing theory, pavement engineering, Intelligent Transportation System (ITS), and infrastructure management.\n\n\"Municipal engineering\" is concerned with municipal infrastructure. This involves specifying, designing, constructing, and maintaining streets, sidewalks, water supply networks, sewers, street lighting, municipal solid waste management and disposal, storage depots for various bulk materials used for maintenance and public works (salt, sand, etc.), public parks and cycling infrastructure. In the case of underground utility networks, it may also include the civil portion (conduits and access chambers) of the local distribution networks of electrical and telecommunications services. It can also include the optimizing of waste collection and bus service networks. Some of these disciplines overlap with other civil engineering specialties, however municipal engineering focuses on the coordination of these infrastructure networks and services, as they are often built simultaneously, and managed by the same municipal authority. Municipal engineers may also design the site civil works for large buildings, industrial plants or campuses (i.e. access roads, parking lots, potable water supply, treatment or pretreatment of waste water, site drainage, etc.)\n\n\"Water resources engineering\" is concerned with the collection and management of water (as a natural resource). As a discipline it therefore combines elements of hydrology, environmental science, meteorology, conservation, and resource management. This area of civil engineering relates to the prediction and management of both the quality and the quantity of water in both underground (aquifers) and above ground (lakes, rivers, and streams) resources. Water resource engineers analyze and model very small to very large areas of the earth to predict the amount and content of water as it flows into, through, or out of a facility. Although the actual design of the facility may be left to other engineers.\n\n\"Hydraulic engineering\" is concerned with the flow and conveyance of fluids, principally water. This area of civil engineering is intimately related to the design of pipelines, water supply network, drainage facilities (including bridges, dams, channels, culverts, levees, storm sewers), and canals. Hydraulic engineers design these facilities using the concepts of fluid pressure, fluid statics, fluid dynamics, and hydraulics, among others.\nCivil engineering systems is a discipline that promotes the use of systems thinking to manage complexity and change in civil engineering within its wider public context. It posits that the proper development of civil engineering infrastructure requires a holistic, coherent understanding of the relationships between all of the important factors that contribute to successful projects while at the same time emphasising the importance of attention to technical detail. Its purpose is to help integrate the entire civil engineering project life cycle from conception, through planning, designing, making, operating to decommissioning.\n\n\n"}
{"id": "5763", "url": "https://en.wikipedia.org/wiki?curid=5763", "title": "Cantonese (disambiguation)", "text": "Cantonese (disambiguation)\n\nCantonese is a language originating in Guangzhou City, Southern China\n\nCantonese may also refer to:\n\n"}
{"id": "5765", "url": "https://en.wikipedia.org/wiki?curid=5765", "title": "Çatalhöyük", "text": "Çatalhöyük\n\nÇatalhöyük (; also \"Çatal Höyük\" and \"Çatal Hüyük\"; from Turkish \"çatal\" \"fork\" + \"höyük\" \"tumulus\") was a very large Neolithic and Chalcolithic proto-city settlement in southern Anatolia, which existed from approximately 7500 BC to 5700 BC, and flourished around 7000 BC. In July 2012, it was inscribed as a UNESCO World Heritage Site.\n\nÇatalhöyük is located overlooking the Konya Plain, southeast of the present-day city of Konya (ancient Iconium) in Turkey, approximately 140 km (87 mi) from the twin-coned volcano of Mount Hasan. The eastern settlement forms a mound which would have risen about 20 m (66 ft) above the plain at the time of the latest Neolithic occupation. There is also a smaller settlement mound to the west and a Byzantine settlement a few hundred meters to the east. The prehistoric mound settlements were abandoned before the Bronze Age. A channel of the Çarşamba River once flowed between the two mounds, and the settlement was built on alluvial clay which may have been favorable for early agriculture.\n\nThe site was first excavated by James Mellaart in 1958. He later led a team which further excavated there for four seasons between 1961 and 1965. These excavations revealed this section of Anatolia as a centre of advanced culture in the Neolithic period. Excavation revealed 18 successive layers of buildings signifying various stages of the settlement and eras of history. The bottom layer of buildings can be dated as early as 7100 BC while the top layer is of 5600 BC.\n\nMellaart was banned from Turkey for his involvement in the Dorak affair in which he published drawings of supposedly important Bronze Age artifacts that later went missing. After this scandal, the site lay idle until 1993, when investigations began under the leadership of Ian Hodder, then at the University of Cambridge. These investigations are among the most ambitious excavation projects currently in progress according to archaeologist Colin Renfrew, among others. In addition to extensive use of archaeological science, psychological and artistic interpretations of the symbolism of the wall paintings have been employed. Hodder, a former student of Mellaart, chose the site as the first \"real world\" test of his then-controversial theory of post-processual archaeology. The site has always had a strong research emphasis upon engagement with digital methodologies, driven by the project's experimental and reflexive methodological framework. Sponsors and collaborators of the current dig include Yapi Kredi, Boeing, University of York, Selçuk University, British Institute at Ankara, Cardiff University, Stanford University, Turkish Cultural Foundation, and University at Buffalo.\n\nÇatalhöyük was composed entirely of domestic buildings, with no obvious public buildings. While some of the larger ones have rather ornate murals, the purpose of some rooms remains unclear.\n\nThe population of the eastern mound has been estimated to be, at maximum, 10,000 people, but the population likely varied over the community’s history. An average population of between 5,000 and 7,000 is a reasonable estimate. The sites were set up as large numbers of buildings clustered together. Households looked to their neighbors for help, trade, and possible marriage for their children. The inhabitants lived in mudbrick houses that were crammed together in an aggregate structure. No footpaths or streets were used between the dwellings, which were clustered in a honeycomb-like maze. Most were accessed by holes in the ceiling and doors on the side of the houses, with doors reached by ladders and stairs. The rooftops were effectively streets. The ceiling openings also served as the only source of ventilation, allowing smoke from the houses' open hearths and ovens to escape. \n\nHouses had plaster interiors characterized by squared-off timber ladders or steep stairs. These were usually on the south wall of the room, as were cooking hearths and ovens. The main rooms contained raised platforms that may have been used for a range of domestic activities. Typical houses contained two rooms for everyday activity, such as cooking and crafting. All interior walls and platforms were plastered to a smooth finish. Ancillary rooms were used as storage, and were accessed through low openings from main rooms.\n\nAll rooms were kept scrupulously clean. Archaeologists identified very little rubbish in the buildings, finding middens outside the ruins, with sewage and food waste, as well as significant amounts of wood ash. In good weather, many daily activities may also have taken place on the rooftops, which may have formed a plaza. In later periods, large communal ovens appear to have been built on these rooftops. Over time, houses were renewed by partial demolition and rebuilding on a foundation of rubble, which was how the mound was gradually built up. As many as eighteen levels of settlement have been uncovered.\n\nAs a part of ritual life, the people of Çatalhöyük buried their dead within the village. Human remains have been found in pits beneath the floors and, especially, beneath hearths, the platforms within the main rooms, and under beds. Bodies were tightly flexed before burial and were often placed in baskets or wound and wrapped in reed mats. Disarticulated bones in some graves suggest that bodies may have been exposed in the open air for a time before the bones were gathered and buried. In some cases, graves were disturbed, and the individual’s head removed from the skeleton. These heads may have been used in rituals, as some were found in other areas of the community. In a woman's grave spinning whorls were recovered and in a man's grave, stone axes. Some skulls were plastered and painted with ochre to recreate faces, a custom more characteristic of Neolithic sites in Syria and at Neolithic Jericho than at sites closer by.\n\nVivid murals and figurines are found throughout the settlement, on interior and exterior walls. Distinctive clay figurines of women, notably the Seated Woman of Çatalhöyük, have been found in the upper levels of the site. Although no identifiable temples have been found, the graves, murals, and figurines suggest that the people of Çatalhöyük had a religion rich in symbols. Rooms with concentrations of these items may have been shrines or public meeting areas. Predominant images include men with erect phalluses, hunting scenes, red images of the now extinct aurochs (wild cattle) and stags, and vultures swooping down on headless figures. Relief figures are carved on walls, such as of lionesses facing one another.\n\nHeads of animals, especially of cattle, were mounted on walls. A painting of the village, with the twin mountain peaks of Hasan Dağ in the background, is frequently cited as the world's oldest map, and the first landscape painting. However, some archaeologists question this interpretation. Stephanie Meece, for example, argues that it is more likely a painting of a leopard skin instead of a volcano, and a decorative geometric design instead of a map.\n\nA striking feature of Çatalhöyük are its female figurines. Mellaart, the original excavator, argued that these well-formed, carefully made figurines, carved and molded from marble, blue and brown limestone, schist, calcite, basalt, alabaster, and clay, represented a female deity. Although a male deity existed as well, \"statues of a female deity far outnumber those of the male deity, who moreover, does not appear to be represented at all after Level VI\". To date, eighteen levels have been identified. These artfully-hewn figurines were found primarily in areas Mellaart believed to be shrines. The stately goddess seated on a throne flanked by two lionesses (\"illustration\") was found in a grain bin, which Mellaart suggests might have been a means of ensuring the harvest or protecting the food supply. In later cultures, similar depictions are seen of Cybele, a mountain goddess.\n\nWhereas Mellaart excavated nearly two hundred buildings in four seasons, the current excavator, Ian Hodder, spent an entire season excavating one building alone. Hodder and his team, in 2004 and 2005, began to believe that the patterns suggested by Mellaart were false. They found one similar figurine, but the vast majority did not imitate the Mother Goddess style that Mellaart suggested. Instead of a Mother Goddess culture, Hodder points out that the site gives little indication of a matriarchy or patriarchy.\n\nIn an article in the \"Turkish Daily News\", Hodder is reported as denying that Çatalhöyük was a matriarchal society and quoted as saying \"When we look at what they eat and drink and at their social statues, we see that men and women had the same social status. There was a balance of power. Another example is the skulls found. If one's social status was of high importance in Çatalhöyük, the body and head were separated after death. The number of female and male skulls found during the excavations is almost equal.\" In another article in the \"Hurriyet Daily News\" Hodder is reported to say \"We have learned that men and women were equally approached\".\n\nIn a report in September 2009 on the discovery of around 2000 figurines Hodder is quoted as saying:\nÇatalhöyük was excavated in the 1960s in a methodical way, but not using the full range of natural science techniques that are available to us today. Sir James Mellaart who excavated the site in the 1960s came up with all sorts of ideas about the way the site was organized and how it was lived in and so on ... We’ve now started working there since the mid 1990s and come up with very different ideas about the site. One of the most obvious examples of that is that Çatalhöyük is perhaps best known for the idea of the mother goddess. But our work more recently has tended to show that in fact there is very little evidence of a mother goddess and very little evidence of some sort of female-based matriarchy. That’s just one of the many myths that the modern scientific work is undermining.\n\nProfessor Lynn Meskell explained that while the original excavations had found only 200 figures, the new excavations had uncovered 2,000 figurines of which most were animals, with less than 5% of the figurines women.\n\nEstonian folklorist Uku Masing has suggested as early as in 1976, that Çatalhöyük was probably a hunting and gathering religion and the Mother Goddess figurine did not represent a female deity. He implied that perhaps a longer period of time was needed in order to develop symbols for agricultural rites. His theory was developed in the paper \"Some remarks on the mythology of the people of Catal Hüyük\".\n\nÇatalhöyük has strong evidence of an egalitarian society, as no houses with distinctive features (belonging to royalty or religious hierarchy, for example) have been found so far. The most recent investigations also reveal little social distinction based on gender, with men and women receiving equivalent nutrition and seeming to have equal social status, as typically found in Paleolithic cultures. Children observed domestic areas. They learned how to perform rituals and how to build or repair houses by watching the adults make statues, beads and other objects.\nÇatalhöyük's spatial layout may be due to the close kin relations exhibited amongst the people. It can be seen, in the layout, that the people were \"divided into two groups who lived on opposite sides of the town, separated by a gully.\" Furthermore, because no nearby towns were found from which marriage partners could be drawn, \"this spatial separation must have marked two intermarrying kinship groups.\" This would help explain how a settlement so early on would become so large.\n\nIn upper levels of the site, it becomes apparent that the people of Çatalhöyük were gaining skills in agriculture and the domestication of animals. Female figurines have been found within bins used for storage of cereals, such as wheat and barley, and the figurines are presumed to be of a deity protecting the grain. Peas were also grown, and almonds, pistachios, and fruit were harvested from trees in the surrounding hills. Sheep were domesticated and evidence suggests the beginning of cattle domestication as well. However, hunting continued to be a major source of food for the community. Pottery and obsidian tools appear to have been major industries; obsidian tools were probably both used and also traded for items such as Mediterranean sea shells and flint from Syria. There is also evidence that the settlement was the first place in the world to mine and smelt metal in the form of lead. Noting the lack of hierarchy and economic inequality, historian Murray Bookchin has argued that Çatalhöyük was an early example of anarcho-communism.\n\n\n\n"}
{"id": "5766", "url": "https://en.wikipedia.org/wiki?curid=5766", "title": "Clement Attlee", "text": "Clement Attlee\n\nClement Richard Attlee, 1st Earl Attlee, (3 January 1883 – 8 October 1967) was a British statesman and Labour Party politician who served as Prime Minister of the United Kingdom from 1945 to 1951.\n\nHe was the Leader of the Labour Party from 1935 to 1955. In 1940, Attlee took Labour into the wartime coalition government and served under Winston Churchill, becoming, in 1942, the first person to hold the office of Deputy Prime Minister of the United Kingdom. He went on to lead the Labour Party to an unexpected landslide victory at the 1945 general election; forming the first Labour majority government, and a mandate to implement its postwar reforms. The 12 per cent national swing from the Conservatives to Labour was unprecedented at that time and remains the largest ever achieved by any party at a general election in British electoral history. He was re-elected with a narrow majority at the 1950 general election. In the following year, Attlee called a snap general election, hoping to increase his parliamentary majority. However, he was narrowly defeated by the Conservatives under the leadership of Winston Churchill, despite winning the most votes of any political party in any general election in British political history until the Conservative Party's fourth consecutive victory in 1992. Attlee remains the longest-ever serving Leader of the Labour Party. \n\nFirst elected to the House of Commons in 1922 as the MP for Limehouse, Attlee rose quickly to become a junior minister in the first Labour minority government led by Ramsay MacDonald in 1924, and then joined the Cabinet during MacDonald's second ministry of 1929–1931. One of only a handful of Labour frontbenchers to retain his seat in the landslide defeat of 1931, he became the party's Deputy Leader. After the resignation of George Lansbury in 1935, he was elected as Leader of the Labour Party and Leader of the Opposition in the subsequent leadership election. At first advocating pacificism and opposing rearmament, he later reversed his position; by 1938, he became a strong critic of Neville Chamberlain's attempts to appease Adolf Hitler and Benito Mussolini. He took Labour into the Churchill war ministry in 1940. Initially serving as Lord Privy Seal, he was appointed as Deputy Prime Minister in 1942. Attlee and Churchill worked together very smoothly, with Attlee working backstage to handle much of the detail and organisational work in Parliament, as Churchill took centre stage with his attention on diplomacy, military policy, and broader issues. With victory in Europe in May 1945, the coalition government was dissolved. Attlee led Labour to win a huge majority in the ensuing 1945 general election two months later.\nThe government he led built the post-war consensus, based upon the assumption that full employment would be maintained by Keynesian policies and that a greatly enlarged system of social services would be created – aspirations that had been outlined in the 1942 Beveridge Report. Within this context, his government undertook the nationalisation of public utilities and major industries, as well as the creation of the National Health Service. Attlee himself had little interest in economic matters but this settlement was broadly accepted by all parties for three decades. Foreign policy was the special domain of Ernest Bevin, but Attlee took special interest in India. He supervised the process by which India was partitioned into India and Pakistan in 1947. He also arranged the independence of Burma (Myanmar), and Ceylon (Sri Lanka). His government ended the British Mandates of Palestine and Jordan. From 1947 onwards, he and Bevin pushed the United States to take a more vigorous role in the emerging Cold War against Soviet Communism. When the budgetary crisis forced Britain out of Greece in 1947, he called on Washington to counter the Communists with the Truman Doctrine. He avidly supported the Marshall Plan to rebuild Western Europe with American money. In 1949, he promoted the NATO military alliance against the Soviet bloc. He sent British troops to fight in the Malayan Emergency in 1948 and sent the RAF to participate in the Berlin Airlift. He commissioned an independent nuclear deterrent for the UK. He used 13,000 troops and passed special legislation to promptly end the London dock strike in 1949. After leading Labour to a narrow victory at the 1950 general election, he sent British troops to fight in the Korean War. Attlee was narrowly defeated by the Conservatives under Churchill in the 1951 general election. He continued as Labour leader but had lost his effectiveness by then. He retired after losing the 1955 general election and was elevated to the House of Lords.\nIn public, Attlee was modest and unassuming; he was ineffective at public relations and lacked charisma. His strengths emerged behind the scenes, especially in committees where his depth of knowledge, quiet demeanour, objectivity, and pragmatism proved decisive. His achievements in politics owed much to lucky breaks and the unsuitability of his rivals. He saw himself as spokesman on behalf of his entire party and successfully kept its multiple factions in harness. Attlee is consistently rated by scholars, critics and the public as one of the greatest British Prime Ministers. His reputation among scholars in recent decades has been much higher than during his years as Prime Minister, thanks to his roles in leading the Labour Party, creating the welfare state and building the coalition opposing Stalin in the Cold War.\n\nAttlee was born on 3 January 1883 in Putney, Surrey (now part of London), into a middle-class family, the seventh of eight children. His father was Henry Attlee (1841–1908), a solicitor, and his mother was Ellen Bravery Watson (1847–1920), daughter of Thomas Simons Watson, secretary for the Art Union of London. He was educated at Northaw School, a boys' preparatory school near Pluckley in Kent; Haileybury College; and University College, Oxford, where in 1904 he graduated as a Bachelor of Arts with second-class honours in modern history.\n\nAttlee then trained as a barrister at the Inner Temple and was called to the bar in March 1906. He worked for a time at his father's law firm Druces and Attlee but did not enjoy the work, and had no particular ambition to succeed in the legal profession. He also played football for non-League club Fleet.\n\nIn 1906, he became a volunteer at Haileybury House, a charitable club for working-class boys in Stepney in the East End of London run by his old school, and from 1907 to 1909 he served as the club's manager. Until then, his political views had been more conservative. However, after his shock at the poverty and deprivation he saw while working with the slum children, he came to the view that private charity would never be sufficient to alleviate poverty and that only direct action and income redistribution by the state would have any serious effect. This sparked a process that caused him to convert to socialism. He subsequently joined the Independent Labour Party (ILP) in 1908 and became active in local politics. In 1909, he stood unsuccessfully at his first election, as an ILP candidate for Stepney Borough Council.\n\nHe also worked briefly as a secretary for Beatrice Webb in 1909, before becoming a secretary for Toynbee Hall. In 1911, he was employed by the UK Government as an \"official explainer\"—touring the country to explain Chancellor of the Exchequer David Lloyd George's National Insurance Act. He spent the summer of that year touring Essex and Somerset on a bicycle, explaining the act at public meetings. A year later, he became a lecturer at the London School of Economics.\n\nFollowing the outbreak of the First World War in August 1914, Attlee applied to join the British Army. Initially his application was turned down, as at age 31 he was seen as too old; however, he was finally allowed to join in September, and was commissioned in the rank of Captain with the 6th (Service) Battalion, South Lancashire Regiment, part of the 38th Brigade of the 13th (Western) Division, and was sent to fight in the Gallipoli Campaign in Turkey. His decision to fight caused a rift between him and his older brother Tom, who, as a conscientious objector, spent much of the war in prison.\n\nAfter a period fighting in Gallipoli, he collapsed after falling ill with dysentery and was put on a ship bound for England to recover. When he woke up he wanted to get back to action as soon as possible, and asked to be let off the ship in Malta where he stayed in hospital to recover. His hospitalisation coincided with the Battle of Sari Bair, which saw a large number of his comrades killed. Upon returning to action, he was informed that his company had been chosen to hold the final lines during the evacuation of Suvla. As such, he was the penultimate man to be evacuated from Suvla Bay, the last being General Stanley Maude.\n\nThe Gallipoli Campaign had been engineered by the First Lord of the Admiralty, Winston Churchill. Although it was unsuccessful, Attlee believed that it was a bold strategy, which could have been a success if it had been better implemented on the ground. This gave him an admiration for Churchill as a military strategist, which would make their working relationship in later years productive.\n\nHe later served in the Mesopotamian Campaign in what is now Iraq, where in April 1916 he was badly wounded, being hit in the leg by shrapnel while storming an enemy trench during the Battle of Hanna. He was sent firstly to India, and then back to the UK to recover. In February 1917, he was promoted to the rank of Major, leading him to be known as \"Major Attlee\" for much of the inter-war period. He would spend most of 1917 training soldiers at various locations in England. From 2 to 9 July 1917, he was the temporary commanding officer (CO) of the newly formed L (later 10th) Battalion, the Tank Corps at Bovington Camp, Dorset. From 9 July, he assumed command of 30th Company of the same battalion however he did not deploy to France with it in December 1917.\n\nAfter fully recovering from his injuries, he was sent to France in June 1918 to serve on the Western Front for the final months of the war. After being discharged from the Army in January 1919, he returned to Stepney, and returned to his old job lecturing part-time at the London School of Economics.\n\nAttlee met Violet Millar while on a long trip with friends to Italy in 1921. They bonded immediately and were soon engaged, marrying at Christ Church, Hampstead, on 10 January 1922. It would come to be a devoted marriage, with Attlee providing protection and Violet providing a home that was an escape for Attlee from political turmoil. She died in 1964. They had four children:\n\nAttlee returned to local politics in the immediate post-war period, becoming mayor of the Metropolitan Borough of Stepney, one of London's most deprived inner-city boroughs, in 1919. During his time as mayor, the council undertook action to tackle slum landlords who charged high rents but refused to spend money on keeping their property in habitable condition. The council served and enforced legal orders on homeowners to repair their property. It also appointed health visitors and sanitary inspectors, reducing the infant mortality rate, and took action to find work for returning unemployed ex-servicemen.\n\nIn 1920, while mayor, he wrote his first book, \"The Social Worker\", which set out many of the principles that informed his political philosophy and that were to underpin the actions of his government in later years. The book attacked the idea that looking after the poor could be left to voluntary action. He wrote on page 30:In a civilised community, although it may be composed of self-reliant individuals, there will be some persons who will be unable at some period of their lives to look after themselves, and the question of what is to happen to them may be solved in three ways – they may be neglected, they may be cared for by the organised community as of right, or they may be left to the goodwill of individuals in the community.\n\nand went on to say at page 75:Charity is only possible without loss of dignity between equals. A right established by law, such as that to an old age pension, is less galling than an allowance made by a rich man to a poor one, dependent on his view of the recipient's character, and terminable at his caprice.\n\nIn 1921, George Lansbury, the Labour mayor of the neighbouring borough of Poplar, and future Labour Party leader, launched the Poplar Rates Rebellion; a campaign of disobedience seeking to equalise the poor relief burden across all the London boroughs. Attlee, who was a personal friend of Lansbury strongly supported this, however Herbert Morrison, the Labour mayor of nearby Hackney, and one of the main figures in the London Labour Party, strongly denounced Lansbury and the rebellion. During this period, Attlee developed a lifelong dislike of Morrison.\n\nAt the 1922 general election, Attlee became the Member of Parliament (MP) for the constituency of Limehouse in Stepney. At the time, he admired Ramsay MacDonald and helped him get elected as Labour Party leader at the 1922 leadership election. He served as MacDonald's Parliamentary Private Secretary for the brief 1922 parliament. His first taste of ministerial office came in 1924, when he served as Under-Secretary of State for War in the short-lived first Labour government, led by MacDonald.\n\nAttlee opposed the 1926 General Strike, believing that strike action should not be used as a political weapon. However, when it happened, he did not attempt to undermine it. At the time of the strike, he was chairman of the Stepney Borough Electricity Committee. He negotiated a deal with the Electrical Trade Union so that they would continue to supply power to hospitals, but would end supplies to factories. One firm, Scammell and Nephew Ltd, took a civil action against Attlee and the other Labour members of the committee (although not against the Conservative members who had also supported this). The court found against Attlee and his fellow councillors and they were ordered to pay £300 damages. The decision was later reversed on appeal, but the financial problems caused by the episode almost forced Attlee out of politics.\n\nIn 1927, he was appointed a member of the multi-party Simon Commission, a royal commission set up to examine the possibility of granting self-rule to India. Due to the time he needed to devote to the commission, and contrary to a promise MacDonald made to Attlee to induce him to serve on the commission, he was not initially offered a ministerial post in the Second Labour Government, which entered office after the 1929 general election. Attlee's service on the Commission equipped him with a thorough exposure to India and many of its political leaders. By 1933 he argued that British rule was alien to India and was unable to make the social and economic reforms necessary for India's progress. He became the British leader most sympathetic to Indian independence (as a dominion), preparing him for his role in deciding on independence in 1947.\n\nIn May 1930, Labour MP Oswald Mosley left the party after its rejection of his proposals for solving the unemployment problem, and Attlee was given Mosley's post of Chancellor of the Duchy of Lancaster. In March 1931, he became Postmaster General, a post he held for five months until August, when the Labour government fell, after failing to agree on how to tackle the financial crisis of the Great Depression. That month MacDonald and a few of his allies formed a National Government with the Conservatives and Liberals, leading them to be expelled from Labour. MacDonald offered Attlee a job in the National Government, but he turned down the offer and opted to stay loyal to the main Labour party.\n\nAfter Ramsay MacDonald formed the National Government, Labour was deeply divided. Attlee had long been close to MacDonald and now felt betrayed—as did most Labour politicians. During the course of the second Labour government, Attlee had become increasingly disillusioned with MacDonald, whom he came to regard as vain and incompetent, and of whom he later wrote scathingly in his autobiography. He would write:\n\nIn the old days I had looked up to MacDonald as a great leader. He had a fine presence and great oratorical power. The unpopular line which he took during the First World War seemed to mark him as a man of character. Despite his mishandling of the Red Letter episode, I had not appreciated his defects until he took office a second time. I then realised his reluctance to take positive action and noted with dismay his increasing vanity and snobbery, while his habit of telling me, a junior Minister, the poor opinion he had of all his Cabinet colleagues made an unpleasant impression. I had not, however, expected that he would perpetrate the greatest betrayal in the political history of this country... The shock to the Party was very great, especially to the loyal workers of the rank-and-file who had made great sacrifices for these men.\n\nThe 1931 general election held later that year was a disaster for the Labour Party, which lost over 200 seats, returning only 52 MPs to Parliament. The vast majority of the party's senior figures lost their seats, including the Leader Arthur Henderson, Attlee narrowly retained his Limehouse seat in the election, with his majority being slashed from 7,288 to just 551. He was one of only three Labour MPs who had experience of government to retain their seats, along with George Lansbury and Stafford Cripps, accordingly Lansbury was elected Leader unopposed with Attlee as his deputy.\n\nMost of the remaining Labour MPs after 1931 were elderly trade union officials who could not contribute much to debates, Lansbury was in his 70s, and Stafford Cripps another main figure of the Labour front bench who had entered Parliament in 1931, was inexperienced. As one of the most capable and experienced of the remaining Labour MPs, Attlee therefore shouldered a lot of the burden of providing an opposition to the National Government in the years 1931–35, during this time he had to extend his knowledge of subjects which he had not studied in any depth before, such as finance and foreign affairs in order to provide an effective opposition to the government.\n\nAttlee effectively served as acting leader for nine months from December 1933, after Lansbury fractured his thigh in an accident, which raised Attlee's public profile considerably. It was during this period, however, that personal financial problems almost forced Attlee to quit politics altogether. His wife had become ill, and at that time there was no separate salary for the Leader of the Opposition. On the verge of resigning from Parliament, he was persuaded to stay by Stafford Cripps, a wealthy socialist, who agreed to make a donation to party funds to pay him an additional salary until Lansbury could take over again.\n\nDuring 1932–33 Attlee flirted with, and then drew back from radicalism, influenced by Stafford Cripps who was then on the radical wing of the party, he was briefly a member of the Socialist League, which had been formed by former Independent Labour Party (ILP) members, who opposed the ILP's disaffiliation from the main Labour Party in 1932. At one point he agreed with the proposition put forward by Cripps that gradual reform was inadequate and that a socialist government would have to pass an emergency powers act, allowing it to rule by decree to overcome any opposition by vested interests until it was safe to restore democracy. He admired Oliver Cromwell's strong-armed rule and use of major generals to control England. After looking more closely at Hitler, Mussolini, Stalin, and even his former colleague Oswald Mosley, leader of the new blackshirt fascist movement in Britain, Attlee retreated from his radicalism, and distanced himself from the League, and argued instead that the Labour Party must adhere to constitutional methods and stand forthright for democracy and against totalitarianism of either the left or right. He always supported the crown, and as Prime Minister was close to King George VI.\n\nGeorge Lansbury, a committed pacifist, resigned as the Leader of the Labour Party at the 1935 Party Conference on 8 October, after delegates voted in favour of sanctions against Italy for its aggression against Abyssinia. Lansbury had strongly opposed the policy, and felt unable to continue leading the party. Taking advantage of the disarray in the Labour Party, the Prime Minister Stanley Baldwin announced on 19 October that a general election would be held on 14 November. With no time for a leadership contest, the party agreed that Attlee should serve as interim leader, on the understanding that a leadership election would be held after the general election. Attlee therefore led Labour through the 1935 election, which saw the party stage a partial comeback from its disastrous 1931 performance, winning 38 per cent of the vote, the highest share Labour had won up to that point, and gaining over one hundred seats.\n\nAttlee stood in the subsequent leadership election, held soon after, where he was opposed by Herbert Morrison, who had just re-entered parliament in the recent election, and Arthur Greenwood: Morrison was seen as the favourite, but was distrusted by many sections of the party, especially the left-wing. Arthur Greenwood meanwhile was a popular figure in the party; however his leadership bid was severely hampered by his alcohol problem. Attlee was able to come across as a competent and unifying figure, particularly having already led the party through a general election. He went on to come first in both the first and second ballots, formally being elected Leader of the Labour Party on 3 December 1935.\n\nThroughout the 1920s and most of the 1930s, the Labour Party's official policy had been to oppose rearmament, instead supporting internationalism and collective security under the League of Nations. At the 1934 Labour Party Conference, Attlee declared that, \"We have absolutely abandoned any idea of nationalist loyalty. We are deliberately putting a world order before our loyalty to our own country. We say we want to see put on the statute book something which will make our people citizens of the world before they are citizens of this country.\" During a debate on defence in Commons a year later, Attlee said \"We are told (in the White Paper) that there is danger against which we have to guard ourselves. We do not think you can do it by national defence. We think you can only do it by moving forward to a new world. A world of law, the abolition of national armaments with a world force and a world economic system. I shall be told that that is quite impossible.\" Shortly after those comments, Adolf Hitler proclaimed that German rearmament offered no threat to world peace. Attlee responded the next day noting that Hitler's speech, although containing unfavourable references to the Soviet Union, created \"A chance to call a halt in the armaments race...We do not think that our answer to Herr Hitler should be just rearmament. We are in an age of rearmaments, but we on this side cannot accept that position.\"\n\nIn April 1936, the Chancellor of the Exchequer, Neville Chamberlain, introduced a Budget which increased the amount spent on the armed forces. Attlee made a radio broadcast in opposition to it, saying the budget:\nIn June 1936, the Conservative MP Duff Cooper called for an Anglo-French alliance against possible German aggression and called for all parties to support one. Attlee condemned this: \"We say that any suggestion of an alliance of this kind—an alliance in which one country is bound to another, right or wrong, by some overwhelming necessity—is contrary to the spirit of the League of Nations, is contrary to the Covenant, is contrary to Locarno is contrary to the obligations which this country has undertaken, and is contrary to the professed policy of this Government.\" At the Labour Party conference at Edinburgh in October Attlee reiterated that \"There can be no question of our supporting the Government in its rearmament policy.\"\n\nHowever, with the rising threat from Nazi Germany, and the ineffectiveness of the League of Nations, this policy eventually lost credibility. By 1937, Labour had jettisoned its pacifist position and came to support rearmament and oppose Neville Chamberlain's policy of appeasement.\n\nIn 1938, Attlee opposed the Munich Agreement, in which Chamberlain negotiated with Hitler to give Germany the German-speaking parts of Czechoslovakia, the Sudetenland: We all feel relief that war has not come this time. Every one of us has been passing through days of anxiety; we cannot, however, feel that peace has been established, but that we have nothing but an armistice in a state of war. We have been unable to go in for care-free rejoicing. We have felt that we are in the midst of a tragedy. We have felt humiliation. This has not been a victory for reason and humanity. It has been a victory for brute force. At every stage of the proceedings there have been time limits laid down by the owner and ruler of armed force. The terms have not been terms negotiated; they have been terms laid down as ultimata. We have seen to-day a gallant, civilised and democratic people betrayed and handed over to a ruthless despotism. We have seen something more. We have seen the cause of democracy, which is, in our view, the cause of civilisation and humanity, receive a terrible defeat. ... The events of these last few days constitute one of the greatest diplomatic defeats that this country and France have ever sustained. There can be no doubt that it is a tremendous victory for Herr Hitler. Without firing a shot, by the mere display of military force, he has achieved a dominating position in Europe which Germany failed to win after four years of war. He has overturned the balance of power in Europe. He has destroyed the last fortress of democracy in Eastern Europe which stood in the way of his ambition. He has opened his way to the food, the oil and the resources which he requires in order to consolidate his military power, and he has successfully defeated and reduced to impotence the forces that might have stood against the rule of violence.\n\nAt the end of 1937, Attlee and a party of three Labour MPs visited Spain and visited the British Battalion of the International Brigades fighting in the Spanish Civil War. One of the companies was named the \"Major Attlee Company\" in his honour.\n\nIn 1937, Attlee wrote a book entitled \"The Labour Party in Perspective\", which sold fairly well, in which he set out some of his views. He argued that there was no point in Labour compromising on its socialist principles in the belief that this would achieve electoral success. He wrote:\n\n'I find that the proposition often reduces itself to this – that if the Labour Party would drop its socialism and adopt a Liberal platform, many Liberals would be pleased to support it. I have heard it said more than once that if Labour would only drop its policy of nationalisation everyone would be pleased, and it would soon obtain a majority. I am convinced it would be fatal for the Labour Party...'\n\nHe also wrote, that there was no point in:\n\n'watering down Labour's socialist creed in order to attract new adherents who cannot accept the full socialist faith. On the contrary, I believe that it is only a clear and bold policy that will attract this support'In the late 1930s, Attlee sponsored a Jewish mother and her two children, enabling them to leave Germany in 1939 and move to the UK. On arriving in Britain, Attlee invited one of the children into his home in Stanmore, north-west London, where he stayed for several months.\n\nAttlee remained as Leader of the Opposition when the Second World War broke out in September 1939. The ensuing disastrous Norwegian Campaign would result in a motion of no confidence in Neville Chamberlain. Although Chamberlain survived this, the reputation of his administration was so badly and publicly damaged that it became clear a coalition government would be necessary. Even if Attlee had personally been prepared to serve under Chamberlain in an emergency coalition government, he would never have been able to carry Labour with him. Consequently, Chamberlain tendered his resignation, and Labour and the Conservatives entered a coalition government led by Winston Churchill on 10 May 1940.\n\nAttlee and Churchill quickly agreed that the War Cabinet would consist of three Conservatives (initially Churchill, Chamberlain and Lord Halifax) and two Labour members (initially himself and Arthur Greenwood) and that Labour should have slightly more than one third of the posts in the coalition government. Attlee and Greenwood played a vital role in supporting Churchill during a series of War Cabinet debates over whether or not to negotiate peace terms with Hitler following the Fall of France in May 1940, both supported Churchill and gave him the majority he needed in the War Cabinet to continue Britain's resistance.\n\nOnly Attlee and Churchill remained in the War Cabinet from the formation of the Government of National Unity in May 1940 through to the election in May 1945. Attlee was initially the Lord Privy Seal, before becoming Britain's first ever Deputy Prime Minister in 1942, as well as becoming the Dominions Secretary and the Lord President of the Council.\n\nAttlee himself played a generally low key but vital role in the wartime government, working behind the scenes and in committees to ensure the smooth operation of government. In the coalition government, three inter-connected committees effectively ran the country. Churchill chaired the first two, the War Cabinet and the Defence Committee, with Attlee deputising for him in these, and answering for the government in Parliament when Churchill was absent. Attlee himself instituted, and later chaired the third body, the Lord President's Committee, which was responsible for overseeing domestic affairs. As Churchill was most concerned with overseeing the war effort, this arrangement suited both men. Attlee himself had largely been responsible for creating these arrangements with Churchill's backing, streamlining the machinery of government and abolishing many committees. He also acted as a concilliator in the government, smoothing over tensions which frequently arose between Labour and Conservative Ministers.\n\nMany Labour activists were baffled by the top leadership role for a man they regarded as having little charisma; Beatrice Webb wrote in her diary in early 1940:\n\nFollowing the defeat of Nazi Germany and the end of the War in Europe in May 1945, Attlee and Churchill favoured the coalition government remaining in place until Japan had been defeated. However, Herbert Morrison made it clear that the Labour Party would not be willing to accept this, and Churchill was forced to tender his resignation as Prime Minister and call an immediate election.\n\nThe war had set in motion profound social changes within Britain, and had ultimately led to a widespread popular desire for social reform. This mood was epitomised in the Beveridge Report of 1942, by the Liberal economist William Beveridge. The \"Report\" assumed that the maintenance of full employment would be the aim of post-war governments, and that this would provide the basis for the welfare state. Immediately on its release, it sold hundreds of thousands of copies. All major parties committed themselves to fulfilling this aim, but most historians say that Attlee's Labour Party were seen by the electorate as the party most likely to follow it through.\n\nLabour campaigned on the theme of \"Let Us Face the Future,\" positioning themselves as the party best placed to rebuild Britain after the war, and were widely viewed as having run a strong and positive campaign, while the Conservative campaign centred entirely around Churchill. Despite opinion polls indicating a strong Labour lead, opinion polls were then viewed as a novelty which had not proven their worth, and most commentators expected that Churchill's prestige and status as a \"war hero\" would ensure a comfortable Conservative victory. Before polling day, \"The Manchester Guardian\" surmised that \"the chances of Labour sweeping the country and obtaining a clear majority ... are pretty remote\". The \"News of the World\" predicted a working Conservative majority, while in Glasgow a pundit forecast the result as Conservatives 360, Labour 220, Others 60. Churchill however made some costly errors during the campaign. In particular, his suggestion during one radio broadcast that a future Labour Government would require \"some form of a gestapo\" to implement their policies was widely regarded as being in very bad taste, and massively backfired.\n\nWhen the results of the election were announced on 26 July, they came as a surprise to most, including Attlee himself. Labour had won power by a huge landslide, winning 47.7 per cent of the vote to the Conservatives' 36 per cent. This gave them 393 seats in the House of Commons, a working majority of 146. This was the first time in history that the Labour Party had won a majority in Parliament. When Attlee went to see King George VI at Buckingham Palace to be appointed Prime Minister, the notoriously laconic Attlee and the famously tongue-tied King stood in silence; Attlee finally volunteered the remark, \"I've won the election.\" The King replied \"I know. I heard it on the Six O'Clock News.\"\nAs Prime Minister, Attlee appointed Hugh Dalton as Chancellor of the Exchequer, Ernest Bevin as Foreign Secretary, and Herbert Morrison as Deputy Prime Minister, with overall responsibility for nationalisation. Additionally, Stafford Cripps was made President of the Board of Trade, Aneurin Bevan became Minister of Health, and Ellen Wilkinson, the only woman to serve in Attlee's government, was appointed Minister of Education. The Attlee government proved itself to be a radical, reforming government. From 1945 to 1948, over 200 public Acts of Parliament were passed, with eight major pieces of legislation placed on the statute book in 1946 alone.\n\nFrancis (1995) argues there was consensus both in the Labour's national executive committee and at party conferences on a definition of socialism that stressed moral improvement as well as material improvement. The Attlee government was committed to rebuilding British society as an ethical commonwealth, using public ownership and controls to abolish extremes of wealth and poverty. Labour's ideology contrasted sharply with the contemporary Conservative Party's defence of individualism, inherited privileges, and income inequality. On 5 July 1948, Clement Attlee replied to a letter dated 22 June from James Murray and ten other MPs who raised concerns about West Indians who arrived on board the HMT Empire Windrush. As for the prime minister himself, he was not much focused on economic policy, letting others handle the issues.\n\nAttlee's Health Minister, Aneurin Bevan, fought hard against the general disapproval of the medical establishment, including the British Medical Association, by creating the National Health Service (NHS) in 1948. This was a publicly funded healthcare system, which offered treatment free of charge for all at the point of use. Reflecting pent-up demand that had long existed for medical services, the NHS treated some 8 and a half million dental patients and dispensed more than 5 million pairs of spectacles during its first year of operation.\n\nThe government set about implementing the wartime plans of Liberal William Beveridge for the creation of a \"cradle to grave\" welfare state. It set in place an entirely new system of social security. Among the most important pieces of legislation was the National Insurance Act 1946, in which people in work were required to pay a flat rate of national insurance. In return, they (and the wives of male contributors) were eligible for a wide range of benefits, including pensions, sickness benefit, unemployment benefit, and funeral benefit. Various other pieces of legislation provided for child benefit and support for people with no other source of income. In 1949, unemployment, sickness and maternity benefits were exempted from tax.\n\nThe New Towns Act of 1946 set up development corporations to construct new towns, while the Town and Country Planning Act of 1947 instructed county councils to prepare development plans and also provided compulsory purchase powers. The Attlee government also extended the powers of local authorities to requisition houses and parts of houses, and made the acquisition of land less difficult than before. The Housing (Scotland) Act of 1949 provided grants of 75 per cent (87.5 per cent in the highlands and islands) towards modernisation costs payable by Treasury to local authorities.\n\nIn 1949, local authorities were empowered to provide people suffering from poor health with public housing at subsidised rents.\n\nTo assist home ownership, the limit on the amount of money that people could borrow from their local authority to purchase or build a home was raised from £800 to £1,500 in 1945, and to £5,000 in 1949. Under the National Assistance act of 1948, local authorities had a duty \"to provide emergency temporary accommodation for families which become homeless through no fault of their own.\"\n\nA large house-building programme was carried out with the intention of providing millions of people with high-quality homes. A housing bill passed in 1946 increased Treasury subsidies for the construction of local authority housing in England and Wales. Four out of five houses constructed under Labour were council properties built to more generous specifications than before the Second World War, and subsidies kept down council rents. Altogether, these policies provided public-sector housing with its biggest-ever boost up until that point, while low-wage earners particularly benefited from these developments. Although the Attlee government failed to meet its targets, primarily due to economic constraints, over a million new homes were built between 1945 and 1951 (a significant achievement under the circumstances) which ensured that decent, affordable housing was available to many low-income families for the first time ever.\n\nA number of reforms were embarked upon to improve conditions for women and children. In 1946, universal family allowances were introduced to provide financial support to households for raising children. These benefits had been legislated for the previous year by Churchill's Family Allowances Act 1945, and was the first measure pushed through parliament by Attlee's government. Conservatives would later criticise Labour for having been \"too hasty\" in introducing family allowances.\n\nA Married Women (Restraint Upon Anticipation) Act was passed in 1949 \"to equalise, to render inoperative any restrictions upon anticipation or alienation attached to the enjoyment of property by a woman,\" while the Married Women (Maintenance) Act of 1949 was enacted with the intention of improving the adequacy and duration of financial benefits for married women.\n\nThe Criminal Law (Amendment) Act of 1950 amended an Act of 1885 to bring prostitutes within the law and safeguard them from abduction and abuse. The Criminal Justice Act of 1948 restricted imprisonment for juveniles and brought improvements to the probation and remand centres systems, while the passage of the Justices of the Peace Act of 1949 led to extensive reforms of magistrates' courts. The Attlee government also abolished the marriage bar in the Civil Service, thereby enabling married women to work in that institution.\n\nIn 1946, the government set up a National Institute of Houseworkers as a means of providing a social democratic variety of domestic service.\n\nBy late 1946, agreed standards of training were established, which was followed by the opening of a training headquarters and the opening of an additional nine (9) training centres in Wales, Scotland, and then throughout Great Britain. The National Health Service Act of 1946 indicated that domestic help should be provided for households where that help is required \"owing to the presence of any person who is ill, lying-in, an expectant mother, mentally defective, aged or a child not over compulsory school age.\" 'Home help' therefore included the provision of home-helps for nursing and expectant mothers and for mothers with children under the age of five, and by 1952 some 20,000 women were engaged in this service.\n\nDevelopment rights were nationalised while the government attempted to take all development profits for the State. Strong planning authorities were set up to control land use, and issued manuals of guidance which stressed the importance of safeguarding agricultural land. A chain of regional offices was set up within its planning ministry to provide a strong lead in regional development policies.\n\nComprehensive Development Areas (CDAs), a designation under the Town and Country Planning Act of 1947, allowed local authorities to acquire property in the designated areas using powers of compulsory purchase in order to re-plan and develop urban areas suffering from urban blight or war damage.\n\nVarious measures were carried out to improve conditions in the workplace. Entitlement to sick leave was greatly extended, and sick pay schemes were introduced for local authority administrative, professional and technical workers in 1946 and for various categories of manual workers in 1948. Worker's compensation was also significantly improved.\n\nThe Fair Wages Resolution of 1946 required any contractor working on a public project to at least match the pay rates and other employment conditions set in the appropriate collective agreement. In 1946, purchase tax was removed completely from kitchen fittings and crockery, while the rate was reduced on various gardening items.\n\nThe Fire Services Act 1947 introduced a new pension scheme for fire-fighters, while the Electricity Act 1947 introduced better retirement benefits for workers in that industry. A Workers' Compensation (Supplementation) Act was passed in 1948 that introduced benefits for workers with certain asbestos-related diseases which had occurred before 1948. The Merchant Shipping Act of 1948 and the Merchant Shipping (Safety Convention) Act of 1949 were passed to improve conditions for seamen. The Shops Act of 1950 consolidated previous legislation which provided that no one could be employed in a shop for more than six hours without having a break for at least 20 minutes. The legislation also required a lunch break of at least 45 minutes for anyone for worked between 11:30 am and 2:30 pm and a half-hour tea break for anyone working between 4 pm and 7 pm. The government also strengthened a Fair Wages Resolution, with a clause that required all employers getting government contracts to recognise the rights of their workers to join trade unions.\n\nThe Trades Disputes Act 1927 was repealed, and a Dock Labour Scheme was introduced in 1947 to put an end to the casual system of hiring labour in the docks. This scheme gave registered dockers the legal right to minimum work and decent conditions. Through the National Dock Labour Board (on which trade unions and employers had equal representation) the unions acquired control over recruitment and dismissal. Registered dockers laid off by employers within the Scheme had the right either to be taken on by another, or to generous compensation. All dockers were registered under the Dock Labour Scheme, giving them a legal right to minimum work, holidays and sick pay.\n\nWages for members of the police force were significantly increased. The introduction of a Miner's Charter in 1946 instituted a five-day work week for miners and a standardised day wage structure, and in 1948 a Colliery Workers Supplementary Scheme was approved, providing supplementary allowances to disabled coal-workers and their dependants. In 1948, a pension scheme was set up to provide pension benefits for employees of the new NHS, as well as their dependents. Under the Coal Industry Nationalisation (Superannuation) Regulations of 1950, a pension scheme for mineworkers was established. Improvements were also made in farmworkers' wages, and the Agricultural Wages Board in 1948 not only safeguarded wage levels, but also ensured that workers were provided with accommodation.\n\nA number of regulations aimed at safeguarding the health and safety of people at work were also introduced during Attlee's time in office. Regulations issued in February 1946 applied to factories involved with \"manufacturing briquettes or blocks of fuel consisting of coal, coal dust, coke or slurry with pitch as a binding substance,\" and concerned \"dust and ventilation, washing facilities and clothing accommodation, medical supervision and examination, skin and eye protection and messrooms.\"\n\nAttlee's government also carried out their manifesto commitment for nationalisation of basic industries and public utilities. The Bank of England and civil aviation were nationalised in 1946. Coal mining, the railways, road haulage, canals and Cable and Wireless were nationalised in 1947, electricity and gas followed in 1948. The steel industry was nationalised in 1951. By 1951 about 20 per cent of the British economy had been taken into public ownership.\n\nNationalisation failed to provide workers with a greater say in the running of the industries in which they worked. It did, however, bring about significant material gains for workers in the form of higher wages, reduced working hours, and improvements in working conditions, especially in regards to safety. As historian Eric Shaw noted of the years following nationalisation, the electricity and gas supply companies became \"impressive models of public enterprise\" in terms of efficiency, and the National Coal Board was not only profitable, but working conditions for miners had significantly improved as well.\n\nWithin a few years of nationalisation, a number of progressive measures had been carried out which did much to improve conditions in the mines, including better pay, a five-day working week, a national safety scheme (with proper standards at all the collieries), a ban on boys under the age of 16 going underground, the introduction of training for newcomers before going down to the coalface, and the making of pithead baths into a standard facility.\n\nThe newly established National Coal Board offered sick pay and holiday pay to miners. As noted by Martin Francis:\nUnion leaders saw nationalisation as a means to pursue a more advantageous position within a framework of continued conflict, rather than as an opportunity to replace the old adversarial form of industrial relations. Moreover, most workers in nationalised industries exhibited an essentially instrumentalist attitude, favouring public ownership because it secured job security and improved wages rather than because it promised the creation of a new set of socialists relationships in the workplace.\n\nThe Attlee government placed strong emphasis on improving the quality of life in rural areas, benefiting both farmers and other consumers. Security of tenure for farmers was introduced, while consumers were protected by food subsidies and the redistributive effects of deficiency payments. Between 1945 and 1951, the quality of rural life was improved by improvements in gas, electricity, and water services, as well as in leisure and public amenities. In addition, the 1947 Transport Act improved provision of rural bus services, while the Agriculture Act 1947 established a more generous subsidy system for farmers. Legislation was also passed in 1947 and 1948 which established a permanent Agricultural Wages Board to fix minimum wages for agricultural workers.\n\nAttlee's government made it possible for farm workers to borrow up to 90 per cent of the cost of building their own houses, and received a subsidy of £15 a year for 40 years towards that cost. Grants were also made to meet up to half the cost of supplying water to farm buildings and fields, the government met half the cost of bracken eradication and lime spreading, and grants were paid for bringing hill farming land into use that had previously been considered unfit for farming purposes.\n\nIn 1946, the National Agricultural Advisory Service was set up to supply agricultural advice and information. The Hill Farming Act of 1946 introduced for upland areas a system of grants for buildings, land improvement, and infrastructural improvements such as roads and electrification. The act also continued a system of headage payments for hill sheep and cattle that had been introduced during the war. The Agricultural Holdings Act of 1948 enabled (in effect) tenant farmers to have lifelong tenancies and made provision for compensation in the event of cessations of tenancies. In addition, the Livestock Rearing Act of March 1951 extended the provisions of the 1946 Hill Farming Act to the upland store cattle and sheep sector.\n\nAt a time of world food shortages, it was vital that farmers produced the maximum possible quantities. The government encouraged farmers via subsidies for modernisation, while the National Agricultural Advisory Service provided expertise and price guarantees. As a result of the Attlee government's initiatives in agriculture, there was a 20 per cent increase in output between 1947 and 1952, while Britain adopted one of the most mechanised and efficient farming industries in the world.\n\nThe Attlee government ensured provisions of the Education Act 1944 were fully implemented, with free secondary education becoming a right for the first time. Fees in state grammar schools were eliminated, while new, modern secondary schools were constructed.\n\nThe school leaving age was raised to 15 in 1947, an accomplishment helped brought into fruition by initiatives such as the H.O.R.S.A. (\"Huts Operation for Raising the School-leaving Age\") scheme and the S.F.O.R.S.A. (furniture) scheme. University scholarships were introduced to ensure that no one who was qualified “should be deprived of a university education for financial reasons,” while a large school building programme was organised. A rapid increase in the number of trained teachers took place, and the number of new school places was increased.\n\nIncreased Treasury funds were made available for education, particularly for upgrading school buildings suffering from years of neglect and war damage. Prefabricated classrooms were built and 928 new primary schools were constructed between 1945 and 1950. The provision of free school meals was expanded, and opportunities for university entrants were increased. State scholarships to universities were increased, and the government adopted a policy of supplementing university scholarships awards to a level sufficient to cover fees plus maintenance.\n\nMany thousands of ex-servicemen were assisted to go through college who could never have contemplated it before the war. Free milk was also made available to all schoolchildren for the first time. In addition, spending on technical education rose, and the number of nursery schools was increased. Salaries for teachers were also improved, and funds were allocated towards improving existing schools.\n\nIn 1947, the Arts Council of Great Britain was set up to encourage the arts.\n\nA Ministry of Education was established, and free County Colleges were set up for the compulsory part-time instruction of teenagers between the ages of 15 and 18 who were not in full-time education. An Emergency Training Scheme was also introduced which turned out an extra 25,000 teachers in 1945–1951. In 1947, Regional Advisory Councils were set up to bring together industry and education to find out the needs of young workers \"and advise on the provision required, and to secure reasonable economy of provision.\" That same year, thirteen Area Training Organisations were set up in England and one in Wales to coordinate teacher training.\n\nAttlee's government, however, failed to introduce the comprehensive education for which many socialists had hoped (as a means of making the educational system based more on merit and less on hereditary privilege.) This reform was eventually carried out by Harold Wilson's government. During its time in office, the Attlee government increased spending on education by over 50 per cent, from £6.5 billion to £10 billion.\n\nThe most significant problem facing Attlee and his ministers remained the economy, as the war effort had left Britain nearly bankrupt. The war had cost Britain about a quarter of her national wealth. Overseas investments had been used up to pay for the war. The transition to a peacetime economy, and the maintaining of strategic military commitments abroad led to continuous and severe problems with the balance of trade. This resulted in strict rationing of food and other essential goods continuing in the post war period to force a reduction in consumption in an effort to limit imports, boost exports, and stabilise the Pound Sterling so that Britain could trade its way out of its financial state.\n\nThe abrupt end of the American Lend-Lease program in August 1945 almost caused a crisis. Some relief was provided by the Anglo-American loan, negotiated in December 1945. The conditions attached to the loan included making the pound fully convertible to the US dollar. When this was introduced in July 1947, it led to a currency crisis and convertibility had to be suspended after just five weeks. The UK benefited from the American Marshall Aid program in 1948, and the economic situation improved significantly. Another balance of payments crisis in 1949 forced Chancellor of the Exchequer, Stafford Cripps, into devaluation of the pound.\n\nDespite these problems, one of the main achievements of Attlee's government was the maintenance of near full employment. The government maintained most of the wartime controls over the economy, including control over the allocation of materials and manpower, and unemployment rarely rose above 500,000, or 3 per cent of the total workforce. Labour shortages proved a more frequent problem. The inflation rate was also kept low during his term. The rate of unemployment rarely rose above 2 per cent during Attlee's time in office, whilst there was no hard-core of long-term unemployed. Both production and productivity rose as a result of new equipment, while the average working week was shortened.\n\nThe government was less successful in housing, which was the responsibility of Aneurin Bevan. The government had a target to build 400,000 new houses a year to replace those which had been destroyed in the war, but shortages of materials and manpower meant that less than half this number were built. Nevertheless, millions of people were rehoused as a result of the Attlee government's housing policies. Between August 1945 and December 1951, 1,016,349 new homes were completed in England, Scotland, and Wales.\n\nWhen the Attlee government was voted out of office in 1951, the economy had been improved compared to 1945. The period from 1946 to 1951 saw continuous full employment and steadily rising living standards, which increased by about 10 per cent each year. During that same period, the economy grew by 3 per cent a year, and by 1951 the UK had \"the best economic performance in Europe, while output per person was increasing faster than in the United States.\" Careful planning after 1945 also ensured that demobilisation was carried out without having a negative impact upon economic recovery, and that unemployment stayed at very low levels. In addition, the number of motor cars on the roads rose from 3 million to 5 million from 1945 to 1951, and seaside holidays were taken by far more people than ever before. A Monopolies and Restrictive Practices (Inquiry and Control) Act was passed in 1948, which allowed for investigations of restrictive practices and monopolies.\n\n1947 proved a particularly difficult year for the government; an exceptionally cold winter that year caused coal mines to freeze and cease production, creating widespread power cuts and food shortages. The Minister of Fuel and Power, Emanuel Shinwell was widely blamed for failing to ensure adequate coal stocks, and soon resigned from his post. The Conservatives capitalised on the crisis with the slogan 'Starve with Strachey and shiver with Shinwell' (referring to the Minister of Food John Strachey).\n\nThe crisis led to an unsuccessful plot by Hugh Dalton to replace Attlee as Prime Minister with Ernest Bevin. Later that year Stafford Cripps tried to persuade Attlee to stand aside for Bevin. These plots petered out after Bevin refused to cooperate. Later that year, Hugh Dalton resigned as Chancellor after inadvertently leaking details of the budget to a journalist. He was replaced by Cripps.\n\nIn foreign affairs, the Attlee government was concerned with four main issues; post-war Europe, the onset of the Cold War, the establishment of the United Nations, and decolonisation. The first two were closely related, and Attlee was assisted by Foreign Secretary Ernest Bevin. Attlee also attended the later stages of the Potsdam Conference, where he negotiated with President Harry S. Truman and Joseph Stalin.\n\nIn the immediate aftermath of the war, the Government faced the challenge of managing relations with Britain's former war-time ally, Stalin and the Soviet Union. Ernest Bevin was a passionate anti-communist, based largely on his experience of fighting communist influence in the trade union movement. Bevin's initial approach to the USSR as Foreign Secretary was \"wary and suspicious, but not automatically hostile.\" Attlee himself sought warm relations with Stalin. He put his trust in the United Nations, rejected notions that the Soviet Union was bent on world conquest, and warned that treating Moscow as an enemy would turn it into one. This put Attlee at sword's point with his foreign minister, the Foreign Office, and the military who all saw the Soviets as a growing threat to Britain's role in the Middle East. Suddenly in January 1947, Attlee reversed his position and agreed with Bevin on a hard-line anti-Soviet policy.\n\nIn an early \"good-will\" gesture that was later heavily criticised, the Attlee government allowed the Soviets to purchase, under the terms of a 1946 UK-USSR Trade Agreement, a total of 25 Rolls-Royce Nene jet engines in September 1947 and March 1948. The agreement included an agreement not to use them for military purposes. The price was fixed under a commercial contract; a total of 55 jet engines were sold to the USSR in 1947. However, the Cold War intensified during this period and the Soviets, who at the time were well behind the West in jet technology, reverse-engineered the Nene and installed their own version in the MiG-15 interceptor. This was used to good effect against US-UK forces in the subsequent Korean War, as well as in several later MiG models.\n\nAfter Stalin took political control of most of Eastern Europe, and began to subvert other governments in the Balkans, Attlee's and Bevin's worst fears of Soviet intentions were realised. The Attlee government then became instrumental in the creation of the successful NATO defence alliance to protect Western Europe against any Soviet aggression. In a crucial contribution to the economic stability of post-war Europe, Attlee's Cabinet was instrumental in promoting the American Marshall Plan for the economic recovery of Europe. He called it, one of the \"most bold, enlightened and good-natured acts in the history of nations.\"\n\nA group of Labour MPs, organised under the banner of \"Keep Left\" urged the government to steer a middle way between the two emerging superpowers, and advocated the creation of a \"third force\" of European powers to stand between the US and USSR. However, deteriorating relations between Britain and the USSR, as well as Britain's economic reliance on America following the Marshall Plan, steered policy towards supporting the US. In January 1947, fear of both Soviet and American nuclear intentions led to a secret meeting of the Cabinet, where the decision was made to press ahead with the development of Britain's independent nuclear deterrent, an issue which later caused a split in the Labour Party. Britain's first successful nuclear test, however, did not occur until 1952, one year after Attlee had left office.\n\nThe London dock strike of July 1949, led by Communists, was suppressed when the Attlee government sent in 13,000 Army troops and passed special legislation to promptly end the strike. His response reveals Attlee's growing concern that Soviet expansionism, supported by the British Communist Party, was a genuine threat to national security, and that the docks were highly vulnerable to sabotage ordered by Moscow. He noted that the strike was caused not by local grievances, but to help communist unions who were on strike in Canada. Attlee agreed with MI5 that he faced \"a very present menace.\"\nDecolonisation was never a major election issue but Attlee gave the matter a great deal of attention and was the chief leader in planning and achieving the process of decolonisation of the British Empire, starting in Asia.\n\nAttlee orchestrated the granting of independence to India and Pakistan in 1947. Attlee in 1928–1934 had been a member of the Indian Statutory Commission, called the Simon Commission. He became the Labour Party expert on India and by 1934 was committed to granting India the same independent dominion status that Canada and Australia recently were given. He faced strong resistance from the die-hard Conservative imperialists, led by Churchill, who opposed both independence and efforts led by Prime Minister Stanley Baldwin to set up a system of limited local control by Indians themselves. Attlee and the Labour leadership were sympathetic to the Congress movement led by Mahatma Gandhi and Jawaharlal Nehru. During the Second World War Attlee was in charge of Indian affairs. He set up the Cripps Mission in 1942, which tried and failed to bring the factions together. When the Congress called for passive resistance in the \"Quit India\" movement of 1942–1945, it was Attlee who ordered the arrest and internment for the duration of tens of thousands of Congress leaders and crushed the revolt.\n\nLabour's election Manifesto in 1945 called for \"the advancement of India to responsible self-government,\" but did not mention independence. In 1942 the British Raj tried to enlist all major political factors parties in support of the war effort. Congress, led by Nehru and Gandhi, demanded immediate independence and full control by Congress of all of India. That demand was rejected by the British, and Congress opposed the war effort with its \"Quit India campaign\". The Raj immediately responded in 1942 by imprisoning the major national, regional and local Congress leaders for the duration. Attlee did not object. By contrast, the Muslim League led by Muhammad Ali Jinnah, and also the Sikh community, strongly supported the war effort. They greatly enlarged their membership and won favour from London for their decision. Attlee retained a fondness for Congress and until 1946, accepted their thesis that they were a non-religious party that accepted Hindus, Muslims, Sikhs, and everyone else.\nThe Muslim league insisted that it was the only true representative of all of the Muslims of India, and by 1946 Attlee had come to agree with them.\nWith violence escalating in India after the war, but with British financial power at a low ebb, large-scale military involvement was impossible. Viceroy Wavell said he needed a further seven army divisions to prevent communal violence if independence negotiations failed. No divisions were available; independence was the only option. Given the demands of the Muslim league, independence implied a partition that set off heavily Muslim Pakistan from the main portion of India.\n\nThe Labour government gave independence to India and Pakistan in an unexpectedly quick move in 1947. Historian Andrew Roberts says the independence of India was a \"national humiliation\" but it was necessitated by urgent financial, administrative, strategic and political needs. Churchill in 1940–1945 had tightened the hold on India and imprisoned the Congress leadership, with Attlee's approval. Labour had looked forward to making it a fully independent dominion like Canada or Australia. Many of the Congress leaders in the India had studied in England, and were highly regarded as fellow idealistic socialists by Labour leaders. Attlee was the Labour expert on India and took special charge of decolonisation. Attlee found that Churchill's viceroy, Field Marshal Wavell, was too imperialistic, too keen on military solutions (he wanted seven more Army divisions) and too neglectful of Indian political alignments. The new Viceroy was Lord Mountbatten, the dashing war hero and a cousin of the King. The boundary between the newly created states of Pakistan and India involved the widespread resettlement of millions of Muslims and Hindus (and many Sikhs). Extreme violence ensued when Punjab and Bengal provinces were split. Historian Yasmin Khan estimates that between a half-million and a million men, women and children were killed. Gandhi himself was assassinated by a Hindu activist in January 1948.\n\nThe final result was two nations consisting of a Hindu-majority India and a Muslim-majority Pakistan (which incorporated East Pakistan, now Bangladesh). Both joined the Commonwealth.\n\nAttlee also sponsored the peaceful transition to independence in 1948 of Burma (Myanmar) and Ceylon (Sri Lanka).\n\nOne of the most urgent problems concerned the future of the Palestine Mandate. It had become too troublesome and much too expensive to handle. British policies there were perceived by the Zionist movement and the Truman Administration as pro-Arab and anti-Jewish. In the face of an armed revolt of Jewish militant groups and increasing violence of the local Arab population, Britain had found itself unable to control events. This was a very unpopular commitment, and the evacuation of British troops and subsequent handing over of the issue to the United Nations was widely supported by the British public.\n\nThe government's policies with regard to the other colonies, particularly those in Africa, focused on keeping them as strategic Cold War assets while modernising their economies. The Labour Party had long attracted aspiring leaders from Africa and had developed elaborate plans before the war. Implementing them overnight with an empty treasury proved too challenging. A major military base was built in Kenya, and the African colonies came under an unprecedented degree of direct control from London. Development schemes were implemented to help solve Britain's post-war balance of payments crisis and raise African living standards. This \"new colonialism\" worked slowly and had failures such as the Tanganyika groundnut scheme.\n\nThe 1950 election gave Labour a massively reduced majority of five seats compared to the triple-digit majority of 1945. Although re-elected, the result was seen by Attlee as very disappointing, and was widely attributed to the effects of post-war austerity denting Labour's appeal to middle-class voters. With such a small majority leaving him dependent on a small number of MPs to govern, Attlee's second term was much tamer than his first. Some major reforms were nevertheless passed, particularly regarding industry in urban areas and regulations to limit air and water pollution.\n\nBy 1951, the Attlee government was exhausted, with several of its most senior ministers ailing or ageing, and with a lack of new ideas. Attlee's record for settling internal differences in the Labour Party fell in April 1951, when there was a damaging split over an austerity Budget brought in by the Chancellor, Hugh Gaitskell, to pay for the cost of Britain's participation in the Korean War. Aneurin Bevan resigned to protest against the new charges for \"teeth and spectacles\" in the National Health Service introduced by that Budget, and was joined in this action by several senior ministers, including the future Prime Minister Harold Wilson, then the President of the Board of Trade. Thus escalated a battle between the left and right wings of the Party that continues today.\n\nFinding it increasingly impossible to govern, Attlee's only chance was to call a snap election in October 1951, in the hope of achieving a more workable majority and to regain authority. The gamble failed: Labour narrowly lost to the Conservative Party, despite winning considerably more votes (achieving the largest Labour vote in electoral history). Attlee tendered his resignation as Prime Minister the following day, after six years and three months in office.\n\nFollowing the defeat in 1951, Attlee continued to lead the party as Leader of the Opposition. His last four years as leader were, however, widely seen as one of the Labour Party's weaker periods.\n\nThe period was dominated by infighting between the Labour Party's right wing, led by Hugh Gaitskell, and its left, led by Aneurin Bevan. Many Labour MPs felt that Attlee should have retired after the 1951 election and allowed a younger man to lead the party. Bevan openly called for him to stand down in the summer of 1954. One of his main reasons for staying on as leader was to frustrate the leadership ambitions of Herbert Morrison, whom Attlee disliked for both political and personal reasons. At one time, Attlee had favoured Aneurin Bevan to succeed him as leader, but this became problematic after Bevan almost irrevocably split the party.\n\nIn an interview with the \"News Chronicle\" columnist Percy Cudlipp in mid-September 1955, Attlee made clear his own thinking together with his preference for the leadership succession, stating:\nAttlee, now aged 72, contested the 1955 general election against Anthony Eden, which saw Labour lose 18 seats, and the Conservatives increase their majority. He retired as Leader of the Labour Party on 7 December 1955, having led the party for twenty years, and on 14 December Hugh Gaitskell was elected as his replacement.\n\nHe subsequently retired from the House of Commons and was elevated to the peerage to take his seat in the House of Lords as Earl Attlee and Viscount Prestwood on 16 December 1955. He believed Eden had been forced into taking a strong stand on the Suez Crisis by his backbenchers. In 1958, he was, along with numerous notables; to establish the Homosexual Law Reform Society. The society campaigned for the decriminalisation of homosexual acts in private by consenting adults, a reform which was voted through Parliament nine years later.\n\nIn 1962, he spoke twice in the House of Lords against the British government's application for the UK to join the European Economic Community (\"Common Market\"). In his second speech delivered in November, Attlee claimed that Britain had a separate parliamentary tradition from the Continental countries that composed the EEC. He also claimed that if Britain was a member, EEC rules would prevent the British government from planning the economy and that Britain's traditional policy had been outward looking rather than Continental.\n\nHe attended Winston Churchill's funeral in January 1965. He was elderly and frail by that time, and had to remain seated in the freezing cold as the coffin was carried, having tired himself out by standing at the rehearsal the previous day. He lived to see the Labour Party return to power under Harold Wilson in 1964, but also to see his old constituency of Walthamstow West fall to the Conservatives in a by-election in September 1967.\n\nAttlee died peacefully in his sleep of pneumonia, at the age of 84 at Westminster Hospital on 8 October 1967. Two thousand people attended his funeral in November, including the then-Prime Minister Harold Wilson and the Duke of Kent, representing the Queen. He was cremated and his ashes were buried at Westminster Abbey.\n\nUpon his death, the title passed to his son Martin Richard Attlee, 2nd Earl Attlee (1927–1991). It is now held by Clement Attlee's grandson John Richard Attlee, 3rd Earl Attlee. The third earl (a member of the Conservative Party) retained his seat in the Lords as one of the hereditary peers to remain under an amendment to Labour's 1999 House of Lords Act.\n\nAttlee's estate was sworn for probate purposes at a value of £7,295, a relatively modest sum for so prominent a figure, and only a fraction of the £75,394 in his father's estate when he died in 1908.\n\nThe quotation about Attlee, \"A modest man, but then he has so much to be modest about\", is commonly ascribed to Churchill—though Churchill denied saying it, and respected Attlee's service in the War Cabinet. Attlee's modesty and quiet manner hid a great deal that has only come to light with historical reappraisal. In terms of the machinery of government, he was one of the most businesslike and effective of all the UK Prime Ministers. The journalist and broadcaster Anthony Howard called him \"the greatest Prime Minister of the 20th century.\"\n\nHis leadership style of consensual government, acting as a chairman rather than a president, won him much praise from historians and politicians alike. Christopher Soames, the British Ambassador to France during the Conservative government of Edward Heath and cabinet minister under Margaret Thatcher, remarked that \"Mrs Thatcher was not really running a team. Every time you have a Prime Minister who wants to make all the decisions, it mainly leads to bad results. Attlee didn't. That's why he was so damn good.\"\n\nThatcher herself wrote in her 1995 memoirs, which charted her beginnings in Grantham to her victory at the 1979 general election, that she admired Attlee, writing: \"Of Clement Attlee, however, I was an admirer. He was a serious man and a patriot. Quite contrary to the general tendency of politicians in the 1990s, he was all substance and no show.\"\n\nAttlee's government presided over the successful transition from a wartime economy to peacetime, tackling problems of demobilisation, shortages of foreign currency, and adverse deficits in trade balances and government expenditure. Further domestic policies that he brought about included the creation of the National Health Service and the post-war Welfare State, which became key to the reconstruction of post-war Britain. Attlee and his ministers did much to transform the UK into a more prosperous and egalitarian society during their time in office with reductions in poverty and a rise in the general economic security of the population.\n\nIn foreign affairs, he did much to assist with the post-war economic recovery of Europe. He proved a loyal ally of the United States at the onset of the Cold War. Due to his style of leadership, it was not he, but Ernest Bevin who masterminded foreign policy. It was Attlee's government that decided Britain should have an independent nuclear weapons programme, and work on it began in 1947.\n\nBevin, Attlee's Foreign Secretary, famously stated that \"We've got to have it and it's got to have a bloody Union Jack on it.\" The first operational British A Bomb was not detonated until October 1952, about one year after Attlee had left office. Independent British atomic research was prompted partly by the US McMahon Act, which nullified wartime expectations of postwar US–UK collaboration in nuclear research, and prohibited Americans from communicating nuclear technology even to allied countries. British atomic bomb research was kept secret even from some members of Attlee's own cabinet, whose loyalty or discretion seemed uncertain.\n\nAlthough a socialist, Attlee still believed in the British Empire of his youth. He thought of it as an institution that was a power for good in the world. Nevertheless, he saw that a large part of it needed to be self-governing. Using the Dominions of Canada, Australia, and New Zealand as a model, he continued the transformation of the empire into the modern-day British Commonwealth.\n\nHis greatest achievement, surpassing many of these, was perhaps the establishment of a political and economic consensus about the governance of Britain that all three major parties subscribed to for three decades, fixing the arena of political discourse until the late-1970s. In 2004, he was voted the most successful British Prime Minister of the 20th century by a poll of 139 academics organised by Ipsos MORI.\n\nSeveral years after his death, a street on a new housing development in Tividale, West Midlands, was named Attlee Close in his memory. The Birks Holt social housing estate in Maltby, South Yorkshire, has its streets named after Labour politicians, including Attlee, Sir Stafford Cripps, Hugh Gaitskell, and George Lansbury.\nA blue plaque unveiled in 1979 commemorates Attlee at 17 Monkhams Avenue, in Woodford Green in the London borough of Redbridge.\n\nAttlee was elected a Fellow of the Royal Society in 1947. Attlee was awarded an Honorary Fellowship of Queen Mary College on 15 December 1948.\n\nOn 30 November 1988, a bronze statue of Clement Attlee was unveiled by Harold Wilson (the next Labour Prime Minister after Attlee) outside Limehouse Library in Attlee's former constituency. By then Wilson was the last surviving member of Attlee's cabinet, and the unveiling of the statue would be one of the last public appearances by Wilson, who was by that point in the early stages of Alzheimer's Disease; he died at the age of 79 in May 1995.\n\nLimehouse Library was closed in 2003, after which the statue was vandalised. The council surrounded it with protective hoarding for four years, before eventually removing it for repair and recasting in 2009. In April 2011 the restored statue was unveiled by Peter Mandelson in its new position less than a mile away at the Queen Mary University of London's Mile End campus.\n\nThere is also a statue of Clement Attlee in the Houses of Parliament that was erected, instead of a bust, by parliamentary vote in 1979. The sculptor was Ivor Roberts-Jones.\n\n\n\nAlthough one of his brothers became a clergyman and one of his sisters a missionary, Attlee himself is usually regarded as an agnostic. In an interview he described himself as \"incapable of religious feeling\", saying that he believed in \"the ethics of Christianity\" but not \"the mumbo-jumbo.\" When asked whether he was an agnostic, Attlee replied \"I don't know.\"\n\n\n\n\n"}
{"id": "5768", "url": "https://en.wikipedia.org/wiki?curid=5768", "title": "Catullus", "text": "Catullus\n\nGaius Valerius Catullus (; ; c. 84 – c. 54 BC) was a Latin poet of the late Roman Republic who wrote chiefly in the neoteric style of poetry, which is about personal life rather than classical heroes. His surviving works are still read widely and continue to influence poetry and other forms of art.\n\nCatullus's poems were widely appreciated by other poets, significantly influencing Ovid, Horace, and Virgil, among others. After his rediscovery in the Late Middle Ages, Catullus again found admirers. The explicit sexual imagery which he uses in some of his poems has shocked many readers. Indeed, Catullus's work was never canonical in schools, although his body of work is still frequently read from secondary school to graduate programs across the world, with his 64th poem often considered his greatest.\n\nGaius Valerius Catullus () was born to a leading equestrian family of Verona, in Cisalpine Gaul. The social prominence of the Catullus family allowed the father of Gaius Valerius to entertain Julius Caesar when he was the Promagistrate (proconsul) of both Gallic provinces. In a poem, Catullus describes his happy homecoming to the family villa at Sirmio, on Lake Garda, near Verona; he also owned a villa near the resort of Tibur (Tivoli).\n\nCatullus appears to have spent most of his young adult years in Rome. His friends there included the poets Licinius Calvus, and Helvius Cinna, Quintus Hortensius (son of the orator and rival of Cicero) and the biographer Cornelius Nepos, to whom Catullus dedicated a \"libellus\" of poems, the relation of which to the extant collection remains a matter of debate. He appears to have been acquainted with the poet Marcus Furius Bibaculus. A number of prominent contemporaries appear in his poetry, including Cicero, Caesar and Pompey. According to an anecdote preserved by Suetonius, Caesar did not deny that Catullus's lampoons left an indelible stain on his reputation, but when Catullus apologized, he invited the poet for dinner the very same day.\nIt was probably in Rome that Catullus fell deeply in love with the \"Lesbia\" of his poems, who is usually identified with Clodia Metelli, a sophisticated woman from the aristocratic house of patrician family Claudii Pulchri, sister of the infamous Publius Clodius Pulcher, and wife to proconsul Quintus Caecilius Metellus Celer. In his poems Catullus describes several stages of their relationship: initial euphoria, doubts, separation, and his wrenching feelings of loss. Clodia had several other partners; \"From the poems one can adduce no fewer than five lovers in addition to Catullus: Egnatius (poem 37), Gellius (poem 91), Quintius (poem 82), Rufus (poem 77), and Lesbius (poem 79).\" There is also some question surrounding her husband's mysterious death in 59 B.C., some critics believing he was domestically poisoned. Yet, a sensitive and passionate Catullus could not relinquish his flame for Clodia, regardless of her obvious indifference to his desire for a deep and permanent relationship. In his poems, Catullus wavers between devout, sweltering love and bitter, scornful insults that he directs at her blatant infidelity (as demonstrated in poems 11 and 58). His passion for her is unrelenting—yet it is unclear when exactly the couple split up for good. Catullus's poems about the relationship display striking depth and psychological insight.\n\nHe spent the provincial command year summer 57 to summer 56 BC in Bithynia on the staff of the commander Gaius Memmius. While in the East, he traveled to the Troad to perform rites at his brother's tomb, an event recorded in a moving poem.\nThere survives no ancient biography of Catullus: his life has to be pieced together from scattered references to him in other ancient authors and from his poems. Thus it is uncertain when he was born and when he died. St. Jerome says that he died in his 30th year, and was born in 87 BC. But the poems include references to events of 55 and 54 BC. Since the Roman consular fasti make it somewhat easy to confuse 87–57 BC with 84–54 BC, many scholars accept the dates 84 BC–54 BC, supposing that his latest poems and the publication of his \"libellus\" coincided with the year of his death. Other authors suggest 52 or 51 BC as the year of the poet's death. Though upon his elder brother's death Catullus lamented that their \"whole house was buried along\" with the deceased, the existence (and prominence) of \"Valerii Catulli\" is attested in the following centuries. T.P. Wiseman argues that after the brother's death Catullus could have married, and that, in this case, the later \"Valerii Catulli\" may have been his descendants.\n\nCatullus's poems have been preserved in an anthology of 116 \"carmina\" (the actual number of poems may slightly vary in various editions), which can be divided into three parts according to their form: sixty short poems in varying meters, called \"polymetra\", eight longer poems, and forty-eight epigrams.\n\nThere is no scholarly consensus on whether Catullus himself arranged the order of the poems. The longer poems differ from the \"polymetra\" and the epigrams not only in length but also in their subjects: There are seven hymns and one mini-epic, or epyllion, the most highly prized form for the \"new poets\".\n\nThe \"polymetra\" and the epigrams can be divided into four major thematic groups (ignoring a rather large number of poems that elude such categorization):\n\n\nAll these poems describe the lifestyle of Catullus and his friends, who, despite Catullus's temporary political post in Bithynia, lived their lives withdrawn from politics. They were interested mainly in poetry and love. Above all other qualities, Catullus seems to have valued \"venustas\", or charm, in his acquaintances, a theme which he explores in a number of his poems. The ancient Roman concept of \"virtus\" (i.e. of virtue that had to be proved by a political or military career), which Cicero suggested as the solution to the societal problems of the late Republic, meant little to them.\n\nHowever Catullus does not reject traditional notions, but rather their particular application to the \"vita activa\" of politics and war. Indeed, he tries to reinvent these notions from a personal point of view and to introduce them into human relationships. For example, he applies the word \"fides\", which traditionally meant faithfulness towards one's political allies, to his relationship with Lesbia and reinterprets it as unconditional faithfulness in love. So, despite the seeming frivolity of his lifestyle, Catullus measured himself and his friends by quite ambitious standards.\n\nCatullus's poetry was influenced by the innovative poetry of the Hellenistic Age, and especially by Callimachus and the Alexandrian school, which had propagated a new style of poetry that deliberately turned away from the classical epic poetry in the tradition of Homer. Cicero called these local innovators \"neoteroi\" (νεώτεροι) or 'moderns' (in Latin \"poetae novi\" or 'new poets'), in that they cast off the heroic model handed down from Ennius in order to strike new ground and ring a contemporary note. Catullus and Callimachus did not describe the feats of ancient heroes and gods (except perhaps in re-evaluating and predominantly artistic circumstances, e.g. poems 63 and 64), focusing instead on small-scale personal themes. Although these poems sometimes seem quite superficial and their subjects often are mere everyday concerns, they are accomplished works of art. Catullus described his work as \"expolitum\", or polished, to show that the language he used was very carefully and artistically composed.\n\nCatullus was also an admirer of Sappho, a female poet of the seventh century BC, and is the source for much of what we know or infer about her. Catullus 51 follows Sappho 31 so closely that some believe the later poem to be, in part, a direct translation of the earlier poem, and 61 and 62 are certainly inspired by and perhaps translated directly from lost works of Sappho. Both of the latter are \"epithalamia\", a form of laudatory or erotic wedding-poetry that Sappho had been famous for but that had gone out of fashion in the intervening centuries. Catullus twice used a meter that Sappho developed, called the Sapphic strophe, in poems 11 and 51. In fact, Catullus may have brought about a substantial revival of that form in Rome.\n\nCatullus, as was common to his era, was greatly influenced by stories from Greek and Roman myth. His longer poems—such as 63, 64, 65, 66, and 68—allude to mythology in various ways. Some stories he refers to are the wedding of Peleus and Thetis, the departure of the Argonauts, Theseus and the Minotaur, Ariadne's abandonment, Tereus and Procne, as well as Protesilaus and Laodamia.\n\nCatullus wrote in many different meters including hendecasyllabic verse and elegiac couplets (common in love poetry). A great part of his poetry shows strong and occasionally wild emotions, especially in regard to Lesbia. His love poems are very emotional and ardent, and can be related to even today. Catullus describes his Lesbia as having multiple suitors and often showing little affection towards him. He also demonstrates a great sense of humour such as in Catullus 13.\n\n\"Catullus Dreams\" (2011) is a song cycle by David Glaser set to texts of Catullus. The cycle is scored for soprano and seven instruments. It was premiered at Symphony Space in New York by soprano Linda Larson and Sequitur Ensemble.\n\n\"Catulli Carmina\" is a cantata by Carl Orff to the texts of Catullus.\n\n\"Carmina Catulli\" is a song cycle arranged from 17 of Catullus' poems by American composer Michael Linton. The cycle was recorded in December 2013 and premiered at Carnegie Hall's Weill Recital Hall in March 2014 by French baritone Edwin Crossley-Mercer and pianist Jason Paul Peterson.\n\nCatullus 5, the love poem \"Vivamus mea Lesbia atque amemus\", in the translation by Ben Jonson was set to music (lute accompanied song) by Alfonso Ferrabosco the younger. Thomas Campion also wrote a lute-song using his own translation of the first six lines of Catullus 5 followed by two verses of his own. The translation by Richard Crashaw was set to music in a four-part glee by Samuel Webbe Jr. It was also set to music in a three-part glee by John Stafford Smith.\n\nFinnish jazz singer Reine Rimón has recorded poems of Catullus set to standard jazz tunes.\n\nThe American composer, Ned Rorem, set Catullus 101 to music for voice and piano. The song, \"Catallus: on the Burial of His Brother\" was originally published in 1969.\n\nThe Icelandic composer, Johann Johannsson, set Catullus 85 to music. The poem is sung through a vocoder. The music is played by a string quartet and piano. Titled \"Odi Et Amo\", the song is found on Johannsson's album Englaborn.\n\n\n\n\n"}
{"id": "5769", "url": "https://en.wikipedia.org/wiki?curid=5769", "title": "C. S. Forester", "text": "C. S. Forester\n\nCecil Louis Troughton Smith (27 August 1899 – 2 April 1966), known by his pen name Cecil Scott \"C. S.\" Forester, was an English novelist known for writing tales of naval warfare such as the 12-book Horatio Hornblower series, depicting a Royal Navy officer during the Napoleonic wars. Two of the Hornblower books, \"A Ship of the Line\" and \"Flying Colours\", were jointly awarded the James Tait Black Memorial Prize for fiction in 1938. His other works include \"The African Queen\" (1935; filmed in 1951 by John Huston).\n\nForester was born in Cairo and, after a family breakup at an early age, moved with his mother to London, where he was educated at Alleyn's School and Dulwich College, south London. He began to study medicine at Guy's Hospital, London, but left without completing his degree. Forester had always worn glasses and been of slender physique. Trying to enlist in the army, he failed his physical and was told there was not a chance that he would be accepted, even though he was of good height and somewhat athletic. Around 1921, after leaving Guy's, he began writing seriously using his pen name. The name of his primary literary character Hornblower was apparently taken from a fellow Alleyn's pupil. A Hornblower is listed on the 1914–1918 War Memorial at the School.\n\nDuring the Second World War, Forester moved to the United States, where he worked for the British Ministry of Information and wrote propaganda to encourage the US to join the Allies. He eventually settled in Berkeley, California. In 1942, while living in Washington, D.C., he met a young British intelligence officer named Roald Dahl, whose experiences in the RAF he had heard about, and encouraged him to write about them. According to Dahl's autobiographical \"Lucky Break\", Forester asked Dahl about his experiences as a fighter pilot. This prompted Dahl to write his first story, \"A Piece of Cake\".\n\nForester wrote many novels. He is best known for the 12-book Horatio Hornblower series, depicting a Royal Navy officer during the Napoleonic Wars. He began the series with Hornblower fairly high in rank in the first novel, published in 1937. The last completed novel was published in 1962. With demand for more stories, Forester filled in Hornblower's life story, in effect. Hornblower's fictional feats were based on real events, but Forester wrote the body of the works carefully to avoid entanglements with real world history, so that Hornblower is always off on another mission when a great naval battle occurs during the Napoleonic Wars.\n\nForester's other novels include \"The African Queen\" (1935) and \"The General\" (1936); Peninsular War novels in \"Death to the French\" (published in the United States as \"Rifleman Dodd\") and \"The Gun\" (filmed as \"The Pride and the Passion\" in 1957); and seafaring stories that did not involve Hornblower, such as \"Brown on Resolution\" (1929), \"The Captain from Connecticut\" (1941), \"The Ship\" (1943), and \"Hunting the Bismarck\" (1959), which was used as the basis of the screenplay for the film \"Sink the Bismarck!\" (1960). Several of his works were filmed, including \"The African Queen\" (1951), directed by John Huston. Forester is also credited as story writer for several movies not based on his published fiction, including \"Commandos Strike at Dawn\" (1942).\n\nHe wrote several volumes of short stories set during the Second World War. Those in \"The Nightmare\" (1954) were based on events in Nazi Germany, ending at the Nuremberg trials. Stories in \"The Man in the Yellow Raft\" (1969) followed the career of the destroyer USS \"Boon\", while many of those in \"Gold from Crete\" (1971) followed the destroyer HMS \"Apache\". The last of the stories in \"Gold from Crete\" was \"If Hitler had invaded England\", which offers an imagined sequence of events starting with Hitler's attempt to implement Operation Sea Lion, and culminating in the early military defeat of Nazi Germany in the summer of 1941. His non-fiction seafaring works include \"The Age of Fighting Sail\" (1956), an account of the sea battles between Great Britain and the United States in the War of 1812.\n\nIn addition to his novels of seafaring life, Forester published two crime novels (\"Payment Deferred\" (1926) and \"Plain Murder\" (1930)) and two children's books. \"Poo-Poo and the Dragons\" (1942) was created as a series of stories told to his younger son George to encourage him to finish his meals. George had mild food allergies that kept him feeling unwell, and he needed encouragement to eat. \"The Barbary Pirates\" (1953) is a children's history of early 19th-century pirates.\n\nC. S. Forester appeared as a contestant on the television quiz programme \"You Bet Your Life\", hosted by Groucho Marx, in an episode broadcast on 1 November 1956.\n\nA \"lost\" novel of Forester's, \"The Pursued\", was discovered in 2003 and bought at an auction and was published by Penguin Classics on 3 November 2011.\n\nHe married Kathleen Belcher in 1926 and they had two sons, John and George Forester. The couple divorced in 1945. In 1947, he married Dorothy Foster.\nJohn Forester wrote a two-volume biography of his father, including many elements of Forester's life which only became clear to his son after his death.\n\n\n\n\n\n\n\n\n\nSternlicht, Sanford V., \"C.S. Forester and the Hornblower saga\" (Syracuse University Press, 1999)\n\nVan der Kiste, John, \"C.S. Forester's Crime Noir: A view of the murder stories\" (KDP, 2018)\n\n"}
{"id": "5770", "url": "https://en.wikipedia.org/wiki?curid=5770", "title": "List of country calling codes", "text": "List of country calling codes\n\nCountry calling codes or country dial in codes are telephone dialing prefixes for the member countries or regions of the International Telecommunication Union (ITU). They are defined by the ITU-T in standards E.123 and E.164. The prefixes enable international direct dialing (IDD), and are also referred to as \"international subscriber dialing\" (ISD) codes.\nCountry codes are a component of the international telephone numbering plan, and are necessary only when dialing a telephone number to establish a call to another country. Country codes are dialed before the national telephone number. By convention, international telephone numbers are represented by prefixing the country code with a plus sign (+), which also indicates to the subscriber that the local international call prefix must first be dialed. For example, the international call prefix in all countries belonging to the North American Numbering Plan is 011, while it is 00 in most European, Asian and African countries. On GSM (cellular) networks, the prefix may automatically be inserted when the user prefixes a dialed number with the plus sign.\n\nCountry calling codes are prefix codes and can be organized as a tree. In each row of the table, the country codes given in the left-most column share the same first digit; then subsequent columns give the second digit in ascending order.\nWhile there is a general geographic grouping to the zones, some exceptions exist for political and historical reasons. Thus, the geographical indicators below are approximations only.\n\nMember countries of the North American Numbering Plan (NANP) are assigned three-digit area codes under the common country prefix \"1\", shown in the format \"+1 XXX\".\n\nThe North American Numbering Plan includes:\n\n\nOriginally, larger countries such as Spain, the United Kingdom or France, were assigned two-digit codes to compensate for their usually longer domestic numbers. Small countries, such as Iceland, were assigned three-digit codes. Since the 1980s, all new assignments have been three-digit regardless of countries’ populations.\n\n\n\n\n\nIn Antarctica, dialing is dependent on the parent country of each base:\n\nOther places with no country codes in use, although a code may be reserved:\n\n"}
{"id": "5771", "url": "https://en.wikipedia.org/wiki?curid=5771", "title": "Christopher Marlowe", "text": "Christopher Marlowe\n\nChristopher Marlowe, also known as Kit Marlowe (; baptised 26 February 156430 May 1593), was an English playwright, poet and translator of the Elizabethan era. Marlowe was the foremost Elizabethan tragedian of his day. He greatly influenced William Shakespeare, who was born in the same year as Marlowe and who rose to become the pre-eminent Elizabethan playwright after Marlowe's mysterious early death. Marlowe's plays are known for the use of blank verse and their overreaching protagonists.\n\nA warrant was issued for Marlowe's arrest on 18 May 1593. No reason was given for it, though it was thought to be connected to allegations of blasphemy—a manuscript believed to have been written by Marlowe was said to contain \"vile heretical conceipts\". On 20 May, he was brought to the court to attend upon the Privy Council for questioning. There is no record of their having met that day, however, and he was commanded to attend upon them each day thereafter until \"licensed to the contrary\". Ten days later, he was stabbed to death by Ingram Frizer. Whether or not the stabbing was connected to his arrest remains unknown.\n\nMarlowe was born in Canterbury to shoemaker John Marlowe and his wife Catherine. His date of birth is not known, but he was baptised on 26 February 1564, and is likely to have been born a few days before. Thus, he was just two months older than his contemporary William Shakespeare, who was baptised on 26 April 1564 in Stratford-upon-Avon.\n\nMarlowe attended The King's School in Canterbury (where a house is now named after him) and Corpus Christi College, Cambridge, where he studied on a scholarship and received his Bachelor of Arts degree in 1584. In 1587, the university hesitated to award him his Master of Arts degree because of a rumour that he intended to go to the English college at Rheims, presumably to prepare for ordination as a Roman Catholic priest. However, his degree was awarded on schedule when the Privy Council intervened on his behalf, commending him for his \"faithful dealing\" and \"good service\" to the Queen. The nature of Marlowe's service was not specified by the Council, but its letter to the Cambridge authorities has provoked much speculation, notably the theory that Marlowe was operating as a secret agent working for Sir Francis Walsingham's intelligence service. No direct evidence supports this theory, although the Council's letter is evidence that Marlowe had served the government in some secret capacity.\n\nOf the dramas attributed to Marlowe, \"Dido, Queen of Carthage\" is believed to have been his first. It was performed by the Children of the Chapel, a company of boy actors, between 1587 and 1593. The play was first published in 1594; the title page attributes the play to Marlowe and Thomas Nashe.\n\nMarlowe's first play performed on the regular stage in London, in 1587, was \"Tamburlaine the Great\", about the conqueror Timur (Tamerlane), who rises from shepherd to warlord. It is among the first English plays in blank verse, and, with Thomas Kyd's \"The Spanish Tragedy\", generally is considered the beginning of the mature phase of the Elizabethan theatre. \"Tamburlaine\" was a success, and was followed with \"Tamburlaine the Great, Part II\".\n\nThe two parts of \"Tamburlaine\" were published in 1590; all Marlowe's other works were published posthumously. The sequence of the writing of his other four plays is unknown; all deal with controversial themes.\n\nMarlowe's plays were enormously successful, thanks in part, no doubt, to the imposing stage presence of Edward Alleyn. Alleyn was unusually tall for the time, and the haughty roles of Tamburlaine, Faustus, and Barabas were probably written especially for him. Marlowe's plays were the foundation of the repertoire of Alleyn's company, the Admiral's Men, throughout the 1590s.\n\nMarlowe also wrote the poem \"Hero and Leander\" (published in 1598, and with a continuation by George Chapman the same year), the popular lyric \"The Passionate Shepherd to His Love\", and translations of Ovid's \"Amores\" and the first book of Lucan's \"Pharsalia\". In 1599, his translation of Ovid was banned and copies publicly burned as part of Archbishop Whitgift's crackdown on offensive material.\n\nMarlowe has been credited in the New Oxford Shakespeare series as co-author of the three \"Henry VI\" plays, though some scholars doubt any actual collaboration.\n\nAs with other writers of the period, little is known about Marlowe. What evidence there is can be found in legal records and other official documents. This has not stopped writers of both fiction and non-fiction from speculating about his activities and character. Marlowe has often been described as a spy, a brawler, and a heretic, as well as a \"magician\", \"duellist\", \"tobacco-user\", \"counterfeiter\", and \"rakehell\". J. A. Downie and Constance Kuriyama have argued against the more lurid speculation, but J. B. Steane remarked, \"it seems absurd to dismiss all of these Elizabethan rumours and accusations as 'the Marlowe myth.\n\nMarlowe is alleged to have been a government spy (Park Honan's 2005 biography even had \"Spy\" in its title). The author Charles Nicholl speculates this was the case and suggests that Marlowe's recruitment took place when he was at Cambridge. As noted above, in 1587 the Privy Council ordered the University of Cambridge to award Marlowe his degree of Master of Arts, denying rumours that he intended to go to the English Catholic college in Rheims, saying instead that he had been engaged in unspecified \"affaires\" on \"matters touching the benefit of his country\". Surviving college records from the period also indicate that Marlowe had had a series of unusually lengthy absences from the university – much longer than permitted by university regulations – that began in the academic year 1584–1585. Surviving college buttery (provisions store) accounts indicate he began spending lavishly on food and drink during the periods he was in attendance – more than he could have afforded on his known scholarship income.\n\nIt has sometimes been theorised that Marlowe was the \"Morley\" who was tutor to Arbella Stuart in 1589. This possibility was first raised in a \"TLS\" letter by E. St John Brooks in 1937; in a letter to \"Notes and Queries\", John Baker has added that only Marlowe could be Arbella's tutor due to the absence of any other known \"Morley\" from the period with an MA and not otherwise occupied. If Marlowe was Arbella's tutor (and some biographers think that the \"Morley\" in question may have been a brother of the musician Thomas Morley), it might indicate that he was there as a spy, since Arbella, niece of Mary, Queen of Scots, and cousin of James VI of Scotland, later James I of England, was at the time a strong candidate for the succession to Elizabeth's throne. Frederick S. Boas dismisses the possibility of this identification, based on surviving legal records which document his \"residence in London between September and December 1589\". Marlowe had been party to a fatal quarrel involving his neighbours and the poet Thomas Watson in Norton Folgate and was held in Newgate Prison for a fortnight. In fact, the quarrel and his arrest was on 18 September, he was released on bail on 1 October, and he had to attend court – where he was cleared of any wrongdoing – on 3 December, but there is no record of where he was for the intervening two months.\n\nIn 1592 Marlowe was arrested in the town of Flushing (Vlissingen) (then an English garrison town) in the Netherlands for his alleged involvement in the counterfeiting of coins, presumably related to the activities of seditious Catholics. He was sent to be dealt with by the Lord Treasurer (Burghley) but no charge or imprisonment resulted. This arrest may have disrupted another of Marlowe's spying missions, perhaps by giving the resulting coinage to the Catholic cause. He was to infiltrate the followers of the active Catholic plotter William Stanley and report back to Burghley.\n\nIn early May 1593 several bills were posted about London threatening Protestant refugees from France and the Netherlands who had settled in the city. One of these, the \"Dutch church libel\", written in rhymed iambic pentameter, contained allusions to several of Marlowe's plays and was signed, \"Tamburlaine\". On 11 May the Privy Council ordered the arrest of those responsible for the libels. The next day, Marlowe's colleague Thomas Kyd was arrested. Kyd's lodgings were searched and a 3-page fragment of a heretical tract was found. In a letter to Sir John Puckering, Kyd asserted that it had belonged to Marlowe, with whom he had been writing \"in one chamber\" some two years earlier. In a second letter, Kyd described Marlowe as blasphemous, disorderly, holding treasonous opinions, being an irreligious reprobate, and 'intemperate & of a cruel hart'. At that time they had both been working for an aristocratic patron, probably Ferdinando Stanley, Lord Strange. A warrant for Marlowe's arrest was issued on 18 May, when the Privy Council apparently knew that he might be found staying with Thomas Walsingham, whose father was a first cousin of the late Sir Francis Walsingham, Elizabeth's principal secretary in the 1580s and a man more deeply involved in state espionage than any other member of the Privy Council. Marlowe duly presented himself on 20 May but, there apparently being no Privy Council meeting on that day, was instructed to \"give his daily attendance on their Lordships, until he shall be licensed to the contrary\". On Wednesday, 30 May, Marlowe was killed.\n\nVarious accounts of Marlowe's death were current over the next few years. In his \"Palladis Tamia\", published in 1598, Francis Meres says Marlowe was \"stabbed to death by a bawdy serving-man, a rival of his in his lewd love\" as punishment for his \"epicurism and atheism.\" In 1917, in the \"Dictionary of National Biography\", Sir Sidney Lee wrote that Marlowe was killed in a drunken fight, and this is still often stated as fact today.\n\nThe official account came to light only in 1925 when the scholar Leslie Hotson discovered the coroner's report of the inquest on Marlowe's death, held two days later on Friday 1 June 1593, by the Coroner of the Queen's Household, William Danby. Marlowe had spent all day in a house in Deptford, owned by the widow Eleanor Bull, and together with three men: Ingram Frizer, Nicholas Skeres and Robert Poley. All three had been employed by one or other of the Walsinghams. Skeres and Poley had helped snare the conspirators in the Babington plot and Frizer would later describe Thomas Walsingham as his \"master\" at that time although his role was probably more that of a financial or business agent as he was for Walsingham's wife Audrey a few years later. These witnesses testified that Frizer and Marlowe had argued over payment of the bill (now famously known as the 'Reckoning') exchanging \"divers malicious words\" while Frizer was sitting at a table between the other two and Marlowe was lying behind him on a couch. Marlowe snatched Frizer's dagger and wounded him on the head. In the ensuing struggle, according to the coroner's report, Marlowe was stabbed above the right eye, killing him instantly. The jury concluded that Frizer acted in self-defence, and within a month he was pardoned. Marlowe was buried in an unmarked grave in the churchyard of St. Nicholas, Deptford immediately after the inquest, on 1 June 1593.\n\nThe complete text of the inquest report was published by Leslie Hotson in his book, \"The Death of Christopher Marlowe\", in the introduction to which Prof. G. L. Kittredge said \"The mystery of Marlowe's death, heretofore involved in a cloud of contradictory gossip and irresponsible guess-work, is now cleared up for good and all on the authority of public records of complete authenticity and gratifying fullness\", but this confidence proved fairly short-lived.\n\nHotson himself had considered the possibility that the witnesses had \"concocted a lying account of Marlowe's behaviour, to which they swore at the inquest, and with which they deceived the jury\" but came down against that scenario. Others, however, began to suspect that this was indeed the case. Writing to the \"Times Literary Supplement\" shortly after the book's publication, Eugénie de Kalb disputed that the struggle and outcome as described were even possible, and Samuel A. Tannenbaum insisted the following year that such a wound could not have possibly resulted in instant death, as had been claimed. Even Marlowe's biographer John Bakeless acknowledged that \"some scholars have been inclined to question the truthfulness of the coroner's report. There is something queer about the whole episode\" and said that Hotson's discovery \"raises almost as many questions as it answers.\" It has also been discovered more recently that the apparent absence of a local county coroner to accompany the Coroner of the Queen's Household would, if noticed, have made the inquest null and void.\n\nOne of the main reasons for doubting the truth of the inquest concerns the reliability of Marlowe's companions as witnesses. As an \"agent provocateur\" for the late Sir Francis Walsingham, Robert Poley was a consummate liar, the \"very genius of the Elizabethan underworld\", and is even on record as saying \"I will swear and forswear myself, rather than I will accuse myself to do me any harm.\" The other witness, Nicholas Skeres, had for many years acted as a confidence trickster, drawing young men into the clutches of people in the money-lending racket, including Marlowe's apparent killer, Ingram Frizer, with whom he was currently engaged in just such a swindle. In other words, despite their being referred to as \"generosi\" (gentlemen) in the inquest report, they were all professional liars.\n\nSome biographers, such as Kuriyama and Downie, nevertheless take the inquest to be a true account of what occurred, but in trying to explain what really happened if the account was \"not\" true, others have come up with a variety of murder theories.\n\nSince there are only written documents on which to base any conclusions, and since it is probable that the most crucial information about his death was never committed to writing at all, it is unlikely that the full circumstances of Marlowe's death will ever be known.\n\nDuring his lifetime, Marlowe was reputed to be an atheist which, at that time, held the dangerous implication of being an enemy of God and, by association, the state. With the rise of public fears concerning The School of Night, or \"School of Atheism\" in the late 16th century, accusations of atheism were closely associated with disloyalty to the Protestant monarchy of England.\n\nSome modern historians consider that Marlowe's professed atheism, as with his supposed Catholicism, may have been no more than an elaborate and sustained pretence adopted to further his work as a government spy. Contemporary evidence comes from Marlowe's accuser in Flushing, an informer called Richard Baines. The governor of Flushing had reported that each of the men had \"of malice\" accused the other of instigating the counterfeiting, and of intending to go over to the Catholic \"enemy\"; such an action was considered atheistic by the Church of England. Following Marlowe's arrest in 1593, Baines submitted to the authorities a \"note containing the opinion of one Christopher Marly concerning his damnable judgment of religion, and scorn of God's word.\" Baines attributes to Marlowe a total of eighteen items which \"scoff at the pretensions of the Old and New Testament\" such as, \"Christ was a bastard and his mother dishonest [unchaste]\", \"the woman of Samaria and her sister were whores and that Christ knew them dishonestly\", and, \"St John the Evangelist was bedfellow to Christ and leaned always in his bosom\" (cf. John 13:23–25), and, \"that he used him as the sinners of Sodom\". He also implies that Marlowe had Catholic sympathies. Other passages are merely sceptical in tone: \"he persuades men to atheism, willing them not to be afraid of bugbears and hobgoblins\". The final paragraph of Baines's document reads:\n\nSimilar examples of Marlowe's statements were given by Thomas Kyd after his imprisonment and possible torture (see above); both Kyd and Baines connect Marlowe with the mathematician Thomas Harriot and Sir Walter Raleigh's circle. Another document claimed at around the same time that \"one Marlowe is able to show more sound reasons for Atheism than any divine in England is able to give to prove divinity, and that ... he hath read the Atheist lecture to Sir Walter Raleigh and others.\"\n\nSome critics believe that Marlowe sought to disseminate these views in his work and that he identified with his rebellious and iconoclastic protagonists. However, plays had to be approved by the Master of the Revels before they could be performed, and the censorship of publications was under the control of the Archbishop of Canterbury. Presumably these authorities did not consider any of Marlowe's works to be unacceptable other than the \"Amores\".\n\nMarlowe is frequently claimed to have been homosexual. Others argue that the question of whether an Elizabethan was gay or homosexual in a modern sense is anachronistic. For the Elizabethans, what is often today termed homosexual or bisexual was more likely to be recognised as a sexual act, rather than an exclusive sexual orientation and identity. Some scholars argue that the evidence is inconclusive and that the reports of Marlowe's homosexuality may simply be exaggerated rumours produced after his death. Richard Baines reported Marlowe as saying: \"all they that love not Tobacco & Boies were fools\". David Bevington and Eric Rasmussen describe Baines's evidence as \"unreliable testimony\" and make the comment: \"These and other testimonials need to be discounted for their exaggeration and for their having been produced under legal circumstances we would regard as a witch-hunt\". One critic, J.B. Steane, remarked that he considers there to be \"no \"evidence\" for Marlowe's homosexuality at all.\" Other scholars, however, point to homosexual themes in Marlowe's writing: in \"Hero and Leander\", Marlowe writes of the male youth Leander, \"in his looks were all that men desire\" and that when the youth swims to visit Hero at Sestos, the sea god Neptune becomes sexually excited, \"[i]magining that Ganymede, displeas'd, [h]ad left the Heavens ... [t]he lusty god embrac'd him, call'd him love ... He watched his arms and, as they opened wide [a]t every stroke, betwixt them would he slide [a]nd steal a kiss, ... And dive into the water, and there pry [u]pon his breast, his thighs, and every limb, ... [a]nd talk of love\", while the boy, naive and unaware of Greek love practices, protests, You are deceiv'd, I am no woman, I.' Thereat smil'd Neptune.\" \"Edward the Second\" contains the following passage supporting homosexual relationships:\n\nMarlowe wrote the only play about the life of Edward II up to his time, taking the humanist literary discussion of male sexuality much further than his contemporaries. The play was extremely bold, dealing with a star-crossed love story between Edward II and Piers Gaveston. Though it was common practice at the time to reveal characters as gay to give audiences reason to suspect them as culprits of a given crime, Christopher Marlowe's Edward II is portrayed as a sympathetic character.\n\nWhatever the particular focus of modern critics, biographers and novelists, for his contemporaries in the literary world, Marlowe was above all an admired and influential artist. Within weeks of his death, George Peele remembered him as \"Marley, the Muses' darling\"; Michael Drayton noted that he \"Had in him those brave translunary things / That the first poets had\", and Ben Jonson wrote of \"Marlowe's mighty line\". Thomas Nashe wrote warmly of his friend, \"poor deceased Kit Marlowe\". So too did the publisher Edward Blount, in the dedication of \"Hero and Leander\" to Sir Thomas Walsingham.\n\nAmong the few contemporary dramatists to say anything negative about Marlowe was the anonymous author of the Cambridge University play \"The Return From Parnassus\" (1598) who wrote, \"Pity it is that wit so ill should dwell, / Wit lent from heaven, but vices sent from hell.\"\n\nThe most famous tribute to Marlowe was paid by Shakespeare in \"As You Like It\", where he not only quotes a line from \"Hero and Leander\" (\"Dead Shepherd, now I find thy saw of might, 'Who ever lov'd that lov'd not at first sight?) but also gives to the clown Touchstone the words \"When a man's verses cannot be understood, nor a man's good wit seconded with the forward child, understanding, it strikes a man more dead than a great reckoning in a little room.\" This appears to be a reference to Marlowe's murder which involved a fight over the \"reckoning\", the bill, as well as to a line in Marlowe's \"Jew of Malta\"\"Infinite riches in a little room\".\n\nShakespeare was heavily influenced by Marlowe in his work, as can be seen in the re-using of Marlovian themes in \"Antony and Cleopatra\", \"The Merchant of Venice\", \"Richard II\", and \"Macbeth\" (\"Dido\", \"Jew of Malta\", \"Edward II\" and \"Doctor Faustus\", respectively). In \"Hamlet\", after meeting with the travelling actors, Hamlet requests the Player perform a speech about the Trojan War, which at 2.2.429–32 has an echo of Marlowe's \"Dido, Queen of Carthage\". In \"Love's Labour's Lost\" Shakespeare brings on a character \"Marcade\" (three syllables) in conscious acknowledgement of Marlowe's character \"Mercury\", also attending the King of Navarre, in \"Massacre at Paris\". The significance, to those of Shakespeare's audience who had read \"Hero and Leander\", was Marlowe's identification of himself with the god Mercury.\n\nAn argument has arisen centred on the notion that Marlowe may have faked his death and then continued to write under the assumed name of William Shakespeare. Orthodox academic consensus rejects alternative candidates for authorship of Shakespeare's plays and sonnets, including Marlowe.\n\nA Marlowe Memorial in the form of a bronze sculpture of \"The Muse of Poetry\" by Edward Onslow Ford was erected by subscription in Buttermarket, Canterbury in 1891.\n\nIn July 2002, a memorial window to Marlowe – a gift of the Marlowe Society – was unveiled in Poets' Corner in Westminster Abbey. Controversially, a question mark was added to the generally accepted date of death. On 25 October 2011 a letter from Paul Edmondson and Stanley Wells was published by \"The Times\" newspaper, in which they called on the Dean and Chapter to remove the question mark on the grounds that it \"flew in the face of a mass of unimpugnable evidence\". In 2012, they renewed this call in their e-book \"Shakespeare Bites Back\", adding that it \"denies history\", and again the following year in their book \"Shakespeare Beyond Doubt\".\n\nMarlowe has been used as a character in plays, novels, films and TV-productions.\n\nThe dates of composition are approximate.\n\n\nThe play \"Lust's Dominion\" was attributed to Marlowe upon its initial publication in 1657, though scholars and critics have almost unanimously rejected the attribution. He may also have written or co-written \"Arden of Faversham\".\n\n\n"}
{"id": "5772", "url": "https://en.wikipedia.org/wiki?curid=5772", "title": "Cricket (disambiguation)", "text": "Cricket (disambiguation)\n\nCricket is a bat-and-ball sport contested by two teams.\n\nCricket also commonly refers to:\n\n\nCricket(s) or The Cricket(s) may also refer to:\n\n\n\n\n\n\n"}
{"id": "5776", "url": "https://en.wikipedia.org/wiki?curid=5776", "title": "Caving", "text": "Caving\n\nCaving – also traditionally known as spelunking in the United States and Canada and potholing in the United Kingdom and Ireland – is the recreational pastime of exploring wild (generally non-commercial) cave systems. In contrast, speleology is the scientific study of caves and the cave environment.\n\nThe challenges involved in caving vary according to the cave being visited; in addition to the total absence of light beyond the entrance negotiating pitches, squeezes, and water hazards can be difficult. Cave diving is a distinct, and more hazardous, sub-speciality undertaken by a small minority of technically proficient cavers. In an area of overlap between recreational pursuit and scientific study, the most devoted and serious-minded cavers become accomplished at the surveying and mapping of caves and the formal publication of their efforts. In the US, these are generally private, but in the UK and other European countries are published freely and publicly. \n\nSometimes categorized as an \"extreme sport\", it is not commonly considered as such by long-time enthusiasts, who may dislike the term for its connotation of disregard for safety.\n\nMany caving skills overlap with those involved in canyoning, and mine and urban exploration.\n\nCaving is often undertaken for the enjoyment of the outdoor activity or for physical exercise, as well as original exploration, similar to mountaineering or diving. Physical or biological science is also an important goal for some cavers, while others are engaged in cave photography. Virgin cave systems comprise some of the last unexplored regions on Earth and much effort is put into trying to locate, enter and survey them. In well-explored regions (such as most developed nations), the most accessible caves have already been explored, and gaining access to new caves often requires cave digging or cave diving.\n\nCaving, in certain areas, has also been utilized as a form of eco and adventure tourism. Tour companies have established an industry leading and guiding tours into and through caves. Depending on the type of cave and the type of tour, the experience could be adventure-based or ecological-based. In many areas (e.g. America, the oceanic islands of Tenerife, Iceland and Hawaii), there are tours led through lava tubes by a guiding service.\n\nCaving has also been described as an \"individualist's team sport\" by some, as cavers can often make a trip without direct physical assistance from others but will generally go in a group for companionship or to provide emergency help if needed. Some however consider the assistance cavers give each other as a typical team sport activity.\n\nClay Perry, an American caver of the 1940s, wrote about a group of men and boys who explored and studied caves throughout New England. This group referred to themselves as \"spelunkers\", a term derived from the Latin \"\" \"cave, cavern, den\", itself from the Greek \"spēlynks\" \"cave\". This is regarded as the first use of the word in the Americas. Throughout the 1950s, \"spelunking\" was the general term used for exploring caves in US English. It was used freely, without any positive or negative connotations, although only rarely outside the US.\n\nIn the 1960s, the terms \"spelunking\" and \"spelunker\" began to be considered déclassé among experienced enthusiasts. In 1985, Steve Knutson – editor of the National Speleological Society (NSS) publication \"American Caving Accidents\" – made the following distinction:\n\nThis sentiment is exemplified by bumper stickers and T-shirts displayed by some cavers: \"Cavers rescue spelunkers\". Nevertheless, outside the caving community, \"spelunking\" and \"spelunkers\" predominately remain neutral terms referring to the practice and practitioners, without any respect to skill level.\n\n\"Potholing\" refers to the act of exploring \"potholes\", a word originating in the north of England for predominantly vertical caves.\n\nThe base term \"caving\" comes from the Latin \"cavea\" or \"caverna\", meaning simply, a cave.\n\nCaving was pioneered by Édouard-Alfred Martel (1859–1938), who first achieved the descent and exploration of the Gouffre de Padirac, in France, as early as 1889 and the first complete descent of a 110-metre wet vertical shaft at Gaping Gill, in Yorkshire, England, in 1895. He developed his own techniques based on ropes and metallic ladders. Martel visited Kentucky and notably Mammoth Cave National Park in October 1912. In the 1920s famous US caver Floyd Collins made important explorations in the area and in the 1930s, as caving became increasingly popular, small exploration teams both in the Alps and in the karstic high plateaus of southwest France (Causses and Pyrenees) transformed cave exploration into both a scientific and recreational activity. Robert de Joly, Guy de Lavaur and Norbert Casteret were prominent figures of that time, surveying mostly caves in Southwest France. During World War II, an alpine team composed of Pierre Chevalier, Fernand Petzl, Charles Petit-Didier and others explored the Dent de Crolles cave system near Grenoble, which became the deepest explored system in the world (-658m) at that time. The lack of available equipment during the war forced Pierre Chevalier and the rest of the team to develop their own equipment, leading to technical innovation. The scaling-pole (1940), nylon ropes (1942), use of explosives in caves (1947) and mechanical rope-ascenders (Henri Brenot's \"monkeys\", first used by Chevalier and Brenot in a cave in 1934) can be directly associated to the exploration of the Dent de Crolles cave system.\n\nIn 1941, American cavers organized themselves into the National Speleological Society (NSS) to advance the exploration, conservation, study and understanding of caves in the United States. American caver Bill Cuddington, known as \"Vertical Bill\", further developed the single-rope technique (SRT) in the late 1950s. In 1958, two Swiss alpinists, Juesi and Marti teamed together, creating the first rope ascender known as the Jumar. In 1968 Bruno Dressler asked Fernand Petzl, who worked as a metals machinist, to build a rope-ascending tool, today known as the Petzl Croll, that he had developed by adapting the Jumar to pit caving. Pursuing these developments, Petzl started in the 1970s a caving equipment manufacturing company named Petzl. The development of the rappel rack and the evolution of mechanical ascension systems extended the practice and safety of pit exploration to a wider range of cavers.\n\nHard hats are worn to protect the head from bumps and falling rocks. The caver's primary light source is usually mounted on the helmet in order to keep the hands free. Electric LED lights are most common. Many cavers carry two or more sources of light – one as primary and the others as backup in case the first fails. More often than not, a second light will be mounted to the helmet for quick transition if the primary fails. Carbide lamp systems are an older form of illumination, inspired by miner's equipment, and are still used by some cavers, particularly on remote expeditions where electric charging facilities are not available.\n\nThe type of clothes worn underground varies according to the environment of the cave being explored, and the local culture. In cold caves, the caver may wear a warm base layer that retains its insulating properties when wet, such as a fleece (\"furry\") suit or polypropylene underwear, and an oversuit of hard-wearing (e.g., cordura) or waterproof (e.g., PVC) material. Lighter clothing may be worn in warm caves, particularly if the cave is dry, and in tropical caves thin polypropylene clothing is used, to provide some abrasion protection while remaining as cool as possible. Wetsuits may be worn if the cave is particularly wet or involves stream passages. On the feet boots are worn – hiking-style boots in drier caves, or rubber boots (such as wellies) often with neoprene socks (\"wetsocks\") in wetter caves. Knee-pads (and sometimes elbow-pads) are popular for protecting joints during crawls. Depending on the nature of the cave, gloves are sometimes worn to protect the hands against abrasion or cold. In pristine areas and for restoration, clean oversuits and powder-free, non-latex surgical gloves are used to protect the cave itself from contaminants.\nRopes are used for descending or ascending pitches (single rope technique or SRT) or for protection. Knots commonly used in caving are the figure-of-eight- (or figure-of-nine-) loop, bowline, alpine butterfly, and Italian hitch. Ropes are usually rigged using bolts, slings, and carabiners. In some cases cavers may choose to bring and use a flexible metal ladder.\n\nIn addition to the equipment already described, cavers frequently carry packs containing first-aid kits, emergency equipment, and food. Containers for securely transporting urine are also commonly carried. On longer trips, containers for securely transporting feces out of the cave are carried.\n\nDuring very long trips, it may be necessary to camp in the cave – some cavers have stayed underground for many days, or in particularly extreme cases, for weeks at a time. This is particularly the case when exploring or mapping very extended cave systems, where it would be impractical to retrace the route back to the surface regularly. Such long trips necessitate the cavers carrying provisions, sleeping and cooking equipment.\n\nCaves can be dangerous places; hypothermia, falling, flooding, falling rocks and physical exhaustion are the main risks. Rescuing people from underground is difficult and time-consuming, and requires special skills, training, and equipment. Full-scale cave rescues often involve the efforts of dozens of rescue workers (often other long-time cavers who have participated in specialized courses, as normal rescue staff are not sufficiently experienced in cave environments), who may themselves be put in jeopardy in effecting the rescue. This said, caving is not necessarily a high-risk sport (especially if it does not involve difficult climbs or diving). As in all physical sports, knowing one's limitations is key.\n\nCaving in warmer climates carries the risk of contracting histoplasmosis, a fungal infection that is contracted from bird or bat droppings. It can cause pneumonia and can disseminate in the body to cause continued infections.\n\nIn many parts of the world, leptospirosis (\"a type of bacterial infection spread by animals\" including rats) is a distinct threat due to the presence of rat urine in rainwater or precipitation that enters the caves water system. Complications are uncommon, but can be serious.\nSafety risks while caving can be minimized by using a number of techniques:\n\n\nMany cave environments are very fragile. Many speleothems can be damaged by even the slightest touch and some by impacts as slight as a breath. Research suggests that increased carbon dioxide levels can lead to \"a higher equilibrium concentration of calcium within the drip waters feeding the speleothems, and hence causes dissolution of existing features.\" In 2008, researchers found evidence that respiration from cave visitors may generate elevated carbon dioxide concentrations in caves, leading to increased temperatures of up to 3 °C and a dissolution of existing features.\n\nPollution is also of concern. Since water that flows through a cave eventually comes out in streams and rivers, any pollution may ultimately end up in someone's drinking water, and can even seriously affect the surface environment, as well. Even minor pollution such as dropping organic material can have a dramatic effect on the cave biota.\n\nCave-dwelling species are also very fragile, and often, a particular species found in a cave may live within that cave alone, and be found nowhere else in the world, such as Alabama cave shrimp. Cave-dwelling species are accustomed to a near-constant climate of temperature and humidity, and any disturbance can be disruptive to the species' life cycles. Though cave wildlife may not always be immediately visible, it is typically nonetheless present in most caves.\n\nBats are one such fragile species of cave-dwelling animal. Bats which hibernate are most vulnerable during the winter season, when no food supply exists on the surface to replenish the bat's store of energy should it be awakened from hibernation. Bats which migrate are most sensitive during the summer months when they are raising their young. For these reasons, visiting caves inhabited by hibernating bats is discouraged during cold months; and visiting caves inhabited by migratory bats is discouraged during the warmer months when they are most sensitive and vulnerable. Due to an affliction affecting bats in the northeastern US known as white nose syndrome (WNS), the US Fish & Wildlife Service has called for a moratorium effective March 26, 2009, on caving activity in states known to have hibernacula (MD, NY, VT, NH, MA, CT, NJ, PA, VA, and WV) affected by WNS, as well as adjoining states.\n\nSome cave passages may be marked with flagging tape or other indicators to show biologically, aesthetically, or archaeologically sensitive areas. Marked paths may show ways around notably fragile areas such as a pristine floor of sand or silt which may be thousands of years old, dating from the last time water flowed through the cave. Such deposits may easily be spoiled forever by a single misplaced step. Active formations such as flowstone can be similarly marred with a muddy footprint or handprint, and ancient human artifacts, such as fiber products, may even crumble to dust under all but the most gentle touch.\n\nIn 1988, concerned that cave resources were becoming increasingly damaged through unregulated use, Congress enacted the Federal Cave Resources Protection Act, giving land management agencies in the United States expanded authority to manage cave conservation on public land.\n\nIn Europe there have been some panoramic 360° records and VR projects as a means of sharing interesting caves or quarries:\n\nCavers in many countries have created organizations for the administration and oversight of caving activities within their nations. The oldest of these is the French Federation of Speleology (originally Société de spéléologie) founded by Édouard-Alfred Martel in 1895, which produced the first periodical journal in speleology, \"Spelunca\". The National Speleological Society in the US was founded in 1941 (originally formed as the Speleological Society of the District of Columbia on May 6, 1939), and the first speleological institute in the world was founded in 1920 in Cluj-Napoca, Romania, by Emil Racovita, a Romanian biologist, zoologist, speleologist and explorer of Antarctica.\n\n\n\n"}
{"id": "5778", "url": "https://en.wikipedia.org/wiki?curid=5778", "title": "Cave", "text": "Cave\n\nA cave or cavern is a natural void in the ground, specifically a space large enough for a human to enter. Caves often form by the weathering of rock and often extend deep underground. The word \"cave\" can also refer to much smaller openings such as sea caves, rock shelters, and grottos, though strictly speaking a cave is exogene, meaning it is deeper than its opening is wide, and a rock shelter is endogene. \n\nSpeleology is the science of exploration and study of all aspects of caves and the cave environment. Visiting or exploring caves for recreation may be called \"caving\", \"potholing\", or \"spelunking\".\n\nThe formation and development of caves is known as \"speleogenesis\"; it can occur over the course of millions of years. Caves can range widely in size, and are formed by various geological processes. These may involve a combination of chemical processes, erosion by water, tectonic forces, microorganisms, pressure, and atmospheric influences. Isotopic dating techniques can be applied to cave sediments, to determine the timescale of the geological events which formed and shaped present-day caves.\n\nIt is estimated that a cave cannot exceed in depth due to the pressure of overlying rocks. For karst caves the maximum depth is determined on the basis of the lower limit of karst forming processes, coinciding with the base of the soluble carbonate rocks. Most caves are formed in limestone by dissolution.\n\nCaves can be classified in various other ways as well, including a contrast between active and relict: active caves have water flowing through them; relict caves do not, though water may be retained in them. Types of active caves include inflow caves (\"into which a stream sinks\"), outflow caves (\"from which a stream emerges\"), and through caves (\"traversed by a stream\").\n\nSolutional caves or karst caves are the most frequently occurring caves. Such caves form in rock that is soluble; most occur in limestone, but they can also form in other rocks including chalk, dolomite, marble, salt, and gypsum. Rock is dissolved by natural acid in groundwater that seeps through bedding planes, faults, joints, and comparable features. Over time cracks enlarge to become caves and cave systems.\n\nThe largest and most abundant solutional caves are located in limestone. Limestone dissolves under the action of rainwater and groundwater charged with HCO (carbonic acid) and naturally occurring organic acids. The dissolution process produces a distinctive landform known as \"karst\", characterized by sinkholes and underground drainage. Limestone caves are often adorned with calcium carbonate formations produced through slow precipitation. These include flowstones, stalactites, stalagmites, helictites, soda straws and columns. These secondary mineral deposits in caves are called \"speleothems\".\n\nThe portions of a solutional cave that are below the water table or the local level of the groundwater will be flooded.\n\nLechuguilla Cave in New Mexico and nearby Carlsbad Cavern are now believed to be examples of another type of solutional cave. They were formed by HS (hydrogen sulfide) gas rising from below, where reservoirs of oil give off sulfurous fumes. This gas mixes with groundwater and forms HSO (sulfuric acid). The acid then dissolves the limestone from below, rather than from above, by acidic water percolating from the surface.\n\nCaves formed at the same time as the surrounding rock are called primary caves.\n\nLava tubes are formed through volcanic activity and are the most common primary caves. As lava flows downhill, its surface cools and solidifies. Hot liquid lava continues to flow under that crust, and if most of it flows out, a hollow tube remains. Such caves can be found in the Canary Islands, Jeju-do, the basaltic plains of Eastern Idaho, and in other places. Kazumura Cave near Hilo, Hawaii is a remarkably long and deep lava tube; it is .\n\nLava caves include but are not limited to lava tubes. Other caves formed through volcanic activity include rifts, lava molds, open vertical conduits, inflationary, blisters, among others.\n\nSea caves are found along coasts around the world. A special case is littoral caves, which are formed by wave action in zones of weakness in sea cliffs. Often these weaknesses are faults, but they may also be dykes or bedding-plane contacts. Some wave-cut caves are now above sea level because of later uplift. Elsewhere, in places such as Thailand's Phang Nga Bay, solutional caves have been flooded by the sea and are now subject to littoral erosion. Sea caves are generally around in length, but may exceed .\n\nCorrasional or erosional caves are those that form entirely by erosion by flowing streams carrying rocks and other sediments. These can form in any type of rock, including hard rocks such as granite. Generally there must be some zone of weakness to guide the water, such as a fault or joint. A subtype of the erosional cave is the wind or aeolian cave, carved by wind-born sediments. Many caves formed initially by solutional processes often undergo a subsequent phase of erosional or vadose enlargement where active streams or rivers pass through them.\n\nGlacier caves are formed by melting ice and flowing water within and under glaciers. The cavities are influenced by the very slow flow of the ice, which tends to collapse the caves again. Glacier caves are sometimes misidentified as \"ice caves\", though this latter term is properly reserved for bedrock caves that contain year-round ice formations.\n\nFracture caves are formed when layers of more soluble minerals, such as gypsum, dissolve out from between layers of less soluble rock. These rocks fracture and collapse in blocks of stone.\n\nTalus caves are formed by the openings among large boulders that have fallen down into a random heap, often at the bases of cliffs. These unstable deposits are called talus or scree, and may be subject to frequent rockfalls and landslides.\n\nAnchialine caves are caves, usually coastal, containing a mixture of freshwater and saline water (usually sea water). They occur in many parts of the world, and often contain highly specialized and endemic fauna.\n\n\nCaves are found throughout the world, but only a small portion of them have been explored and documented by cavers. The distribution of documented cave systems is widely skewed toward countries where caving has been popular for many years (such as France, Italy, Australia, the UK, the United States, etc.). As a result, explored caves are found widely in Europe, Asia, North America and Oceania, but are sparse in South America, Africa, and Antarctica.\n\nThis is a rough generalization, as large expanses of North America and Asia contain no documented caves, whereas areas such as the Madagascar dry deciduous forests and parts of Brazil contain many documented caves. As the world's expanses of soluble bedrock are researched by cavers, the distribution of documented caves is likely to shift. For example, China, despite containing around half the world's exposed limestone—more than —has relatively few documented caves.\n\n\n\nCave-inhabiting animals are often categorized as troglobites (cave-limited species), troglophiles (species that can live their entire lives in caves, but also occur in other environments), trogloxenes (species that use caves, but cannot complete their life cycle fully in caves) and accidentals (animals not in one of the previous categories). Some authors use separate terminology for aquatic forms (for example, stygobites, stygophiles, and stygoxenes).\n\nOf these animals, the troglobites are perhaps the most unusual organisms. Troglobitic species often show a number of characteristics, termed troglomorphic, associated with their adaptation to subterranean life. These characteristics may include a loss of pigment (often resulting in a pale or white coloration), a loss of eyes (or at least of optical functionality), an elongation of appendages, and an enhancement of other senses (such as the ability to sense vibrations in water). Aquatic troglobites (or stygobites), such as the endangered Alabama cave shrimp, live in bodies of water found in caves and get nutrients from detritus washed into their caves and from the feces of bats and other cave inhabitants. Other aquatic troglobites include cave fish, and cave salamanders such as the olm and the Texas blind salamander.\n\nCave insects such as Oligaphorura (formerly Archaphorura) schoetti are troglophiles, reaching in length. They have extensive distribution and have been studied fairly widely. Most specimens are female, but a male specimen was collected from St Cuthberts Swallet in 1969.\n\nBats, such as the gray bat and Mexican free-tailed bat, are trogloxenes and are often found in caves; they forage outside of the caves. Some species of cave crickets are classified as trogloxenes, because they roost in caves by day and forage above ground at night.\n\nBecause of the fragile nature of the cave ecosystem, and the fact that cave regions tend to be isolated from one another, caves harbor a number of endangered species, such as the Tooth cave spider, liphistius trapdoor spider, and the gray bat.\n\nCaves are visited by many surface-living animals, including humans. These are usually relatively short-lived incursions, due to the lack of light and sustenance.\n\nCave entrances often have typical florae. For instance, in the eastern temperate United States, cave entrances are most frequently (and often densely) populated by the bulblet fern, \"Cystopteris bulbifera\".\n\nThroughout history, primitive peoples have made use of caves. The earliest human fossils found in caves come from a series of caves near Krugersdorp and Mokopane in South Africa. The cave sites of Sterkfontein, Swartkrans, Kromdraai B, Drimolen, Malapa, Cooper's D, Gladysvale, Gondolin and Makapansgat have yielded a range of early human species dating back to between three and one million years ago, including Australopithecus africanus, Australopithecus sediba and Paranthropus robustus. However, it is not generally thought that these early humans were living in the caves, but that they were brought into the caves by carnivores that had killed them.\n\nThe first early hominid ever found in Africa, the Taung Child in 1924, was also thought for many years to come from a cave, where it had been deposited after being predated on by an eagle. However, this is now debated (Hopley et al., 2013; Am. J. Phys. Anthrop.). Caves do form in the dolomite of the Ghaap Plateau, including the Early, Middle and Later Stone Age site of Wonderwerk Cave; however, the caves that form along the escarpment's edge, like that hypothesised for the Taung Child, are formed within a secondary limestone deposit called tufa. There is numerous evidence for other early human species inhabiting caves from at least one million years ago in different parts of the world, including Homo erectus in China at Zhoukoudian, Homo rhodesiensis in South Africa at the Cave of Hearths (Makapansgat), Homo neandertalensis and Homo heidelbergensis in Europe at Archaeological Site of Atapuerca, Homo floresiensis in Indonesia, and the Denisovans in southern Siberia.\n\nIn southern Africa, early modern humans regularly used sea caves as shelter starting about 180,000 years ago when they learned to exploit the sea for the first time (Marean et al., 2007; Nature). The oldest known site is PP13B at Pinnacle Point. This may have allowed rapid expansion of humans out of Africa and colonization of areas of the world such as Australia by 60-50,000 years ago. Throughout southern Africa, Australia, and Europe, early modern humans used caves and rock shelters as sites for rock art, such as those at Giants Castle. Caves such as the yaodong in China were used for shelter; other caves were used for burials (such as rock-cut tombs), or as religious sites (such as ). Among the known sacred caves are China's Cave of a Thousand Buddhas and the sacred caves of Crete.\n"}
{"id": "5781", "url": "https://en.wikipedia.org/wiki?curid=5781", "title": "Chinese numerals", "text": "Chinese numerals\n\nChinese numerals are words and characters used to denote numbers in Chinese.\n\nToday, speakers of Chinese use three written numeral systems: the system of Hindu-Arabic numerals used worldwide, and two indigenous systems. The more familiar indigenous system is based on Chinese characters that correspond to numerals in the spoken language. These are shared with other languages of the Chinese cultural sphere such as Japanese, Korean and Vietnamese. Most people and institutions in China and Taiwan primarily use the Hindu-Arabic or mixed Arabic-Chinese systems for convenience, with traditional Chinese numerals used in finance, mainly for writing amounts on checks, banknotes, some ceremonial occasions, some boxes, and on commercials.\nThe other indigenous system is the Suzhou numerals, or \"huama\", a positional system, the only surviving form of the rod numerals. These were once used by Chinese mathematicians, and later in Chinese markets, such as those in Hong Kong before the 1990s, but have been gradually supplanted by Hindu-Arabic (and also Roman) numerals.\n\nThe Chinese character numeral system consists of the Chinese characters used by the Chinese written language to write spoken numerals. Similar to spelling-out numbers in English (e.g., \"one thousand nine hundred forty-five\"), it is not an independent system \"per se\". Since it reflects spoken language, it does not use the positional system as in Arabic numerals, in the same way that spelling out numbers in English does not.\n\nThere are characters representing the numbers zero through nine, and other characters representing larger numbers such as tens, hundreds, thousands and so on. There are two sets of characters for Chinese numerals: one for everyday writing, known as \"xiǎoxiě\" (), and one for use in commercial or financial contexts, known as \"dàxiě\" (). The latter arose because the characters used for writing numerals are geometrically simple, so simply using those numerals cannot prevent forgeries in the same way spelling numbers out in English would. A forger could easily change the everyday characters 三十 (30) to 五千 (5000) just by adding a few strokes. That would not be possible when writing using the financial characters 叁拾 (30) and 伍仟 (5000). They are also referred to as \"banker's numerals\", \"anti-fraud numerals\", or \"banker's anti-fraud numerals\". For the same reason, rod numerals were never used in commercial records.\n\nT denotes Traditional Chinese characters, while S denotes Simplified Chinese characters.\n\nIn the People's Liberation Army of the People's Republic of China, some numbers will have altered names when used for clearer radio communications. They are:\n\nFor numbers larger than 10,000, similarly to the long and short scales in the West, there have been four systems in ancient and modern usage. The original one, with unique names for all powers of ten up to the 14th, is ascribed to the Yellow Emperor in the 6th century book by Zhen Luan, \"Wujing suanshu\" (Arithmetic in Five Classics). In modern Chinese only the second system is used, in which the same ancient names are used, but each represents a number 10,000 (myriad, 萬 wàn) times the previous:\n\nIn practice, this situation does not lead to ambiguity, with the exception of 兆 (zhào), which means 10 according to the system in common usage throughout the Chinese communities as well as in Japan and Korea, but has also been used for 10 in recent years (especially in mainland China for megabyte). To avoid problems arising from the ambiguity, the PRC government never uses this character in official documents, but uses 万亿 (wànyì) instead. Partly due to this, combinations of 万 and 亿 are often used instead of the larger units of the traditional system as well, for example 亿亿 (yìyì) instead of 京. The ROC government in Taiwan uses 兆 (zhào) to mean 10 in official documents.\n\nNumerals beyond 載 zǎi come from Buddhist texts in Sanskrit, but are mostly found in ancient texts. Some of the following words are still being used today, but may have transferred meanings.\n\nThe following are characters used to denote small order of magnitude in Chinese historically. With the introduction of SI units, some of them have been incorporated as SI prefixes, while the rest have fallen into disuse.\n\nIn the People's Republic of China, the translations for the SI prefixes in 1981 were different from those used today. The larger (兆, 京, 垓, 秭, 穰) and smaller Chinese numerals (微, 纖, 沙, 塵, 渺) were defined as translations for the SI prefixes as \"mega\", \"giga\", \"tera\", \"peta\", \"exa\", \"micro\", \"nano\", \"pico\", \"femto\", \"atto\", resulting in the creation of yet more values for each numeral.\n\nThe Republic of China (Taiwan) defined 百萬 as the translation for \"mega\". This translation is widely used in official documents, academic communities, informational industries, etc. However, the civil broadcasting industries sometimes use 兆赫 to represent \"megahertz\".\n\nToday, the governments of both China and Taiwan use phonetic transliterations for the SI prefixes. However, the governments have each chosen different Chinese characters for certain prefixes. The following table lists the two different standards together with the early translation.\n\nMultiple-digit numbers are constructed using a multiplicative principle; first the digit itself (from 1 to 9), then the place (such as 10 or 100); then the next digit.\n\nIn Mandarin, the multiplier (\"liǎng\") is often used rather than (\"èr\") for all numbers 200 and greater with the \"2\" numeral (although as noted earlier this varies from dialect to dialect and person to person). Use of both 兩 (\"liǎng\") or 二 (\"èr\") are acceptable for the number 200. When writing in the Cantonese dialect, 二 (\"yi\") is used to represent the \"2\" numeral for all numbers. In the southern Min dialect of Chaozhou (Teochew), 兩 (\"no\") is used to represent the \"2\" numeral in all numbers from 200 onwards. Thus:\nFor the numbers 11 through 19, the leading \"one\" () is usually omitted. In some dialects, like Shanghainese, when there are only two significant digits in the number, the leading \"one\" and the trailing zeroes are omitted. Sometimes, the one before \"ten\" in the middle of a number, such as 213, is omitted. Thus:\n\nNotes:\n\nIn certain older texts like the Protestant Bible or in poetic usage, numbers such as 114 may be \"written\" as [100] [10] [4] ().\n\nOutside of Taiwan, digits are sometimes grouped by myriads instead of thousands. Hence it is more convenient to think of numbers here as in groups of four, thus 1,234,567,890 is regrouped here as 12,3456,7890. Larger than a myriad, each number is therefore four zeroes longer than the one before it, thus 10000 × () = (). If one of the numbers is between 10 and 19, the leading \"one\" is omitted as per the above point. Hence (numbers in parentheses indicate that the number has been written as one number rather than expanded):\nIn Taiwan, pure Arabic numerals are officially always and only grouped by thousands. Unofficially, they are often not grouped, particularly for numbers below 100,000. Mixed Arabic-Chinese numerals are often used in order to denote myriads. This is used both officially and unofficially, and come in a variety of styles:\nInterior zeroes before the unit position (as in 1002) must be spelt explicitly. The reason for this is that trailing zeroes (as in 1200) are often omitted as shorthand, so ambiguity occurs. One zero is sufficient to resolve the ambiguity. Where the zero is before a digit other than the units digit, the explicit zero is not ambiguous and is therefore optional, but preferred. Thus:\nTo construct a fraction, the denominator is written first, followed by (\"parts of/dividing\") and then the numerator. This is the opposite of how fractions are read in English, which is numerator first. Each half of the fraction is written the same as a whole number. Mixed numbers are written with the whole-number part first, followed by (\"and\"), then the fractional part.\nPercentages are constructed similarly, using (100) as the denominator. The (one) before is omitted. (Like the English \"one hundred\" or \"a hundred\", typically the quantity 100 is denoted in Chinese.)\nDecimal numbers are constructed by first writing the whole number part, then inserting a point (), and finally the decimal expression. The decimal expression is written using only the digits for 0 to 9, without multiplicative words.\n, [half] functions as a number and therefore requires a measure word. Example: is \"half a glass of water\".\n\nOrdinal numbers are formed by adding (\"sequence\") before the number.\nNegative numbers are formed by adding fù () before the number.\n\nChinese grammar requires the use of classifiers (measure words) when a numeral is used together with a noun to express a quantity. For example, \"three people\" is expressed as , \"three ( particle) person\", where / \"\" is a classifier. There exist many different classifiers, for use with different sets of nouns, although / is the most common, and may be used informally in place of other classifiers.\n\nChinese uses cardinal numbers in certain situations in which English would use ordinals. For example, (literally \"three story/storey\") means \"third floor\" (\"second floor\" in British ). Likewise, (literally \"twenty-one century\") is used for \"21st century\".\n\nNumbers of years are commonly spoken as a sequence of digits, as in (\"two zero zero one\") for the year 2001. Names of months and days (in the Western system) are also expressed using numbers: (\"one month\") for January, etc.; and (\"week one\") for Monday, etc. Only one exception, Sunday is , or informally , both literally \"week day\". When meaning \"week\", \"\" and \"\" are interchangeable. \"\" or \"\" means \"day of worship\". Chinese Catholics call Sunday \"\" \"\", \"Lord's day\".\n\nFull dates are usually written in the format 2001年1月20日 for January 20, 2001 (using \"year\", \"month\", and \"day\") – all the numbers are read as cardinals, not ordinals, with no leading zeroes, and the year is read as a sequence of digits. For brevity the , and may be dropped to give a date composed of just numbers. For example \"6-4\" in Chinese is \"six-four\", short for \"month six, day four\" i.e. June Fourth, a common Chinese shorthand for the 1989 Tiananmen Square protests (because of the violence that occurred on June 4th). For another example 67, in Chinese is sixty seven, short for year nineteen sixty seven, a common Chinese shorthand for the Hong Kong 1967 leftist riots. \n\nIn the same way that Roman numerals were standard in ancient and medieval Europe for mathematics and commerce, the Chinese formerly used the rod numerals, which is a positional system. The Suzhou numerals () system is a variation of the Southern Song rod numerals. Nowadays, the \"huāmǎ\" system is only used for displaying prices in Chinese markets or on traditional handwritten invoices.\n\nThere is a common method of using of one hand to signify the numbers one to ten. While the five digits on one hand can express the numbers one to five, six to ten have special signs that can be used in commerce or day-to-day communication.\n\nMost Chinese numerals of later periods were descendants of the Shang dynasty oracle numerals of the 14th century BC. The oracle bone script numerals were found on tortoise shell and animal bones. In early civilizations, the Shang were able to express any numbers, however large, with only nine symbols and a counting board.\n\nSome of the bronze script numerals such as 1, 2, 3, 4, 10, 11, 12, and 13 became part of the system of rod numerals.\n\nIn this system, horizontal rod numbers are used for the tens, thousands, hundred thousands etc. It's written in \"Sunzi Suanjing\" that \"one is vertical, ten is horizontal\".\n\nThe counting rod numerals system has place value and decimal numerals for computation, and was used widely by Chinese merchants, mathematicians and astronomers from the Han dynasty to the 16th century.\n\nIn AD 690, Empress Wǔ promulgated Zetian characters, one of which was \"〇\". The word is now used as a synonym for the number zero.\n\nAlexander Wylie, Christian missionary to China, in 1853 already refuted the notion that \"the Chinese numbers were written in words at length\", and stated that in ancient China, calculation was carried out by means of counting rods, and \"the written character is evidently a rude presentation of these\". After being introduced to the rod numerals, he said \"Having thus obtained a simple but effective system of figures, we find the Chinese in actual use of a method of notation depending on the theory of local value [i.e. place-value], several centuries before such theory was understood in Europe, and while yet the science of numbers had scarcely dawned among the Arabs.\"\n\nDuring the Ming and Qing dynasties (after Arabic numerals were introduced into China), some Chinese mathematicians used Chinese numeral characters as positional system digits. After the Qing period, both the Chinese numeral characters and the Suzhou numerals were replaced by Arabic numerals in mathematical writings.\n\nTraditional Chinese numeric characters are also used in Japan and Korea and were used in Vietnam before the 20th century. In vertical text (that is, read top to bottom), using characters for numbers is the norm, while in horizontal text, Arabic numerals are most common. Chinese numeric characters are also used in much the same formal or decorative fashion that Roman numerals are in Western cultures. Chinese numerals may appear together with Arabic numbers on the same sign or document.\n\n"}
{"id": "5783", "url": "https://en.wikipedia.org/wiki?curid=5783", "title": "Computer program", "text": "Computer program\n\nA computer program is a collection of instructions that performs a specific task when executed by a computer. A computer requires programs to function.\n\nA computer program is usually written by a computer programmer in a programming language. From the program in its human-readable form of source code, a compiler can derive machine code—a form consisting of instructions that the computer can directly execute. Alternatively, a computer program may be executed with the aid of an interpreter.\n\nA collection of computer programs, libraries, and related data are referred to as software. Computer programs may be categorized along functional lines, such as application software and system software. The underlying method used for some calculation or manipulation is known as an algorithm.\n\nThe earliest programmable machines preceded the invention of the digital computer. In 1801, Joseph-Marie Jacquard devised a loom that would weave a pattern by following a series of perforated cards. Patterns could be woven and repeated by arranging the cards.\n\nIn 1837, Charles Babbage was inspired by Jacquard's loom to attempt to build the Analytical Engine.\nThe names of the components of the calculating device were borrowed from the textile industry. In the textile industry, yarn was brought from the store to be milled. The device would have had a \"store\"—memory to hold 1,000 numbers of 40 decimal digits each. Numbers from the \"store\" would then have then been transferred to the \"mill\" (analogous to the CPU of a modern machine), for processing. It was programmed using two sets of perforated cards—one to direct the operation and the other for the input variables.\n\nDuring a nine-month period in 1842–43, Ada Lovelace translated the memoir of Italian mathematician Luigi Menabrea. The memoir covered the Analytical Engine. The translation contained Note G which completely detailed a method for calculating Bernoulli numbers using the Analytical Engine. This note is recognized by some historians as the world's first written computer program.\n\nIn 1936, Alan Turing introduced the Universal Turing machine—a theoretical device that can model every computation that can be performed on a Turing complete computing machine.\nIt is a finite-state machine that has an infinitely long read/write tape. The machine can move the tape back and forth, changing its contents as it performs an algorithm. The machine starts in the initial state, goes through a sequence of steps, and halts when it encounters the halt state.\nThis machine is considered by some to be the origin of the stored-program computer—used by John von Neumann (1946) for the \"Electronic Computing Instrument\" that now bears the von Neumann architecture name.\n\nThe Z3 computer, invented by Konrad Zuse (1941) in Germany, was a digital and programmable computer. A digital computer uses electricity as the calculating component. The Z3 contained 2,400 relays to create the circuits. The circuits provided a binary, floating-point, nine-instruction computer. Programming the Z3 was through a specially designed keyboard and punched tape.\n\nThe Electronic Numerical Integrator And Computer (Fall 1945) was a Turing complete, general-purpose computer that used 17,468 vacuum tubes to create the circuits. At its core, it was a series of Pascalines wired together. Its 40 units weighed 30 tons, occupied , and consumed $650 per hour (in 1940s currency) in electricity when idle. It had 20 base-10 accumulators. Programming the ENIAC took up to two months. Three function tables were on wheels and needed to be rolled to fixed function panels. Function tables were connected to function panels using heavy black cables. Each function table had 728 rotating knobs. Programming the ENIAC also involved setting some of the 3,000 switches. Debugging a program took a week. The programmers of the ENIAC were women who were known collectively as the \"ENIAC girls.\" The ENIAC featured parallel operations. Different sets of accumulators could simultaneously work on different algorithms. It used punched card machines for input and output, and it was controlled with a clock signal. It ran for eight years, calculating hydrogen bomb parameters, predicting weather patterns, and producing firing tables to aim artillery guns. \n\nThe Manchester Baby (June 1948) was a stored-program computer. Programming transitioned away from moving cables and setting dials; instead, a computer program was stored in memory as numbers. Only three bits of memory were available to store each instruction, so it was limited to eight instructions. 32 switches were available for programming.\n\nComputers manufactured until the 1970s had front-panel switches for programming. The computer program was written on paper for reference. An instruction was represented by a configuration of on/off settings. After setting the configuration, an execute button was pressed. This process was then repeated. Computer programs also were manually input via paper tape or punched cards. After the medium was loaded, the starting address was set via switches and the execute button pressed.\n\nIn 1961, the Burroughs B5000 was built specifically to be programmed in the ALGOL 60 language. The hardware featured circuits to ease the compile phase.\n\nIn 1964, the IBM System/360 was a line of six computers each having the same instruction set architecture. The Model 30 was the smallest and least expensive. Customers could upgrade and retain the same application software. Each System/360 model featured multiprogramming. With operating system support, multiple programs could be in memory at once. When one was waiting for input/output, another could compute. Each model also could emulate other computers. Customers could upgrade to the System/360 and retain their IBM 7094 or IBM 1401 application software.\n\nComputer programming is the process of writing or editing source code. Editing source code involves testing, analyzing, refining, and sometimes coordinating with other programmers on a jointly developed program. A person who practices this skill is referred to as a computer programmer, software developer, and sometimes coder.\n\nThe sometimes lengthy process of computer programming is usually referred to as software development. The term software engineering is becoming popular as the process is seen as an engineering discipline.\n\nComputer programs can be categorized by the programming language paradigm used to produce them. Two of the main paradigms are imperative and declarative.\n\n\"Imperative programming languages\" specify a sequential algorithm using declarations, expressions, and statements:\n\nOne criticism of imperative languages is the side effect of an assignment statement on a class of variables called non-local variables.\n\n\"Declarative programming languages\" describe \"what\" computation should be performed and not \"how\" to compute it. Declarative programs omit the control flow and are considered \"sets\" of instructions. Two broad categories of declarative languages are functional languages and logical languages. The principle behind functional languages (like Haskell) is to not allow side effects, which makes it easier to reason about programs like mathematical functions. The principle behind logical languages (like Prolog) is to define the problem to be solved – the goal – and leave the detailed solution to the Prolog system itself. The goal is defined by providing a list of subgoals. Then each subgoal is defined by further providing a list of its subgoals, etc. If a path of subgoals fails to find a solution, then that subgoal is backtracked and another path is systematically attempted.\n\nA computer program in the form of a human-readable, computer programming language is called source code. Source code may be converted into an executable image by a compiler or executed immediately with the aid of an interpreter.\n\nCompilers are used to translate source code from a programming language into either object code or machine code. Object code needs further processing to become machine code, and machine code consists of the central processing unit's native instructions, ready for execution. Compiled computer programs are commonly referred to as executables, binary images, or simply as binaries – a reference to the binary file format used to store the executable code.\n\nInterpreters are used to execute source code from a programming language line-by-line. The interpreter decodes each statement and performs its behavior. One advantage of interpreters is that they can easily be extended to an interactive session. The programmer is presented with a prompt, and individual lines of code are typed in and performed immediately.\n\nThe main disadvantage of interpreters is computer programs run slower than when compiled. Interpreting code is slower because the interpreter must decode each statement and then perform it. However, software development may be faster using an interpreter because testing is immediate when the compiling step is omitted. Another disadvantage of interpreters is an interpreter must be present on the executing computer. By contrast, compiled computer programs need no compiler present during execution.\n\nJust in time compilers pre-compile computer programs just before execution. For example, the Java virtual machine Hotspot contains a Just In Time Compiler which selectively compiles Java bytecode into machine code - but only code which Hotspot predicts is likely to be used many times.\n\nEither compiled or interpreted programs might be executed in a batch process without human interaction.\n\nScripting languages are often used to create batch processes. One common scripting language is Unix shell, and its executing environment is called the command-line interface.\n\nNo properties of a programming language require it to be exclusively compiled or exclusively interpreted. The categorization usually reflects the most popular method of language execution. For example, Java is thought of as an interpreted language and C a compiled language, despite the existence of Java compilers and C interpreters.\n\nTypically, computer programs are stored in non-volatile memory until requested either directly or indirectly to be executed by the computer user. Upon such a request, the program is loaded into random-access memory, by a computer program called an operating system, where it can be accessed directly by the central processor. The central processor then executes (\"runs\") the program, instruction by instruction, until termination. A program in execution is called a process. Termination is either by normal self-termination or by error – software or hardware error.\n\nMany operating systems support multitasking which enables many computer programs to appear to run simultaneously on one computer. Operating systems may run multiple programs through process scheduling – a software mechanism to switch the CPU among processes often so users can interact with each program while it runs. Within hardware, modern day multiprocessor computers or computers with multicore processors may run multiple programs.\n\nA computer program in execution is normally treated as being different from the data the program operates on. However, in some cases, this distinction is blurred when a computer program modifies itself. The modified computer program is subsequently executed as part of the same program. Self-modifying code is possible for programs written in machine code, assembly language, Lisp, C, COBOL, PL/1, and Prolog.\n\nComputer programs may be categorized along functional lines. The main functional categories are application software and system software. System software includes the operating system which couples computer hardware with application software. The purpose of the operating system is to provide an environment in which application software executes in a convenient and efficient manner. In addition to the operating system, system software includes embedded programs, boot programs, and micro programs. Application software designed for end users have a user interface. Application software not designed for the end user includes middleware, which couples one application with another. Application software also includes utility programs. The distinction between system software and application software is under debate.\n\nThere are many types of application software:\n\nUtility programs are application programs designed to aid system administrators and computer programmers.\n\nAn operating system is a computer program that acts as an intermediary between a user of a computer and the computer hardware.\nIn the 1950s, the programmer, who was also the operator, would write a program and run it.\nAfter the program finished executing, the output may have been printed, or it may have been punched onto paper tape or cards for later processing.\nMore often than not the program did not work.\n\nThe programmer then looked at the console lights and fiddled with the console switches. If less fortunate, a memory printout was made for further study.\nIn the 1960s, programmers reduced the amount of wasted time by automating the operator's job. A program called an \"operating system\" was kept in the computer at all times.\n\nOriginally, operating systems were programmed in assembly; however, modern operating systems are typically written in C.\n\nA stored-program computer requires an initial computer program stored in its read-only memory to boot. The boot process is to identify and initialize all aspects of the system, from processor registers to device controllers to memory contents. Following the initialization process, this initial computer program loads the operating system and sets the program counter to begin normal operations.\n\nIndependent of the host computer, a hardware device might have embedded firmware to control its operation. Firmware is used when the computer program is rarely or never expected to change, or when the program must not be lost when the power is off.\n\nMicrocode programs control some central processing units and some other hardware. This code moves data between the registers, buses, arithmetic logic units, and other functional units in the CPU. Unlike conventional programs, microcode is not usually written by, or even visible to, the end users of systems, and is usually provided by the manufacturer, and is considered internal to the device.\n\n\n"}
{"id": "5785", "url": "https://en.wikipedia.org/wiki?curid=5785", "title": "Crime", "text": "Crime\n\nIn ordinary language, a crime is an unlawful act punishable by a state or other authority. The term \"crime\" does not, in modern criminal law, have any simple and universally accepted definition, though statutory definitions have been provided for certain purposes. The most popular view is that crime is a category created by law; in other words, something is a crime if declared as such by the relevant and applicable law. One proposed definition is that a crime or offence (or criminal offence) is an act harmful not only to some individual but also to a community, society or the state (\"a public wrong\"). Such acts are forbidden and punishable by law.\n\nThe notion that acts such as murder, rape and theft are to be prohibited exists worldwide. What precisely is a criminal offence is defined by criminal law of each country. While many have a catalogue of crimes called the criminal code, in some common law countries no such comprehensive statute exists.\n\nThe state (government) has the power to severely restrict one's liberty for committing a crime. In modern societies, there are procedures to which investigations and trials must adhere. If found guilty, an offender may be sentenced to a form of reparation such as a community sentence, or, depending on the nature of their offence, to undergo imprisonment, life imprisonment or, in some jurisdictions, execution.\n\nUsually, to be classified as a crime, the \"act of doing something criminal\" (\"actus reus\") mustwith certain exceptionsbe accompanied by the \"intention to do something criminal\" (\"mens rea\").\n\nWhile every crime violates the law, not every violation of the law counts as a crime. Breaches of private law (torts and breaches of contract) are not automatically punished by the state, but can be enforced through civil procedure.\n\nWhen informal relationships and sanctions prove insufficient to establish and maintain a desired social order, a government or a state may impose more formalized or stricter systems of social control. With institutional and legal machinery at their disposal, agents of the State can compel populations to conform to codes and can opt to punish or attempt to reform those who do not conform.\n\nAuthorities employ various mechanisms to regulate (encouraging or discouraging) certain behaviors in general. Governing or administering agencies may for example codify rules into laws, police citizens and visitors to ensure that they comply with those laws, and implement other policies and practices that legislators or administrators have prescribed with the aim of discouraging or preventing crime. In addition, authorities provide remedies and sanctions, and collectively these constitute a criminal justice system. Legal sanctions vary widely in their severity; they may include (for example) incarceration of temporary character aimed at reforming the convict. Some jurisdictions have penal codes written to inflict permanent harsh punishments: legal mutilation, capital punishment or life without parole.\n\nUsually, a natural person perpetrates a crime, but legal persons may also commit crimes. Conversely, at least under U.S. law, nonpersons such as animals cannot commit crimes.\n\nThe sociologist Richard Quinney has written about the relationship between society and crime. When Quinney states \"crime is a social phenomenon\" he envisages both how individuals conceive crime and how populations perceive it, based on societal norms.\n\nThe word \"crime\" is derived from the Latin root \"cernō\", meaning \"I decide, I give judgment\". Originally the Latin word \"crīmen\" meant \"charge\" or \"cry of distress.\" The Ancient Greek word \"krima\" (κρίμα), from which the Latin cognate derives, typically referred to an intellectual mistake or an offense against the community, rather than a private or moral wrong.\n\nIn 13th century English \"crime\" meant \"sinfulness\", according to etymonline.com. It was probably brought to England as Old French \"crimne\" (12th century form of Modern French \"crime\"), from Latin \"crimen\" (in the genitive case: \"criminis\"). In Latin, \"crimen\" could have signified any one of the following: \"charge, indictment, accusation; crime, fault, offense\".\n\nThe word may derive from the Latin \"cernere\" – \"to decide, to sift\" (see crisis, mapped on Kairos and Chronos). But Ernest Klein (citing Karl Brugmann) rejects this and suggests *cri-men, which originally would have meant \"cry of distress\". Thomas G. Tucker suggests a root in \"cry\" words and refers to English plaint, plaintiff, and so on. The meaning \"offense punishable by law\" dates from the late 14th century. The Latin word is glossed in Old English by \"facen\", also \"deceit, fraud, treachery\", [cf. fake]. \"Crime wave\" is first attested in 1893 in American English.\n\nWhether a given act or omission constitutes a crime does not depend on the nature of that act or omission. It depends on the nature of the legal consequences that may follow it. An act or omission is a crime if it is capable of being followed by what are called criminal proceedings.\n\nHistory\n\nThe following definition of \"crime\" was provided by the Prevention of Crimes Act 1871, and applied for the purposes of section 10 of the Prevention of Crime Act 1908:\n\nFor the purpose of section 243 of the Trade Union and Labour Relations (Consolidation) Act 1992, a crime means an offence punishable on indictment, or an offence punishable on summary conviction, and for the commission of which the offender is liable under the statute making the offence punishable to be imprisoned either absolutely or at the discretion of the court as an alternative for some other punishment.\n\nA normative definition views crime as deviant behavior that violates prevailing normscultural standards prescribing how humans ought to behave normally. This approach considers the complex realities surrounding the concept of crime and seeks to understand how changing social, political, psychological, and economic conditions may affect changing definitions of crime and the form of the legal, law-enforcement, and penal responses made by society.\n\nThese structural realities remain fluid and often contentious. For example: as cultures change and the political environment shifts, societies may criminalise or decriminalise certain behaviours, which directly affects the statistical crime rates, influence the allocation of resources for the enforcement of laws, and (re-)influence the general public opinion.\n\nSimilarly, changes in the collection and/or calculation of data on crime may affect the public perceptions of the extent of any given \"crime problem\". All such adjustments to crime statistics, allied with the experience of people in their everyday lives, shape attitudes on the extent to which the State should use law or social engineering to enforce or encourage any particular social norm. Behaviour can be controlled and influenced by a society in many ways without having to resort to the criminal justice system.\n\nIndeed, in those cases where no clear consensus exists on a given norm, the drafting of criminal law by the group in power to prohibit the behaviour of another group may seem to some observers an improper limitation of the second group's freedom, and the ordinary members of society have less respect for the law or laws in generalwhether the authorities actually enforce the disputed law or not.\n\nLegislatures can pass laws (called \"mala prohibita\") that define crimes against social norms. These laws vary from time to time and from place to place: note variations in gambling laws, for example, and the prohibition or encouragement of duelling in history. Other crimes, called \"mala in se\", count as outlawed in almost all societies, (murder, theft and rape, for example).\n\nEnglish criminal law and the related criminal law of Commonwealth countries can define offences that the courts alone have developed over the years, without any actual legislation: common law offences. The courts used the concept of \"malum in se\" to develop various common law offences.\n\nOne can view criminalization as a procedure deployed by society as a preemptive harm-reduction device, using the threat of punishment as a deterrent to anyone proposing to engage in the behavior causing harm. The State becomes involved because governing entities can become convinced that the costs of not criminalizing (through allowing the harms to continue unabated) outweigh the costs of criminalizing it (restricting individual liberty, for example, to minimize harm to others).\n\nStates control the process of criminalization because:\n\nThe label of \"crime\" and the accompanying social stigma normally confine their scope to those activities seen as injurious to the general population or to the State, including some that cause serious loss or damage to individuals. Those who apply the labels of \"crime\" or \"criminal\" intend to assert the hegemony of a dominant population, or to reflect a consensus of condemnation for the identified behavior and to justify any punishments prescribed by the State (in the event that standard processing tries and convicts an accused person of a crime).\n\nJustifying the State's use of force to coerce compliance with its laws has proven a consistent theoretical problem. One of the earliest justifications involved the theory of natural law. This posits that the nature of the world or of human beings underlies the standards of morality or constructs them. Thomas Aquinas wrote in the 13th century: \"the rule and measure of human acts is the reason, which is the first principle of human acts\" (Aquinas, ST I-II, Q.90, A.I). He regarded people as by nature rational beings, concluding that it becomes morally appropriate that they should behave in a way that conforms to their rational nature. Thus, to be valid, any law must conform to natural law and coercing people to conform to that law is morally acceptable. In the 1760s William Blackstone (1979: 41) described the thesis:\n\nBut John Austin (1790–1859), an early positivist, applied utilitarianism in accepting the calculating nature of human beings and the existence of an objective morality. He denied that the legal validity of a norm depends on whether its content conforms to morality. Thus in Austinian terms a moral code can objectively determine what people ought to do, the law can embody whatever norms the legislature decrees to achieve social utility, but every individual remains free to choose what to do. Similarly, Hart (1961) saw the law as an aspect of sovereignty, with lawmakers able to adopt any law as a means to a moral end.\n\nThus the necessary and sufficient conditions for the truth of a proposition of law simply involved internal logic and consistency, and that the state's agents used state power with responsibility. Ronald Dworkin (2005) rejects Hart's theory and proposes that all individuals should expect the equal respect and concern of those who govern them as a fundamental political right. He offers a theory of compliance overlaid by a theory of deference (the citizen's duty to obey the law) and a theory of enforcement, which identifies the legitimate goals of enforcement and punishment. Legislation must conform to a theory of legitimacy, which describes the circumstances under which a particular person or group is entitled to make law, and a theory of legislative justice, which describes the law they are entitled or obliged to make.\n\nIndeed, despite everything, the majority of natural-law theorists have accepted the idea of enforcing the prevailing morality as a primary function of the law. This view entails the problem that it makes any moral criticism of the law impossible: if conformity with natural law forms a necessary condition for legal validity, all valid law must, by definition, count as morally just. Thus, on this line of reasoning, the legal validity of a norm necessarily entails its moral justice.\n\nOne can solve this problem by granting some degree of moral relativism and accepting that norms may evolve over time and, therefore, one can criticize the continued enforcement of old laws in the light of the current norms. People may find such law acceptable, but the use of State power to coerce citizens to comply with that law lacks moral justification. More recent conceptions of the theory characterise crime as the violation of individual rights.\n\nSince society considers so many rights as natural (hence the term \"right\") rather than man-made, what constitutes a crime also counts as natural, in contrast to laws (seen as man-made). Adam Smith illustrates this view, saying that a smuggler would be an excellent citizen, \"...had not the laws of his country made that a crime which nature never meant to be so.\"\n\nNatural-law theory therefore distinguishes between \"criminality\" (which derives from human nature) and \"illegality\" (which originates with the interests of those in power). Lawyers sometimes express the two concepts with the phrases \"malum in se\" and \"malum prohibitum\" respectively. They regard a \"crime \"malum in se\"\" as inherently criminal; whereas a \"crime \"malum prohibitum\"\" (the argument goes) counts as criminal only because the law has decreed it so.\n\nIt follows from this view that one can perform an illegal act without committing a crime, while a criminal act could be perfectly legal. Many Enlightenment thinkers (such as Adam Smith and the American Founding Fathers) subscribed to this view to some extent, and it remains influential among so-called classical liberals and libertarians.\n\nSome religious communities regard sin as a crime; some may even highlight the crime of sin very early in legendary or mythological accounts of originsnote the tale of Adam and Eve and the theory of original sin. What one group considers a crime may cause or ignite war or conflict. However, the earliest known civilizations had codes of law, containing both civil and penal rules mixed together, though not always in recorded form.\n\nThe Sumerians produced the earliest surviving written codes. Urukagina (reigned , short chronology) had an early code that has not survived; a later king, Ur-Nammu, left the earliest extant written law system, the Code of Ur-Nammu (), which prescribed a formal system of penalties for specific cases in 57 articles. The Sumerians later issued other codes, including the \"code of Lipit-Ishtar\". This code, from the 20th century BCE, contains some fifty articles, and scholars have reconstructed it by comparing several sources. \n\nSuccessive legal codes in Babylon, including the code of Hammurabi (c. 1790 BC), reflected Mesopotamian society's belief that law derived from the will of the gods (see Babylonian law).\nMany states at this time functioned as theocracies, with codes of conduct largely religious in origin or reference. In the Sanskrit texts of Dharmaśāstra (), issues such as legal and religious duties, code of conduct, penalties and remedies, etc. have been discussed and forms one of the elaborate and earliest source of legal code.\n\nSir Henry Maine (1861) studied the ancient codes available in his day, and failed to find any criminal law in the \"modern\" sense of the word. While modern systems distinguish between offences against the \"State\" or \"community\", and offences against the \"individual\", the so-called penal law of ancient communities did not deal with \"crimes\" (Latin: \"crimina\"), but with \"wrongs\" (Latin: \"delicta\"). Thus the Hellenic laws treated all forms of theft, assault, rape, and murder as private wrongs, and left action for enforcement up to the victims or their survivors. The earliest systems seem to have lacked formal courts.\n\nThe Romans systematized law and applied their system across the Roman Empire. Again, the initial rules of Roman law regarded assaults as a matter of private compensation. The most significant Roman law concept involved \"dominion\". The \"pater familias\" owned all the family and its property (including slaves); the \"pater\" enforced matters involving interference with any property. The \"Commentaries\" of Gaius (written between 130 and 180 AD) on the Twelve Tables treated \"furtum\" (in modern parlance: \"theft\") as a tort.\n\nSimilarly, assault and violent robbery involved trespass as to the \"pater's\" property (so, for example, the rape of a slave could become the subject of compensation to the \"pater\" as having trespassed on his \"property\"), and breach of such laws created a \"vinculum juris\" (an obligation of law) that only the payment of monetary compensation (modern \"damages\") could discharge. Similarly, the consolidated Teutonic laws of the Germanic tribes, included a complex system of monetary compensations for what courts would consider the complete range of criminal offences against the person, from murder down.\n\nEven though Rome abandoned its Britannic provinces around 400 AD, the Germanic mercenarieswho had largely become instrumental in enforcing Roman rule in Britanniaacquired ownership of land there and continued to use a mixture of Roman and Teutonic Law, with much written down under the early Anglo-Saxon kings. But only when a more centralized English monarchy emerged following the Norman invasion, and when the kings of England attempted to assert power over the land and its peoples, did the modern concept emerge, namely of a crime not only as an offence against the \"individual\", but also as a wrong against the \"State\".\n\nThis idea came from common law, and the earliest conception of a criminal act involved events of such major significance that the \"State\" had to usurp the usual functions of the civil tribunals, and direct a special law or \"privilegium\" against the perpetrator. All the earliest English criminal trials involved wholly extraordinary and arbitrary courts without any settled law to apply, whereas the civil (delictual) law operated in a highly developed and consistent manner (except where a king wanted to raise money by selling a new form of writ). The development of the idea that the \"State\" dispenses justice in a court only emerges in parallel with or after the emergence of the concept of sovereignty.\n\nIn continental Europe, Roman law persisted, but with a stronger influence from the Christian Church.\nCoupled with the more diffuse political structure based on smaller feudal units, various legal traditions emerged, remaining more strongly rooted in Roman jurisprudence, but modified to meet the prevailing political climate.\n\nIn Scandinavia the effect of Roman law did not become apparent until the 17th century, and the courts grew out of the \"things\"the assemblies of the people. The people decided the cases (usually with largest freeholders dominating). This system later gradually developed into a system with a royal judge nominating a number of the most esteemed men of the parish as his board, fulfilling the function of \"the people\" of yore.\n\nFrom the Hellenic system onwards, the policy rationale for requiring the payment of monetary compensation for wrongs committed has involved the avoidance of feuding between clans and families.\nIf compensation could mollify families' feelings, this would help to keep the peace. On the other hand, the institution of oaths also played down the threat of feudal warfare. Both in archaic Greece and in medieval Scandinavia, an accused person walked free if he could get a sufficient number of male relatives to swear him not guilty. (Compare the United Nations Security Council, in which the veto power of the permanent members ensures that the organization does not become involved in crises where it could not enforce its decisions.)\n\nThese means of restraining private feuds did not always work, and sometimes prevented the fulfillment of justice. But in the earliest times the \"state\" did not always provide an independent policing force. Thus criminal law grew out of what 21st-century lawyers would call torts; and, in real terms, many acts and omissions classified as crimes actually overlap with civil-law concepts.\n\nThe development of sociological thought from the 19th century onwards prompted some fresh views on crime and criminality, and fostered the beginnings of criminology as a study of crime in society. Nietzsche noted a link between crime and creativityin \"The Birth of Tragedy\" he asserted: \"The best and brightest that man can acquire he must obtain by crime\". In the 20th century Michel Foucault in \"Discipline and Punish\" made a study of criminalization as a coercive method of state control.\n\nThe following classes of offences are used, or have been used, as legal terms of art:\n\nResearchers and commentators have classified crimes into the following categories, in addition to those above:\n\nOne can categorise crimes depending on the related punishment, with sentencing tariffs prescribed in line with the perceived seriousness of the offence. Thus fines and noncustodial sentences may address the crimes seen as least serious, with lengthy imprisonment or (in some jurisdictions) capital punishment reserved for the most serious.\n\nUnder the common law of England, crimes were classified as either treason, felony or misdemeanour, with treason sometimes being included with the felonies. This system was based on the perceived seriousness of the offence. It is still used in the United States but the distinction between felony and misdemeanour is abolished in England and Wales and Northern Ireland.\n\nThe following classes of offence are based on mode of trial:\n\nIn common law countries, crimes may be categorised into common law offences and statutory offences. In the US, Australia and Canada (in particular), they are divided into federal crimes and under state crimes.\n\n\nIn the United States since 1930, the FBI has tabulated Uniform Crime Reports (UCR) annually from crime data submitted by law enforcement agencies across the United States.\nOfficials compile this data at the city, county, and state levels into the UCR. They classify violations of laws based on common law as Part I (index) crimes in UCR data. These are further categorized as violent or property crimes. Part I violent crimes include murder and criminal homicide (voluntary manslaughter), forcible rape, aggravated assault, and robbery; while Part I property crimes include burglary, arson, larceny/theft, and motor-vehicle theft. All other crimes count come under Part II.\n\nFor convenience, such lists usually include infractions although, in the U.S., they may come into the sphere not of the criminal law, but rather of the civil law. Compare tortfeasance.\n\nBooking arrests require detention for a time-frame ranging 1 to 24 hours.\n\nThere are several national and International organizations offering studies and statistics about global and local crime activity, such as United Nations Office on Drugs and Crime, the United States of America Overseas Security Advisory Council (OSAC) safety report or national reports generated by the law-enforcement authorities of EU state member reported to the Europol.\n\nIn England and Wales, as well as in Hong Kong, the term \"offence\" means the same thing as, and is interchangeable with, the term \"crime\", They are further split into:\n\nMany different causes and correlates of crime have been proposed with varying degree of empirical support. They include socioeconomic, psychological, biological, and behavioral factors. Controversial topics include media violence research and effects of gun politics.\n\nEmotional state (both chronic and current) have a tremendous impact on individual thought processes and, as a result, can be linked to criminal activities. The positive psychology concept of Broaden and Build posits that cognitive functioning expands when an individual is in a good-feeling emotional state and contracts as emotional state declines. In positive emotional states an individual is able to consider more possible solutions to problems, but in lower emotional states fewer solutions can be ascertained. The narrowed thought-action repertoires can result in the only paths perceptible to an individual being ones they would never use if they saw an alternative, but if they can't conceive of the alternatives that carry less risk they will choose one that they can see. Criminals who commit even the most horrendous of crimes, such as mass murders, did not see another solution.\n\nCrimes defined by treaty as crimes against international law include:\n\nFrom the point of view of state-centric law, extraordinary procedures (usually international courts) may prosecute such crimes. Note the role of the International Criminal Court at The Hague in the Netherlands.\n\nDifferent religious traditions may promote distinct norms of behaviour, and these in turn may clash or harmonise with the perceived interests of a state. Socially accepted or imposed religious morality has influenced secular jurisdictions on issues that may otherwise concern only an individual's conscience. Activities sometimes criminalized on religious grounds include (for example) alcohol consumption (prohibition), abortion and stem-cell research. In various historical and present-day societies, institutionalized religions have established systems of earthly justice that punish crimes against the divine will and against specific devotional, organizational and other rules under specific codes, such as Roman Catholic canon law.\n\nIn the military sphere, authorities can prosecute both regular crimes and specific acts (such as mutiny or desertion) under martial-law codes that either supplant or extend civil codes in times of (for example) war.\n\nMany constitutions contain provisions to curtail freedoms and criminalize otherwise tolerated behaviors under a state of emergency in the event of war, natural disaster or civil unrest. Undesired activities at such times may include assembly in the streets, violation of curfew, or possession of firearms.\n\nTwo common types of employee crime exist: embezzlement and wage theft.\n\nThe complexity and anonymity of computer systems may help criminal employees camouflage their operations. The victims of the most costly scams include banks, brokerage houses, insurance companies, and other large financial institutions.\n\nIn the United States, it is estimated that workers are not paid at least $19 billion every year in overtime and that in total $40 billion to $60 billion are lost annually due to all forms of wage theft. This compares to national annual losses of $340 million due to robbery, $4.1 billion due to burglary, $5.3 billion due to larceny, and $3.8 billion due to auto theft in 2012. In Singapore, as in the United States, wage theft was found to be widespread and severe. In a 2014 survey it was found that as many as one-third of low wage male foreign workers in Singapore, or about 130,000, were affected by wage theft from partial to full denial of pay.\n\n\n"}
{"id": "5786", "url": "https://en.wikipedia.org/wiki?curid=5786", "title": "California Institute of Technology", "text": "California Institute of Technology\n\nThe California Institute of Technology (abbreviated Caltech) is a private doctorate-granting research university located in Pasadena, California, United States. Known for its strength in natural science and engineering, Caltech is often ranked as one of the world's top-ten universities.\n\nAlthough founded as a preparatory and vocational school by Amos G. Throop in 1891, the college attracted influential scientists such as George Ellery Hale, Arthur Amos Noyes and Robert Andrews Millikan in the early 20th century. The vocational and preparatory schools were disbanded and spun off in 1910 and the college assumed its present name in 1921. In 1934, Caltech was elected to the Association of American Universities and the antecedents of NASA's Jet Propulsion Laboratory, which Caltech continues to manage and operate, were established between 1936 and 1943 under Theodore von Kármán. The university is one among a small group of institutes of technology in the United States which is primarily devoted to the instruction of pure and applied sciences.\n\nCaltech has six academic divisions with strong emphasis on science and engineering, managing $332 million in 2011 in sponsored research. Its primary campus is located approximately northeast of downtown Los Angeles. First-year students are required to live on campus and 95% of undergraduates remain in the on-campus House System at Caltech. Although Caltech has a strong tradition of practical jokes and pranks, student life is governed by an honor code which allows faculty to assign take-home examinations. The Caltech Beavers compete in 13 intercollegiate sports in the NCAA Division III's Southern California Intercollegiate Athletic Conference.\n\n, Caltech alumni, faculty and researchers include 73 Nobel Laureates (chemist Linus Pauling being the only individual in history to win two unshared prizes), 4 Fields Medalists, and 6 Turing Award winners. In addition, there are 53 non-emeritus faculty members (as well as many emeritus faculty members) who have been elected to one of the United States National Academies, 4 Chief Scientists of the U.S. Air Force and 71 have won the United States National Medal of Science or Technology. Numerous faculty members are associated with the Howard Hughes Medical Institute as well as NASA. According to a 2015 Pomona College study, Caltech ranked number one in the U.S. for the percentage of its graduates who go on to earn a PhD.\n\nCaltech started as a vocational school founded in Pasadena in 1891 by local businessman and politician Amos G. Throop. The school was known successively as Throop University, Throop Polytechnic Institute (and Manual Training School) and Throop College of Technology before acquiring its current name in 1920. The vocational school was disbanded and the preparatory program was split off to form an independent Polytechnic School in 1907.\n\nAt a time when scientific research in the United States was still in its infancy, George Ellery Hale, a solar astronomer from the University of Chicago, founded the Mount Wilson Observatory in 1904. He joined Throop's board of trustees in 1907, and soon began developing it and the whole of Pasadena into a major scientific and cultural destination. He engineered the appointment of James A. B. Scherer, a literary scholar untutored in science but a capable administrator and fund raiser, to Throop's presidency in 1908. Scherer persuaded retired businessman and trustee Charles W. Gates to donate $25,000 in seed money to build Gates Laboratory, the first science building on campus.\n\nIn 1910, Throop moved to its current site. Arthur Fleming donated the land for the permanent campus site. Theodore Roosevelt delivered an address at Throop Institute on March 21, 1911, and he declared:\n\nI want to see institutions like Throop turn out perhaps ninety-nine of every hundred students as men who are to do given pieces of industrial work better than any one else can do them; I want to see those men do the kind of work that is now being done on the Panama Canal and on the great irrigation projects in the interior of this country—and the one-hundredth man I want to see with the kind of cultural scientific training that will make him and his fellows the matrix out of which you can occasionally develop a man like your great astronomer, George Ellery Hale.\nIn the same year, a bill was introduced in the California Legislature calling for the establishment of a publicly funded \"California Institute of Technology\", with an initial budget of a million dollars, ten times the budget of Throop at the time. The board of trustees offered to turn Throop over to the state, but the presidents of Stanford University and the University of California successfully lobbied to defeat the bill, which allowed Throop to develop as the only scientific research-oriented education institute in southern California, public or private, until the onset of the World War II necessitated the broader development of research-based science education. The promise of Throop attracted physical chemist Arthur Amos Noyes from MIT to develop the institution and assist in establishing it as a center for science and technology.\n\nWith the onset of World War I, Hale organized the National Research Council to coordinate and support scientific work on military problems. While he supported the idea of federal appropriations for science, he took exception to a federal bill that would have funded engineering research at land-grant colleges, and instead sought to raise a $1 million national research fund entirely from private sources. To that end, as Hale wrote in \"The New York Times\":\n\nThroop College of Technology, in Pasadena California has recently afforded a striking illustration of one way in which the Research Council can secure co-operation and advance scientific investigation. This institution, with its able investigators and excellent research laboratories, could be of great service in any broad scheme of cooperation. President Scherer, hearing of the formation of the council, immediately offered to take part in its work, and with this object, he secured within three days an additional research endowment of one hundred thousand dollars.\nThrough the National Research Council, Hale simultaneously lobbied for science to play a larger role in national affairs, and for Throop to play a national role in science. The new funds were designated for physics research, and ultimately led to the establishment of the Norman Bridge Laboratory, which attracted experimental physicist Robert Andrews Millikan from the University of Chicago in 1917. During the course of the war, Hale, Noyes and Millikan worked together in Washington on the NRC. Subsequently, they continued their partnership in developing Caltech.\nUnder the leadership of Hale, Noyes, and Millikan (aided by the booming economy of Southern California), Caltech grew to national prominence in the 1920s and concentrated on the development of Roosevelt's \"Hundredth Man\". On November 29, 1921, the trustees declared it to be the express policy of the Institute to pursue scientific research of the greatest importance and at the same time \"to continue to conduct thorough courses in engineering and pure science, basing the work of these courses on exceptionally strong instruction in the fundamental sciences of mathematics, physics, and chemistry; broadening and enriching the curriculum by a liberal amount of instruction in such subjects as English, history, and economics; and vitalizing all the work of the Institute by the infusion in generous measure of the spirit of research\". In 1923, Millikan was awarded the Nobel Prize in Physics. In 1925, the school established a department of geology and hired William Bennett Munro, then chairman of the division of History, Government, and Economics at Harvard University, to create a division of humanities and social sciences at Caltech. In 1928, a division of biology was established under the leadership of Thomas Hunt Morgan, the most distinguished biologist in the United States at the time, and discoverer of the role of genes and the chromosome in heredity. In 1930, Kerckhoff Marine Laboratory was established in Corona del Mar under the care of Professor George MacGinitie. In 1926, a graduate school of aeronautics was created, which eventually attracted Theodore von Kármán. Kármán later helped create the Jet Propulsion Laboratory, and played an integral part in establishing Caltech as one of the world's centers for rocket science. In 1928, construction of the Palomar Observatory began.\n\nMillikan served as \"Chairman of the Executive Council\" (effectively Caltech's president) from 1921 to 1945, and his influence was such that the Institute was occasionally referred to as \"Millikan's School.\" Millikan initiated a visiting-scholars program soon after joining Caltech. Scientists who accepted his invitation include luminaries such as Paul Dirac, Erwin Schrödinger, Werner Heisenberg, Hendrik Lorentz and Niels Bohr. Albert Einstein arrived on the Caltech campus for the first time in 1931 to polish up his Theory of General Relativity, and he returned to Caltech subsequently as a visiting professor in 1932 and 1933.\n\nDuring World War II, Caltech was one of 131 colleges and universities nationally that took part in the V-12 Navy College Training Program which offered students a path to a Navy commission. The United States Navy also maintained a naval training school for aeronautical engineering, resident inspectors of ordinance and naval material, and a liaison officer to the National Defense Research Committee on campus.\nFrom April 1 to December 1, 1951, Caltech was the host of a federal classified study, Project Vista. The selection of Caltech as host for the project was based on the university's expertise in rocketry and nuclear physics. In response to the war in Korea and the pressure from the Soviet Union, the project was Caltech's way of assisting the federal government in its effort to increase national security. The project was created to study new ways of improving the relationship between tactical air support and ground troops. The Army, Air Force, and Navy sponsored the project, however it was under contract with the Army. The study was named after the hotel, Vista del Arroyo Hotel, which housed the study. The study operated under a committee with the supervision of President Lee A. DuBridge. William A. Fowler, a professor at Caltech, was selected as research director. More than a fourth of Caltech's faculty and a group of outside scientists staffed the project. Moreover, the number increases if one takes into account visiting scientists, military liaisons, secretarial, and security staff. In compensation for its participation, the university received about $750,000.\n\nIn the 1950s–1970s, Caltech was the home of Murray Gell-Mann and Richard Feynman, whose work was central to the establishment of the Standard Model of particle physics. Feynman was also widely known outside the physics community as an exceptional teacher and colorful, unconventional character.\n\nDuring Lee A. DuBridge's tenure as Caltech's president (1946–1969), Caltech's faculty doubled and the campus tripled in size. DuBridge, unlike his predecessors, welcomed federal funding of science. New research fields flourished, including chemical biology, planetary science, nuclear astrophysics, and geochemistry. A 200-inch telescope was dedicated on nearby Palomar Mountain in 1948 and remained the world's most powerful optical telescope for over forty years.\n\nCaltech opened its doors to female undergraduates during the presidency of Harold Brown in 1970, and they made up 14% of the entering class. The fraction of female undergraduates has been increasing since then.\n\nCaltech undergraduates have historically been so apathetic to politics that there has been only one organized student protest in January 1968 outside the Burbank studios of NBC, in response to rumors that NBC was to cancel \"\". In 1973, the students from Dabney House protested a presidential visit with a sign on the library bearing the simple phrase \"Impeach Nixon\". The following week, Ross McCollum, president of the National Oil Company, wrote an open letter to Dabney House stating that in light of their actions he had decided not to donate one million dollars to Caltech. The Dabney family, being Republicans, disowned Dabney House after hearing of the prank.\n\nSince 2000, the Einstein Papers Project has been located at Caltech. The project was established in 1986 to assemble, preserve, translate, and publish papers selected from the literary estate of Albert Einstein and from other collections.\nIn fall 2008, the freshman class was 42% female, a record for Caltech's undergraduate enrollment. In the same year, the Institute concluded a six-year-long fund-raising campaign. The campaign raised more than $1.4 billion from about 16,000 donors. Nearly half of the funds went into the support of Caltech programs and projects.\n\nIn 2010, Caltech, in partnership with Lawrence Berkeley National Laboratory and headed by Professor Nathan Lewis, established a DOE Energy Innovation Hub aimed at developing revolutionary methods to generate fuels directly from sunlight. This hub, the Joint Center for Artificial Photosynthesis, will receive up to $122 million in federal funding over five years.\n\nSince 2012, Caltech began to offer classes through massive open online courses (MOOCs) under Coursera, and from 2013, edX.\n\nJean-Lou Chameau, the eighth president, announced on February 19, 2013, that he would be stepping down to accept the presidency at King Abdullah University of Science and Technology. Thomas F. Rosenbaum was announced to be the ninth president of Caltech on October 24, 2013, and his term began on July 1, 2014.\n\nCaltech's primary campus is located in Pasadena, California, approximately northeast of downtown Los Angeles. It is within walking distance of Old Town Pasadena and the Pasadena Playhouse District and therefore the two locations are frequent getaways for Caltech students.\n\nIn 1917 Hale hired architect Bertram Goodhue to produce a master plan for the campus. Goodhue conceived the overall layout of the campus and designed the physics building, Dabney Hall, and several other structures, in which he sought to be consistent with the local climate, the character of the school, and Hale's educational philosophy. Goodhue's designs for Caltech were also influenced by the traditional Spanish mission architecture of Southern California.\n\nDuring the 1960s, Caltech underwent considerable expansion, in part due to the philanthropy of alumnus Arnold O. Beckman. In 1953, Beckman was asked to join the Caltech Board of Trustees. In 1964, he became its chairman. Over the next few years, as Caltech's president emeritus David Baltimore describes it, Arnold Beckman and his wife Mabel \"shaped the destiny of Caltech\".\n\nIn 1971 a magnitude-6.6 earthquake in San Fernando caused some damage to the Caltech campus. Engineers who evaluated the damage found that two historic buildings dating from the early days of the Institute—Throop Hall and the Goodhue-designed Culbertson Auditorium—had cracked.\n\nNew additions to the campus include the Cahill Center for Astronomy and Astrophysics and the Walter and Leonore Annenberg Center for Information Science and Technology, which opened in 2009, and the Warren and Katherine Schlinger Laboratory for Chemistry and Chemical Engineering followed in March 2010. The Institute also concluded an upgrading of the south houses in 2006. In late 2010, Caltech completed a 1.3 MW solar array projected to produce approximately 1.6 GWh in 2011.\n\nCaltech is incorporated as a non-profit corporation and is governed by a privately appointed 46-member board of trustees who serve five-year terms of office and retire at the age of 72. The current board is chaired by David L. Lee, co-founder of Global Crossing Ltd. The Trustees elect a President to serve as the chief executive officer of the Institute and administer the affairs on the Institute on behalf of the board, a Provost who serves as the chief academic officer of the Institute below the President, and ten other vice presidential and other senior positions. Former Georgia Tech provost Jean-Lou Chameau became the eighth president of Caltech on September 1, 2006, replacing David Baltimore who had served since 1997. Chameau's compensation for 2008–2009 totaled $799,472. Chameau served until June 30, 2013. Thomas F. Rosenbaum was announced to be the ninth president of Caltech on October 24, 2013, and his term began on July 1, 2014. Caltech's endowment is governed by a permanent Trustee committee and administered by an Investment Office.\n\nThe Institute is organized into six primary academic divisions: Biology and Biological Engineering, Chemistry and Chemical Engineering, Engineering and Applied Science, Geological and Planetary Sciences, Humanities and Social Sciences, and Physics, Mathematics, and Astronomy. The voting faculty of Caltech include all professors, instructors, research associates and fellows, and the University Librarian. Faculty are responsible for establishing admission requirements, academic standards, and curricula. The Faculty Board is the faculty's representative body and consists of 18 elected faculty representatives as well as other senior administration officials. Full-time professors are expected to teach classes, conduct research, advise students, and perform administrative work such as serving on committees.\n\nFounded in 1930s, the Jet Propulsion Laboratory (JPL) is a federally funded research and development center (FFRDC) owned by NASA and operated as a division of Caltech through a contract between NASA and Caltech. In 2008, JPL spent over $1.6 billion on research and development and employed over 5,000 project-related and support employees. The JPL Director also serves as a Caltech Vice President and is responsible to the President of the Institute for the management of the laboratory.\n\nCaltech is a small four-year, highly residential research university with a slight majority in graduate programs. The Institute has been accredited by the Western Association of Schools and Colleges since 1949. Caltech is on the quarter system: the fall term starts in late September and ends before Christmas, the second term starts after New Years Day and ends in mid-March, and the third term starts in late March or early April and ends in early June.\n\nIn 2016, \"U.S. News & World Report\" ranked Caltech as tied for the 10th in the United States in their 2016 national rankings, with the graduate programs in chemistry and earth sciences ranked first in the nation. The \"Academic Ranking of World Universities\" (ARWU), ranked Caltech 8th in the world and 6th in the U.S. in 2016.\n\nIn 2014 the\"The Daily Beast\" ranked Caltech 12th nationally its Guide to Best Colleges. That same year \"Money Magazine\" ranked Caltech 10th nationally.\n\nIn 2010, the United States National Research Council released its latest \"Assessment of Research Doctorate Programs\", and, 23 of the 24 graduate programs of Caltech were ranked within the top four programs in the nation for small sized universities.\n\nCaltech was ranked 1st internationally between 2011 and 2016 by the \"Times Higher Education World University Rankings\". Caltech was ranked as the best university in the world in two categories: Engineering & Technology and Physical Sciences. It was also found to have the highest faculty citation rate in the world.\n\nFor the Class of 2022 (enrolled Fall 2018), Caltech received 8208 applications and accepted 6.6% of applicants; 231 enrolled. The class included 46% women and 54% men. 25% were of underrepresented ancestry, and 8% were international students.\n\nAdmission to Caltech is extremely rigorous and requires the highest test scores in the nation.\n\nThe middle 50% range of SAT scores for enrolled freshmen for the class of 2022 were 740-780 for evidence based reading and writing and 790-800 for math, and 1520-1570 total. \nThe middle 50% range ACT Composite score was 35-36. \nThe SAT Math Level 2 middle 50% range was 800–800. \nThe middle 50% range for the SAT Physics Subject Test was 770-800; SAT Chemistry Subject Test was 780-800; \nSAT Biology Subject Tests was 770-800\n\nUndergraduate tuition for the 2013–2014 school year was $39,990 and total annual costs were estimated to be $58,755. In 2012–2013, Caltech awarded $17.1 million in need-based aid, $438k in non-need-based aid, and $2.51 million in self-help support to enrolled undergraduate students. The average financial aid package of all students eligible for aid was $38,756 and students graduated with an average debt of $15,090.\n\nThe full-time, four-year undergraduate program emphasizes instruction in the arts and sciences and has high graduate coexistence. Caltech offers 24 majors (called \"options\") and six minors across all six academic divisions. Caltech also offers interdisciplinary programs in Applied Physics, Biochemistry, Bioengineering, Computation and Neural Systems, Control and Dynamical Systems, Environmental Science and Engineering, Geobiology and Astrobiology, Geochemistry, and Planetary Astronomy. The most popular options are Chemical Engineering, Computer Science, Electrical Engineering, Mechanical Engineering and Physics.\n\nPrior to the entering class of 2017, Caltech requires students to take a core curriculum of three terms of mathematics, three terms of physics, two terms of chemistry, one term of biology, two terms of lab courses, one terms of scientific communication, three terms of physical education, and 12 terms of humanities and social science.\n\nA typical class is worth 9 academic units and given the extensive core curriculum requirements in addition to individual options' degree requirements, students need to take an average of 40.5 units per term (more than four classes) in order to graduate in four years. 36 units is the minimum full-time load, 48 units is considered a heavy load, and registrations above 51 units require an overload petition. Approximately 20 percent of students double-major. This is achievable since the humanities and social sciences majors have been designed to be done in conjunction with a science major. Although choosing two options in the same division is discouraged, it is still possible.\n\nFirst-year students are enrolled in first-term classes based upon results of placement exams in math, physics, chemistry, and writing and take all classes in their first two terms on a Pass/Fail basis. There is little competition; collaboration on homework is encouraged and the Honor System encourages take-home tests and flexible homework schedules. Caltech offers co-operative programs with other schools, such as the Pasadena Art Center College of Design and Occidental College.\n\nAccording to a PayScale study, Caltech graduates earn a median early career salary of $83,400 and $143,100 mid-career, placing them in the top 5 among graduates of US colleges and universities. The average net return on investment over a period of 20 years is $887,000, the tenth-highest among US colleges.\n\nCaltech offers Army and Air Force ROTC in cooperation with the University of Southern California.\n\nThe graduate instructional programs emphasize doctoral studies and are dominated by science, technology, engineering, and mathematics fields. The Institute offers graduate degree programs for the Master of Science, Engineer's Degree, Doctor of Philosophy, BS/MS and MD/PhD, with the majority of students in the PhD program. The most popular options are Chemistry, Physics, Biology, Electrical Engineering and Chemical Engineering. Applicants for graduate studies are required to take the GRE. GRE Subject scores are either required or strongly recommended by several options. A joint program between Caltech and the Keck School of Medicine of the University of Southern California, and the UCLA David Geffen School of Medicine grants MD/PhD degrees. Students in this program do their preclinical and clinical work at USC or UCLA, and their PhD work with any member of the Caltech faculty, including the Biology, Chemistry, and Engineering and Applied Sciences Divisions. The MD degree would be from USC or UCLA and the PhD would be awarded from Caltech.\n\nThe research facilities at Caltech are available to graduate students, but there are opportunities for students to work in facilities of other universities, research centers as well as private industries. The graduate student to faculty ratio is 4:1.\n\nApproximately 99 percent of doctoral students have full financial support. Financial support for graduate students comes in the form of fellowships, research assistantships, teaching assistantships or a combination of fellowship and assistantship support.\n\nGraduate students are bound by the Honor Code, as are the undergraduates, and the Graduate Honor Council oversees any violations of the code.\nCaltech was elected to the Association of American Universities in 1934 and remains a research university with \"very high\" research activity, primarily in STEM fields. Caltech manages research expenditures of $270 million annually, 66th among all universities in the U.S. and 17th among private institutions without medical schools for 2008. The largest federal agencies contributing to research are NASA, National Science Foundation, Department of Health and Human Services, Department of Defense, and Department of Energy. Caltech received $144 million in federal funding for the physical sciences, $40.8 million for the life sciences, $33.5 million for engineering, $14.4 million for environmental sciences, $7.16 million for computer sciences, and $1.97 million for mathematical sciences in 2008.\n\nThe Institute was awarded an all-time high funding of $357 million in 2009. Active funding from the National Science Foundation Directorate of Mathematical and Physical Science (MPS) for Caltech stands at $343 million , the highest for any educational institution in the nation, and higher than the total funds allocated to any state except California and New York.\nIn 2005, Caltech had dedicated to research: to physical sciences, to engineering, and to biological sciences.\n\nIn addition to managing JPL, Caltech also operates the Palomar Observatory in San Diego County, the Owens Valley Radio Observatory in Bishop, California, the Submillimeter Observatory and W. M. Keck Observatory at the Mauna Kea Observatory, the Laser Interferometer Gravitational-Wave Observatory at Livingston, Louisiana and Richland, Washington, and Kerckhoff Marine Laboratory in Corona del Mar, California. The Institute launched the Kavli Nanoscience Institute at Caltech in 2006, the Keck Institute for Space Studies in 2008, and is also the current home for the Einstein Papers Project. The Spitzer Science Center (SSC), part of the Infrared Processing and Analysis Center located on the Caltech campus, is the data analysis and community support center for NASA's Spitzer Space Telescope.\n\nCaltech partnered with UCLA to establish a Joint Center for Translational Medicine (UCLA-Caltech JCTM), which conducts experimental research into clinical applications, including the diagnosis and treatment of diseases such as cancer.\n\nCaltech operates several TCCON stations as part of an international collaborative effort of measuring greenhouse gases globally. One station is on campus.\nUndergraduates at Caltech are also encouraged to participate in research. About 80% of the class of 2010 did research through the annual Summer Undergraduate Research Fellowships (SURF) program at least once during their stay, and many continued during the school year. Students write and submit SURF proposals for research projects in collaboration with professors, and about 70 percent of applicants are awarded SURFs. The program is open to both Caltech and non-Caltech undergraduate students. It serves as preparation for graduate school and helps to explain why Caltech has the highest percentage of alumni who go on to receive a PhD of all the major universities.\n\nThe licensing and transferring of technology to the commercial sector is managed by the Office of Technology Transfer (OTT). OTT protects and manages the intellectual property developed by faculty members, students, other researchers, and JPL technologists. Caltech receives more invention disclosures per faculty member than any other university in the nation. , 1891 patents were granted to Caltech researchers since 1969.\n\nDuring the early 20th century, a Caltech committee visited several universities and decided to transform the undergraduate housing system from regular fraternities to a house system. Four south houses (or \"hovses\") were built: Blacker House, Dabney House, Fleming House and Ricketts House. In the 1960s, three north houses were built: Lloyd House, Page House, and Ruddock House, and during the 1990s, Avery House. The four south houses closed for renovation in 2005 and reopened in 2006. All first year students live in the house system and 95% of undergraduates remain affiliated with a house.\n\nCaltech has athletic teams in baseball, men's and women's basketball, cross country, fencing, men's and women's soccer, swimming and diving, men's and women's tennis, track and field, women's volleyball, and men's and women's water polo. Caltech's mascot is the Beaver, a homage to nature's engineer. Its teams (with the exception of the fencing team) play in the Southern California Intercollegiate Athletic Conference, which Caltech co-founded in 1915. The fencing team competes in the NCAA's Division I, facing teams from UCLA, USC, UCSD, and Stanford, among others.\n\nOn January 6, 2007, the Beavers' men's basketball team snapped a 207-game losing streak to Division III schools, beating Bard College 81–52. It was their first Division III victory since 1996.\nUntil their win over Occidental on February 22, 2011 the team had not won a game in conference play since 1985. Ryan Elmquist's free throw with 3.3 seconds in regulation gave the Beavers the victory. The documentary film \"Quantum Hoops\" concerns the events of the Beavers' 2005–06 season.\n\nOn January 13, 2007, the Caltech women's basketball team snapped a 50-game losing streak, defeating the Pomona–Pitzer Sagehens 55–53. The women's program, which entered the SCIAC in 2002, garnered their first conference win. On the bench as honorary coach for the evening was Dr. Robert Grubbs, 2005 Nobel laureate in Chemistry. The team went on to beat Whittier College on February 10, for its second SCIAC win, and placed its first member on the All Conference team. The 2006–2007 season is the most successful season in the history of the program.\n\nIn 2007, 2008, and 2009, the women's table tennis team (a club team) competed in nationals. The women's Ultimate club team, known as \"Snatch\", has also been very successful in recent years, ranking 44 of over 200 college teams in the Ultimate Player's Association.\n\nOn February 2, 2013, the Caltech baseball team ended a 228-game losing streak, the team's first win in nearly 10 years.\n\nThe track and field team plays at the South Athletic Field in Tournament Park, the site of the first Rose Bowl Game.\n\nThe school also sponsored a football team prior to 1976, which played part of its home schedule at the Rose Bowl, or, as Caltech students put it, \"to the largest number of empty seats in the nation\".\n\nThe Caltech/Occidental College Orchestra is a full seventy-piece orchestra composed of students, faculty, and staff at Caltech and nearby Occidental College. The orchestra gives three pairs of concerts annually, at both Caltech and Occidental College. There are also two Caltech Jazz Bands and a Concert Band. For vocal music, Caltech has a mixed voice Glee Club and a smaller Chamber Singer group. The theater program at Caltech is known as TACIT, or Theater Arts at the California Institute of Technology. There are two to three plays organized by TACIT per year, and they were involved in the production of the , released in 2011.\n\nEvery Halloween, Dabney House conducts the infamous \"Millikan pumpkin-drop experiment\" from the top of Millikan Library, the highest point on campus. According to tradition, a claim was once made that the shattering of a pumpkin frozen in liquid nitrogen and dropped from a sufficient height would produce a triboluminescent spark. This yearly event involves a crowd of observers, who try to spot the elusive spark. The title of the event is an oblique reference to the famous Millikan oil-drop experiment which measured \"e\", the elemental unit of electrical charge.\n\nOn Ditch Day, the seniors ditch school, leaving behind elaborately designed tasks and traps at the doors of their rooms to prevent underclassmen from entering. Over the years this has evolved to the point where many seniors spend months designing mechanical, electrical, and software obstacles to confound the underclassmen. Each group of seniors designs a \"stack\" to be solved by a handful of underclassmen. The faculty have been drawn into the event as well, and cancel all classes on Ditch Day so the underclassmen can participate in what has become a highlight of the academic year.\n\nAnother long-standing tradition is the playing of Wagner's \"Ride of the Valkyries\" at 7:00 each morning during finals week with the largest, loudest speakers available. The playing of that piece is not allowed at any other time (except if one happens to be listening to the entire 14 hours and 5 minutes of \"The Ring Cycle\"), and any offender is dragged into the showers to be drenched in cold water fully dressed. \nCaltech students have been known for the many pranks (also known as \"RFs\").\n\nThe two most famous in recent history are the changing of the Hollywood Sign to read \"Caltech\", by judiciously covering up certain parts of the letters, and the changing of the scoreboard to read Caltech 38, MIT 9 during the 1984 Rose Bowl Game. But the most famous of all occurred during the 1961 Rose Bowl Game, where Caltech students altered the flip-cards that were raised by the stadium attendees to display \"Caltech\", and several other \"unintended\" messages. This event is now referred to as the Great Rose Bowl Hoax.\n\nIn recent years, pranking has been officially encouraged by Tom Mannion, Caltech's Assistant VP for Student Affairs and Campus Life. \"The grand old days of pranking have gone away at Caltech, and that's what we are trying to bring back,\" reported the \"Boston Globe\".\n\nIn December 2011, Caltech students went to New York and pulled a prank on Manhattan's Greenwich Village. The prank involved making The Cube sculpture look like the Aperture Science Weighted Companion Cube from the video game \"Portal\".\n\nCaltech pranks have been documented in three Legends of Caltech books, the most recent of which was edited by alumni Autumn Looijen '99 and Mason Porter '98 and published in May 2007.\n\nIn 2005, a group of Caltech students pulled a string of pranks during MIT's Campus Preview Weekend for admitted students. These include covering up the word Massachusetts in the \"Massachusetts Institute of Technology\" engraving on the main building façade with a banner so that it read \"That Other Institute of Technology\". A group of MIT hackers responded by altering the banner so that the inscription read \"The Only Institute of Technology.\" Caltech students also passed out T-shirts to MIT's incoming freshman class that had MIT written on the front and \"...because not everyone can go to Caltech\" along with an image of a palm tree on the back.\n\nMIT retaliated in April 2006, when students posing as the Howe & Ser (Howitzer) Moving Company stole the 130-year-old, 1.7-ton Fleming House cannon and moved it over 3000 miles to their campus in Cambridge, Massachusetts for their 2006 Campus Preview Weekend, repeating a similar prank performed by nearby Harvey Mudd College in 1986. Thirty members of Fleming House traveled to MIT and reclaimed their cannon on April 10, 2006.\n\nOn April 13, 2007 (Friday the 13th), a group of students from \"The California Tech\", Caltech's campus newspaper, arrived and distributed fake copies of \"The Tech\", MIT's campus newspaper, while prospective students were visiting for their Campus Preview Weekend. Articles included \"MIT Invents the Interweb\", \"Architects Deem Campus 'Unfortunate'\", and \"Infinite Corridor Not Actually Infinite\".\n\nIn December 2009, some Caltech students declared that MIT had been sold and had become the Caltech East campus. A \"sold\" banner was hung on front of the MIT dome building and a \"Welcome to Caltech East: School of the Humanities\" banner over the Massachusetts Avenue Entrance. Newspapers and T-shirts were distributed, and door labels and fliers in the infinite corridor were put up in accordance with the \"curriculum change.\"\n\nIn September 2010, MIT students attempted to put a TARDIS, the time machine from the BBC's \"Doctor Who\", onto a roof. Caught in midact, the prank was aborted. In January 2011, Caltech students in conjunction with MIT students helped put the TARDIS on top of Baxter. Caltech students then moved the TARDIS to UC Berkeley and Stanford.\n\nIn April 2014, during MIT's Campus Preview Weekend, a group of Caltech students handed out mugs emblazoned with the MIT logo on the front and the words \"The Institute of Technology\" on the back. When heated, the mugs turn orange, display a palm tree, and read \"Caltech The Hotter Institute of Technology.\" Identical mugs continue to be sold at the Caltech campus store.\n\nLife in the Caltech community is governed by the honor code, which simply states: \"No member of the Caltech community shall take unfair advantage of any other member of the Caltech community.\" This is enforced by a Board of Control, which consists of undergraduate students, and by a similar body at the graduate level, called the Graduate Honor Council.\n\nThe honor code aims at promoting an atmosphere of respect and trust that allows Caltech students to enjoy privileges that make for a more relaxed atmosphere. For example, the honor code allows professors to make the majority of exams as take-home, allowing students to take them on their own schedule and in their preferred environment.\n\nThrough the late 1990s, the only exception to the honor code, implemented earlier in the decade in response to changes in federal regulations, concerned the sexual harassment policy. Today, there are myriad exceptions to the honor code in the form of new institute policies such as the fire policy, and alcohol policy. Although both policies are presented in the Honor System Handbook given to new members of the Caltech community, some undergraduates regard them as a slight against the honor code and the implicit trust and respect it represents within the community. In recent years, the Student Affairs Office has also become fond of pursuing investigations independently of the Board of Control and Conduct Review Committee, an implicit violation of both the Honor Code and written disciplinary policy that has contributed to further erosion of trust between some parts of the undergraduate community and the administration.\n\n, Caltech has 38 Nobel laureates to its name awarded to 22 alumni, which includes 5 Caltech professors who are also alumni (Carl D. Anderson, Linus Pauling, William A. Fowler, Edward B. Lewis, and Kip Thorne), and 15 non-alumni professors. The total number of Nobel Prizes is 39 because Pauling received prizes in both Chemistry and Peace. The official Nobel Prize count is 48 affiliates in total when including temporary academic staff such as visiting professors and postdoctoral scholars. Seven faculty and alumni have received a Crafoord Prize from the Royal Swedish Academy of Sciences, while 58 have been awarded the U.S. National Medal of Science, and 13 have received the National Medal of Technology. One alumnus, Stanislav Smirnov, won the Fields Medal in 2010. Other distinguished researchers have been affiliated with Caltech as postdoctoral scholars (for example, Barbara McClintock, James D. Watson, Sheldon Glashow and John Gurdon) or visiting professors (for example, Albert Einstein, Stephen Hawking and Edward Witten).\n\nCaltech enrolled 961 undergraduate students and 1,277 graduate students for the 2017–2018 school year. Women made up 45% of the undergraduate and 30% of the graduate student body. The racial demographics of the school substantially differ from those of the nation as a whole.\n\nThe four-year graduation rate is 79% and the six-year rate is 92%, which is low compared to most leading U.S. universities, but substantially higher than it was in the 1960s and 1970s. Students majoring in STEM fields traditionally have graduation rates below 70%.\n\nRichard Feynman was among the most well-known physicists associated with Caltech, having published the \"Feynman Lectures on Physics\", an undergraduate physics text, and a few other popular science texts such as \"Six Easy Pieces\" for the general audience. The promotion of physics made him a public figure of science, although his Nobel-winning work in quantum electrodynamics was already very established in the scientific community. Murray Gell-Mann, a Nobel-winning physicist, introduced a classification of hadrons and went on to postulate the existence of quarks, which is currently accepted as part of the Standard Model. Long-time Caltech President Robert Andrews Millikan was the first to calculate the charge of the electron with his well-known oil-drop experiment, while Richard Chace Tolman is remembered for his contributions to cosmology and statistical mechanics. 2004 Nobel Prize in Physics winner H. David Politzer is a current professor at Caltech, as is astrophysicist and author Kip Thorne and eminent mathematician Barry Simon. Linus Pauling pioneered quantum chemistry and molecular biology, and went on to discover the nature of the chemical bond in 1939. Seismologist Charles Richter, also an alumnus, developed the magnitude scale that bears his name, the Richter magnitude scale for measuring the power of earthquakes. One of the founders of the geochemistry department, Clair Patterson was the first to accurately determine the age of the Earth via lead:uranium ratio in meteorites. In engineering, Theodore von Kármán made many key advances in aerodynamics, notably his work on supersonic and hypersonic airflow characterization. A repeating pattern of swirling vortices is named after him, the von Kármán vortex street. Participants in von Kármán's GALCIT project included Frank Malina, who helped develop the WAC Corporal which was the first U.S. rocket to reach the edge of space, Jack Parsons, a pioneer in the development of liquid and solid rocket fuels who designed the first castable composite-based rocket motor, and Qian Xuesen, who was dubbed the \"Father of Chinese Rocketry\". More recently, Michael Brown, a professor of planetary astronomy, discovered many trans-Neptunian objects, most notably the dwarf planet Eris, which prompted the International Astronomical Union to redefine the term \"planet\".\n\nDavid Baltimore, the Robert A. Millikan Professor of Biology, and Alice Huang, Senior Faculty Associate in Biology, have served as the President of AAAS from 2007–2008 and 2010–2011 respectively.\n\n33% of the faculty are members of the National Academy of Science or Engineering and/or fellows of the American Academy of Arts and Sciences. This is the highest percentage of any faculty in the country with the exception of the graduate institution Rockefeller University.\n\nThe average salary for assistant professors at Caltech is $111,300, associate professors $121,300, and full professors $172,800. Caltech faculty are active in applied physics, astronomy and astrophysics, biology, biochemistry, biological engineering, chemical engineering, computer science, geology, mechanical engineering and physics.\n\nThere are 22,930 total living alumni in the U.S. and around the world. Twenty-two alumni and 15 non-alumni faculty have won the Nobel Prize. The Turing Award, the \"Nobel Prize of Computer Science\", has been awarded to six alumni, and one has won the Fields Medal.\n\nAlumni have participated in scientific research. Some have concentrated their studies on the very small universe of atoms and molecules. Nobel laureate Carl D. Anderson (BS 1927, PhD 1930) proved the existence of positrons and muons, Nobel laureate Edwin McMillan (BS 1928, MS 1929) synthesized the first transuranium element, Nobel laureate Leo James Rainwater (BS 1939) investigated the non-spherical shapes of atomic nuclei, and Nobel laureate Douglas D. Osheroff (BS 1967) studied the superfluid nature of helium-3. Donald Knuth (PhD 1963), the \"father\" of the analysis of algorithms, wrote \"The Art of Computer Programming\" and created the TeX computer typesetting system, which is commonly used in the scientific community. Narendra Karmarkar (MS 1979) is known for the interior point method, a polynomial algorithm for linear programming known as Karmarkar's algorithm.\nOther alumni have turned their gaze to the universe. C. Gordon Fullerton (BS 1957, MS 1958) piloted the third Space Shuttle mission. Astronaut (and later, United States Senator) Harrison Schmitt (BS 1957) was the only geologist to have ever walked on the surface of the moon. Astronomer Eugene Merle Shoemaker (BS 1947, MS 1948) co-discovered Comet Shoemaker-Levy 9 (a comet which crashed into the planet Jupiter) and was the first person buried on the moon (by having his ashes crashed into the moon). Astronomer George O. Abell (BS 1951, MS 1952, PhD 1957) while a grad student at Cal Tech participated in the National Geographic Society-Palomar Sky Survey. This ultimately resulted in the publication of the \"Abell Catalogue of Clusters of Galaxies,\" the definitive work in the field.\n\nUndergraduate alumni founded, or co-founded, companies such as LCD manufacturer Varitronix, Hotmail, Compaq, and MathWorks (which created Matlab), while graduate students founded, or co-founded, companies such as Intel, TRW, and the non-profit educational organization, the Exploratorium.\n\nArnold Beckman (PhD 1928) invented the pH meter and commercialized it with the founding of Beckman Instruments. His success with that company enabled him to provide seed funding for William Shockley (BS 1932), who had co-invented semiconductor transistors and wanted to commercialize them. Shockley became the founding Director of the Shockley Semiconductor Laboratory division of Beckman Instruments. Shockley had previously worked at Bell Labs, whose first president was another alumnus, Frank Jewett (BS 1898). Because his aging mother lived in Palo Alto, California, Shockley established his laboratory near her in Mountain View, California. Shockley was a co-recipient of the Nobel Prize in physics in 1956, but his aggressive management style and odd personality at the Shockley Lab became unbearable. In late 1957, eight of his researchers resigned and with support from Sherman Fairchild formed Fairchild Semiconductor. Among the \"traitorous eight\" was Gordon E. Moore (PhD 1954), who later left Fairchild to co-found Intel. Other offspring companies of Fairchild Semiconductor include National Semiconductor and Advanced Micro Devices, which in turn spawned more technology companies in the area. Shockley's decision to use silicon – instead of germanium – as the semiconductor material, coupled with the abundance of silicon semiconductor related companies in the area, gave rise to the term \"Silicon Valley\" to describe that geographic region surrounding Palo Alto.\n\nCaltech alumni also held public offices, with Mustafa A.G. Abushagur (PhD 1984) the Deputy Prime Minister of Libya, James Fletcher (PhD 1948) the 4th and 7th Administrator of NASA, Steven Koonin (PhD 1972) the Undersecretary of Energy for Science, and Regina Dugan (PhD 1993) the 19th director of DARPA. The 20th director for DARPA, Arati Prabhakar, is also a Caltech alumna (PhD 1984). Arvind Virmani is a former Chief Economic Adviser to the Government of India. In 2013, President Obama announced the nomination of France Cordova (PhD 1979) as the director of the National Science Foundation and Ellen Williams (PhD 1982) as the director for ARPA-E.\n\n\nOver the years Caltech has actively promoted the commercialization of technologies developed within its walls. Through its Office of Technology Transfer & Corporate Partnerships, scientific breakthroughs have led to the transfer of numerous technologies in a wide variety of scientific-related fields such as photovoltaic, radio-frequency identification (RFID), semiconductors, hyperspectral imaging, electronic devices, protein design, solid state amplifiers and many more. Companies such as Contour Energy Systems, Impinj, Fulcrum Microsystems, Nanosys, Inc., Photon etc., Xencor, Wavestream Wireless have emerged from Caltech.\n\nCaltech has appeared in many works of popular culture, both as itself and in disguised form. As with MIT, a Caltech reference is often used to establish a character's high level of intelligence or a technical background; for example, in the novel \"Contact\" by Carl Sagan, Eleanor Arroway holds a Ph.D. in radio astronomy from there. On television, all four male lead characters in the sitcom \"The Big Bang Theory\" are employed at the Institute. Caltech is also the inspiration, and frequent film location, for the California Institute of Science of \"Numb3rs\". On film, the Pacific Tech of \"The War of the Worlds\" and \"Real Genius\" is based on Caltech.\n\nIn nonfiction, two 2007 documentaries examine aspects of Caltech: \"Curious\", its researchers, and \"Quantum Hoops\", its men's basketball team.\n\nGiven its Los Angeles-area location, the grounds of the Institute are often host to short scenes in movies and television. The Athenaeum dining club appears in the \"Beverly Hills Cop\" series, \"The X-Files\", \"True Romance\", and \"The West Wing\".\n\n"}
{"id": "5790", "url": "https://en.wikipedia.org/wiki?curid=5790", "title": "Carlo Goldoni", "text": "Carlo Goldoni\n\nCarlo Osvaldo Goldoni (; 25 February 1707 – 6 February 1793) was an Italian playwright and librettist from the Republic of Venice. His works include some of Italy's most famous and best-loved plays. Audiences have admired the plays of Goldoni for their ingenious mix of wit and honesty. His plays offered his contemporaries images of themselves, often dramatizing the lives, values, and conflicts of the emerging middle classes. Though he wrote in French and Italian, his plays make rich use of the Venetian language, regional vernacular, and colloquialisms. Goldoni also wrote under the pen name and title \"Polisseno Fegeio, Pastor Arcade,\" which he claimed in his memoirs the \"Arcadians of Rome\" bestowed on him.\n\nOne of his best known works is the comic play \"Servant of Two Masters\", which has been translated and adapted internationally numerous times. In 2011, Richard Bean adapted the play for the National Theatre of Great Britain as \"One Man, Two Guvnors\". Its popularity led to a transfer to the West End and in 2012 to Broadway.\n\nThere is an abundance of autobiographical information on Goldoni, most of which comes from the introductions to his plays and from his \"Memoirs\". However, these memoirs are known to contain many errors of fact, especially about his earlier years.\n\nIn these memoirs, he paints himself as a born comedian, careless, light-hearted and with a happy temperament, proof against all strokes of fate, yet thoroughly respectable and honorable.\n\nGoldoni was born in Venice in 1707, the son of Margherita and Giulio Goldoni. In his memoirs, Goldoni describes his father as a physician, and claims that he was introduced to theatre by his grandfather Carlo Alessandro Goldoni. In reality, it seems that Giulio was an apothecary; as for the grandfather, he had died four years before Carlo's birth. In any case, Goldoni was deeply interested in theatre from his earliest years, and all attempts to direct his activity into other channels were of no avail; his toys were puppets, and his books, plays.\n\nHis father placed him under the care of the philosopher Caldini at Rimini but the youth soon ran away with a company of strolling players and returned to Venice. In 1723 his father matriculated him into the stern Collegio Ghislieri in Pavia, which imposed the tonsure and monastic habits on its students. However, he relates in his \"Memoirs\" that a considerable part of his time was spent in reading Greek and Latin comedies. He had already begun writing at this time and, in his third year, he composed a libellous poem (\"Il colosso\") in which he ridiculed the daughters of certain Pavian families. As a result of that incident (and/or of a visit paid with some schoolmates to a local brothel) he was expelled from the school and had to leave the city (1725). He studied law at Udine, and eventually took his degree at University of Modena. He was employed as a law clerk at Chioggia and Feltre, after which he returned to his native city and began practicing.\n\nEducated as a lawyer, and holding lucrative positions as secretary and counsellor, he seemed, indeed, at one time to have settled down to the practice of law, but following an unexpected summons to Venice, after an absence of several years, he changed his career, and thenceforth he devoted himself to writing plays and managing theatres. His father died in 1731. In 1732, to avoid an unwanted marriage, he left the town for Milan and then for Verona where the theatre manager Giuseppe Imer helped him on his way to becoming a comical poet as well as introducing him to his future wife, Nicoletta Conio. Goldoni returned with her to Venice, where he stayed until 1743.\n\nGoldoni entered the Italian theatre scene with a tragedy, \"Amalasunta\", produced in Milan. The play was a critical and financial failure.\n\nSubmitting it to Count Prata, director of the opera, he was told that his piece \"was composed with due regard for the rules of Aristotle and Horace, but not according to those laid down for the Italian drama.\" \"In France\", continued the count, \"you can try to please the public, but here in Italy it is the actors and actresses whom you must consult, as well as the composer of the music and the stage decorators. Everything must be done according to a certain form which I will explain to you.\"\n\nGoldoni thanked his critic, went back to his inn and ordered a fire, into which he threw the manuscript of his \"Amalasunta\".\n\nHis next play, \"Belisario\", written in 1734, was more successful, though of its success he afterward professed himself ashamed.\n\nDuring this period he also wrote librettos for opera seria and served for a time as literary director of the San Giovanni Grisostomo, Venice's most distinguished opera house.\n\nHe wrote other tragedies for a time, but he was not long in discovering that his bent was for comedy. He had come to realize that the Italian stage needed reforming; adopting Molière as his model, he went to work in earnest and in 1738 produced his first real comedy, \"L'uomo di mondo\" (\"The Man of the World\"). During his many wanderings and adventures in Italy, he was constantly at work and when, at Livorno, he became acquainted with the manager Medebac, he determined to pursue the profession of playwriting in order to make a living. He was employed by Medebac to write plays for his theater in Venice. He worked for other managers and produced during his stay in that city some of his most characteristic works. He also wrote \"Momolo Cortesan\" in 1738. By 1743, he had perfected his hybrid style of playwriting (combining the model of Molière with the strengths of Commedia dell'arte and his own wit and sincerity). This style was typified in \"La Donna di garbo\", the first Italian comedy of its kind.\n\nAfter 1748, Goldoni collaborated with the composer Baldassare Galuppi, making significant contributions to the new form of 'opera buffa'. Galuppi composed the score for more than twenty of Goldoni's librettos. As with his comedies, Goldoni's \"opera buffa\" integrate elements of the Commedia dell'arte with recognisable local and middle-class realities. His operatic works include two of the most successful musical comedies of the eighteenth century, \"Il filosofo di campagna\" (\"The Country Philosopher\"), set by Galuppi (1752) and \"La buona figliuola\" (\"The Good Girl\"), set by Niccolò Piccinni (1760).\n\nIn 1753, following his return from Bologna he defected to the Teatro San Luca of the Vendramin family where he performed most of his plays to 1762.\n\nIn 1757, he engaged in a bitter dispute with playwright Carlo Gozzi, which left him utterly disgusted with the tastes of his countrymen; so much so that in 1761 he moved to Paris, where he received a position at court and was put in charge of the Theatre Italien. He spent the rest of his life in France, composing most of his plays in French and writing his memoirs in that language.\n\nAmong the plays which he wrote in French, the most successful was \"Le bourru bienfaisant\", produced on the occasion of the marriage of Louis XVI and Marie Antoinette in 1771. He enjoyed considerable popularity in France; when he retired to Versailles, the King gave him a pension. He lost this pension after the French Revolution. The Convention eventually voted to restore his pension the day after his death. It was restored to his widow, at the pleading of the poet André Chénier; \"She is old\", he urged, \"she is seventy-six, and her husband has left her no heritage save his illustrious name, his virtues and his poverty.\"\nIn his \"Memoirs\" Goldoni amply discusses the state of Italian comedy when he began writing. At that time, Italian comedy revolved around the conventionality of the Commedia dell'arte, or improvised comedy. Goldoni took to himself the task of superseding the comedy of masks and the comedy of intrigue by representations of actual life and manners through the characters and their behaviors. He rightly maintained that Italian life and manners were susceptible of artistic treatment such as had not been given them before.\n\nHis works are a lasting monument to the changes that he initiated: a dramatic revolution that had been attempted but not achieved before. Goldoni's importance lay in providing good examples rather than precepts. Goldoni says that he took for his models the plays of Molière and that whenever a piece of his own succeeded he whispered to himself: \"Good, but not yet Molière.\" Goldoni's plays are gentler and more optimistic in tone than Molière's.\n\nIt was this very success that was the object of harsh critiques by Carlo Gozzi, who accused Goldoni of having deprived the Italian theatre of the charms of poetry and imagination. The great success of Gozzi's fairy dramas so irritated Goldoni that it led to his self-exile to France.\n\nGoldoni gave to his country a classical form, which, though it has since been cultivated, has yet to be cultivated by a master.\n\nGoldoni's plays that were written while he was still in Italy ignore religious and ecclesiastical subjects. This may be surprising, considering his staunch Catholic upbringing. No thoughts are expressed about death or repentance in his memoirs or in his comedies. After his move to France, his position became clearer, as his plays took on a clear anti-clerical tone and often satirized the hypocrisy of monks and of the Church.\n\nGoldoni was inspired by his love of humanity and the admiration he had for his fellow men. He wrote, and was obsessed with, the relationships that humans establish with one another, their cities and homes, the Humanist movement, and the study of philosophy. The moral and civil values that Goldoni promotes in his plays are those of rationality, civility, humanism, the importance of the rising middle-class, a progressive stance to state affairs, honor and honesty. Goldoni had a dislike for arrogance, intolerance and the abuse of power.\n\nGoldoni's main characters are no abstract examples of human virtue, nor monstrous examples of human vice. They occupy the middle ground of human temperament. Goldoni maintains an acute sensibility for the differences in social classes between his characters as well as environmental and generational changes. Goldoni pokes fun at the arrogant nobility and the pauper who lacks dignity.\n\nAs in other theatrical works of the time and place, the characters in Goldoni's Italian comedies spoke originally either the literary Tuscan variety (which became modern Italian) or the Venetian dialect, depending on their station in life. However, in some printed editions of his plays he often turned the Venetian texts into Tuscan, too.\n\n\nThe following is a small sampling of Goldoni's enormous output.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "5793", "url": "https://en.wikipedia.org/wiki?curid=5793", "title": "Cumulative distribution function", "text": "Cumulative distribution function\n\nIn probability theory and statistics, the cumulative distribution function (CDF) of a real-valued random variable formula_1, or just distribution function of formula_1, evaluated at formula_3, is the probability that formula_1 will take a value less than or equal to formula_3.\n\nIn the case of a continuous distribution, it gives the area under the probability density function from minus infinity to formula_3. Cumulative distribution functions are also used to specify the distribution of multivariate random variables.\n\nThe cumulative distribution function of a real-valued random variable formula_1 is the function given by\n\nwhere the right-hand side represents the probability that the random variable formula_1 takes on a value less than or\nequal to formula_3. The probability that formula_1 lies in the semi-closed interval formula_11, where formula_12, is therefore\n\nIn the definition above, the \"less than or equal to\" sign, \"≤\", is a convention, not a universally used one (e.g. Hungarian literature uses \"<\"), but is important for discrete distributions. The proper use of tables of the binomial and Poisson distributions depends upon this convention. Moreover, important formulas like Paul Lévy's inversion formula for the characteristic function also rely on the \"less than or equal\" formulation.\n\nIf treating several random variables formula_13 etc. the corresponding letters are used as subscripts while, if treating only one, the subscript is usually omitted. It is conventional to use a capital formula_14 for a cumulative distribution function, in contrast to the lower-case formula_15 used for probability density functions and probability mass functions. This applies when discussing general distributions: some specific distributions have their own conventional notation, for example the normal distribution.\n\nThe CDF of a continuous random variable formula_1 can be expressed as the integral of its probability density function formula_17 as follows:\n\nIn the case of a random variable formula_1 which has distribution having a discrete component at a value formula_20,\n\nIf formula_22 is continuous at formula_20, this equals zero and there is no discrete component at formula_20.\n\nEvery cumulative distribution function formula_22 is non-decreasing and right-continuous, which makes it a càdlàg function. Furthermore,\n\nEvery function with these four properties is a CDF, i.e., for every such function, a random variable can be defined such that the function is the cumulative distribution function of that random variable.\n\nIf formula_1 is a purely discrete random variable, then it attains values formula_28 with probability formula_29, and the CDF of formula_1 will be discontinuous at the points formula_31 and constant in between:\n\nIf the CDF formula_22 of a real valued random variable formula_1 is continuous, then formula_1 is a continuous random variable; if furthermore formula_22 is absolutely continuous, then there exists a Lebesgue-integrable function formula_37 such that\n\nfor all real numbers formula_39 and formula_20. The function formula_17 is equal to the derivative of formula_22 almost everywhere, and it is called the probability density function of the distribution of formula_1.\n\nAs an example, suppose formula_1 is uniformly distributed on the unit interval formula_45.\nThen the CDF of formula_1 is given by\n\nSuppose instead that formula_1 takes only the discrete values 0 and 1, with equal probability.\nThen the CDF of formula_1 is given by\n\nSometimes, it is useful to study the opposite question and ask how often the random variable is \"above\" a particular level. This is called the complementary cumulative distribution function (ccdf) or simply the tail distribution or exceedance, and is defined as\n\nThis has applications in statistical hypothesis testing, for example, because the one-sided p-value is the probability of observing a test statistic \"at least\" as extreme as the one observed. Thus, provided that the test statistic, \"T\", has a continuous distribution, the one-sided p-value is simply given by the ccdf: for an observed value formula_52 of the test statistic\n\nIn survival analysis, formula_54 is called the survival function and denoted formula_55, while the term \"reliability function\" is common in engineering.\n\n\nWhile the plot of a cumulative distribution often has an S-like shape, an alternative illustration is the folded cumulative distribution or mountain plot, which folds the top half of the graph over,\nthus using two scales, one for the upslope and another for the downslope. This form of illustration emphasises the median and dispersion (specifically, the mean absolute deviation from the median) of the distribution or of the empirical results.\n\nIf the CDF \"F\" is strictly increasing and continuous then formula_66 is the unique real number formula_67 such that formula_68. In such a case, this defines the inverse distribution function or quantile function.\n\nSome distributions do not have a unique inverse (for example in the case where formula_69 for all <math>a<x, causing formula_22 to be constant). This problem can be solved by defining, for formula_71, the generalized inverse distribution function:\n\nSome useful properties of the inverse cdf (which are also preserved in the definition of the generalized inverse distribution function) are:\n\nThe inverse of the cdf can be used to translate results obtained for the uniform distribution to other distributions.\n\nWhen dealing simultaneously with more than one random variable the joint cumulative distribution function can also be defined. For example, for a pair of random variables formula_92, the joint CDF formula_93 is given by\n\nwhere the right-hand side represents the probability that the random variable formula_1 takes on a value less than or\nequal to formula_3 and that formula_81 takes on a value less than or\nequal to formula_97.\n\nFor formula_98 random variables formula_99, the joint CDF formula_100 is given by\n\nInterpreting the formula_98 random variables as a random vector formula_102 yields a shorter notation:\n\nEvery multivariate CDF is:\n\nThe generalization of the cumulative distribution function from real to complex random variables is not obvious because expressions of the form formula_106 make no sense. However expessions of the form formula_107 make sense. Therefore, we define the cumulative distribution of a complex random variables via the joint distribution of their real and imaginary parts:\n\nGeneralization of yields\nas definition for the CDS of a complex random vector formula_110.\n\nThe concept of the cumulative distribution function makes an explicit appearance in statistical analysis in two (similar) ways. Cumulative frequency analysis is the analysis of the frequency of occurrence of values of a phenomenon less than a reference value. The empirical distribution function is a formal direct estimate of the cumulative distribution function for which simple statistical properties can be derived and which can form the basis of various statistical hypothesis tests. Such tests can assess whether there is evidence against a sample of data having arisen from a given distribution, or evidence against two samples of data having arisen from the same (unknown) population distribution.\n\nThe Kolmogorov–Smirnov test is based on cumulative distribution functions and can be used to test to see whether two empirical distributions are different or whether an empirical distribution is different from an ideal distribution. The closely related Kuiper's test is useful if the domain of the distribution is cyclic as in day of the week. For instance Kuiper's test might be used to see if the number of tornadoes varies during the year or if sales of a product vary by day of the week or day of the month.\n\n"}
{"id": "5794", "url": "https://en.wikipedia.org/wiki?curid=5794", "title": "Central tendency", "text": "Central tendency\n\nIn statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution. It may also be called a center or location of the distribution. Colloquially, measures of central tendency are often called \"averages.\" The term \"central tendency\" dates from the late 1920s.\n\nThe most common measures of central tendency are the arithmetic mean, the median and the mode. A central tendency can be calculated for either a finite set of values or for a theoretical distribution, such as the normal distribution. Occasionally authors use central tendency to denote \"the tendency of quantitative data to cluster around some central value.\"\n\nThe central tendency of a distribution is typically contrasted with its \"dispersion\" or \"variability\"; dispersion and central tendency are the often characterized properties of distributions. Analysts may judge whether data has a strong or a weak central tendency based on its dispersion.\n\nThe following may be applied to one-dimensional data. Depending on the circumstances, it may be appropriate to transform the data before calculating a central tendency. Examples are squaring the values or taking logarithms. Whether a transformation is appropriate and what it should be, depend heavily on the data being analyzed.\n\n\nAny of the above may be applied to each dimension of multi-dimensional data, but the results may not be invariant to rotations of the multi-dimensional space. In addition, there are the\n\n\nSeveral measures of central tendency can be characterized as solving a variational problem, in the sense of the calculus of variations, namely minimizing variation from the center. That is, given a measure of statistical dispersion, one asks for a measure of central tendency that minimizes variation: such that variation from the center is minimal among all choices of center. In a quip, \"dispersion precedes location\". This center may or may not be unique. In the sense of \"L\" spaces, the correspondence is:\nThe associated functions are called \"p\"-norms: respectively 0-\"norm\", 1-norm, 2-norm, and ∞-norm. The function corresponding to the \"L\" space is not a norm, and is thus often referred to in quotes: 0-\"norm\".\n\nIn equations, for a given (finite) data set \"X\", thought of as a vector formula_1, the dispersion about a point \"c\" is the \"distance\" from x to the constant vector formula_2 in the \"p\"-norm (normalized by the number of points \"n\"):\n\nNote that for formula_4 and formula_5 these functions are defined by taking limits, respectively as formula_6 and formula_7. For formula_4 the limiting values are formula_9 and formula_10 for formula_11, so the difference becomes simply equality, so the 0-norm counts the number of \"unequal\" points. For formula_5 the largest number dominates, and thus the ∞-norm is the maximum difference.\n\nThe mean (\"L\" center) and midrange (\"L\" center) are unique (when they exist), while the median (\"L\" center) and mode (\"L\" center) are not in general unique. This can be understood in terms of convexity of the associated functions (coercive functions).\n\nThe 2-norm and ∞-norm are strictly convex, and thus (by convex optimization) the minimizer is unique (if it exists), and exists for bounded distributions. Thus standard deviation about the mean is lower than standard deviation about any other point, and the maximum deviation about the midrange is lower than the maximum deviation about any other point.\n\nThe 1-norm is not \"strictly\" convex, whereas strict convexity is needed to ensure uniqueness of the minimizer. Correspondingly, the median (in this sense of minimizing) is not in general unique, and in fact any point between the two central points of a discrete distribution minimizes average absolute deviation.\n\nThe 0-\"norm\" is not convex (hence not a norm). Correspondingly, the mode is not unique – for example, in a uniform distribution \"any\" point is the mode.\n\nFor unimodal distributions the following bounds are known and are sharp:\n\nwhere \"μ\" is the mean, \"ν\" is the median, \"θ\" is the mode, and \"σ\" is the standard deviation.\n\nFor every distribution,\n\n"}
